# System requirements

## SBOM

The table below provides the required versions of the various components for reference. For the latest version of the component Compatibility Matrix use the [official tracker](https://nvidia-my.sharepoint.com/:x:/r/personal/khushalis_nvidia_com/Documents/DGX%20SPOD_BCM_Component%20Matrix.xlsx?d=wae259e4b54934003b1bda194a0f42992&csf=1&web=1&e=hZAhW9&nav=MTVfezVFRTg0MjhCLTFFNjYtNEI2Ri1CN0MzLUYwNjI0M0VENUM4MX0).

| **Component** | **Version** | 
| --- | --- |
| *Base Command Manager* | 10.0 (10.24.11 or 10.25.2) | 
| *Run:ai* | 2.19 | 
| *Kubernetes* | 1.31 | 
| *DGX OS* | 6.3 or later | 
| *DGX Kernel* | 5.15.0-1070-nvidia | 
| *CPU node OS* | 22.04.4 LTS | 
| *GPU driver* | 550.127.08 | 
| *NVIDIA container toolkit* | 1.17.3-1 | 
| *CUDA* | 12.4 | 
| *Mellanox OFED* | 23.10-3.2.2.0 | 
| *Lustre kernel driver* | 2.14.0_ddn145 | 
| *VAST multi-path driver* | 4.0.32 | 
| *GPU Operator* | 24.9.0 | 
| *Network Operator* | 24.7.0 | 
| *Kubeflow Training operator*
*(Run:ai distributed workloads)* | 1.7.0 | 
| *Knative Serving* | 1.15.0 | 

**Kubernetes CSI/COSI drivers:**

| **Component** | **Version** | 
| --- | --- |
| *DDN Exa* | 2.3.0 | 
| *VAST CSI* | 2.4.21 | 
| *VAST COSI (Object Storage)* | 2.4.21 | 

1) 2.5.x is not currently validated or supported.


## Customer-provided Prerequisites

### Customer DNS records
* The Run:ai cluster creation wizard requires a domain name (FQDN) to the Kubernetes cluster as well as a **trusted** certificate for that domain. The domain name needs to be resolvable from the corporate DNS servers. For more information see Run:ai Domain name requirements:
[https://docs.run.ai/v2.19/admin/runai-setup/cluster-setup/cluster-prerequisites/#domain-name-requirement](https://docs.run.ai/v2.19/admin/runai-setup/cluster-setup/cluster-prerequisites/#domain-name-requirement)
* The DNS record needs to point to the IP address (A record) of the shared, alias interface that is active on the active BCM headnode ( <NIC device name>:cmha i.e. eth0:cmha) or be a CNAME alias to a host DNS record pointing to that same IP address

!!! Note 
    Validate that both the headnode and nodes outside the DGX SuperPOD can resolve the hostname before beginning the deployment of Run:ai.
    
    - From the BCM headnode:
      ```
      pdsh -g all host -4 <Run:ai endpoint FQDN>
      ```
    
    - From a system external to the DGX:
      ```
      nslookup <Run:ai endpoint FQDN>
      ```


### TLS/SSL certificates
* The customer organization should provide a FQDN with a corresponding DNS record, and a **signed, trusted** certificate for the domain name. The Run:ai installation requires both private key and full-chain in PEM format. The certificate CN should be the cluster’s FQDN domain name and it should also contain at least one SAN DNS entry pointing for the same FQDN. See [https://docs.run.ai/v2.19/admin/runai-setup/cluster-setup/cluster-install/#tls-certificates](https://docs.run.ai/v2.19/admin/runai-setup/cluster-setup/cluster-install/#tls-certificates) for more details.
The certificate will be installed on the Kubernetes control-plane nodes as well as a Kubernetes secret for the Run:ai backend and the NCM Kubernetes Ingress controller.
* The certificate CN name needs to be equal to the FQDN name is Section 2.1.
* The certificate needs to include at least one Subject Alternative Name DNS entry (SAN) for the same FQDN.
* The certificate needs to include the full trust chain (signing CA public keys)
* The customer needs to provide the private key and certificate pair.
**Optional** - NVPS engineer can  generate a Certificate Signing Request) if needed and a CSR can be generated by running the following command:

```
openssl req -new -newkey rsa:2048 -nodes -out bcm-runai-1_nvidia_com.csr \
-keyout bcm-runai-1_nvidia_com.key \
-subj "/C=US/ST=CA/L=Santa Clara/O=Nvidia Corporation/OU=SW-GPU/CN=bcm-runai-1.nvidia.com" \ 
-addext "subjectAltName = DNS:bcm-runai-1.nvidia.com"
```

Note that the CSR needs both the hostname to be provided in the Subject  Canonical Name (CN)  stanza as well as in an extra SAN (Subject Alternate Name) field.

The CSR file can be shared with the customer’s security team so that it can be signed  by the customer’s certificate  authority.

#### Certificate Validation

1. Optional - If the certificate is in P7B format).  PKCS#7 (also known as P7B) is a container format for digital certificates that is most  found in Windows and Java server contexts, and usually has the extension .p7b. PKCS#7 files are not used to store private keys. Microsoft PKI typically generates P7B certificates (including NVIDIA’s internal PKI), and they need to be converted to PEM format in order to be used by Linux services. The following command can be used to convert the certificate:
   ```
   openssl pkcs7 -print_certs -in <P7B CERTIFICATE>.p7b -out <PEM CERTIFICATE>.pem
   ```

2. Verify that the private key and certificate pair match:
   ```
   diff --color <(openssl x509 -noout -modulus -in "<CERTIFICATE PEM FILE>") <(openssl rsa -noout -modulus -in "<PRIVATE KEY FILE>")
   ```

3. Inspect the certificate and verify that:

      a. The Subject’s CN contains the right FQDN 
         ```
         # openssl x509 -in <CERTIFICATE PEM FILE>  -text  | egrep  "^\s+Subject:"
               Subject: C = US, ST = CA, L = Santa Clara, O = Nvidia Corporation, OU = SW-GPU, CN = bcm-runai-1.nvidia.com
         ```

      b. The certificate contains at least one SAN DNS entry pointing to the same FQDN:
         ```
         # openssl x509 -in <CERTIFICATE PEM FILE>  -text  | egrep  "^\s+X509v3 Subject Alternative Name:" -A10 | grep DNS
                        DNS:bcm-runai-1.nvidia.com
         ```

      c. Verify that the certificate is signed by the CA and that the PEM contains the full trust chain:
         ```
         # openssl verify -verbose -CAfile <rootCA PEM FILE> <CERTIFICATE PEM FILE>

         runai-bcm-1220.pem: OK
         ```

## Pre-installation Checklist

The following checklist is provided for convenience and can be seen as part of an expanded site survey for Run:ai deployments on SuperPOD. This information needs to be collected and validated (as per the steps Chapter 2), before the actual Run:ai deployment begins.

| **Component** | **Type** | 
| --- | --- |
| *[SSL] Full-chain SSL certificate* | <*.p7b, *.der or *.pem file> | 
| *[SSL] SSL Private Key* | Private certificate (e.g. *.key) | 
| *[SSL] CA trust chain public certificate* | X509 PEM file | 
| *[Networking] FQDN name/ Reserved IP address* | DNS A or CNAME record pointing to the BCM headnode shared/floating IP or Load Balancer reserved IP | 
| *[Networking]  Load Balancer IP address range* | Additional IP address space (8 or more) for the Kubernetes LoadBalancer (Inference, DataMover workloads) | 
| *[SSO] OpenID Discovery URL* | URL | 
| *[SSO] OpenID ClientID* | string | 
| *[SSO] OpenID Client Secret* | credential | 
| *[SSO] (Optional) LDAP attribute mapping* | Only if non-standard attribute names are used | 
| *[Storage] Lustre filesystem* | string (FS path) | 
| *[Storage] Lustre MGS NIDs* | String (Lustre NIDs) | 

## Installation Access prerequisites

The configuration of BCM as well the deployment of Run:ai can be performed through SSH access on the BCM headnodes, provided that the system accessing the headnode has SSH (Port TCP/22) and HTTP(s) connectivity (Ports: TCP/22, TCP/80, TCP/443 and TCP/10443 for Kubernetes) to the BCM headnodes.

Note: If the Run:ai endpoint FQDN cannot be resolved add a local hostfile (i.e. /etc/hosts on Linux/MacOS) static entry.

* Install kubectl:
[https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

* Install Helm (v3.14 or greater):
[https://docs.run.ai/v2.19/admin/runai-setup/cluster-setup/cluster-install/#helm](https://docs.run.ai/v2.19/admin/runai-setup/cluster-setup/cluster-install/#helm)


## BCM Prerequisites

The items below are non-default DGX SuperPOD items which need special attention prior to proceeding with the installation of K8S and Run:ai.  Pay special attention that these are incorporated before proceeding with the installation

* BCM has been deployed and HA has been configured with shared storage as per the [DGX SuperPOD runbook](https://docs.nvidia.com/dgx-superpod-deployment-guide-dgx-a100.pdf).
* All the networks that will be used for Kubernetes nodes have the same MTU.
* The NVIDIA OFED has been installed in the **all **of the used software-images
* The VAST Multi-path or DDN EXA Lustre driver has been installed in **all **of the used the software image and the Lustre/VAST has been mounted on all DGX and K8S  nodes using a BCM FSMount
* Enable SR-IOV in the DGX H100 image by ensuring that the following kernel parameters are included: **intel_iommu=on iommu=pt (H100) **or **amd_iommu=on iommu=pt (H100). **After that, enable SR-IOV on the Mellanox HCAs, set the number of VFs to the desired number and reboot the DGX nodes.

For example:
```
root@bcmhead1:~# cmsh
[bcmhead1]% softwareimage use dgx-os-6.3-h100-image
[bcmhead1->softwareimage[dgx-os-6.3-h100-image]]% append kernelparameters  " intel_iommu=on"
[bcmhead1->softwareimage*[dgx-os-6.3-h100-image*]]% commit
```

Note the leading whitespace in `“ intel_iommu=on” above.`

* The /var filesystem of the headnodes, CPU and DGX compute nodes have **at least 100GB** of space available. **DO NOT** proceed with installing Kubernetes until  this requirement is met on the control plane nodes.

Run the following command on the headnode to verify requirement:
```
pdsh -g category=dgx-a100,k8s-control-plane,runai-control-plane df -h /var
```

* Ensure that NVME disk devices are named consistently across reboots. See [Appendix C](?tab=t.0#heading=h.afp9y7rd26kr) for instructions and example disk layouts.
* The following three BCM node categories and software images need to be present:
 
   * A node category for the DGX H100 nodes with a DGX Base OS software image (DGX OS 6.1 tested).
   * A node category for the Kubernetes master nodes (3 DGX SuperPOD management nodes - Dell R760 tested) with an Ubuntu 22.04 software image.
   * A node category for the Run:ai control plane nodes (2 DGX SuperPOD management nodes - Dell R760 tested) with an Ubuntu 22.04 software image.  We assume below (section 7.1) that this is called `runai-control-plane`

* All compute nodes have been assigned to the right category, are provisioned and are up and running.

