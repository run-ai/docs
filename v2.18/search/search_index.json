{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Researcher/overview-researcher/","title":"Overview: Researcher Documentation","text":"<p>Researchers, or AI practitioners, use Run:ai to submit Workloads. </p> <p>As part of the Researcher documentation you will find:</p> <ul> <li>Quickstart Guides which provide step-by-step guides to Run:ai technology.</li> <li>Command line interface reference documentation.</li> <li>Best Practices for Deep Learning with Run:ai.</li> <li>Information about the Run:ai Scheduler.</li> <li>Using Run:ai with various developer tools. </li> </ul>"},{"location":"Researcher/use-cases/","title":"Use Cases","text":"<p>This is a collection of various client-requested use cases. Each use case is accompanied by a short live-demo video, along with all the files used.</p> <p>Note</p> <p>For the most up-to-date information, check out the official Run:ai use-cases GitHub page.  </p> <ul> <li>MLflow with Run:ai: experiment management is important for Data Scientists. This is a demo of how to set up and use MLflow with Run:ai.  </li> <li>Introduction to Docker: Run:ai runs using Docker images. This is a brief introduction to Docker, image creation, and how to use them in the context of Run:ai. Please also check out the Persistent Environments use case if you wish to keep the creation of Docker images to a minimum.  </li> <li>Tensorboard with Jupyter (ResNet demo): Many Data Scientists like to use Tensorboard to keep an eye on the their current training experiments. They also like to have it side-by-side with Jupyter. In this demo, we will show how to integrate Tensorboard and Jupyter Lab within the context of Run:ai.  </li> <li>Persistent Environments (with Conda/Mamba &amp; Jupyter): Some Data Scientists find creating Docker images for every single one of their environments a bit of a hindrance. They would often prefer the ability to create and alter environments on the fly and to have those environments remain, even after an image has finished running in a job. This demo shows users how they can create and persist Conda/Mamba environments using an NFS.  </li> <li>Weights &amp; Biases with Run:ai: W&amp;B (Weights &amp; Biases) is one of the best tools for experiment tracking and management. W&amp;B is an official Run:ai partner. In this tutorial, we will demo how to use W&amp;B alongside Run:ai</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/","title":"Quickstart: Launch an Inference Workload","text":""},{"location":"Researcher/Walkthroughs/quickstart-inference/#introduction","title":"Introduction","text":"<p>Machine learning (ML) inference refers to the process of using a trained machine learning model to make predictions or generate outputs based on new, unseen data. After a model has been trained on a dataset, inference involves applying this model to new examples to produce results such as classifications, predictions, or other types of insights.</p> <p>The quickstart below shows an inference server running the model and an inference client.</p> <p>There are various ways to submit a Workload:</p> <ul> <li>Run:ai command-line interface (CLI)</li> <li>Run:ai user interface</li> <li>Run:ai API</li> </ul> <p>At this time, Inference services cannot be created via the CLI. The CLI can be used for creating a client to query the inference service.</p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Infrastructure Administrator will need to install some optional inference prerequisites as described here.</p> <p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>ML Engineer access to Project in Run:ai named \"team-a\"</li> <li>The project should be assigned a quota of at least 1 GPU. </li> <li>The URL of the Run:ai Console. E.g. https://acme.run.ai.</li> </ul> <p>As described, the inference client can be created via CLI. To perform this, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/quickstart-inference/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#create-an-inference-server-environment","title":"Create an Inference Server Environment","text":"<p>To complete this Quickstart via the UI, you will need to create a new Inference Server Environment asset. </p> <p>This is a one-time step for all Inference workloads using the same image.</p> <p>Under <code>Environments</code> Select NEW ENVIRONMENT. Then select:</p> <ul> <li>A default (cluster) scope.</li> <li>Use the environment name <code>inference-server</code>.</li> <li>The image <code>runai.jfrog.io/demo/example-triton-server</code>.</li> <li>Under <code>type of workload</code> select <code>inference</code>.</li> <li>Under <code>endpoint</code> set the container port as <code>8000</code> which is the port that the triton server is using. </li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#run-an-inference-workload","title":"Run an Inference Workload","text":"CLI V1CLI V2User InterfaceAPI <p>Not available right now.</p> <p>Not available right now.</p> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Inference</li> <li>You should already have <code>Cluster</code> and <code>Project</code> selected. Enter <code>inference-server-1</code> as the name and press CONTINUE.</li> <li>Under <code>Environment</code>,  select <code>inference-server</code>.</li> <li>Under <code>Compute Resource</code>, select <code>half-gpu</code>. </li> <li>Under `Replica autoscaling, select a minimum of 1 and a maximum of 2. </li> <li>Under <code>conditions for a new replica</code> select <code>Concurrency</code> and set the value as 3.</li> <li>Set the <code>scale to zero</code> option to <code>5 minutes</code></li> <li>Select CREATE INFERENCE.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/inferences' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"inference-server-1\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"image\": \"runai.jfrog.io/demo/example-triton-server\",\n        \"servingPort\": {\n            \"protocol\": \"http\",\n            \"container\": 8000\n        },\n        \"autoscaling\": {\n            \"minReplicas\": 1,\n            \"maxReplicas\": 2,\n            \"metric\": \"concurrency\",\n            \"metricThreshold\": 3,\n            \"scaleToZeroRetentionSeconds\": 300\n        },\n        \"compute\": {\n            \"cpuCoreRequest\": 0.1,\n            \"gpuRequestType\": \"portion\",\n            \"cpuMemoryRequest\": \"100M\",\n            \"gpuDevicesRequest\": 1,\n            \"gpuPortionRequest\": 0.5\n        }\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Inference Submit API see API Documentation </li> </ul> <p>This would start a triton inference server with a maximum of 2 instances, each instance consumes half a GPU. </p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#query-the-inference-server","title":"Query the Inference Server","text":"<p>You can use the Run:ai Triton demo client to send requests to the server</p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#find-the-inference-server-endpoint","title":"Find the Inference Server Endpoint","text":"<ul> <li>Under <code>Workloads</code>, select <code>Columns</code> on the top right. Add the column <code>Connections</code>.</li> <li>See the connections of the <code>inference-server-1</code> workload: </li> </ul> <ul> <li>Copy the inference endpoint URL.</li> </ul> CLI V1CLI V2User Interface <p>Open a terminal and run:</p> <pre><code>runai config project team-a   \nrunai submit inference-client-1  -i runai.jfrog.io/demo/example-triton-client \\\n-- perf_analyzer -m inception_graphdef  -p 3600000 -u  &lt;INFERENCE-ENDPOINT&gt;    \n</code></pre> <p>Open a terminal and run:</p> <pre><code>runai project set team-a\nrunai training submit inference-client-1  -i runai.jfrog.io/demo/example-triton-client \\\n-- perf_analyzer -m inception_graphdef  -p 3600000 -u  &lt;INFERENCE-ENDPOINT&gt;    \n</code></pre> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Training</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. Enter <code>inference-client-1</code> as the name and press CONTINUE.</li> <li>Select NEW ENVIRONMENT. Enter <code>inference-client</code> as the name and <code>runai.jfrog.io/demo/example-triton-client</code> as the image.  Select CREATE ENVIRONMENT.</li> <li>When the previous screen comes up, select <code>cpu-only</code> under the Compute resource.</li> <li>Under <code>runtime settings</code> enter the command as <code>perf_analyzer</code> and arguments <code>-m inception_graphdef  -p 3600000 -u  &lt;INFERENCE-ENDPOINT&gt;</code> (replace inference endpoint with the above URL).</li> <li>Select CREATE TRAINING.</li> </ul> <p>In the user interface, under <code>inference-server-1</code>, go to the <code>Metrics</code> tab and watch as the various GPU and inference metrics graphs rise. </p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <p>Not available right now</p> <p>Not available right now</p> <p>Select the two workloads and press DELETE.</p>"},{"location":"Researcher/Walkthroughs/quickstart-jupyter/","title":"Quickstart: Launch WorkSpace with a Jupyter Notebook","text":""},{"location":"Researcher/Walkthroughs/quickstart-jupyter/#introduction","title":"Introduction","text":"<p>The purpose of this article is to provide a quick ramp-up to running a Jupyter Notebook Workspace. Workspaces are containers that live forever until deleted by the user. </p> <p>There are various ways to submit a Workspace:</p> <ul> <li>Run:ai command-line interface (CLI)</li> <li>Run:ai user interface</li> <li>Run:ai API</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-jupyter/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Researcher access to Project in Run:ai named \"team-a\"</li> <li>The project should be assigned a quota of at least 1 GPU. </li> <li>A URL of the Run:ai Console. E.g. https://acme.run.ai.</li> </ul> <p>To complete this Quickstart via the CLI, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-jupyter/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/quickstart-jupyter/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/quickstart-jupyter/#run-workload","title":"Run Workload","text":"CLI V1CLI V2User InterfaceAPI <p>Open a terminal and run:</p> <pre><code>runai config project team-a   \nrunai submit jup1 --jupyter -g 1\n</code></pre> <p>Note</p> <p>For more information on the workload submit command, see cli documentation.</p> <p>Open a terminal and run:</p> <pre><code>runai project set team-a\nrunai workspace submit jup1  --image jupyter/scipy-notebook --gpu-devices-request 1 \\\n    --external-url container=8888  --command start-notebook.sh  \\\n    -- --NotebookApp.base_url=/\\${RUNAI_PROJECT}/\\${RUNAI_JOB_NAME} --NotebookApp.token=''\n</code></pre> <p>Note</p> <p>For more information on the workspace submit command, see cli documentation.</p> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Workspace</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. Enter <code>jup1</code> as the name and press CONTINUE.</li> <li>Under <code>Environment</code>,  select <code>jupyter-lab</code>.</li> <li>Under <code>Compute Resource</code>, select <code>one-gpu</code>. </li> <li>Select CREATE WORKSPACE.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/workspaces' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"jup1\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"command\" : \"start-notebook.sh\",\n        \"args\" : \"--NotebookApp.base_url=/${RUNAI_PROJECT}/${RUNAI_JOB_NAME} --NotebookApp.token=''\",\n        \"image\": \"jupyter/scipy-notebook\",\n        \"compute\": {\n            \"gpuDevicesRequest\": 1\n        },\n        \"exposedUrls\" : [\n            { \n                \"container\" : 8888,\n                \"toolType\": \"jupyter-notebook\", \\ # (5)\n                \"toolName\": \"Jupyter\" \\ # (6)\n            }\n        ]\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> <li><code>toolType</code> will show the Jupyter icon when connecting to the Jupyter tool via the user interface. </li> <li><code>toolName</code> text will show when connecting to the Jupyter tool via the user interface.</li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Training Submit API see API Documentation </li> </ul> <p>This would start a Workspace with a pre-configured Jupyter image with an allocation of a single GPU. </p>"},{"location":"Researcher/Walkthroughs/quickstart-jupyter/#accessing-the-jupyter-notebook","title":"Accessing the Jupyter Notebook","text":"<p>Via the Run:ai user interface, go to <code>Workloads</code>, select the <code>jup1</code> Workspace and press <code>Connect</code>.</p> <p>Alternatively, browse directly to <code>https://&lt;COMPANY-URL&gt;/team-a/jup1</code>.</p>"},{"location":"Researcher/Walkthroughs/quickstart-mig/","title":"Quickstart: Launch Workloads with NVIDIA Dynamic MIG","text":""},{"location":"Researcher/Walkthroughs/quickstart-mig/#introduction","title":"Introduction","text":"<p>A single GPU has a significant amount of memory. Ranging from a couple of gigabytes in older generations and up to 80GB per GPU in the later models of the latest NVIDIA GPU technology. A single GPU also has a vast amount of computing power. </p> <p>This amount of memory and computing power is important for processing large amounts of data, such as in training deep learning models. However, there are quite a few applications that do not need this power. Examples can be inference workloads and the model-creation phase. It would thus be convenient if we could divide up a GPU between various workloads, thus achieving better GPU utilization. </p> <p>Run:ai provides two alternatives for splitting GPUs: Fractions and Dynamic MIG allocation. The focus of this article is Dynamic MIG allocation.  A detailed explanation of the two Run:ai offerings can be found here.</p>"},{"location":"Researcher/Walkthroughs/quickstart-mig/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> <li>A machine with a single available NVIDIA A100 GPU. This can be achieved by allocating filler workloads to the other GPUs on the node, or by using Google Cloud which allows for the creation of a virtual node with a single A100 GPU. </li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-mig/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/quickstart-mig/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Allocate 2 GPUs to the Project.</li> <li>Mark the node as a dynamic MIG node as described here.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-mig/#run-an-inference-workload-single-replica","title":"Run an Inference Workload - Single Replica","text":"<p>At the GPU node level, run: <code>nvidia-smi</code>:</p> <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                   On |\n| N/A   32C    P0    42W / 400W |      0MiB / 40536MiB |     N/A      Default |\n|                               |                      |              Enabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  No MIG devices found                                                       |\n+-----------------------------------------------------------------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>In the highlighted text above, note that:</p> <ul> <li>MIG is enabled (if <code>Enabled</code> has a star next to it, you need to reboot your machine).</li> <li>The GPU is not yet divided into devices.</li> </ul> <p>At the command-line run:</p> <pre><code>runai config project team-a\nrunai submit mig1 -i runai.jfrog.io/demo/quickstart-cuda  --gpu-memory 10GB\nrunai submit mig2 -i runai.jfrog.io/demo/quickstart-cuda  --mig-profile 2g.10gb \nrunai submit mig3 -i runai.jfrog.io/demo/quickstart-cuda  --mig-profile 2g.10gb \n</code></pre> <p>We used two different methods to create MIG partitions: </p> <ol> <li>Stating the amount of GPU memory we require </li> <li>Requiring a partition of explicit size using NVIDIA terminology. </li> </ol> <p>Both methods achieve the same effect. They result in three MIG partitions of 10GB each. You can verify that by running <code>nvidia-smi</code>, at the GPU node level:</p> <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                   On |\n| N/A   47C    P0   194W / 400W |  27254MiB / 40536MiB |     N/A      Default |\n|                               |                      |              Enabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  0    3   0   0  |   9118MiB /  9984MiB | 28      0 |  2   0    1    0    0 |\n|                  |      4MiB / 16383MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n|  0    4   0   1  |   9118MiB /  9984MiB | 28      0 |  2   0    1    0    0 |\n|                  |      4MiB / 16383MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n|  0    5   0   2  |   9016MiB /  9984MiB | 28      0 |  2   0    1    0    0 |\n|                  |      2MiB / 16383MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0    3    0     142213      C   ./quickstart                     9111MiB |\n|    0    4    0     146799      C   ./quickstart                     9111MiB |\n|    0    5    0     132219      C   ./quickstart                     9009MiB |\n+-----------------------------------------------------------------------------+\n</code></pre> <ul> <li>Highlighted above is a list of 3 MIG devices, each 10GB large. Total of 30GB (out of the 40GB on the GPU)</li> <li>You can also run the same command inside one of the containers: <code>runai exec mig1 nvidia-smi</code>. This will show a single device (the only one that the container sees from its point of view).</li> <li>Run: <code>runai list</code> to see the 3 jobs in <code>Running</code> state.</li> </ul> <p>We now want to allocate an interactive job with 20GB. Interactive jobs take precedence over the default training jobs:</p> <p><pre><code>runai submit mig1-int -i runai.jfrog.io/demo/quickstart-cuda \\\n    --interactive --gpu-memory 20G \n</code></pre> or similarly, <pre><code>runai submit mig1-int -i runai.jfrog.io/demo/quickstart-cuda \\\n    --interactive --mig-profile 3g.20gb  \n</code></pre></p> <p>Using <code>runai list</code> and <code>nvidia-smi</code> on the host machine, you can see that:</p> <ul> <li>One training job is preempted, and its device is deleted.</li> <li>The new, interactive job starts running.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-overview/","title":"Run:ai Quickstart Guides","text":"<p>Below is a set of Quickstart documents. The purpose of these documents is to get you acquainted with an aspect of Run:ai in the simplest possible form.</p> <p>Note</p> <p>The Quickstart documents are based solely on the command-line interface. The same functionality can be achieved by using the Workloads User interface which allows for Workload submission and log viewing. </p> <p>Follow the Quickstart documents below to learn more:</p> <ul> <li>Training Quickstart documents:<ul> <li>Unattended training sessions</li> <li>Distributed Training</li> </ul> </li> <li>Build Quickstart documents: <ul> <li>Basic Interactive build sessions</li> <li>Interfactive build session with connected ports</li> <li>Jupyter Notebook</li> <li>Visual Studio Web</li> </ul> </li> <li>Inference</li> <li>GPU Allocation documents:<ul> <li>Using GPU Fractions</li> <li>Dynamic MIG</li> </ul> </li> <li>Scheduling documents:<ul> <li>Over-Quota, Basic Fairness &amp; Bin Packing</li> <li>Fairness</li> </ul> </li> </ul> <p>Most quickstarts rely on an image called <code>runai.jfrog.io/demo/quickstart</code>. The image is based on  TensorFlow Release 20-08. This TensorFlow image has minimal requirements for CUDA and NVIDIA Compute Capability. </p> <p>If your GPUs do not meet these requirements, use <code>runai.jfrog.io/demo/quickstart:legacy</code> instead. </p>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/","title":"Quickstart: Launch Workspace with a Visual Studio Code for Web","text":""},{"location":"Researcher/Walkthroughs/quickstart-vscode/#introduction","title":"Introduction","text":"<p>The purpose of this article is to provide a quick ramp-up to running a Workspace running Visual Studio Code (Web edition). Workspaces are containers that live forever until deleted by the user. </p> <p>There are various ways to submit a Workspace:</p> <ul> <li>Run:ai command-line interface (CLI)</li> <li>Run:ai user interface</li> <li>Run:ai API</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Infrastructure Administrator will need to configure a wildcard certificate to Run:ai as described here.</p> <p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Researcher access to Project in Run:ai named \"team-a\"</li> <li>The project should be assigned a quota of at least 1 GPU. </li> <li>A URL of the Run:ai Console. E.g. https://acme.run.ai.</li> </ul> <p>To complete this Quickstart via the CLI, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/quickstart-vscode/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/#create-a-visual-studio-environment","title":"Create a Visual Studio Environment","text":"<p>To complete this Quickstart via the UI, you will need to create a new Visual Studio Environment asset. </p> <p>This is a one-time step for all VSCode Workloads.</p> <p>Under <code>Environments</code> Select NEW ENVIRONMENT. Then select:</p> <ul> <li>A default (cluster) scope.</li> <li>Use the environment name <code>vscode</code>.</li> <li>The image <code>quay.io/opendatahub-contrib/workbench-images:vscode-datascience-c9s-py311_2023c_latest</code>.</li> <li>Under <code>Tools</code>, add Visual Studio Code and change the port to <code>8787</code>.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/#run-workload","title":"Run Workload","text":"CLI V1CLI V2User InterfaceAPI <p>Open a terminal and run:</p> <pre><code>runai config project team-a   \nrunai submit vs1 --jupyter -g 1\n</code></pre> <p>Note</p> <p>For more information on the workload submit command, see cli documentation.</p> <p>Open a terminal and run:</p> <pre><code>runai project set team-a\nrunai workspace submit vs1  --image quay.io/opendatahub-contrib/workbench-images:vscode-datascience-c9s-py311_2023c_latest \\\n    --gpu-devices-request 1  --external-url container=8787  \n</code></pre> <p>Note</p> <p>For more information on the workspace submit command, see cli documentation.</p> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Workspace</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. Enter <code>vs1</code> as the name and press CONTINUE.</li> <li>Under <code>Environment</code>,  select <code>vscode</code>.</li> <li>Under <code>Compute Resource</code>, select <code>one-gpu</code>. </li> <li>Select CREATE WORKSPACE.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/workspaces' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"vs1\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"image\": \"quay.io/opendatahub-contrib/workbench-images:vscode-datascience-c9s-py311_2023c_latest\",\n        \"compute\": {\n            \"gpuDevicesRequest\": 1\n        },\n        \"exposedUrls\" : [\n            { \n                \"container\" : 8787,\n                \"toolType\": \"visual-studio-code\", \\ # (5)\n                \"toolName\": \"Visual Studio\" \\ # (6)\n            }\n        ]\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> <li><code>toolType</code> will show the Visual Studio icon when connecting to the Visual Studio tool via the user interface. </li> <li><code>toolName</code> text will show when connecting to the Visual Studio tool via the user interface.</li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Training Submit API see API Documentation </li> </ul> <p>This would start a Workspace with a pre-configured Visual Studio Code image with an allocation of a single GPU. </p>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/#accessing-visual-studio-web","title":"Accessing Visual Studio Web","text":"<p>Via the Run:ai user interface, go to <code>Workloads</code>, select the <code>vs1</code> Workspace and press <code>Connect</code>.</p>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <pre><code>runai delete job vs1\n</code></pre> <pre><code>runai workspace delete vs1\n</code></pre> <p>Select the Workspace and press DELETE.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/","title":"Quickstart: Launch Interactive Build Workloads with Connected Ports","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#introduction","title":"Introduction","text":"<p>This Quickstart is an extension of the Quickstart document: Start and Use Interactive Build Workloads </p> <p>When starting a container with the Run:ai Command-Line Interface (CLI), it is sometimes needed to expose internal ports to the user. Examples are: accessing a Jupyter notebook, using the container from a development environment such as PyCharm. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#exposing-a-container-port","title":"Exposing a Container Port","text":"<p>There are three ways to expose ports in Kubernetes: Port Forwarding, NodePort, and LoadBalancer. The first two will always work. The other requires a special setup by your administrator. The four methods are explained here. </p> <p>The document below provides an example based on Port Forwarding.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#port-forwarding-step-by-step-walkthrough","title":"Port Forwarding, Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named <code>team-a</code>.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#run-workload","title":"Run Workload","text":"<ul> <li>At the command-line run:</li> </ul> <pre><code>runai config project team-a\nrunai submit nginx-test -i zembutsu/docker-sample-nginx --interactive\nrunai port-forward nginx-test --port 8080:80\n</code></pre> <ul> <li>The Job is based on a sample NGINX webserver docker image <code>zembutsu/docker-sample-nginx</code>. Once accessed via a browser, the page shows the container name. </li> <li>Note the interactive flag which means the Job will not have a start or end. It is the Researcher's responsibility to close the Job.  </li> <li>In this example, we have chosen the simplest scheme to expose ports which is port forwarding. We temporarily expose port 8080 to localhost as long as the <code>runai port-forward</code> command is not stopped</li> <li>It is possible to forward traffic from multiple IP addresses by using the \"--address\" parameter. Check the CLI reference for further details. </li> </ul> <p>The result will be:</p> <pre><code>The job 'nginx-test-0' has been submitted successfully\nYou can run `runai describe job nginx-test-0 -p team-a` to check the job status\n\nForwarding from 127.0.0.1:8080 -&gt; 80\nForwarding from [::1]:8080 -&gt; 80\n</code></pre>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#access-the-webserver","title":"Access the Webserver","text":"<p>Open the following in the browser at http://localhost:8080.</p> <p>You should see a web page with the name of the container.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#stop-workload","title":"Stop Workload","text":"<p>Press Ctrl-C in the shell to stop port forwarding. Then delete the Job by running <code>runai delete job nginx-test</code></p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#see-also","title":"See Also","text":"<ul> <li>Develop on Run:ai using Visual Studio Code</li> <li>Develop on Run:ai using PyCharm</li> <li>Use a Jupyter notbook with Run:ai.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/","title":"Quickstart: Launch Interactive Build Workloads","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build/#introduction","title":"Introduction","text":"<p>The purpose of this article is to provide a quick ramp-up to running an interactive Workspace to allow building data science programs. data scientists typically use various tools such as Jupyter Notebook,  PyCharm, or Visual Studio code. However, in this quickstart, we will start by launching a bare-bones Workspace without such tools. </p> <p>With this Quickstart you will learn how to:</p> <ul> <li>Start a workspace.</li> <li>Open an ssh session to the workspace.</li> <li>Stop the workspace.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Researcher access to Project in Run:ai named \"team-a\"</li> <li>The project should be assigned a quota of at least 1 GPU. </li> <li>A URL of the Run:ai Console. E.g. https://acme.run.ai.</li> </ul> <p>To complete this Quickstart via the CLI, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#step-by-step-quickstart","title":"Step by Step Quickstart","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#create-a-workspace","title":"Create a Workspace","text":"CLI V1CLI V2User InterfaceAPI <p>Open a terminal and run:</p> <pre><code>runai config project team-a   \nrunai submit build1 -i ubuntu -g 1 --interactive -- sleep infinity\n</code></pre> <p>Note</p> <p>For more information on the workload submit command, see cli documentation.</p> <p>Open a terminal and run:</p> <pre><code>runai project set team-a\nrunai workspace submit build1 -i ubuntu -g 1 --command -- sleep infinity\n</code></pre> <p>Note</p> <p>For more information on the workspace submit command, see cli documentation.</p> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Workspace</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. Enter <code>build1</code> as the name and press CONTINUE.</li> <li>Select NEW ENVIRONMENT. Enter <code>ubuntu</code> as the name and <code>ubuntu</code> as the image. Then select CREATE ENVIRONMENT.</li> <li>When the previous screen comes up, select <code>one-gpu</code> under the Compute resource. </li> <li>Select CREATE WORKSPACE.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/workspaces' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"build1\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"command\" : \"sleep\",\n        \"args\" : \"infinity\"\n        \"image\": \"ubuntu\",\n        \"compute\": {\n        \"gpuDevicesRequest\": 1\n        }\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Workspace Submit API see API Documentation </li> </ul> <ul> <li>This would start a workload of type Workspace for <code>team-a</code> with an allocation of a single GPU. </li> <li>We named the Workload <code>build1</code>. </li> <li>Note that, unlike a Training workload, a Workspace workload will not end automatically. It is the Researcher's responsibility to stop the Workload. </li> <li>The command provided is <code>sleep infinity</code>. You must provide a command or the container will start and then exit immediately. Alternatively, when using the command line, replace these flags with <code>--attach</code> to attach immediately to a session.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#list-workloads","title":"List Workloads","text":"<p>Follow up on the Workload's progress by running:</p> CLI V1CLI V2User Interface <p><pre><code>runai list jobs\n</code></pre> The result: </p> <pre><code>runai workspace list\n</code></pre> <p>The result:</p> <pre><code>Workload     Type        Status      Project     Preemptible      Running/Requested Pods     GPU Allocation\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nvs1          Workspace   Running     team-a      No               1/1                        1.00\n</code></pre> <ul> <li>Open the Run:ai user interface.</li> <li>Under \"Workloads\" you can view the new Workspace:</li> </ul> <p></p> <p>Select the Workloads and press <code>Show Details</code> to see the Workload details</p> <p> </p> <p>Typical statuses you may see:</p> <ul> <li>ContainerCreating - The docker container is being downloaded from the cloud repository</li> <li>Pending - the job is waiting to be scheduled</li> <li>Running - the job is running</li> </ul> <p>A full list of Job statuses can be found here</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#describe-workload","title":"Describe Workload","text":"<p>To get additional status on your Workload run:</p> CLI V1CLI V2User Interface <pre><code>runai describe job build1\n</code></pre> <pre><code>runai workspace describe build1\n</code></pre> <p>Workload parameters can be viewed by adding more columns to the Workload list and by reviewing the <code>Event History</code> tab for the specific Workload. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#get-a-shell-to-the-container","title":"Get a Shell to the container","text":"CLI V1CLI V2 <p>Run: <pre><code>runai bash build1\n</code></pre></p> <pre><code>runai workspace bash build1\n</code></pre> <p>This should provide a direct shell into the computer</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <pre><code>runai delete job build1\n</code></pre> <pre><code>runai workspace delete build1\n</code></pre> <p>Select the Workspace and press DELETE.</p> <p>This would stop the workspace. You can verify this by running the list command again.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/","title":"Quickstart: Launch Distributed Training Workloads","text":""},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#introduction","title":"Introduction","text":"<p>Distributed Training is the ability to split the training of a model among multiple processors. Each processor is called a worker. Worker nodes work in parallel to speed up model training. There is also a master which coordinates the workers. </p> <p>Distributed Training should not be confused with multi-GPU training. Multi-GPU training is the allocation of more than a single GPU to your workload which runs on a single container.</p> <p>Getting distributed training to work is more complex than a single-container training as it requires syncing of data and timing between the different workers. However, it is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Several Deep Learning frameworks support distributed training. This example will focus on PyTorch.</p> <p>Run:ai provides the ability to run, manage, and view distributed training workloads. The following is a Quickstart document for such a scenario.</p> <p>There are various ways to submit a distributed training Workload:</p> <ul> <li>Run:ai command-line interface (CLI)</li> <li>Run:ai user interface</li> <li>Run:ai API</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Infrastructure Administrator will need to install the optional Kubeflow Training Operator as described here</p> <p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Researcher access to Project in Run:ai named \"team-a\"</li> <li>The project should be assigned a quota of at least 1 GPU. </li> <li>A URL of the Run:ai Console. E.g. https://acme.run.ai.</li> </ul> <p>To complete this Quickstart via the CLI, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#run-a-distributed-training-workload","title":"Run a Distributed Training Workload","text":"CLI V1CLI V2User InterfaceAPI <p>Open a terminal and run:</p> <pre><code>runai config project team-a\nrunai submit-dist pytorch dist-train1 --workers=2 -g 0.1 \\\n    -i kubeflow/pytorch-dist-mnist:latest\n</code></pre> <p>Note</p> <p>For more information on the workload submit command, see cli documentation.</p> <p>Open a terminal and run:</p> <pre><code>runai project set team-a\nrunai distributed submit dist-train1  --framework PyTorch \\\n    -i kubeflow/pytorch-dist-mnist:latest --workers 2 \n    --gpu-request-type portion --gpu-portion-request 0.1 --gpu-devices-request 1 --cpu-memory-request 100M\n</code></pre> <p>Note</p> <p>For more information on the training submit command, see cli documentation.</p> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Training</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. </li> <li>Under <code>Workload architecture</code> select <code>Distributed</code> and choose <code>PyTorch</code>. Set the distributed training configuration to <code>Workers &amp; master</code>.</li> <li>Enter <code>train1</code> as the name and press CONTINUE.</li> <li>Select NEW ENVIRONMENT. Enter <code>pytorch-dt</code> as the name and <code>kubeflow/pytorch-dist-mnist:latest</code> as the image. Then select CREATE ENVIRONMENT.</li> <li>When the previous screen comes up, under <code>Compute resource</code> enter 2 workers and select <code>small-fraction</code> as the Compute resource. </li> <li>Select CONTINUE and then CREATE TRAINING.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/distributed' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"dist-train1\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"compute\": {\n            \"cpuCoreRequest\": 0.1,\n            \"gpuRequestType\": \"portion\",\n            \"cpuMemoryRequest\": \"100M\",\n            \"gpuDevicesRequest\": 1,\n            \"gpuPortionRequest\": 0.1\n        },\n        \"image\": \"kubeflow/pytorch-dist-mnist:latest\",  \n        \"numWorkers\": 2,  \\ # (5)\n        \"distributedFramework\": \"PyTorch\" \\ # (6)\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> <li>Use 2 workers.</li> <li>Use PyTorch training operator </li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Distributed Training Submit API see API Documentation </li> </ul> <p>This would start a distributed training Workload for <code>team-a</code>. The Workload will have one master and two workers. We named the Workload <code>dist-train1</code></p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#list-workloads","title":"List Workloads","text":"<p>Follow up on the Workload's progress by running:</p> CLI V1CLI V2User Interface <p><pre><code>runai list jobs\n</code></pre> The result: </p> <pre><code>runai distributed list\n</code></pre> <p>The result:</p> <pre><code>Workload     Type         Status      Project     Preemptible      Running/Requested Pods     GPU Allocation\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndist-train1  Distributed  Running      team-a      Yes              0/2                        0.00\n</code></pre> <ul> <li>Open the Run:ai user interface.</li> <li>Under \"Workloads\" you can view the new Training Workload:</li> </ul> <p></p> <ul> <li>Select the <code>0/2</code> under Running/Requested Pods and watch the worker pod status:</li> </ul> <p></p> <p>Select the <code>dist-train1</code> workload and press <code>Show Details</code> to see the Workload details</p> <p> </p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#describe-workload","title":"Describe Workload","text":"<p>The Run:ai scheduler ensures that all pods can run together. You can see the list of workers as well as the main \"launcher\" pod by running:</p> CLI V1CLI V2User Interface <pre><code>runai describe job train1\n</code></pre> <pre><code>runai training describe train1\n</code></pre> <p>Workload parameters can be viewed by adding more columns to the Workload list and by reviewing the <code>Event History</code> tab for the specific Workload. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#view-logs","title":"View Logs","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <p>Get the name of the worker pods from the above <code>describe</code> command, then run: </p> <pre><code>runai logs dist-train1 --pod dist-train1-worker-0\n</code></pre> <p>(where <code>dist-train1-worker-0</code> is the name of the first worker)</p> <p>You should see a log of a running container</p> <p>Get the name of the worker pods from the above <code>describe</code> command, then run: </p> <pre><code>runai distributed logs dist-train1 --pod dist-train1-worker-0\n</code></pre> <p>(where <code>dist-train1-worker-0</code> is the name of the first worker)</p> <p>You should see a log of a running container:</p> <p>Select the Workload, and press Show Details. Under <code>Logs</code> you can select each of the workers and see the logs emitted from the container</p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <pre><code>runai delete job dist-train1\n</code></pre> <pre><code>runai training delete dist-train1\n</code></pre> <p>Select the Workload and press DELETE.</p> <p>This would stop the training workload. You can verify this by listing training workloads again.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/","title":"Quickstart: Launch Workloads with GPU Fractions","text":""},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#introduction","title":"Introduction","text":"<p>Run:ai provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU, enabling companies to run more workloads such as computer vision, voice recognition and natural language processing on the same hardware, lowering costs.</p> <p>Run:ai\u2019s fractional GPU system effectively creates logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. This enables several workloads to run in containers side-by-side on the same GPU without interfering with each other. The solution is transparent, simple, and portable; it requires no changes to the containers themselves.</p> <p>A typical use-case could see a couple of Workloads running on the same GPU, meaning you could multiply the work with the same hardware.</p> <p>The purpose of this article is to provide a quick ramp-up to running a training Workload with fractions of a GPU.  </p> <p>There are various ways to submit a  Workload:</p> <ul> <li>Run:ai command-line interface (CLI)</li> <li>Run:ai user interface</li> <li>Run:ai API </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Researcher access to Run:ai </li> <li>To a Project named \"team-a\"</li> <li>With at least 1 GPU assigned to the project. </li> <li>A link to the Run:ai Console. E.g. https://acme.run.ai.</li> <li>To complete this Quickstart via the CLI, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:<ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul> </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#run-workload","title":"Run Workload","text":"<p>Open a terminal and run:</p> CLI V1CLI V2User InterfaceAPI <pre><code>runai config project team-a   \nrunai submit frac05 -i runai.jfrog.io/demo/quickstart -g 0.5\nrunai submit frac05-2 -i runai.jfrog.io/demo/quickstart -g 0.5 \n</code></pre> <pre><code>runai project set team-a\nrunai training submit frac05 -i runai.jfrog.io/demo/quickstart --gpu-portion-request 0.5\nrunai training submit frac05-2 -i runai.jfrog.io/demo/quickstart --gpu-portion-request 0.5\n</code></pre> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Training</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. Enter <code>frac05</code> as the name and press CONTINUE.</li> <li>Select NEW ENVIRONMENT. Enter <code>quickstart</code> as the name and <code>runai.jfrog.io/demo/quickstart</code> as the image. Then select CREATE ENVIRONMENT.</li> <li>When the previous screen comes up, select <code>half-gpu</code> under the Compute resource. </li> <li>Select CREATE TRAINING.</li> <li>Follow the process again to submit a second workload called <code>frac05-2</code>.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/trainings' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"frac05\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"image\": \"runai.jfrog.io/demo/quickstart\",\n        \"compute\": {\n        \"gpuRequestType\": \"portion\",\n        \"gpuPortionRequest\" : 0.5\n        }\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Training Submit API see API Documentation </li> </ul> <ul> <li>The Workloads are based on a sample docker image <code>runai.jfrog.io/demo/quickstart</code> the image contains a startup script that runs a deep learning TensorFlow-based workload.</li> <li>We named the Workloads frac05 and frac05-2 respectively. </li> <li>The Workloads are assigned to team-a with an allocation of half a GPU. </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#list-workloads","title":"List Workloads","text":"<p>Follow up on the Workload's progress by running:</p> CLI V1CLI V2User Interface <p><pre><code>runai list jobs\n</code></pre> The result:</p> <pre><code>Showing jobs for project team-a\nNAME      STATUS   AGE  NODE                  IMAGE                          TYPE   PROJECT  USER   GPUs Allocated (Requested)  PODs Running (Pending)  SERVICE URL(S)\nfrac05    Running  9s   runai-cluster-worker  runai.jfrog.io/demo/quickstart  Train  team-a   yaron  0.50 (0.50)                 1 (0)\nfrac05-2  Running  8s   runai-cluster-worker  runai.jfrog.io/demo/quickstart  Train  team-a   yaron  0.50 (0.50)                 1 (0)\n</code></pre> <pre><code>runai training list\n</code></pre> <p>The result:</p> <pre><code>Workload               Type        Status      Project     Preemptible      Running/Requested Pods     GPU Allocation\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfrac05      Training    Running  team-a      Yes              0/1                        0.00\nfrac05-2    Training    Running  team-a      Yes              0/1                        0.00    \n</code></pre> <ul> <li>Open the Run:ai user interface.</li> <li>Under <code>Workloads</code> you can view the two new Training Workloads</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#view-partial-gpu-memory","title":"View Partial GPU memory","text":"<p>To verify that the Workload sees only parts of the GPU memory run:</p> CLI V1CLI V2 <pre><code>runai exec frac05 nvidia-smi\n</code></pre> <pre><code>runai training exec frac05 nvidia-smi\n</code></pre> <p>The result:</p> <p></p> <p>Notes:</p> <ul> <li>The total memory is circled in red. It should be 50% of the GPUs memory size. In the picture above we see 8GB which is half of the 16GB of Tesla V100 GPUs.</li> <li>The script running on the container is limited by 8GB. In this case, TensorFlow, which tends to allocate almost all of the GPU memory has allocated 7.7GB RAM (and not close to 16 GB). Overallocation beyond 8GB will lead to an out-of-memory exception </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#use-exact-gpu-memory","title":"Use Exact GPU Memory","text":"<p>Instead of requesting a fraction of the GPU, you can ask for specific GPU memory requirements. For example:</p> CLI V1CLI V2User Interface <pre><code>runai submit  -i runai.jfrog.io/demo/quickstart --gpu-memory 5G\n</code></pre> <pre><code>runai training submit -i runai.jfrog.io/demo/quickstart --gpu-memory-request 5G\n</code></pre> <p>As part of the Workload submission, Create a new <code>Compute Resource</code>, with 1 GPU Device and 5GB of <code>GPU memory per device</code>. See picture below: </p> <p>Which will provide 5GB of GPU memory. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/","title":"Quickstart: Over-Quota and Bin Packing","text":""},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#goals","title":"Goals","text":"<p>The goal of this Quickstart is to explain the concepts of over-quota and bin-packing (consolidation) and how they help in maximizing cluster utilization: </p> <ul> <li>Show the simplicity of resource provisioning, and how resources are abstracted from users.</li> <li>Show how the system eliminates compute bottlenecks by allowing teams/users to go over their resource quota if there are free GPUs in the cluster.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#setup-and-configuration","title":"Setup and configuration:","text":"<p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Your cluster should have 4 GPUs on 2 machines with 2 GPUs each.</li> <li>Researcher access to two Projects  named \"team-a\" and \"team-b\"</li> <li>Each project should be assigned an exact quota of 2 GPUs. </li> <li>A URL of the Run:ai Console. E.g. https://acme.run.ai.</li> <li> <p>Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul> </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#login","title":"Login","text":"<p>Run <code>runai login</code> and enter your credentials.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-i-over-quota","title":"Part I: Over-quota","text":"<p>Open a terminal and run the following command:</p> CLI V1CLI V2 <pre><code>runai submit a2 -i runai.jfrog.io/demo/quickstart -g 2 -p team-a\nrunai submit a1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai submit b1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\n</code></pre> <pre><code>runai training submit a2 -i runai.jfrog.io/demo/quickstart -g 2 -p team-a\nrunai training submit a1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai training submit b1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <ul> <li>team-a has 3 GPUs allocated. Which is over its quota by 1 GPU. </li> <li>The system allows this over-quota as long as there are available resources</li> <li>The system is at full capacity with all GPUs utilized. </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-2-basic-fairness-via-preemption","title":"Part 2: Basic Fairness via Preemption","text":"<p>Run the following command:</p> CLI V1CLI V2 <pre><code>runai submit b2 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\n</code></pre> <pre><code>runai training submit b2 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <ul> <li>team-a can no longer remain in over-quota. Thus, one Job, must be preempted: moved out to allow team-b to grow.</li> <li>Run:ai scheduler chooses to preempt Job a1.</li> <li>It is important that unattended Jobs will save checkpoints. This will ensure that whenever Job a1 resume, it will do so from where it left off.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-3-bin-packing","title":"Part 3: Bin Packing","text":"<p>Run the following command:</p> CLI V1CLI V2 <p><code>runai delete job a2 -p team-a</code></p> <pre><code>runai training delete a2\n</code></pre> <p>a1 is now going to start running again.</p> <p>Run:</p> CLI V1CLI V2 <pre><code>runai list jobs -A\n</code></pre> <pre><code>runai training list -A\n</code></pre> <p>You have two Jobs that are running on the first node and one Job that is running alone the second node. </p> <p>Choose one of the two Jobs from the full node and delete it:</p> CLI V1CLI V2 <pre><code>runai delete job &lt;job-name&gt; -p &lt;project&gt;\n</code></pre> <pre><code>runai training delete &lt;job-name&gt; -p &lt;project&gt;\n</code></pre> <p>The status now is: </p> <p>Now, run a 2 GPU Job:</p> CLI V1CLI V2 <pre><code>runai submit a2 -i runai.jfrog.io/demo/quickstart -g 2 -p team-a\n</code></pre> <pre><code>runai training submit a2 -i runai.jfrog.io/demo/quickstart -g 2 -p team-a\n</code></pre> <p>_ The status now is: </p> <p>Discussion</p> <p>Note that Job a1 has been preempted and then restarted on the second node, to clear space for the new a2 Job. This is bin-packing or consolidation</p>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/","title":"Quickstart: Queue Fairness","text":""},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#goal","title":"Goal","text":"<p>The goal of this Quickstart is to explain fairness. The over-quota Quickstart shows basic fairness where allocated GPUs per Project are adhered to such that if a Project is in over-quota, its Job will be preempted once another Project requires its resources.</p> <p>This Quickstart is about queue fairness. It shows that Jobs will be scheduled fairly regardless of the time they have been submitted. As such, if a person in Project A has submitted 50 Jobs and soon after that, a person in Project B has submitted 25 Jobs, the Jobs in the queue will be processed fairly.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#setup-and-configuration","title":"Setup and configuration:","text":"<ul> <li>4 GPUs on 2 machines with 2 GPUs each.</li> <li>2 Projects: team-a and team-b with 1 allocated GPU each.</li> <li>Run:ai canonical image runai.jfrog.io/demo/quickstart</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-i-immediate-displacement-of-over-quota","title":"Part I: Immediate Displacement of Over-Quota","text":"<p>Run the following commands:</p> <pre><code>runai submit a1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai submit a2 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai submit a3 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai submit a4 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <p>team-a, even though it has a single GPU as quota, is now using all 4 GPUs.</p> <p>Run the following commands:</p> <pre><code>runai submit b1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\nrunai submit b2 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\nrunai submit b3 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\nrunai submit b4 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <ul> <li>Two team-b Jobs have immediately displaced team-a. </li> <li>team-a and team-b each have a quota of 1 GPU, thus the remaining over-quota (2 GPUs) is distributed equally between the Projects.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-2-queue-fairness","title":"Part 2: Queue Fairness","text":"<p>Now lets start deleting Jobs. Alternatively, you can wait for Jobs to complete.</p> <pre><code>runai delete job b2 -p team-b\n</code></pre> <p>Discussion</p> <p>As the quotas are equal (1 for each Project, the remaining pending Jobs will get scheduled one by one alternating between Projects, regardless of the time in which they were submitted. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/","title":"Quickstart: Launch Unattended Training Workloads","text":""},{"location":"Researcher/Walkthroughs/walkthrough-train/#introduction","title":"Introduction","text":"<p>The purpose of this article is to provide a quick ramp-up to running an unattended training Workload. Training Workloads are containers that execute a program on start and close down automatically when the task is done. </p> <p>With this Quickstart you will learn how to:</p> <ul> <li>Start a deep learning training workload.</li> <li>View training workload status and resource consumption using the Run:ai user interface and the Run:ai CLI.</li> <li>View training workload logs.</li> <li>Stop the training workload.</li> </ul> <p>There are various ways to submit a training Workload:</p> <ul> <li>Run:ai command-line interface (CLI)</li> <li>Run:ai user interface</li> <li>Run:ai API</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Researcher access to Project in Run:ai named \"team-a\"</li> <li>The project should be assigned a quota of at least 1 GPU. </li> <li>A URL of the Run:ai Console. E.g. https://acme.run.ai.</li> </ul> <p>To complete this Quickstart via the CLI, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-train/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#run-workload","title":"Run Workload","text":"CLI V1CLI V2User InterfaceAPI <p>Open a terminal and run:</p> <pre><code>runai config project team-a   \nrunai submit train1 -i runai.jfrog.io/demo/quickstart -g 1\n</code></pre> <p>Note</p> <p>For more information on the workload submit command, see cli documentation.</p> <p>Open a terminal and run:</p> <pre><code>runai project set team-a\nrunai training submit train1 -i runai.jfrog.io/demo/quickstart -g 1\n</code></pre> <p>Note</p> <p>For more information on the training submit command, see cli documentation.</p> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Training</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. Enter <code>train1</code> as the name and press CONTINUE.</li> <li>Select NEW ENVIRONMENT. Enter <code>quickstart</code> as the name and <code>runai.jfrog.io/demo/quickstart</code> as the image. Then select CREATE ENVIRONMENT.</li> <li>When the previous screen comes up, select <code>one-gpu</code> under the Compute resource. </li> <li>Select CREATE TRAINING.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/trainings' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"train1\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"image\": \"runai.jfrog.io/demo/quickstart\",\n        \"compute\": {\n        \"gpuDevicesRequest\": 1\n        }\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Training Submit API see API Documentation </li> </ul> <p>This would start an unattended training Workload for <code>team-a</code> with an allocation of a single GPU. The Workload is based on a sample docker image <code>runai.jfrog.io/demo/quickstart</code>. We named the Workload <code>train1</code></p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#list-workloads","title":"List Workloads","text":"<p>Follow up on the Workload's progress by running:</p> CLI V1CLI V2User Interface <p><pre><code>runai list jobs\n</code></pre> The result: </p> <pre><code>runai training list\n</code></pre> <p>The result:</p> <pre><code>Workload               Type        Status      Project     Preemptible      Running/Requested Pods     GPU Allocation\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntrain1                 Training    Running     team-a      Yes              1/1                        0.00\n</code></pre> <ul> <li>Open the Run:ai user interface.</li> <li>Under \"Workloads\" you can view the new Training Workload:</li> </ul> <p></p> <p>Select the Workloads and press <code>Show Details</code> to see the Workload details</p> <p> </p> <p>Under Metrics you can see utilization graphs:</p> <p></p> <p>Typical statuses you may see:</p> <ul> <li>ContainerCreating - The docker container is being downloaded from the cloud repository</li> <li>Pending - the Workload is waiting to be scheduled</li> <li>Running - the Workload is running</li> <li>Succeeded - the Workload has ended</li> </ul> <p>A full list of Workload statuses can be found here </p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#describe-workload","title":"Describe Workload","text":"<p>To get additional status on your Workload run:</p> CLI V1CLI V2User Interface <pre><code>runai describe job train1\n</code></pre> <pre><code>runai training describe train1\n</code></pre> <p>Workload parameters can be viewed by adding more columns to the Workload list and by reviewing the <code>Event History</code> tab for the specific Workload. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#view-logs","title":"View Logs","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <pre><code>runai logs train1\n</code></pre> <p>You should see a log of a running container:</p> <p></p> <pre><code>runai training logs train1\n</code></pre> <p>You should see a log of a running container:</p> <p></p> <p>Select the Workload, and press Show Details. Under <code>Logs</code> you can see the logs emitted from the container</p> <p></p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <pre><code>runai delete job train1\n</code></pre> <pre><code>runai training delete train1\n</code></pre> <p>Select the Workload and press DELETE.</p> <p>This would stop the training workload. You can verify this by listing training workloads again.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Quickstart document: Launch Interactive Workloads.</li> <li>Use your container to run an unattended training workload.</li> </ul>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/","title":"Best Practice: From Bare Metal to Docker Images","text":""},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#introduction","title":"Introduction","text":"<p>Some Researchers do data science on bare metal. The term bare-metal relates to connecting to a server and working directly on its operating system and disks.</p> <p>This is the fastest way to start working, but it introduces problems when the data science organization scales:</p> <ul> <li>More Researchers mean that the machine resources need to be efficiently shared</li> <li>Researchers need to collaborate and share data, code, and results</li> </ul> <p>To overcome that, people working on bare-metal typically write scripts to gather data, code as well as code dependencies. This soon becomes an overwhelming task.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#why-use-docker-images","title":"Why Use Docker Images?","text":"<p>Docker images and containerization in general provide a level of abstraction which, by large, frees developers and Researchers from the mundane tasks of setting up an environment. The image is an operating system by itself and thus the 'environment' is by large, a part of the image.</p> <p>When a docker image is instantiated, it creates a container. A container is the running manifestation of a docker image.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#moving-a-data-science-environment-to-docker","title":"Moving a Data Science Environment to Docker","text":"<p>A data science environment typically includes:</p> <li>Training data</li> <li>Machine Learning (ML) code and inputs</li> <li>Libraries: Code dependencies that must be installed before the ML code can be run</li>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#training-data","title":"Training data","text":"<p>Training data is usually significantly large (from several Gigabytes to Petabytes) and is read-only in nature. Thus, training data is typically left outside of the docker image. Instead, the data is mounted onto the image when it is instantiated. Mounting a volume allows the code within the container to access the data as though it was within a directory on the local file system.</p> <p>The best practice is to store the training data on a shared file system. This allows the data to be accessed uniformly on whichever machine the Researcher is currently using, allowing the Researcher to easily migrate between machines. </p> <p>Organizations without a shared file system typically write scripts to copy data from machine to machine.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#machine-learning-code-and-inputs","title":"Machine Learning Code and Inputs","text":"<p>As a rule, code needs to be saved and versioned in a code repository.</p> <p>There are two alternative practices:</p> <ul> <li>The code resides in the image and is being periodically pulled from the repository. This practice requires building a new container image each time a change is introduced to the code.</li> <li>When a shared file system exists, the code can reside outside the image on a shared disk and mounted via a volume onto the container. </li> </ul> <p>Both practices are valid.</p> <p>Inputs to machine learning models and artifacts of training sessions, like model checkpoints, are also better stored in and loaded from a shared file system.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#code-dependencies","title":"Code Dependencies","text":"<p>Any code has code dependencies. These libraries must be installed for the code to run. As the code is changing, so do the dependencies.</p> <p>ML Code is typically python and python dependencies are typically declared together in a single <code>requirements.txt</code> file which is saved together with the code.</p> <p>The best practice is to have your docker startup script (see below) run this file using <code>pip install -r requirements.txt</code>. This allows the flexibility of adding and removing code dependencies dynamically.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#ml-lifecycle-build-and-train","title":"ML Lifecycle: Build and Train","text":"<p>Deep learning workloads can be divided into two generic types:</p> <li>Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter Notebook, remote PyCharm, or similar and accesses GPU resources directly. Build workloads are typically meant for debugging and development sessions. </li> <li>Unattended \"training\" sessions. Training is characterized by a machine learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the data scientist can examine the results. A Training session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU and at the end of the run automatically frees the resources. </li> <p>Getting your docker ready is also a matter of which type of workload you are currently running.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#build-workloads","title":"Build Workloads","text":"<p>With \"build\" you are actually coding and debugging small experiments. You are interactive. In that mode, you can typically take a well known standard image (e.g. https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow) and use it directly.</p> <p>Start a docker container by running:</p> <pre><code>docker run -it .... \"the well known image\" -v /where/my/code/resides bash </code></pre> <p>You get a shell prompt to a container with a mounted volume of where your code is. You can then install your prerequisites and run your code via ssh.</p> <p>You can also access the container remotely from tools such as PyCharm, Jupyter Notebook, and more. In this case, the docker image needs to be customized to install the \"server software\" (e.g. a Jupyter Notebook service).</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#training-workloads","title":"Training Workloads","text":"<p>For training workloads, you can use a well-known image (e.g. the TensorFlow image from the link above) but more often than not, you want to create your own docker image. The best practice is to use the well-known image (e.g. TensorFlow from above) as a base image and add your own customizations on top of it. To achieve that, you create a Dockerfile. A Dockerfile is a declarative way to build a docker image and is built in layers. e.g.:</p> <ol><li>Base image is nvidia-tensorflow</li> <li>Install popular software</li> <li>(Optional) Run a script</li> </ol> <p>The script can be part of the image or can be provided as part of the command line to run the docker. It will typically include additional dependencies to install as well as a reference to the ML code to be run. </p> <p>The best practice for running training workloads is to test the container image in a \"build\" session and then send it for execution as a training Job. For further information on how to set up and parameterize a training workload via docker or Run:ai see Converting your Workload to use Unattended Training Execution.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/","title":"Best Practice: Convert your Workload to Run Unattended","text":""},{"location":"Researcher/best-practices/convert-to-unattended/#motivation","title":"Motivation","text":"<p>Run:ai allows non-interactive training workloads to extend beyond guaranteed quotas and into over-quota as long as computing resources are available. To achieve this kind of flexibility, the system needs to be able to safely stop a workload and restart it again later. This requires Researchers to switch workloads from running interactively, to running unattended, thus allowing Run:ai to pause/resume the run.</p> <p>Unattended workloads are a good fit for long-duration runs, or sets of smaller hyperparameter optimization runs.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#best-practices","title":"Best Practices","text":""},{"location":"Researcher/best-practices/convert-to-unattended/#docker-image","title":"Docker Image","text":"<p>A docker container is based on a docker image. Some Researchers use generic images such as ones provided by Nvidia, for example: NVIDIA NGC TensorFlow. Others, use generic images as the base image to a more customized image using Dockerfiles.</p> <p>Realizing that Researchers are not always proficient with building docker files, as a best practice, you will want to:</p> <ul> <li>Use the same docker image both for interactive and unattended jobs. In this way, you can keep the difference between both methods of invocation to a minimum. This can be a stock image from Nvidia or a custom image.</li> <li>Leave some degree of flexibility, which allows the Researcher to add/remove python dependencies without re-creating images.</li> </ul>"},{"location":"Researcher/best-practices/convert-to-unattended/#code-location","title":"Code Location","text":"<p>You will want to minimize the cycle of code change-and-run. There are a couple of best practices which you can choose from:</p> <ol> <li>Code resides on the network file storage. This way you can change the code and immediately run the Job. The Job picks up the new files from the network.</li> <li>Use the <code>runai submit</code> flag <code>--git-sync</code>. The flag allows the Researcher to provide details of a Git repository. The repository will be automatically cloned into a specified directory when the container starts.</li> <li>The code can be embedded within the image. In this case, you will want to create an automatic CI/CD process, which packages the code into a modified image.</li> </ol> <p>The document below assumes option #1.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#create-a-startup-script","title":"Create a Startup Script","text":"<p>Gather the commands you ran inside the interactive Job into a single script. The script will be provided with the command-line at the start of the unattended execution (see the section running the job below). This script should be kept next to your code, on a shared network drive (e.g. /nfs/john).</p> <p>An example of a common startup script start.sh:</p> <pre><code>pip install -r requirements.txt\n...\npython training.py\n</code></pre> <p>The first line of this script is there to make sure that all required python libraries are installed before the training script executes, it also allows the Researcher to add/remove libraries without needing changes to the image itself.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#support-variance-between-different-runs","title":"Support Variance Between Different Runs","text":"<p>Your training script must be flexible enough to support variance in execution without changing the code. For example, you will want to change the number of epochs to run, apply a different set of hyperparameters, etc. There are two ways to handle this in your script. You can use one or both methods:</p> <ol> <li> <p>Your script can read arguments passed to the script:</p> <p><pre><code>python training.py --number-of-epochs=30</code></pre></p> </li> </ol> <p>In which case, change your start.sh script to:</p> <pre><code>pip install -r requirements.txt\n...\npython training.py $@</code></pre> <ol> <li>Your script can read from environment variables during script execution. In case you use environment variables, the variables will be passed to the training script automatically. No special action is required in this case.</li> </ol>"},{"location":"Researcher/best-practices/convert-to-unattended/#checkpoints","title":"Checkpoints","text":"<p>Run:ai can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:ai will give you back the resources and restore your workload. Thus, it is a good practice to save your weights at various checkpoints and start a workload from the latest checkpoint (typically between epochs).</p> <p>TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for PyTorch).</p> <p>It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node</p> <p>For more information on best practices for saving checkpoints, see Saving Deep Learning Checkpoints.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#running-the-job","title":"Running the Job","text":"<p>Using <code>runai submit</code>, drop the flag <code>--interactive</code>. For submitting a Job using the script created above, please use <code>-- [COMMAND]</code> flag to specify a command, use the <code>--</code> syntax to pass arguments, and pass environment variables using the flag <code>--environment</code>.</p> <p>Example with Environment variables:</p> <pre><code>runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3  \n    -v /nfs/john:/mydir -g 1  --working-dir /mydir/  \n    -e 'EPOCHS=30'  -e 'LEARNING_RATE=0.02'  \n    -- ./startup.sh  \n</code></pre> <p>Example with Command-line arguments:</p> <pre><code>runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3  \n    -v /nfs/john:/mydir -g 1  --working-dir /mydir/  \n    -- ./startup.sh batch-size=64 number-of-epochs=3\n</code></pre> <p>Please refer to Command-Line Interface, runai submit for a list of all arguments accepted by the Run:ai CLI.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#use-cli-policies","title":"Use CLI Policies","text":"<p>Different run configurations may vary significantly and can be tedious to be written each time on the command-line. To make life easier, our CLI offers a way to set administrator policies for these configurations and use pre-configured configuration when submitting a Workload. Please refer to Configure Policies.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#attached-files","title":"Attached Files","text":"<p>The 3 relevant files mentioned in this document can be downloaded from Github</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#see-also","title":"See Also","text":"<p>See the unattended training Quickstart: Launch Unattended Training Workloads</p>"},{"location":"Researcher/best-practices/env-variables/","title":"Environment Variables inside a Run:ai Workload","text":""},{"location":"Researcher/best-practices/env-variables/#identifying-a-job","title":"Identifying a Job","text":"<p>There may be use cases where your container may need to uniquely identify the Job it is currently running in. A typical use case is for saving Job artifacts under a unique name.  Run:ai provides pre-defined environment variables you can use. These variables are guaranteed to be unique even if the Job is preempted or evicted and then runs again. </p> <p>Run:ai provides the following environment variables:</p> <ul> <li><code>JOB_NAME</code> - the name of the Job.</li> <li><code>JOB_UUID</code> - a unique identifier for the Job. </li> </ul> <p>Note that the Job can be deleted and then recreated with the same name. A Job UUID will be different even if the Job names are the same.</p>"},{"location":"Researcher/best-practices/env-variables/#gpu-allocation","title":"GPU Allocation","text":"<p>Run:ai provides an environment variable, visible inside the container, to help identify the number of GPUs allocated for the container. Use <code>RUNAI_NUM_OF_GPUS</code></p>"},{"location":"Researcher/best-practices/env-variables/#node-name","title":"Node Name","text":"<p>There may be use cases where your container may need to identify the node it is currently running on. Run:ai provides an environment variable, visible inside the container, to help identify the name of the node on which the pod was scheduled. Use <code>NODE_NAME</code></p>"},{"location":"Researcher/best-practices/env-variables/#usage-example-in-python","title":"Usage Example in Python","text":"<pre><code>import os\n\njobName = os.environ['JOB_NAME']\njobUUID = os.environ['JOB_UUID']\n</code></pre>"},{"location":"Researcher/best-practices/researcher-notifications/","title":"Researcher Email Notifications","text":""},{"location":"Researcher/best-practices/researcher-notifications/#importance-of-email-notifications-for-data-scientists","title":"Importance of Email Notifications for Data Scientists","text":"<p>Managing numerous data science workloads requires monitoring various stages, including submission, scheduling, initialization, execution, and completion. Additionally, handling suspensions and failures is crucial for ensuring timely workload completion. Email Notifications address this need by sending alerts for critical workload life cycle changes. This empowers data scientists to take necessary actions and prevent delays.</p> <p>Once the system administrator configures the email notifications, users will receive notifications about their jobs that transition from one status to another. In addition, the user will get warning notifications before workload termination due to project-defined timeouts. Details included in the email are:</p> <ul> <li>Workload type</li> <li>Project and cluster information</li> <li>Event timestamp</li> </ul> <p>To configure the types of email notifications you can receive:</p> <ol> <li>The user must log in to their account.</li> <li>Press the user icon, then select settings.</li> <li>In the Email notifications, and in the Send me an email about my workloads when section, select the relevant workload statuses.</li> <li>When complete, press Save.</li> </ol>"},{"location":"Researcher/best-practices/save-dl-checkpoints/","title":"Best Practice: Save Deep-Learning Checkpoints","text":""},{"location":"Researcher/best-practices/save-dl-checkpoints/#introduction","title":"Introduction","text":"<p>Run:ai can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:ai will give you back the resources and restore your workload. Thus, it is a good practice to save the state of your run at various checkpoints and start a workload from the latest checkpoint (typically between epochs).</p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#how-to-save-checkpoints","title":"How to Save Checkpoints","text":"<p>TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for PyTorch).</p> <p>This document uses Keras as an example. The code itself can be found here</p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#where-to-save-checkpoints","title":"Where to Save Checkpoints","text":"<p>It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node. Example:</p> <pre><code>runai submit train-with-checkpoints -i tensorflow/tensorflow:1.14.0-gpu-py3 \\\n  -v /mnt/nfs_share/john:/mydir -g 1  --working-dir /mydir --command -- ./startup.sh\n</code></pre> <p>The command saves the checkpoints in an NFS checkpoints folder <code>/mnt/nfs_share/john</code></p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#when-to-save-checkpoints","title":"When to Save Checkpoints","text":""},{"location":"Researcher/best-practices/save-dl-checkpoints/#save-periodically","title":"Save Periodically","text":"<p>It is a best practice to save checkpoints at intervals. For example, every epoch as the Keras code below shows:</p> <pre><code>checkpoints_file = \"weights.best.hdf5\"\ncheckpoint = ModelCheckpoint(checkpoints_file, monitor='val_acc', verbose=1, \n    save_best_only=True, mode='max')\n</code></pre>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#save-on-exit-signal","title":"Save on Exit Signal","text":"<p>If periodic checkpoints are not enough, you can use a signal-hook provided by Run:ai (via Kubernetes). The hook is python code that is called before your Job is suspended and allows you to save your checkpoints as well as other state data you may wish to store.</p> <pre><code>import signal\nimport time\n\ndef graceful_exit_handler(signum, frame):\n    # save your checkpoints to shared storage\n\n    # exit with status \"1\" is important for the Job to return later.  \n    exit(1)\n\nsignal.signal(signal.SIGTERM, graceful_exit_handler)\n</code></pre> <p>By default, you will have 30 seconds to save your checkpoints.</p> <p>Important</p> <p>For the signal to be captured, it must be propagated from the startup script to the python child process. See code here</p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#resuming-using-saved-checkpoints","title":"Resuming using Saved Checkpoints","text":"<p>A Run:ai unattended workload that is resumed, will run the same startup script as on the first run. It is the responsibility of the script developer to add code that:</p> <ul> <li>Checks if saved checkpoints exist (see above)</li> <li>If saved checkpoints exist, load them and start the run using these checkpoints</li> </ul> <pre><code>import os\n\ncheckpoints_file = \"weights.best.hdf5\"\nif os.path.isfile(checkpoints_file):\n    print(\"loading checkpoint file: \" + checkpoints_file)\n    model.load_weights(checkpoints_file)\n</code></pre>"},{"location":"Researcher/cli-reference/Introduction/","title":"Introduction","text":"<p>The Run:ai Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc.</p> <p>To install and configure the Run:ai CLI see Researcher Setup - Start Here</p>"},{"location":"Researcher/cli-reference/runai-attach/","title":"runai attach","text":""},{"location":"Researcher/cli-reference/runai-attach/#description","title":"Description","text":"<p>Attach to a running Job.</p> <p>The command attaches to the standard input, output, and error streams of a running Job. If the Job has multiple pods the job will attach to the first pod unless otherwise set.</p>"},{"location":"Researcher/cli-reference/runai-attach/#synopsis","title":"Synopsis","text":"<pre><code>runai attach &lt;job-name&gt;\n    [--no-stdin ]\n    [--no-tty]   \n    [--pod string]\n    .\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-attach/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-attach/#-no-stdin","title":"--no-stdin","text":"<p>Do not attach STDIN.</p>"},{"location":"Researcher/cli-reference/runai-attach/#-no-tty","title":"--no-tty","text":"<p>Do not allocate a pseudo-TTY</p>"},{"location":"Researcher/cli-reference/runai-attach/#-pod-string","title":"--pod string","text":"<p>Attach to a specific pod within the Job. To find the list of pods run <code>runai describe job &lt;job-name&gt;</code> and then use the pod name with the <code>--pod</code> flag.</p>"},{"location":"Researcher/cli-reference/runai-attach/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-attach/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-attach/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-attach/#output","title":"Output","text":"<p>None</p>"},{"location":"Researcher/cli-reference/runai-bash/","title":"runai bash","text":""},{"location":"Researcher/cli-reference/runai-bash/#description","title":"Description","text":"<p>Get a bash session inside a running Job</p> <p>This command is a shortcut to runai exec (<code>runai exec -it job-name bash</code>). See runai exec for full documentation of the exec command.</p>"},{"location":"Researcher/cli-reference/runai-bash/#synopsis","title":"Synopsis","text":"<pre><code>runai bash &lt;job-name&gt; \n    [--pod string]\n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-bash/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-bash/#-pod-string","title":"--pod string","text":"<p>Specify a pod of a running Job. To get a list of the pods of a specific Job, run <code>runai describe job &lt;job-name&gt;</code> command</p>"},{"location":"Researcher/cli-reference/runai-bash/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-bash/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>"},{"location":"Researcher/cli-reference/runai-bash/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-bash/#-help-h","title":"--help | -h","text":"<p>Show help text</p>"},{"location":"Researcher/cli-reference/runai-bash/#output","title":"Output","text":"<p>The command will access the container that should be currently running in the current cluster and attempt to create a command-line shell based on bash.</p> <p>The command will return an error if the container does not exist or has not been in a running state yet.</p>"},{"location":"Researcher/cli-reference/runai-bash/#see-also","title":"See also","text":"<p>Build Workloads. See Quickstart document: Launch Interactive Build Workloads.</p>"},{"location":"Researcher/cli-reference/runai-config/","title":"runai config","text":""},{"location":"Researcher/cli-reference/runai-config/#description","title":"Description","text":"<p>Set a default Project or Cluster</p>"},{"location":"Researcher/cli-reference/runai-config/#synopsis","title":"Synopsis","text":"<pre><code>runai  config project &lt;project-name&gt;\n    [--loglevel value] \n    [--help | -h]\n\nrunai  config cluster &lt;cluster-name&gt;\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-config/#options","title":"Options","text":"<p>&lt;project-name&gt;  - The name of the Project you want to set as default. Mandatory.</p> <p>&lt;cluster-name&gt; - The name of the cluster you want to set as the current cluster. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-config/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-config/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-config/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-config/#output","title":"Output","text":"<p>None</p>"},{"location":"Researcher/cli-reference/runai-delete/","title":"runai delete","text":""},{"location":"Researcher/cli-reference/runai-delete/#description","title":"Description","text":"<p>Delete a Workload and its associated Pods.</p> <p>Note that once you delete a Workload, its entire data will be gone:</p> <ul> <li>You will no longer be able to enter it via bash.</li> <li>You will no longer be able to access logs.</li> <li>Any data saved on the container and not stored in a shared location will be lost.</li> </ul>"},{"location":"Researcher/cli-reference/runai-delete/#synopsis","title":"Synopsis","text":"<pre><code>runai delete job &lt;job-name&gt; \n    [--all | -A]\n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-delete/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Workload to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-delete/#-all-a","title":"--all | -A","text":"<p>Delete all Workloads.</p>"},{"location":"Researcher/cli-reference/runai-delete/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-delete/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-delete/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-delete/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-delete/#output","title":"Output","text":"<ul> <li> <p>The Workload will be deleted and not available via the command runai list jobs.</p> </li> <li> <p>The Workloads will show as <code>deleted</code> from the Run:ai user interface Job list.</p> </li> </ul>"},{"location":"Researcher/cli-reference/runai-delete/#see-also","title":"See Also","text":"<ul> <li> <p>Build Workloads. See Quickstart document: Launch Interactive Build Workloads.</p> </li> <li> <p>Training Workloads. See Quickstart document:  Launch Unattended Training Workloads.</p> </li> </ul>"},{"location":"Researcher/cli-reference/runai-describe/","title":"runai describe","text":""},{"location":"Researcher/cli-reference/runai-describe/#description","title":"Description","text":"<p>Display details of a Workload or Node.</p>"},{"location":"Researcher/cli-reference/runai-describe/#synopsis","title":"Synopsis","text":"<pre><code>runai describe job &lt;job-name&gt; \n    [--output value | -o value]  \n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n    [--output string | -o string]  \n\n\nrunai describe node [node-name] \n\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-describe/#options","title":"Options","text":"<ul> <li>&lt;job-name&gt; - The name of the Workload to run the command with. Mandatory.</li> <li>&lt;node-name&gt; - The name of the Node to run the command with. If a Node name is not specified, a description of all Nodes is shown.</li> </ul> <p>-o | --output</p> <p>Output format. One of: json|yaml|wide. Default is 'wide'</p>"},{"location":"Researcher/cli-reference/runai-describe/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-describe/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-describe/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project, use: <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-describe/#-help-h","title":"--help | -h","text":"<p>Show help text</p>"},{"location":"Researcher/cli-reference/runai-describe/#output","title":"Output","text":"<ul> <li>The <code>runai describe job</code> command will show Workload properties and status as well as lifecycle events and the list of related resources and pods.</li> <li>The <code>runai describe node</code> command will show Node properties. </li> </ul>"},{"location":"Researcher/cli-reference/runai-exec/","title":"runai exec","text":""},{"location":"Researcher/cli-reference/runai-exec/#description","title":"Description","text":"<p>Execute a command inside a running Job</p> <p>Note: to execute a bash command, you can also use the shorthand runai bash</p>"},{"location":"Researcher/cli-reference/runai-exec/#synopsis","title":"Synopsis","text":"<pre><code>runai exec &lt;job-name&gt; &lt;command&gt; \n    [--stdin | -i] \n    [--tty | -t]\n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-exec/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p> <p>&lt;command&gt; the command itself (e.g. bash).</p>"},{"location":"Researcher/cli-reference/runai-exec/#-stdin-i","title":"--stdin | -i","text":"<p>Keep STDIN open even if not attached.</p>"},{"location":"Researcher/cli-reference/runai-exec/#-tty-t","title":"--tty | -t","text":"<p>Allocate a pseudo-TTY.</p>"},{"location":"Researcher/cli-reference/runai-exec/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-exec/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-exec/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-exec/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-exec/#output","title":"Output","text":"<p>The command will run in the context of the container.</p>"},{"location":"Researcher/cli-reference/runai-exec/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-list/","title":"runai list","text":""},{"location":"Researcher/cli-reference/runai-list/#description","title":"Description","text":"<p>Show lists of Workloads, Projects, Clusters or Nodes.</p>"},{"location":"Researcher/cli-reference/runai-list/#synopsis","title":"Synopsis","text":"<pre><code>runai list jobs \n    [--all-projects | -A]  \n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n\nrunai list projects \n    [--loglevel value] \n    [--help | -h]\n\nrunai list clusters  \n    [--loglevel value] \n    [--help | -h]\n\nrunai list nodes [node-name]\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-list/#options","title":"Options","text":"<p><code>node-name</code> - Name of a specific node to list (optional).</p>"},{"location":"Researcher/cli-reference/runai-list/#-all-projects-a","title":"--all-projects | -A","text":"<p>Show Workloads from all Projects.</p>"},{"location":"Researcher/cli-reference/runai-list/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-list/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-list/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-list/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-list/#output","title":"Output","text":"<ul> <li>A list of Workloads, Nodes, Projects, or Clusters. </li> <li>To filter 'runai list nodes' for a specific Node, add the Node name.</li> </ul>"},{"location":"Researcher/cli-reference/runai-list/#see-also","title":"See Also","text":"<p>To show details for a specific Workload or Node see runai describe.</p>"},{"location":"Researcher/cli-reference/runai-login/","title":"runai login","text":""},{"location":"Researcher/cli-reference/runai-login/#description","title":"Description","text":"<p>Login to Run:ai</p> <p>When Researcher Authentication is enabled, you will need to login to Run:ai using your username and password before accessing resources </p>"},{"location":"Researcher/cli-reference/runai-login/#synopsis","title":"Synopsis","text":"<pre><code>runai login \n    [--loglevel value]\n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-login/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-login/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-login/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-login/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-login/#output","title":"Output","text":"<p>You will be prompted for a user name and password</p>"},{"location":"Researcher/cli-reference/runai-login/#see-also","title":"See Also","text":"<ul> <li>runai logout.</li> </ul>"},{"location":"Researcher/cli-reference/runai-logout/","title":"runai logout","text":""},{"location":"Researcher/cli-reference/runai-logout/#description","title":"Description","text":"<p>Log out from Run:ai</p>"},{"location":"Researcher/cli-reference/runai-logout/#synopsis","title":"Synopsis","text":"<pre><code>runai logout \n    [--loglevel value]\n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-logout/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-logout/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-logout/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-logout/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-logout/#output","title":"Output","text":"<p>You will be logged out from Run:ai</p>"},{"location":"Researcher/cli-reference/runai-logout/#see-also","title":"See Also","text":"<ul> <li>runai login.</li> </ul>"},{"location":"Researcher/cli-reference/runai-logs/","title":"runai logs","text":""},{"location":"Researcher/cli-reference/runai-logs/#description","title":"Description","text":"<p>Show the logs of a Job.</p>"},{"location":"Researcher/cli-reference/runai-logs/#synopsis","title":"Synopsis","text":"<pre><code>runai logs &lt;job-name&gt; \n    [--follow | -f] \n    [--pod string | -p string] \n    [--since duration] \n    [--since-time date-time] \n    [--tail int | -t int] \n    [--timestamps]  \n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-logs/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-follow-f","title":"--follow | -f","text":"<p>Stream the logs.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-pod-p","title":"--pod | -p","text":"<p>Specify a specific pod name. When a Job fails, it may start a couple of times in an attempt to succeed. The flag allows you to see the logs of a specific instance (called 'pod'). Get the name of the pod by running <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-instance-string-i-string","title":"--instance (string) | -i (string)","text":"<p>Show logs for a specific instance in cases where a Job contains multiple pods.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-since-duration","title":"--since (duration)","text":"<p>Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. The flags since and since-time cannot be used together.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-since-time-date-time","title":"--since-time (date-time)","text":"<p>Return logs after specified date. Date format should be RFC3339, example: <code>2020-01-26T15:00:00Z</code>.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-tail-int-t-int","title":"--tail (int) | -t (int)","text":"<p># of lines of recent log file to display.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-timestamps","title":"--timestamps","text":"<p>Include timestamps on each line in the log output.</p>"},{"location":"Researcher/cli-reference/runai-logs/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-logs/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-logs/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-logs/#output","title":"Output","text":"<p>The command will show the logs of the first process in the container. For training Jobs, this would be the command run at startup. For interactive Jobs, the command may not show anything.</p>"},{"location":"Researcher/cli-reference/runai-logs/#see-also","title":"See Also","text":"<ul> <li>Training Workloads. See Quickstart document:  Launch Unattended Training Workloads.</li> </ul>"},{"location":"Researcher/cli-reference/runai-port-forwarding/","title":"runai port-forward","text":""},{"location":"Researcher/cli-reference/runai-port-forwarding/#description","title":"Description","text":"<p>Forward one or more local ports to the selected job or a pod within the job. The forwarding session ends when the selected job terminates or the terminal is interrupted.</p>"},{"location":"Researcher/cli-reference/runai-port-forwarding/#examples","title":"Examples","text":"<ol> <li> <p>Port forward connections from localhost:8080 (localhost is the default) to  on port 8090. <p><code>runai port-forward &lt;job-name&gt; --port 8080:8090</code></p> <li> <p>Port forward connections from 192.168.1.23:8080 to  on port 8080. <p><code>runai port-forward &lt;job-name&gt; --port 8080 --address 192.168.1.23</code></p> <li> <p>Port forward multiple connections from localhost:8080 to  on port 8090 and localhost:6443 to  on port 443. <p><code>runai port-forward &lt;job-name&gt; --port 8080:8090  --port 6443:443</code></p> <li> <p>Port forward into a specific pod in a multi-pod job.</p> <p><code>runai port-forward &lt;job-name&gt; --port 8080:8090 --pod &lt;pod-name&gt;</code></p> </li>"},{"location":"Researcher/cli-reference/runai-port-forwarding/#global-flags","title":"Global flags","text":"<p><code>--loglevel &lt;string&gt;</code>\u2014Set the logging level. Choose:  (default \"info\"). <p><code>-p | --project &lt;string&gt;</code>\u2014Specify the project name. To change the default project use <code>runai config project &lt;project name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-port-forwarding/#flags","title":"Flags","text":"<p><code>--address &lt;string&gt; | [local-interface-ip\\host] |localhost | 0.0.0.0 [privileged]</code>\u2014The listening address of your local machine. (default \"localhost\").</p> <p><code>-h | --help</code>\u2014Help for the command.</p> <p><code>--port</code>\u2014forward ports based on one of the following arguments:</p> <ul> <li> <p><code>&lt;stringArray&gt;</code>\u2014a list of port forwarding combinations.</p> </li> <li> <p><code>[local-port]:[remote-port]</code>\u2014different local and remote ports.</p> </li> <li> <p><code>[local-port=remote-port]</code>\u2014the same port is used for both local and remote.</p> </li> </ul> <p><code>--pod</code>\u2014Specify a pod of a running job. To get a list of the pods of a specific job, run the command <code>runai describe &lt;job-name&gt;</code>.</p> <p><code>--pod-running-timeout</code>\u2014The length of time (like 5s, 2m, or 3h, higher than zero) to wait until the pod is running. Default is 10 minutes.</p> <p>Filter based flags</p> <p><code>--mpi</code>\u2014search only for mpi jobs.</p> <p><code>--interactive</code>\u2014search only for interactive jobs.</p> <p><code>--pytorch</code>\u2014search only for pytorch jobs.</p> <p><code>--tf</code>\u2014search only for tensorflow jobs.</p> <p><code>--train</code>\u2014search only for training jobs.</p>"},{"location":"Researcher/cli-reference/runai-resume/","title":"runai resume","text":""},{"location":"Researcher/cli-reference/runai-resume/#description","title":"Description","text":"<p>Resume a suspended Job</p> <p>Resuming a previously suspended Job will return it to the queue for scheduling. The Job may or may not start immediately, depending on available resources. </p> <p>Suspend and resume do not work with mpi Jobs. </p>"},{"location":"Researcher/cli-reference/runai-resume/#synopsis","title":"Synopsis","text":"<pre><code>runai resume &lt;job-name&gt;\n    [--all | -A]\n\n    [--loglevel value]\n    [--project string | -p string]\n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-resume/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-resume/#-all-a","title":"--all | -A","text":"<p>Resume all suspended Jobs in the current Project.</p>"},{"location":"Researcher/cli-reference/runai-resume/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-resume/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-resume/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-resume/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-resume/#output","title":"Output","text":"<ul> <li>The Job will be resumed. When running runai list jobs the Job status will no longer by Suspended.</li> </ul>"},{"location":"Researcher/cli-reference/runai-resume/#see-also","title":"See Also","text":"<ul> <li>Suspending Jobs: Suspend.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/","title":"runai submit-dist tf","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#description","title":"Description","text":"<p>Submit a distributed TensorFlow training run:ai job to run.</p> <p>Note</p> <p>To use distributed training you need to have installed the TensorFlow operator as specified in Distributed training.</p> <p>Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#examples","title":"Examples","text":"<pre><code>runai submit-dist tf --name distributed-job --workers=2 -g 1 \\\n    -i &lt;image_name\n&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#distributed","title":"Distributed","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-clean-pod-policy-string","title":"--clean-pod-policy &lt; string &gt;","text":"<p>The\u00a0CleanPodPolicy\u00a0controls deletion of pods when a job terminates. The policy can be one of the following values:</p> <ul> <li>Running\u2014only pods still running when a job completes (for example, parameter servers) will be deleted immediately. Completed pods will not be deleted so that the logs will be preserved. (Default)</li> <li>All\u2014all (including completed) pods will be deleted immediately when the job finishes.</li> <li>None\u2014no pods will be deleted when the job completes.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-workers-int","title":"--workers &lt; int&gt;","text":"<p>Number of replicas for Inference jobs</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-args-string-string","title":"--master-args string <code>&lt;string&gt;</code>  <p>Arguments to pass to the master pod container command. If used together with <code>--command</code>, overrides the image's entrypoint of the master pod container with the given command.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-environment-stringarray","title":"--master-environment <code>&lt;stringArray&gt;</code>  <p>Set environment variables in the master pod container. To prevent from a worker environment variable from being set in the master, use the format: <code>name=-</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-extended-resource-stringarray","title":"--master-extended-resource <code>&lt;stringArray&gt;</code>  <p>Request access to an extended resource in the master pod. Use the format: <code>resource_name=quantity</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-gpu-float","title":"--master-gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-no-pvcs","title":"--master-no-pvcs  <p>Do not mount any persistent volumes in the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-no-master","title":"--no-master  <p>Do not create a separate pod for the master.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-mig-profile-string","title":"--mig-profile <code>&lt;string&gt;</code>  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-ttl-after-finish-duration","title":"--ttl-after-finish &lt; duration &gt;  <p>The duration, after which a finished job is automatically deleted (e.g. 5s, 2m, 3h).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#output","title":"Output","text":"<p>The command will attempt to submit an mpi Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#see-also","title":"See Also","text":"<ul> <li>See Quickstart document Running Distributed Training.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/","title":"runai submit-dist mpi","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#description","title":"Description","text":"<p>Submit a Distributed Training (MPI) Run:ai Job to run.</p> <p>Note</p> <p>To use distributed training you need to have installed the Kubeflow MPI Operator as specified in Distributed training.</p> <p>Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#examples","title":"Examples","text":"<p>You can start an unattended mpi training Job of name dist1, based on Project team-a using a quickstart-distributed image:</p> <pre><code>runai submit-dist mpi --name dist1 --workers=2 -g 1 \\\n    -i runai.jfrog.io/demo/quickstart-distributed:v0.3.0 -e RUNAI_SLEEP_SECS=60\n</code></pre> <p>(see: distributed training Quickstart).</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#distributed","title":"Distributed","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-clean-pod-policy-string","title":"--clean-pod-policy &lt; string &gt;","text":"<p>The\u00a0CleanPodPolicy\u00a0controls deletion of pods when a job terminates. The policy can be one of the following values:</p> <ul> <li>Running\u2014only pods still running when a job completes (for example, parameter servers) will be deleted immediately. Completed pods will not be deleted so that the logs will be preserved. (Default)</li> <li>All\u2014all (including completed) pods will be deleted immediately when the job finishes.</li> <li>None\u2014no pods will be deleted when the job completes.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-workers-int","title":"--workers &lt; int &gt;","text":"<p>Number of replicas for Inference jobs.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-slots-per-worker-int","title":"--slots-per-worker &lt; int &gt;","text":"<p>Number of slots to allocate for each worker.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-args-string-string","title":"--master-args string <code>&lt;string&gt;</code>  <p>Arguments to pass to the master pod container command. If used together with <code>--command</code>, overrides the image's entrypoint of the master pod container with the given command.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-environment-stringarray","title":"--master-environment <code>&lt;stringArray&gt;</code>  <p>Set environment variables in the master pod container. To prevent from a worker environment variable from being set in the master, use the format: <code>name=-</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-extended-resource-stringarray","title":"--master-extended-resource <code>&lt;stringArray&gt;</code>  <p>Request access to an extended resource in the master pod. Use the format: <code>resource_name=quantity</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-gpu-float","title":"--master-gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-no-pvcs","title":"--master-no-pvcs  <p>Do not mount any persistent volumes in the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-mig-profile-string","title":"--mig-profile <code>&lt;string&gt;</code>  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-ttl-after-finish-duration","title":"--ttl-after-finish &lt; duration &gt;  <p>The duration, after which a finished job is automatically deleted (e.g. 5s, 2m, 3h).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#output","title":"Output","text":"<p>The command will attempt to submit an mpi Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#see-also","title":"See Also","text":"<ul> <li>See Quickstart document Running Distributed Training.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/","title":"runai submit-dist pytorch","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#description","title":"Description","text":"<p>Submit a distributed PyTorch training run:ai job to run.</p> <p>Note</p> <p>To use distributed training you need to have installed the Pytorch operator as specified in Distributed training.</p> <p>Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#examples","title":"Examples","text":"<pre><code>runai submit-dist pytorch --name distributed-job --workers=2 -g 1 \\\n    -i &lt;image_name&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#distributed","title":"Distributed","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-clean-pod-policy-string","title":"--clean-pod-policy &lt; string &gt;","text":"<p>The\u00a0CleanPodPolicy\u00a0controls deletion of pods when a job terminates. The policy can be one of the following values:</p> <ul> <li>Running\u2014only pods still running when a job completes (for example, parameter servers) will be deleted immediately. Completed pods will not be deleted so that the logs will be preserved. (Default)</li> <li>All\u2014all (including completed) pods will be deleted immediately when the job finishes.</li> <li>None\u2014no pods will be deleted when the job completes.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-max-replicas-int","title":"--max-replicas &lt; int &gt;","text":"<p>Maximum number of replicas for elastic PyTorch job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-min-replicas-int","title":"--min-replicas &lt; int &gt;","text":"<p>Minimum number of replicas for elastic PyTorch job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-workers-int","title":"--workers &lt; int&gt;","text":"<p>Number of replicas for Inference jobs</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-args-string-string","title":"--master-args string <code>&lt;string&gt;</code>  <p>Arguments to pass to the master pod container command. If used together with <code>--command</code>, overrides the image's entrypoint of the master pod container with the given command.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-environment-stringarray","title":"--master-environment <code>&lt;stringArray&gt;</code>  <p>Set environment variables in the master pod container. To prevent from a worker environment variable from being set in the master, use the format: <code>name=-</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-extended-resource-stringarray","title":"--master-extended-resource <code>&lt;stringArray&gt;</code>  <p>Request access to an extended resource in the master pod. Use the format: <code>resource_name=quantity</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-gpu-float","title":"--master-gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-no-pvcs","title":"--master-no-pvcs  <p>Do not mount any persistent volumes in the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-no-master","title":"--no-master  <p>Do not create a separate pod for the master.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-mig-profile-string","title":"--mig-profile <code>&lt;string&gt;</code>  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-ttl-after-finish-duration","title":"--ttl-after-finish &lt; duration &gt;  <p>The duration, after which a finished job is automatically deleted (e.g. 5s, 2m, 3h).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#output","title":"Output","text":"<p>The command will attempt to submit a distributed pytorch workload. You can follow up on the workload by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/","title":"runai submit-dist xgboost","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#description","title":"Description","text":"<p>Submit a distributed XGBoost training run:ai job to run.</p> <p>Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#examples","title":"Examples","text":"<pre><code>runai submit-dist xgboost --name distributed-job --workers=2 -g 1 \\\n    -i &lt;image_name\n&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#distributed","title":"Distributed","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-clean-pod-policy-string","title":"--clean-pod-policy &lt; string &gt;","text":"<p>The\u00a0CleanPodPolicy\u00a0controls deletion of pods when a job terminates. The policy can be one of the following values:</p> <ul> <li>Running\u2014only pods still running when a job completes (for example, parameter servers) will be deleted immediately. Completed pods will not be deleted so that the logs will be preserved. (Default)</li> <li>All\u2014all (including completed) pods will be deleted immediately when the job finishes.</li> <li>None\u2014no pods will be deleted when the job completes.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-workers-int","title":"--workers &lt; int&gt;","text":"<p>Number of replicas for Inference jobs</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-args-string-string","title":"--master-args string <code>&lt;string&gt;</code>  <p>Arguments to pass to the master pod container command. If used together with <code>--command</code>, overrides the image's entrypoint of the master pod container with the given command.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-environment-stringarray","title":"--master-environment <code>&lt;stringArray&gt;</code>  <p>Set environment variables in the master pod container. To prevent from a worker environment variable from being set in the master, use the format: <code>name=-</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-extended-resource-stringarray","title":"--master-extended-resource <code>&lt;stringArray&gt;</code>  <p>Request access to an extended resource in the master pod. Use the format: <code>resource_name=quantity</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-gpu-float","title":"--master-gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-no-pvcs","title":"--master-no-pvcs  <p>Do not mount any persistent volumes in the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-mig-profile-string","title":"--mig-profile <code>&lt;string&gt;</code>  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-ttl-after-finish-duration","title":"--ttl-after-finish &lt; duration &gt;  <p>The duration, after which a finished job is automatically deleted (e.g. 5s, 2m, 3h).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#output","title":"Output","text":"<p>The command will attempt to submit an mpi Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#see-also","title":"See Also","text":"<ul> <li>See Quickstart document Running Distributed Training.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit/","title":"Description","text":"<p>Submit a Run:ai Job for execution.</p> <p>Syntax notes:</p> <ul> <li>Flags of type stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit/#examples","title":"Examples","text":"<p>All examples assume a Run:ai Project has been setup using <code>runai config project &lt;project-name&gt;</code>.</p> <p>Start an interactive Job:</p> <pre><code>runai submit -i ubuntu --interactive --attach -g 1\n</code></pre> <p>Or</p> <pre><code>runai submit --name build1 -i ubuntu -g 1 --interactive -- sleep infinity \n</code></pre> <p>(see: build Quickstart).</p> <p>Externalize ports:</p> <pre><code>runai submit --name build-remote -i rastasheep/ubuntu-sshd:14.04 --interactive \\\n   --service-type=nodeport --port 30022:22\n   -- /usr/sbin/sshd -D\n</code></pre> <p>(see: build with ports Quickstart).</p> <p>Start a Training Job</p> <pre><code>runai submit --name train1 -i runai.jfrog.io/demo/quickstart -g 1 \n</code></pre> <p>(see: training Quickstart).</p> <p>Use GPU Fractions</p> <pre><code>runai submit --name frac05 -i runai.jfrog.io/demo/quickstart -g 0.5\n</code></pre> <p>(see: GPU fractions Quickstart).</p> <p>Submit a Job without a name (automatically generates a name)</p> <pre><code>runai submit -i runai.jfrog.io/demo/quickstart -g 1 \n</code></pre> <p>Submit a job using the system autogenerated name to an external URL:</p> <pre><code>runai submit -i ubuntu --interactive --attach -g 1 service-type=external-url --port 3745 --custom-url=&lt;destination_url&gt;\n</code></pre> <p>Submit a job without a name to a system generated a URL :</p> <pre><code>runai submit -i ubuntu --interactive --attach -g 1 service-type=external-url --port 3745\n</code></pre> <p>Submit a Job without a name with a pre-defined prefix and an incremental index suffix</p> <pre><code>runai submit --job-name-prefix -i runai.jfrog.io/demo/quickstart -g 1 \n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit/#job-type","title":"Job Type","text":""},{"location":"Researcher/cli-reference/runai-submit/#-interactive","title":"--interactive","text":"<p>Mark this Job as interactive.</p>"},{"location":"Researcher/cli-reference/runai-submit/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit/#-completions-int","title":"--completions &lt; int &gt;","text":"<p>Number of successful pods required for this job to be completed. Used with HPO.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-parallelism-int","title":"--parallelism &lt; int &gt;","text":"<p>Number of pods to run in parallel at any given time.  Used with HPO.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-preemptible","title":"--preemptible","text":"<p>Interactive preemptible jobs can be scheduled above guaranteed quota but may be reclaimed at any time.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-auto-deletion-time-after-completion","title":"--auto-deletion-time-after-completion","text":"<p>The timeframe after which a completed or failed job is automatically deleted. Configured in seconds, minutes, or hours (for example 5s, 2m, or 3h). If set to 0, the job will be deleted immediately after completing or failing.</p>"},{"location":"Researcher/cli-reference/runai-submit/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-e-stringarray-environment-stringarray","title":"-e <code>&lt;stringArray&gt;</code> | --environment <code>&lt;stringArray&gt;</code>","text":"<p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>"},{"location":"Researcher/cli-reference/runai-submit/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>","text":"<p>Image to use when creating the container for this Job</p>"},{"location":"Researcher/cli-reference/runai-submit/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>","text":"<p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>","text":"<p>Set labels variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>","text":"<p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>","text":"<p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-stdin","title":"--stdin","text":"<p>Keep stdin open for the container(s) in the pod, even if nothing is attached.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-t-tty","title":"-t | --tty","text":"<p>Allocate a pseudo-TTY.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>","text":"<p>Starts the container with the specified directory as the current directory.</p>"},{"location":"Researcher/cli-reference/runai-submit/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>","text":"<p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>","text":"<p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-extended-resource-stringarray","title":"--extended-resource <code>&lt;stringArray&gt;</code>","text":"<p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>","text":"<p>GPU units to allocate for the Job (0.5, 1).</p>"},{"location":"Researcher/cli-reference/runai-submit/#-gpu-memory","title":"--gpu-memory","text":"<p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-memory-string","title":"--memory <code>&lt;string&gt;</code>","text":"<p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-memory-limit-string","title":"--memory-limit <code>&lt;string&gt;</code>","text":"<p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-mig-profile-string","title":"--mig-profile <code>&lt;string&gt;</code>","text":"<p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>"},{"location":"Researcher/cli-reference/runai-submit/#job-lifecycle_1","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>","text":"<p>The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the <code>--interactive</code> flag is not specified).</p>"},{"location":"Researcher/cli-reference/runai-submit/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>","text":"<p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-large-shm","title":"--large-shm","text":"<p>Mount a large /dev/shm device.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-mount-propagation","title":"--mount-propagation","text":"<p>Enable HostToContainer mount propagation for all container volumes</p>"},{"location":"Researcher/cli-reference/runai-submit/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>","text":"<p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>"},{"location":"Researcher/cli-reference/runai-submit/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>","text":"<p>Mount a persistent volume claim into a container.</p> <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--new-pvc</code>.</p> <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class.</p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p> <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only</p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only</p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write</p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write</p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>"},{"location":"Researcher/cli-reference/runai-submit/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>","text":"<p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running</li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-new-pvc-stringarray","title":"--new-pvc  <code>&lt;stringArray&gt;</code>","text":"<p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p> <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>","text":"<p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'","text":"<p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to jobs. Options are:</p> <ul> <li><code>nodeport</code> - add one or more ports using <code>--port</code>.</li> <li><code>external-url</code> - add one port and an optional custom URL using <code>--custom-url</code>.</li> </ul> <p>For example:</p> <p><code>runai submit test-jup -p team-a -i runai.jfrog.io/demo/jupyter-tensorboard --service-type external-url --port 8888</code></p> <p><code>runai submit test-np -p team-a -i ubuntu --service-type nodeport --port 30000:7070</code></p> <p>This flag supports more than one <code>service-type</code>. Multiple service types are supported in CSV style using multiple instances of the same option and commas to separate the values for them.</p> <p>For example:</p> <p><code>runai submit test-np -p team-a -i ubuntu --service-type nodeport,port=30000:7070 --service-type external-url,port=30001</code></p> <p><code>runai submit test-np -p team-a -i ubuntu --service-type nodeport,port=30000:7070,port=9090 --service-type external-url,port=8080,custom-url=https://my.domain.com/url</code> </p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container. You can use a port number (for example 9090) or use the numbers of <code>hostport:containerport</code> (for example, 30000:7070).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-custom-url-string","title":"--custom-url <code>&lt;string&gt;</code>  <p>An optional argument that specifies a custom URL when using the <code>external-url</code> service type. If not provided, the system will generate a URL automatically.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node. This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#output","title":"Output","text":"<p>The command will attempt to submit a Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p> <p>Note that the submit call may use a policy to provide defaults to any of the above flags.</p>"},{"location":"Researcher/cli-reference/runai-submit/#see-also","title":"See Also","text":"<ul> <li>See any of the Quickstart documents here:.</li> <li>See policy configuration for a description on how policies work.</li> </ul>"},{"location":"Researcher/cli-reference/runai-suspend/","title":"runai suspend","text":""},{"location":"Researcher/cli-reference/runai-suspend/#description","title":"Description","text":"<p>Suspend a Job</p> <p>Suspending a Running Job will stop the Job and will not allow it to be scheduled until it is resumed using <code>runai resume</code>. This means that,</p> <ul> <li>You will no longer be able to enter it via <code>runai bash</code>.</li> <li>The Job logs will be deleted.</li> <li>Any data saved on the container and not stored in a shared location will be lost.</li> </ul> <p>Technically, the command deletes the Kubernetes pods associated with the Job and marks the Job as suspended until it is manually released. </p> <p>Suspend and resume do not work with MPI and Inference </p>"},{"location":"Researcher/cli-reference/runai-suspend/#synopsis","title":"Synopsis","text":"<pre><code>runai suspend &lt;job-name&gt;\n    [--all | -A]\n\n    [--loglevel value]\n    [--project string | -p string]\n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-suspend/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#-all-a","title":"--all | -A","text":"<p>Suspend all Jobs in the current Project.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-suspend/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-suspend/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#output","title":"Output","text":"<ul> <li>The Job will be suspended. When running runai list jobs the Job will be marked as Suspended.</li> </ul>"},{"location":"Researcher/cli-reference/runai-suspend/#see-also","title":"See Also","text":"<ul> <li>Resuming Jobs: Resume.</li> </ul>"},{"location":"Researcher/cli-reference/runai-top-node/","title":"runai top node","text":""},{"location":"Researcher/cli-reference/runai-top-node/#description","title":"Description","text":"<p>Show list of Nodes (machines), their capacity and utilization.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#synopsis","title":"Synopsis","text":"<pre><code>runai top node \n    [--help | -h]\n    [--details | -d]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-top-node/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-top-node/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-top-node/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-top-node/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#-details-d","title":"--details | -d","text":"<p>Show additional details.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#output","title":"Output","text":"<p>Shows a list of Nodes their capacity and utilization.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-update/","title":"runai update","text":""},{"location":"Researcher/cli-reference/runai-update/#description","title":"Description","text":"<p>Find and install the latest version of the runai command-line utility. The command must be run with sudo permissions.</p> <pre><code>sudo runai update\n</code></pre>"},{"location":"Researcher/cli-reference/runai-update/#synopsis","title":"Synopsis","text":"<pre><code>runai update \n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-update/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-update/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-update/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-update/#output","title":"Output","text":"<p>Update of the Run:ai command-line interface.</p>"},{"location":"Researcher/cli-reference/runai-update/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-version/","title":"runai version","text":""},{"location":"Researcher/cli-reference/runai-version/#description","title":"Description","text":"<p>Show the version of this utility.</p>"},{"location":"Researcher/cli-reference/runai-version/#synopsis","title":"Synopsis","text":"<pre><code>runai version \n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-version/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-version/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-version/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-version/#output","title":"Output","text":"<p>The version of the Run:ai command-line interface.</p>"},{"location":"Researcher/cli-reference/runai-version/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-whoami/","title":"runai whoami","text":""},{"location":"Researcher/cli-reference/runai-whoami/#description","title":"Description","text":"<p>Show the user name currently logged in</p>"},{"location":"Researcher/cli-reference/runai-whoami/#synopsis","title":"Synopsis","text":"<pre><code>runai whoami \n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-whoami/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-whoami/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-whoami/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-whoami/#output","title":"Output","text":"<p>The name of the User currently logged in with the Run:ai command-line interface.</p>"},{"location":"Researcher/cli-reference/runai-whoami/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/new-cli/cli-examples/","title":"CLI Examples","text":"<p>This article provides examples of popular use cases illustrating how to use the Command Line Interface (CLI)</p>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#login","title":"Login","text":""},{"location":"Researcher/cli-reference/new-cli/cli-examples/#login-via-runai-sign-in-page-web","title":"Login via run:ai sign in page (web)","text":"<p>You can login from the UI, if you are using SSO or credentials <pre><code>runai login\n</code></pre></p>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#login-via-terminal-credentials","title":"Login via terminal (credentials)","text":"<pre><code>runai login user -u john@acme.com -p \"password\"\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#submitting-a-workload","title":"Submitting a workload","text":""},{"location":"Researcher/cli-reference/new-cli/cli-examples/#naming-a-workload","title":"Naming a workload","text":"<p>Use the commands below to provide a name for a workload.</p>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#setting-a-the-workload-name-my_workload_name","title":"Setting a the workload name ( my_workload_name)","text":"<pre><code>runai workspace submit my-workload-name -p test -i ubuntu \n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#setting-a-random-name-with-prefix-prefixworkload-type","title":"Setting a random name with prefix (prefix=workload type)","text":"<pre><code>    runai workspace submit -p test -i ubuntu \n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#setting-a-random-name-with-specific-prefix-prefix-determined-by-flag","title":"Setting a random name with specific prefix (prefix determined by flag)","text":"<pre><code>runai workspace submit --prefix-name my-prefix-workload-name -p test -i ubuntu \n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#labels-and-annotations","title":"Labels and annotations","text":""},{"location":"Researcher/cli-reference/new-cli/cli-examples/#labels","title":"Labels","text":"<pre><code>runai workspace submit -p test -i ubuntu --label name=value --label name2=value2\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#annotations","title":"Annotations","text":"<pre><code>runai workspace submit -p test -i ubuntu --annotation name=value --annotation name2=value2\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#containers-environment-variables","title":"Container's environment variables","text":"<pre><code>runai workspace submit -p test -i ubuntu -e name=value -e name2=value2\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#requests-and-limits","title":"Requests and limits","text":"<pre><code>runai workspace submit  -p alon -i runai.jfrog.io/demo/quickstart-demo   --cpu-core-request 0.3 --cpu-core-limit 1 --cpu-memory-request 50M --cpu-memory-limit 1G  --gpu-devices-request 1 --gpu-memory-request 1G\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/overview/","title":"Run:ai V2 Command-line Interface","text":"<p>The Run:ai Command-line Interface (CLI) tool for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, and access other features in the Run:ai platform.</p>"},{"location":"Researcher/cli-reference/new-cli/overview/#the-new-v2-command-line-interface","title":"The new V2 Command-line interface","text":"<p>This command-line interface is a complete revamp of the command-line interface. Few highlights:</p> <ul> <li>The CLI internally uses the Control-plane API. This provides a single point of view on Workloads removing dissimilarities between the user interface, programming interface and the command-line interface. </li> <li>As such, it also removes the need to configure the Kubernetes API server for authentication. </li> <li>The CLI is only available for Run:ai cluster version 2.18 and up.</li> <li>The new V2 CLI is backward compatible with the older V1 CLI.</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/overview/#installing-the-improved-command-line-interface","title":"Installing the Improved Command Line Interface","text":"<p>See installation instructions here.</p>"},{"location":"Researcher/cli-reference/new-cli/overview/#reference","title":"Reference","text":"<p>List of all commands can be found here</p>"},{"location":"Researcher/cli-reference/new-cli/runai/","title":"CLI Reference","text":""},{"location":"Researcher/cli-reference/new-cli/runai/#runai","title":"runai","text":"<p>Run:ai Command-line Interface</p>"},{"location":"Researcher/cli-reference/new-cli/runai/#synopsis","title":"Synopsis","text":"<p>runai - The Run:ai Researcher Command Line Interface</p> <p>Description:   A tool for managing Run:ai workloads and monitoring available resources.   It provides researchers with comprehensive control over their AI development environment.</p> <pre><code>runai [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai/#options","title":"Options","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -h, --help                 help for runai\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai/#see-also","title":"SEE ALSO","text":"<ul> <li>runai cluster  - cluster management</li> <li>runai config    - configuration management</li> <li>runai kubeconfig    - kubeconfig management</li> <li>runai describe    - [Deprecated] Display detailed information about resources</li> <li>runai distributed  - distributed management</li> <li>runai exec    - [Deprecated] exec</li> <li>runai list    - [Deprecated] display resource list. By default displays the job list</li> <li>runai login  - login to the control plane</li> <li>runai logout    - logout from control plane</li> <li>runai logs    - [Deprecated] logs</li> <li>runai node    - node management</li> <li>runai nodepool    - node pool management</li> <li>runai port-forward    - [Deprecated] port forward</li> <li>runai project  - project management</li> <li>runai report    - [Experimental] report management</li> <li>runai submit    - [Deprecated] Submit a new workload</li> <li>runai training    - training management</li> <li>runai upgrade  - upgrades the CLI to the latest version</li> <li>runai version  - show the current version of the CLI</li> <li>runai whoami    - show the current logged in user</li> <li>runai workload    - workload management</li> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster/","title":"Runai cluster","text":""},{"location":"Researcher/cli-reference/new-cli/runai_cluster/#runai-cluster","title":"runai cluster","text":"<p>cluster management</p> <pre><code>runai cluster [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster/#options","title":"Options","text":"<pre><code>  -h, --help                 help for cluster\n      --interactive enable   set interactive mode (enabled|disabled)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai cluster list    - cluster list command</li> <li>runai cluster set  - set cluster context</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_list/","title":"Runai cluster list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_cluster_list/#runai-cluster-list","title":"runai cluster list","text":"<p>cluster list command</p> <pre><code>runai cluster list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_list/#options","title":"Options","text":"<pre><code>  -h, --help    help for list\n      --json    Output structure JSON\n      --table   Output structure table\n      --yaml    Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai cluster  - cluster management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_set/","title":"Runai cluster set","text":""},{"location":"Researcher/cli-reference/new-cli/runai_cluster_set/#runai-cluster-set","title":"runai cluster set","text":"<p>set cluster context</p> <pre><code>runai cluster set [CLUSTER_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_set/#options","title":"Options","text":"<pre><code>  -h, --help        help for set\n      --id string   set by cluster ID\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_set/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_set/#see-also","title":"SEE ALSO","text":"<ul> <li>runai cluster  - cluster management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_config/","title":"Runai config","text":""},{"location":"Researcher/cli-reference/new-cli/runai_config/#runai-config","title":"runai config","text":"<p>configuration management</p> <pre><code>runai config [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config/#options","title":"Options","text":"<pre><code>  -h, --help                 help for config\n      --interactive enable   set interactive mode (enabled|disabled)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai config generate  - generate config file</li> <li>runai config project    - Deprecated] Configure a default project</li> <li>runai config set    - Set configuration values</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_config_generate/","title":"Runai config generate","text":""},{"location":"Researcher/cli-reference/new-cli/runai_config_generate/#runai-config-generate","title":"runai config generate","text":"<p>generate config file</p> <pre><code>runai config generate [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_generate/#options","title":"Options","text":"<pre><code>      --file string   Output structure to file\n  -h, --help          help for generate\n      --json          Output structure JSON\n      --yaml          Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_generate/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_generate/#see-also","title":"SEE ALSO","text":"<ul> <li>runai config    - configuration management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_config_project/","title":"Runai config project","text":""},{"location":"Researcher/cli-reference/new-cli/runai_config_project/#runai-config-project","title":"runai config project","text":"<p>Deprecated] Configure a default project</p> <pre><code>runai config project PROJECT_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_project/#options","title":"Options","text":"<pre><code>  -h, --help   help for project\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_project/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_project/#see-also","title":"SEE ALSO","text":"<ul> <li>runai config    - configuration management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_config_set/","title":"Runai config set","text":""},{"location":"Researcher/cli-reference/new-cli/runai_config_set/#runai-config-set","title":"runai config set","text":"<p>Set configuration values</p> <pre><code>runai config set [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_set/#options","title":"Options","text":"<pre><code>      --auth-url string      set the authorization URL; most likely the same as the control plane URL\n      --cp-url string        set the control plane URL\n  -h, --help                 help for set\n      --interactive enable   set interactive mode (enabled|disabled)\n      --output string        set the default output type\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_set/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_set/#see-also","title":"SEE ALSO","text":"<ul> <li>runai config    - configuration management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_describe/","title":"Runai describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_describe/#runai-describe","title":"runai describe","text":"<p>[Deprecated] Display detailed information about resources</p>"},{"location":"Researcher/cli-reference/new-cli/runai_describe/#options","title":"Options","text":"<pre><code>  -h, --help   help for describe\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai describe job    - [Deprecated] Display details of a job</li> <li>runai describe node  - [Deprecated] Display detailed information about nodes in the cluster</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_job/","title":"Runai describe job","text":""},{"location":"Researcher/cli-reference/new-cli/runai_describe_job/#runai-describe-job","title":"runai describe job","text":"<p>[Deprecated] Display details of a job</p> <pre><code>runai describe job JOB_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_job/#options","title":"Options","text":"<pre><code>  -h, --help             help for job\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --type string      The type of the workload (training, workspace, distributed)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_job/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_job/#see-also","title":"SEE ALSO","text":"<ul> <li>runai describe    - [Deprecated] Display detailed information about resources</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_node/","title":"Runai describe node","text":""},{"location":"Researcher/cli-reference/new-cli/runai_describe_node/#runai-describe-node","title":"runai describe node","text":"<p>[Deprecated] Display detailed information about nodes in the cluster</p> <pre><code>runai describe node [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_node/#options","title":"Options","text":"<pre><code>  -h, --help   help for node\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_node/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_node/#see-also","title":"SEE ALSO","text":"<ul> <li>runai describe    - [Deprecated] Display detailed information about resources</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed/","title":"Runai distributed","text":""},{"location":"Researcher/cli-reference/new-cli/runai_distributed/#runai-distributed","title":"runai distributed","text":"<p>distributed management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed/#options","title":"Options","text":"<pre><code>  -h, --help                 help for distributed\n      --interactive enable   set interactive mode (enabled|disabled)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai distributed delete    - delete distributed workload</li> <li>runai distributed describe    - Describe a distributed workload</li> <li>runai distributed exec    - exec management</li> <li>runai distributed list    - list distributed</li> <li>runai distributed logs    - logs management</li> <li>runai distributed port-forward    - port forward management</li> <li>runai distributed submit    - submit distributed</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_delete/","title":"Runai distributed delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_distributed_delete/#runai-distributed-delete","title":"runai distributed delete","text":"<p>delete distributed workload</p> <pre><code>runai distributed delete [DISTRIBUTED_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_delete/#examples","title":"Examples","text":"<pre><code># Delete a distributed workload with a default project\nrunai distributed delete &lt;distributed_name&gt;\n\n# Delete a distributed workload with a specific project\nrunai distributed delete &lt;distributed_name&gt; -p &lt;project_name&gt;\n\n# Delete a distributed workload by UUID\nrunai distributed delete --uuid=&lt;distributed_uuid&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai distributed  - distributed management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_describe/","title":"Runai distributed describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_distributed_describe/#runai-distributed-describe","title":"runai distributed describe","text":"<p>Describe a distributed workload</p> <pre><code>runai distributed describe DISTRIBUTED_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai distributed  - distributed management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_exec/","title":"Runai distributed exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_distributed_exec/#runai-distributed-exec","title":"runai distributed exec","text":"<p>exec management</p> <pre><code>runai distributed exec DISTRIBUTED_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_exec/#examples","title":"Examples","text":"<pre><code># Execute bush to distributed \nrunai distributed exec jup --tty --stdin -- /bin/bash \n\n# Execute ls to workload\nrunai distributed exec jup -- ls\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --stdin                          Pass stdin to the container\n      --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai distributed  - distributed management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_list/","title":"Runai distributed list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_distributed_list/#runai-distributed-list","title":"runai distributed list","text":"<p>list distributed</p> <pre><code>runai distributed list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_list/#examples","title":"Examples","text":"<pre><code># List all distributed workloads\nrunai distributed list -A\n\n# List distributed workloads with a specific state\nrunai distributed list -p=&lt;project_name&gt;\n\n# List distributed workloads with a specific state and limit\nrunai distributed list --state=&lt;distributed_state&gt; --limit=20\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_list/#options","title":"Options","text":"<pre><code>  -A, --all              list jobs from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list, (default 50) (default 50)\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai distributed  - distributed management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_logs/","title":"Runai distributed logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_distributed_logs/#runai-distributed-logs","title":"runai distributed logs","text":"<p>logs management</p> <pre><code>runai distributed logs DISTRIBUTED_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_logs/#examples","title":"Examples","text":"<pre><code>  # Get logs for a distributed\n  runai distributed logs distributed-01\n\n  # Get logs for a specific pod in a distributed workload\n  runai distributed logs distributed-01 --pod=distributed-01-0\n\n  # Get logs for a specific container in a distributed workload\n  runai distributed logs distributed-01 --container=distributed-01\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai distributed  - distributed management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_port-forward/","title":"Runai distributed port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_distributed_port-forward/#runai-distributed-port-forward","title":"runai distributed port-forward","text":"<p>port forward management</p> <pre><code>runai distributed port-forward DISTRIBUTED_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to &lt;distributed-name&gt; on port 8090:\nrunai distributed port-forward &lt;distributed-name&gt; --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to &lt;job-name&gt; on port 8080:\nrunai distributed port-forward &lt;distributed-name&gt; --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to &lt;distributed-name&gt; on port 8090 and from localhost:6443 to &lt;distributed-name&gt; on port 443:\nrunai distributed port-forward &lt;distributed-name&gt; --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout (default 0s)\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai distributed  - distributed management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_submit/","title":"Runai distributed submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_distributed_submit/#runai-distributed-submit","title":"runai distributed submit","text":"<p>submit distributed</p> <pre><code>runai distributed submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_submit/#examples","title":"Examples","text":"<pre><code>runai distributed submit &lt;distributed_name&gt; -p=&lt;project_name&gt; -i=runai.jfrog.io/demo/quickstart -f XGBoost/PyTorch/TF/MPI\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n  -f, --framework string                               The distributed training framework used in the workload.\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --master-args                                    Arguments to pass to the master pod container command. If used together with --master-command, overrides the image's entrypoint of the master pod container with the given command\n      --master-environment stringArray                 Set environment variables in the container\n      --master-extended-resource stringArray           Request access to an extended resource. Use the format: resource_name=quantity\n      --master-gpu-devices-request int32               GPU units to allocate for the job (e.g. 1, 2)\n      --master-gpu-portion-limit float                 GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-gpu-portion-request float               GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-no-pvcs                                 Do not mount any persistent volumes in the master pod\n      --max-replicas int32                             Maximum number of replicas for an elastic PyTorch job\n      --mig-profile string                             MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --min-replicas int32                             Minimum number of replicas for an elastic PyTorch job\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                s3 storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --no-master                                      Do not create a separate pod for the master\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-group int                               Run in the context of the current CLI group rather than the root group\n      --run-as-user int                                Run in the context of the current CLI user rather than the root user\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --slots-per-worker int32                         Number of slots to allocate for each worker\n      --supplemental-groups string                     Comma seperated list of groups that the user running the container belongs to, in addition to the group indicated by --run-as-gid\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --workers int32                                  the number of workers that will be allocated for running the workload\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_distributed_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai distributed  - distributed management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_exec/","title":"Runai exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_exec/#runai-exec","title":"runai exec","text":"<p>[Deprecated] exec</p> <pre><code>runai exec WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --stdin                          Pass stdin to the container\n      --tty                            Stdin is a TTY\n      --type string                    The type of the workload (training, workspace, distributed)\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig/","title":"Runai kubeconfig","text":""},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig/#runai-kubeconfig","title":"runai kubeconfig","text":"<p>kubeconfig management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig/#options","title":"Options","text":"<pre><code>  -h, --help   help for kubeconfig\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai kubeconfig set    - kubeconfig set login token</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig_set/","title":"Runai kubeconfig set","text":""},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig_set/#runai-kubeconfig-set","title":"runai kubeconfig set","text":"<p>kubeconfig set login token</p> <pre><code>runai kubeconfig set [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig_set/#options","title":"Options","text":"<pre><code>  -h, --help   help for set\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig_set/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig_set/#see-also","title":"SEE ALSO","text":"<ul> <li>runai kubeconfig    - kubeconfig management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_list/","title":"Runai list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_list/#runai-list","title":"runai list","text":"<p>[Deprecated] display resource list. By default displays the job list</p> <pre><code>runai list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list/#options","title":"Options","text":"<pre><code>  -A, --all-projects     list jobs from all projects\n  -h, --help             help for list\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai list clusters  - [Deprecated] list all available clusters</li> <li>runai list jobs  - [Deprecated] list all jobs</li> <li>runai list nodes    - [Deprecated] list all nodes</li> <li>runai list projects  - [Deprecated] list all available projects</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_list_clusters/","title":"Runai list clusters","text":""},{"location":"Researcher/cli-reference/new-cli/runai_list_clusters/#runai-list-clusters","title":"runai list clusters","text":"<p>[Deprecated] list all available clusters</p> <pre><code>runai list clusters [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_clusters/#options","title":"Options","text":"<pre><code>  -h, --help    help for clusters\n      --json    Output structure JSON\n      --table   Output structure table\n      --yaml    Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_clusters/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_clusters/#see-also","title":"SEE ALSO","text":"<ul> <li>runai list    - [Deprecated] display resource list. By default displays the job list</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_list_jobs/","title":"Runai list jobs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_list_jobs/#runai-list-jobs","title":"runai list jobs","text":"<p>[Deprecated] list all jobs</p> <pre><code>runai list jobs [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_jobs/#options","title":"Options","text":"<pre><code>  -A, --all-projects     list jobs from all projects\n  -h, --help             help for jobs\n      --json             Output structure JSON\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_jobs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_jobs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai list    - [Deprecated] display resource list. By default displays the job list</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_list_nodes/","title":"Runai list nodes","text":""},{"location":"Researcher/cli-reference/new-cli/runai_list_nodes/#runai-list-nodes","title":"runai list nodes","text":"<p>[Deprecated] list all nodes</p> <pre><code>runai list nodes [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_nodes/#options","title":"Options","text":"<pre><code>  -h, --help    help for nodes\n      --json    Output structure JSON\n      --table   Output structure table\n      --yaml    Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_nodes/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_nodes/#see-also","title":"SEE ALSO","text":"<ul> <li>runai list    - [Deprecated] display resource list. By default displays the job list</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_list_projects/","title":"Runai list projects","text":""},{"location":"Researcher/cli-reference/new-cli/runai_list_projects/#runai-list-projects","title":"runai list projects","text":"<p>[Deprecated] list all available projects</p> <pre><code>runai list projects [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_projects/#options","title":"Options","text":"<pre><code>  -h, --help    help for projects\n      --json    Output structure JSON\n      --table   Output structure table\n      --yaml    Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_projects/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_projects/#see-also","title":"SEE ALSO","text":"<ul> <li>runai list    - [Deprecated] display resource list. By default displays the job list</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_login/","title":"Runai login","text":""},{"location":"Researcher/cli-reference/new-cli/runai_login/#runai-login","title":"runai login","text":"<p>login to the control plane</p> <pre><code>runai login [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login/#examples","title":"Examples","text":"<pre><code>  # Login using browser\n  runai login\n\n  # Login using browser with specific port and host\n  runai login --listen-port=43121 --listen-host=localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login/#options","title":"Options","text":"<pre><code>  -h, --help                 help for login\n      --listen-host string   the host to listen on for the authentication callback (for browser mode only) (default \"localhost\")\n      --listen-port int      the port to listen on for the authentication callback (for browser mode only) (default 43121)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai login application  - login as an application</li> <li>runai login sso  - login using sso without browser</li> <li>runai login user    - login for local user without browser</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_login_application/","title":"Runai login application","text":""},{"location":"Researcher/cli-reference/new-cli/runai_login_application/#runai-login-application","title":"runai login application","text":"<p>login as an application</p> <pre><code>runai login application [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_application/#examples","title":"Examples","text":"<pre><code>  # Login interactive using application credentials\n  runai login app\n\n  # Login using application credentials\n  login app --name=&lt;app_name&gt; --secret=&lt;app_secret&gt; --interactive=disabled\n\n  # Login and Save application credentials\n  login app --name=&lt;app_name&gt; --secret=&lt;app_secret&gt; --interactive=disabled --save\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_application/#options","title":"Options","text":"<pre><code>  -h, --help                 help for application\n      --interactive enable   set interactive mode (enabled|disabled)\n      --name string          application name\n      --save                 save application credentials in config file\n      --secret string        application secret\n      --secret-file string   use application secret from file\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_application/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_application/#see-also","title":"SEE ALSO","text":"<ul> <li>runai login  - login to the control plane</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_login_sso/","title":"Runai login sso","text":""},{"location":"Researcher/cli-reference/new-cli/runai_login_sso/#runai-login-sso","title":"runai login sso","text":"<p>login using sso without browser</p> <pre><code>runai login sso [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_sso/#options","title":"Options","text":"<pre><code>  -h, --help   help for sso\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_sso/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_sso/#see-also","title":"SEE ALSO","text":"<ul> <li>runai login  - login to the control plane</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_login_user/","title":"Runai login user","text":""},{"location":"Researcher/cli-reference/new-cli/runai_login_user/#runai-login-user","title":"runai login user","text":"<p>login for local user without browser</p>"},{"location":"Researcher/cli-reference/new-cli/runai_login_user/#synopsis","title":"Synopsis","text":"<p>Login to the control plane using a local user without browser</p> <pre><code>runai login user [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_user/#examples","title":"Examples","text":"<pre><code># Login with a username. the password will be prompted via stdin afterward (recommended)\nrunai login user -u &lt;username&gt;\n\n# Login with a username and plain password (not recommended for security reasons)\nrunai login user --user=user --password=pass\n\n# Login with a username and password (not recommended for security reasons)\nrunai login user -u=user -p=pass\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_user/#options","title":"Options","text":"<pre><code>  -h, --help              help for user\n  -p, --password string   plaintext password of the given username. not recommended for security reasons\n  -u, --user string       the username to login with\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_user/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_user/#see-also","title":"SEE ALSO","text":"<ul> <li>runai login  - login to the control plane</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_logout/","title":"Runai logout","text":""},{"location":"Researcher/cli-reference/new-cli/runai_logout/#runai-logout","title":"runai logout","text":"<p>logout from control plane</p> <pre><code>runai logout [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_logout/#options","title":"Options","text":"<pre><code>  -h, --help   help for logout\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_logout/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_logout/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_logs/","title":"Runai logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_logs/#runai-logs","title":"runai logs","text":"<p>[Deprecated] logs</p> <pre><code>runai logs WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --type string             The type of the workload (training, workspace, distributed)\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_node/","title":"Runai node","text":""},{"location":"Researcher/cli-reference/new-cli/runai_node/#runai-node","title":"runai node","text":"<p>node management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_node/#options","title":"Options","text":"<pre><code>  -h, --help   help for node\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_node/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_node/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai node list  - List node</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_node_list/","title":"Runai node list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_node_list/#runai-node-list","title":"runai node list","text":"<p>List node</p> <pre><code>runai node list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_node_list/#options","title":"Options","text":"<pre><code>  -h, --help    help for list\n      --json    Output structure JSON\n      --table   Output structure table\n      --yaml    Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_node_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_node_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai node    - node management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool/","title":"Runai nodepool","text":""},{"location":"Researcher/cli-reference/new-cli/runai_nodepool/#runai-nodepool","title":"runai nodepool","text":"<p>node pool management</p> <pre><code>runai nodepool [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool/#options","title":"Options","text":"<pre><code>  -h, --help   help for nodepool\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai nodepool list  - List node pool</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool_list/","title":"Runai nodepool list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_nodepool_list/#runai-nodepool-list","title":"runai nodepool list","text":"<p>List node pool</p> <pre><code>runai nodepool list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool_list/#options","title":"Options","text":"<pre><code>  -h, --help    help for list\n      --json    Output structure JSON\n      --table   Output structure table\n      --yaml    Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai nodepool    - node pool management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_port-forward/","title":"Runai port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_port-forward/#runai-port-forward","title":"runai port-forward","text":"<p>[Deprecated] port forward</p> <pre><code>runai port-forward WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout (default 0s)\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --type string                    The type of the workload (training, workspace, distributed)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_project/","title":"Runai project","text":""},{"location":"Researcher/cli-reference/new-cli/runai_project/#runai-project","title":"runai project","text":"<p>project management</p> <pre><code>runai project [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project/#options","title":"Options","text":"<pre><code>  -h, --help                 help for project\n      --interactive enable   set interactive mode (enabled|disabled)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai project list    - list available project</li> <li>runai project set  - set project context</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_project_list/","title":"Runai project list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_project_list/#runai-project-list","title":"runai project list","text":"<p>list available project</p> <pre><code>runai project list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project_list/#options","title":"Options","text":"<pre><code>  -h, --help    help for list\n      --json    Output structure JSON\n      --table   Output structure table\n      --yaml    Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai project  - project management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_project_set/","title":"Runai project set","text":""},{"location":"Researcher/cli-reference/new-cli/runai_project_set/#runai-project-set","title":"runai project set","text":"<p>set project context</p> <pre><code>runai project set PROJECT_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project_set/#options","title":"Options","text":"<pre><code>  -h, --help   help for set\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project_set/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project_set/#see-also","title":"SEE ALSO","text":"<ul> <li>runai project  - project management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_report/","title":"Runai report","text":""},{"location":"Researcher/cli-reference/new-cli/runai_report/#runai-report","title":"runai report","text":"<p>[Experimental] report management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_report/#options","title":"Options","text":"<pre><code>  -h, --help   help for report\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai report metrics    - [Experimental] metrics management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics/","title":"Runai report metrics","text":""},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics/#runai-report-metrics","title":"runai report metrics","text":"<p>[Experimental] metrics management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics/#options","title":"Options","text":"<pre><code>  -h, --help   help for metrics\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics/#see-also","title":"SEE ALSO","text":"<ul> <li>runai report    - [Experimental] report management</li> <li>runai report metrics clear    - metrics logs deletion</li> <li>runai report metrics config  - metrics configuration</li> <li>runai report metrics output  - metrics logs output</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_clear/","title":"Runai report metrics clear","text":""},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_clear/#runai-report-metrics-clear","title":"runai report metrics clear","text":"<p>metrics logs deletion</p> <pre><code>runai report metrics clear [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_clear/#options","title":"Options","text":"<pre><code>  -h, --help   help for clear\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_clear/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_clear/#see-also","title":"SEE ALSO","text":"<ul> <li>runai report metrics    - [Experimental] metrics management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_config/","title":"Runai report metrics config","text":""},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_config/#runai-report-metrics-config","title":"runai report metrics config","text":"<p>metrics configuration</p> <pre><code>runai report metrics config [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_config/#options","title":"Options","text":"<pre><code>      --age int          metrics max file age (default 14)\n      --files int        metrics max file number (default 30)\n  -h, --help             help for config\n      --metrics enable   metrics enable flag (enabled|disabled)\n      --size int         metrics max file size (default 10)\n      --type reporter    report generated type (none|logger|local)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_config/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_config/#see-also","title":"SEE ALSO","text":"<ul> <li>runai report metrics    - [Experimental] metrics management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_output/","title":"Runai report metrics output","text":""},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_output/#runai-report-metrics-output","title":"runai report metrics output","text":"<p>metrics logs output</p> <pre><code>runai report metrics output [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_output/#options","title":"Options","text":"<pre><code>  -h, --help       help for output\n      --tail int   number of tail metrics (default 100)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_output/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_output/#see-also","title":"SEE ALSO","text":"<ul> <li>runai report metrics    - [Experimental] metrics management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_submit/","title":"Runai submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_submit/#runai-submit","title":"runai submit","text":"<p>[Deprecated] Submit a new workload</p> <pre><code>runai submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_submit/#options","title":"Options","text":"<pre><code>      --add-capability stringArray                     The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --completions int32                              Number of successful pods required for this job to be completed. Used with HPO\n      --configmap-volume stringArray                   Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu float                                      CPU core request (e.g. 0.5, 1)\n      --cpu-limit float                                CPU core limit (e.g. 0.5, 1)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu float                                      GPU units to allocate for the job (e.g. 0.5, 1)\n      --gpu-memory string                              GPU memory to allocate for the job (e.g. 1G, 500M)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --interactive                                    Mark this job as interactive\n      --job-name-prefix string                         Set defined prefix for the workload name and add index as a suffix\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --memory string                                  CPU memory to allocate for the job (e.g. 1G, 500M)\n      --memory-limit string                            CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --mig-profile string                             MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --parallelism int32                              Number of pods to run in parallel at any given time. Used with HPO\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preemptible                                    Interactive preemptible jobs can be scheduled above guaranteed quota but may be reclaimed at any time\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-group int                               Run in the context of the current CLI group rather than the root group\n      --run-as-user int                                Run in the context of the current CLI user rather than the root user\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --supplemental-groups string                     Comma seperated list of groups that the user running the container belongs to, in addition to the group indicated by --run-as-gid\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n  -v, --volume stringArray                             Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training/","title":"Runai training","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training/#runai-training","title":"runai training","text":"<p>training management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_training/#options","title":"Options","text":"<pre><code>  -h, --help                 help for training\n      --interactive enable   set interactive mode (enabled|disabled)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai training delete  - delete training workload</li> <li>runai training describe  - Describe a training workload</li> <li>runai training exec  - exec management</li> <li>runai training list  - list training</li> <li>runai training logs  - logs management</li> <li>runai training mpi    - MPI management</li> <li>runai training port-forward  - port forward management</li> <li>runai training resume  - resume training</li> <li>runai training submit  - submit training</li> <li>runai training suspend    - suspend training</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_delete/","title":"Runai training delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_delete/#runai-training-delete","title":"runai training delete","text":"<p>delete training workload</p> <pre><code>runai training delete [TRAINING_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_delete/#examples","title":"Examples","text":"<pre><code>runai training delete &lt;training_name&gt; (optional)-p=&lt;project_name&gt;\nrunai training delete --uuid=&lt;training_uuid&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_describe/","title":"Runai training describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_describe/#runai-training-describe","title":"runai training describe","text":"<p>Describe a training workload</p> <pre><code>runai training describe TRAINING_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_exec/","title":"Runai training exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_exec/#runai-training-exec","title":"runai training exec","text":"<p>exec management</p> <pre><code>runai training exec TRAINING_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_exec/#examples","title":"Examples","text":"<pre><code># Execute bush to training \nrunai training exec jup --tty --stdin -- /bin/bash \n\n# Execute ls to workload\nrunai training exec jup -- ls\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --stdin                          Pass stdin to the container\n      --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_list/","title":"Runai training list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_list/#runai-training-list","title":"runai training list","text":"<p>list training</p> <pre><code>runai training list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_list/#examples","title":"Examples","text":"<pre><code>runai training list -A\nrunai training list --state=&lt;training_state&gt; --limit=20\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_list/#options","title":"Options","text":"<pre><code>  -A, --all              list jobs from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list, (default 50) (default 50)\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_logs/","title":"Runai training logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_logs/#runai-training-logs","title":"runai training logs","text":"<p>logs management</p> <pre><code>runai training logs TRAINING_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_logs/#examples","title":"Examples","text":"<pre><code>  # Get logs for a training\n  runai training logs training-01\n\n  # Get logs for a specific pod in a training\n  runai training logs training-01 --pod=training-01-0\n\n  # Get logs for a specific container in a training\n  runai training logs training-01 --container=container-01\n\n  # Get the last 100 lines of logs\n  runai training logs training-01 --tail=100\n\n  # Get logs with timestamps\n  runai training logs training-01 --timestamps\n\n  # Follow the logs\n  runai training logs training-01 --follow\n\n  # Get logs for the previous instance of the training\n  runai training logs training-01 --previous\n\n  # GetLimit the logs to 1024 bytes\n  runai training logs training-01 --limit-bytes=1024\n\n  # Get logs since the last 5 minutes\n  runai training logs training-01 --since=300s\n\n  # Get logs since a specific timestamp\n  runai training logs training-01 --since-time=2023-05-30T10:00:00Z\n\n  # Wait up to 30 seconds for training to be ready for logs\n  runai training logs training-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi/","title":"Runai training mpi","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi/#runai-training-mpi","title":"runai training mpi","text":"<p>MPI management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi/#options","title":"Options","text":"<pre><code>  -h, --help   help for mpi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> <li>runai training mpi delete  - delete MPI training</li> <li>runai training mpi describe  - describe MPI training</li> <li>runai training mpi exec  - Execute a command in an MPI training job</li> <li>runai training mpi list  - list MPI training</li> <li>runai training mpi logs  - View logs of an MPI training job</li> <li>runai training mpi port-forward  - Forward one or more local ports to an MPI training job</li> <li>runai training mpi submit  - submit MPI training</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_delete/","title":"Runai training mpi delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_delete/#runai-training-mpi-delete","title":"runai training mpi delete","text":"<p>delete MPI training</p> <pre><code>runai training mpi delete [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - MPI management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_describe/","title":"Runai training mpi describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_describe/#runai-training-mpi-describe","title":"runai training mpi describe","text":"<p>describe MPI training</p> <pre><code>runai training mpi describe [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_describe/#examples","title":"Examples","text":"<pre><code># Describe an MPI training workload with a default project\nrunai training mpi describe &lt;training-mpi-name&gt;\n\n# Describe an MPI training workload in a specific project\nrunai training mpi describe &lt;training-mpi-name&gt; -p &lt;project_name&gt;\n\n# Describe an MPI training workload by UUID\nrunai training mpi describe --uuid=&lt;training_mpi_uuid&gt;\n\n# Describe an MPI training workload with specific output format\nrunai training mpi describe &lt;training-mpi-name&gt; -o json\n\n# Describe an MPI training workload with specific sections\nrunai training mpi describe &lt;training-mpi-name&gt; --general --compute --pods --events --networks\n\n# Describe an MPI training workload with container details and custom limits\nrunai training mpi describe &lt;training-mpi-name&gt; --containers --pod-limit 20 --event-limit 100\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - MPI management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_exec/","title":"Runai training mpi exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_exec/#runai-training-mpi-exec","title":"runai training mpi exec","text":"<p>Execute a command in an MPI training job</p> <pre><code>runai training mpi exec [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_exec/#examples","title":"Examples","text":"<pre><code># Execute bash in the MPI training's main worker\nrunai training mpi exec mpi-training-01 --tty --stdin -- /bin/bash \n\n# Execute ls command in the MPI training's main worker\nrunai training mpi exec mpi-training-01 -- ls\n\n# Execute a command in a specific MPI worker\nrunai training mpi exec mpi-training-01 --pod mpi-training-01-worker-1 -- nvidia-smi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --stdin                          Pass stdin to the container\n      --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - MPI management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_list/","title":"Runai training mpi list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_list/#runai-training-mpi-list","title":"runai training mpi list","text":"<p>list MPI training</p> <pre><code>runai training mpi list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_list/#examples","title":"Examples","text":"<pre><code># List all MPI training workloads\nrunai training mpi list -A\n\n# List MPI training workloads with default project\nrunai training mpi list\n\n# List MPI training workloads in a specific project\nrunai training mpi list -p &lt;project_name&gt;\n\n# List all MPI training workloads with a specific output format\nrunai training mpi list -o wide\n\n# List MPI training workloads with pagination\nrunai training mpi list --limit 20 --offset 40\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_list/#options","title":"Options","text":"<pre><code>  -A, --all              list jobs from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list, (default 50) (default 50)\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - MPI management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_logs/","title":"Runai training mpi logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_logs/#runai-training-mpi-logs","title":"runai training mpi logs","text":"<p>View logs of an MPI training job</p> <pre><code>runai training mpi logs [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_logs/#examples","title":"Examples","text":"<pre><code>  # Get logs for an MPI training\n  runai training mpi logs mpi-training-01\n\n  # Get logs for a specific pod in an MPI training\n  runai training mpi logs mpi-training-01 --pod=mpi-training-01-worker-0\n\n  # Get logs for a specific container in an MPI training\n  runai training mpi logs mpi-training-01 --container=mpi-worker\n\n  # Get the last 100 lines of logs\n  runai training mpi logs mpi-training-01 --tail=100\n\n  # Get logs with timestamps\n  runai training mpi logs mpi-training-01 --timestamps\n\n  # Follow the logs\n  runai training mpi logs mpi-training-01 --follow\n\n  # Get logs for the previous instance of the MPI training\n  runai training mpi logs mpi-training-01 --previous\n\n  # Limit the logs to 1024 bytes\n  runai training mpi logs mpi-training-01 --limit-bytes=1024\n\n  # Get logs since the last 5 minutes\n  runai training mpi logs mpi-training-01 --since=300s\n\n  # Get logs since a specific timestamp\n  runai training mpi logs mpi-training-01 --since-time=2023-05-30T10:00:00Z\n\n  # Wait up to 30 seconds for MPI training to be ready for logs\n  runai training mpi logs mpi-training-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - MPI management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_port-forward/","title":"Runai training mpi port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_port-forward/#runai-training-mpi-port-forward","title":"runai training mpi port-forward","text":"<p>Forward one or more local ports to an MPI training job</p> <pre><code>runai training mpi port-forward [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to MPI training on port 8090:\nrunai training mpi port-forward mpi-training-01 --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to MPI training on port 8080:\nrunai training mpi port-forward mpi-training-01 --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to MPI training on port 8090 and from localhost:6443 to MPI training on port 443:\nrunai training mpi port-forward mpi-training-01 --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout (default 0s)\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - MPI management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_submit/","title":"Runai training mpi submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_submit/#runai-training-mpi-submit","title":"runai training mpi submit","text":"<p>submit MPI training</p> <pre><code>runai training mpi submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_submit/#examples","title":"Examples","text":"<pre><code># Submit an MPI training job\nrunai training mpi submit &lt;name&gt; -p &lt;project_name&gt; -i runai.jfrog.io/demo/quickstart\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --master-args                                    Arguments to pass to the master pod container command. If used together with --master-command, overrides the image's entrypoint of the master pod container with the given command\n      --master-environment stringArray                 Set environment variables in the container\n      --master-extended-resource stringArray           Request access to an extended resource. Use the format: resource_name=quantity\n      --master-gpu-devices-request int32               GPU units to allocate for the job (e.g. 1, 2)\n      --master-gpu-portion-limit float                 GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-gpu-portion-request float               GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-no-pvcs                                 Do not mount any persistent volumes in the master pod\n      --mig-profile string                             MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                s3 storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-group int                               Run in the context of the current CLI group rather than the root group\n      --run-as-user int                                Run in the context of the current CLI user rather than the root user\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --slots-per-worker int32                         Number of slots to allocate for each worker\n      --supplemental-groups string                     Comma seperated list of groups that the user running the container belongs to, in addition to the group indicated by --run-as-gid\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --workers int32                                  the number of workers that will be allocated for running the workload\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - MPI management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_port-forward/","title":"Runai training port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_port-forward/#runai-training-port-forward","title":"runai training port-forward","text":"<p>port forward management</p> <pre><code>runai training port-forward TRAINING_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to &lt;training-name&gt; on port 8090:\nrunai training port-forward &lt;training-name&gt; --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to &lt;job-name&gt; on port 8080:\nrunai training port-forward &lt;training-name&gt; --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to &lt;training-name&gt; on port 8090 and from localhost:6443 to &lt;training-name&gt; on port 443:\nrunai training port-forward &lt;training-name&gt; --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout (default 0s)\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_resume/","title":"Runai training resume","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_resume/#runai-training-resume","title":"runai training resume","text":"<p>resume training</p> <pre><code>runai training resume [TRAINING_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_resume/#examples","title":"Examples","text":"<pre><code>runai training resume &lt;training_name&gt; -p=&lt;project_name&gt;\nrunai training resume --uuid=&lt;training_uuid&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_resume/#options","title":"Options","text":"<pre><code>  -h, --help             help for resume\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_resume/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_resume/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_submit/","title":"Runai training submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_submit/#runai-training-submit","title":"runai training submit","text":"<p>submit training</p> <pre><code>runai training submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_submit/#examples","title":"Examples","text":"<pre><code>runai training submit &lt;training_name&gt; -p=&lt;project_name&gt; -i=runai.jfrog.io/demo/quickstart\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --completions int32                              Number of successful pods required for this job to be completed. Used with HPO\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --mig-profile string                             MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                s3 storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --parallelism int32                              Number of pods to run in parallel at any given time. Used with HPO\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-group int                               Run in the context of the current CLI group rather than the root group\n      --run-as-user int                                Run in the context of the current CLI user rather than the root user\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --supplemental-groups string                     Comma seperated list of groups that the user running the container belongs to, in addition to the group indicated by --run-as-gid\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_suspend/","title":"Runai training suspend","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_suspend/#runai-training-suspend","title":"runai training suspend","text":"<p>suspend training</p> <pre><code>runai training suspend [TRAINING_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_suspend/#examples","title":"Examples","text":"<pre><code>runai training &lt;training_name&gt; -p=&lt;project_name&gt;\nrunai training suspend --uuid=&lt;training_workspace_uuid&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_suspend/#options","title":"Options","text":"<pre><code>  -h, --help             help for suspend\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_suspend/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_suspend/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_upgrade/","title":"Runai upgrade","text":""},{"location":"Researcher/cli-reference/new-cli/runai_upgrade/#runai-upgrade","title":"runai upgrade","text":"<p>upgrades the CLI to the latest version</p> <pre><code>runai upgrade [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_upgrade/#options","title":"Options","text":"<pre><code>      --force   upgrade CLI without checking for new version\n  -h, --help    help for upgrade\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_upgrade/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_upgrade/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_version/","title":"Runai version","text":""},{"location":"Researcher/cli-reference/new-cli/runai_version/#runai-version","title":"runai version","text":"<p>show the current version of the CLI</p> <pre><code>runai version [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_version/#options","title":"Options","text":"<pre><code>  -h, --help   help for version\n      --wide   print full version details\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_version/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_version/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_whoami/","title":"Runai whoami","text":""},{"location":"Researcher/cli-reference/new-cli/runai_whoami/#runai-whoami","title":"runai whoami","text":"<p>show the current logged in user</p> <pre><code>runai whoami [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_whoami/#options","title":"Options","text":"<pre><code>  -h, --help   help for whoami\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_whoami/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_whoami/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload/","title":"Runai workload","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload/#runai-workload","title":"runai workload","text":"<p>workload management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_workload/#options","title":"Options","text":"<pre><code>  -h, --help                 help for workload\n      --interactive enable   set interactive mode (enabled|disabled)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai workload attach  - Attach to a process that is already running inside an existing container.</li> <li>runai workload describe  - Describe a workload</li> <li>runai workload exec  - exec management</li> <li>runai workload list  - List workloads</li> <li>runai workload logs  - logs management</li> <li>runai workload port-forward  - port forward management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_attach/","title":"Runai workload attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload_attach/#runai-workload-attach","title":"runai workload attach","text":"<p>Attach to a process that is already running inside an existing container.</p> <pre><code>runai workload attach WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_attach/#examples","title":"Examples","text":"<pre><code># Attaching to ubuntu workspace \nrunai workload attach ubuntu-wl --type workspace --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --type string                    The type of the workload (training, workspace, distributed)\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workload    - workload management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_describe/","title":"Runai workload describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload_describe/#runai-workload-describe","title":"runai workload describe","text":"<p>Describe a workload</p> <pre><code>runai workload describe WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --type string         The type of the workload (training, workspace, distributed)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workload    - workload management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_exec/","title":"Runai workload exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload_exec/#runai-workload-exec","title":"runai workload exec","text":"<p>exec management</p> <pre><code>runai workload exec WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_exec/#examples","title":"Examples","text":"<pre><code># Execute bush to workspace \nrunai workload exec jup --type workspace --tty --stdin -- /bin/bash \n\n# Execute ls to workload\nrunai workload exec jup --type workspace -- ls\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --stdin                          Pass stdin to the container\n      --tty                            Stdin is a TTY\n      --type string                    The type of the workload (training, workspace, distributed)\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workload    - workload management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_list/","title":"Runai workload list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload_list/#runai-workload-list","title":"runai workload list","text":"<p>List workloads</p> <pre><code>runai workload list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_list/#options","title":"Options","text":"<pre><code>  -A, --all              list jobs from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list, (default 50) (default 50)\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --type string      filter by workload type\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workload    - workload management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_logs/","title":"Runai workload logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload_logs/#runai-workload-logs","title":"runai workload logs","text":"<p>logs management</p> <pre><code>runai workload logs WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_logs/#examples","title":"Examples","text":"<pre><code>  # Get logs for a workspace\n  runai workload logs workspace-01 --type=workspace\n\n  # Get logs for a specific pod in a workspace\n  runai workload logs workspace-01 --type=workspace --pod=workspace-01-0\n\n  # Get logs for a specific container in a workspace\n  runai workload logs workspace-01 --type=workspace --container=container-01\n\n  # Get the last 100 lines of logs\n  runai workload logs workspace-01 --type=workspace --tail=100\n\n  # Get logs with timestamps\n  runai workload logs workspace-01 --type=workspace --timestamps\n\n  # Follow the logs\n  runai workload logs workspace-01 --type=workspace --follow\n\n  # Get logs for the previous instance of the workspace\n  runai workload logs workspace-01 --type=workspace --previous\n\n  # GetLimit the logs to 1024 bytes\n  runai workload logs workspace-01 --type=workspace --limit-bytes=1024\n\n  # Get logs since the last 5 minutes\n  runai workload logs workspace-01 --type=workspace --since=5m\n\n  # Get logs since a specific timestamp\n  runai workload logs workspace-01 --type=workspace --since-time=2023-05-30T10:00:00Z\n\n  # Wait up to 30 seconds for workload to be ready for logs\n  runai workload logs workspace-01 --type=workspace --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --type string             The type of the workload (training, workspace, distributed)\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workload    - workload management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_port-forward/","title":"Runai workload port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload_port-forward/#runai-workload-port-forward","title":"runai workload port-forward","text":"<p>port forward management</p> <pre><code>runai workload port-forward WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to &lt;workload-name&gt; on port 8090:\nrunai workload port-forward &lt;workload-name&gt; --type=&lt;workload-type&gt; --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to &lt;job-name&gt; on port 8080:\nrunai workload port-forward &lt;workload-name&gt; --type=&lt;workload-type&gt; --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to &lt;workload-name&gt; on port 8090 and from localhost:6443 to &lt;workload-name&gt; on port 443:\nrunai workload port-forward &lt;workload-name&gt; --type=&lt;workload-type&gt; --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout (default 0s)\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --type string                    The type of the workload (training, workspace, distributed)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workload    - workload management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace/","title":"Runai workspace","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace/#runai-workspace","title":"runai workspace","text":"<p>workspace management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace/#options","title":"Options","text":"<pre><code>  -h, --help                 help for workspace\n      --interactive enable   set interactive mode (enabled|disabled)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai workspace delete    - delete workspace</li> <li>runai workspace describe    - Describe a training workload</li> <li>runai workspace exec    - exec management</li> <li>runai workspace list    - list workspace</li> <li>runai workspace logs    - logs management</li> <li>runai workspace port-forward    - port forward management</li> <li>runai workspace resume    - resume workspace</li> <li>runai workspace submit    - submit workspace</li> <li>runai workspace suspend  - suspend workspace</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_delete/","title":"Runai workspace delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_delete/#runai-workspace-delete","title":"runai workspace delete","text":"<p>delete workspace</p> <pre><code>runai workspace delete [WORKSPACE_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_delete/#examples","title":"Examples","text":"<pre><code>runai workspace delete &lt;workspace_name&gt; (optional)-p=&lt;project_name&gt;\nrunai workspace delete --uuid=&lt;workload_uuid&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_describe/","title":"Runai workspace describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_describe/#runai-workspace-describe","title":"runai workspace describe","text":"<p>Describe a training workload</p> <pre><code>runai workspace describe WORKSPACE_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_exec/","title":"Runai workspace exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_exec/#runai-workspace-exec","title":"runai workspace exec","text":"<p>exec management</p> <pre><code>runai workspace exec WORKSPACE_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_exec/#examples","title":"Examples","text":"<pre><code># Execute bush to workspace \nrunai workspace exec jup --tty --stdin -- /bin/bash \n\n# Execute ls to workload\nrunai workspace exec jup -- ls\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --stdin                          Pass stdin to the container\n      --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_list/","title":"Runai workspace list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_list/#runai-workspace-list","title":"runai workspace list","text":"<p>list workspace</p> <pre><code>runai workspace list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_list/#examples","title":"Examples","text":"<pre><code>runai workspace list -A\nrunai workspace list --state=&lt;training_state&gt; --limit=20\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_list/#options","title":"Options","text":"<pre><code>  -A, --all              list jobs from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list, (default 50) (default 50)\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_logs/","title":"Runai workspace logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_logs/#runai-workspace-logs","title":"runai workspace logs","text":"<p>logs management</p> <pre><code>runai workspace logs WORKSPACE_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_logs/#examples","title":"Examples","text":"<pre><code>  # Get logs for a workspace\n  runai workspace logs workspace-01\n\n  # Get logs for a specific pod in a workspace\n  runai workspace logs workspace-01 --pod=workspace-01-0\n\n  # Get logs for a specific container in a workspace\n  runai workspace logs workspace-01 --container=container-01\n\n  # Get the last 100 lines of logs\n  runai workspace logs workspace-01 --tail=100\n\n  # Get logs with timestamps\n  runai workspace logs workspace-01 --timestamps\n\n  # Follow the logs\n  runai workspace logs workspace-01 --follow\n\n  # Get logs for the previous instance of the workspace\n  runai workspace logs workspace-01 --previous\n\n  # GetLimit the logs to 1024 bytes\n  runai workspace logs workspace-01 --limit-bytes=1024\n\n  # Get logs since the last 5 minutes\n  runai workspace logs workspace-01 --since=300s\n\n  # Get logs since a specific timestamp\n  runai workspace logs workspace-01 --since-time=2023-05-30T10:00:00Z\n\n  # Wait up to 30 seconds for workspace to be ready for logs\n  runai workspace logs workspace-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_port-forward/","title":"Runai workspace port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_port-forward/#runai-workspace-port-forward","title":"runai workspace port-forward","text":"<p>port forward management</p> <pre><code>runai workspace port-forward WORKSPACE_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to &lt;workspace-name&gt; on port 8090:\nrunai workspace port-forward &lt;workspace-name&gt; --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to &lt;job-name&gt; on port 8080:\nrunai workspace port-forward &lt;workspace-name&gt; --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to &lt;workload-name&gt; on port 8090 and from localhost:6443 to &lt;workspace-name&gt; on port 443:\nrunai workspace port-forward &lt;workload-name&gt; --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout (default 0s)\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_resume/","title":"Runai workspace resume","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_resume/#runai-workspace-resume","title":"runai workspace resume","text":"<p>resume workspace</p> <pre><code>runai workspace resume [WORKSPACE_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_resume/#examples","title":"Examples","text":"<pre><code>runai workspace resume &lt;workspace_name&gt; -p=&lt;project_name&gt;\nrunai workspace resume --uuid=&lt;workspace_uuid&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_resume/#options","title":"Options","text":"<pre><code>  -h, --help             help for resume\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_resume/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_resume/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_submit/","title":"Runai workspace submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_submit/#runai-workspace-submit","title":"runai workspace submit","text":"<p>submit workspace</p> <pre><code>runai workspace submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_submit/#examples","title":"Examples","text":"<pre><code>runai workspace submit &lt;workspace_name&gt; -p=&lt;project_name&gt; -i=runai.jfrog.io/demo/quickstart\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --mig-profile string                             MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                s3 storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preemptible                                    Interactive preemptible jobs can be scheduled above guaranteed quota but may be reclaimed at any time\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-group int                               Run in the context of the current CLI group rather than the root group\n      --run-as-user int                                Run in the context of the current CLI user rather than the root user\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --supplemental-groups string                     Comma seperated list of groups that the user running the container belongs to, in addition to the group indicated by --run-as-gid\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_suspend/","title":"Runai workspace suspend","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_suspend/#runai-workspace-suspend","title":"runai workspace suspend","text":"<p>suspend workspace</p> <pre><code>runai workspace suspend [WORKSPACE_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_suspend/#examples","title":"Examples","text":"<pre><code>runai workspace &lt;workspace_name&gt; -p=&lt;project_name&gt;\nrunai workspace suspend --uuid=&lt;workspace_uuid&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_suspend/#options","title":"Options","text":"<pre><code>  -h, --help             help for suspend\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_suspend/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH (default \"~/.runai/\")\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_suspend/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/guides/set-kubeconfig-with-oidc-parameters/","title":"Add Run:ai authorization to kubeconfig","text":"<p>The runai kubeconfig set command allows users to configure their kubeconfig file with Run:ai authorization token. This setup enables users to gain access to the Kubernetes (k8s) cluster seamlessly.</p> <p>Note</p> <p>Setting kubeconfig is not required in order to use the CLI. This command is used to enable third-party workloads under Run:ai authorization.</p>"},{"location":"Researcher/cli-reference/new-cli/guides/set-kubeconfig-with-oidc-parameters/#usage","title":"Usage","text":"<p>To set the token (will be fetched automatically) inside the kubeconfig file, run the following command:</p> <pre><code>runai kubeconfig set\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/guides/set-kubeconfig-with-oidc-parameters/#prerequisites","title":"Prerequisites","text":"<p>Before executing the command, ensure that</p> <ol> <li>Cluster authentication is configured and enabled.</li> <li>The user has a kubeconfig file configured.</li> <li>The user is logged in (use the runai login command).</li> </ol>"},{"location":"Researcher/cli-reference/new-cli/guides/set-kubeconfig-with-oidc-parameters/#cluster-configuration","title":"Cluster configuration","text":"<p>To enable cluster authentication, add the following flags to the Kubernetes server API of each cluster:</p> <pre><code>spec:\n  containers:\n  - command:\n    ...\n    - --oidc-client-id=&lt;OIDC_CLIENT_ID&gt;\n    - --oidc-issuer-url=url=https://&lt;HOST&gt;/auth/realms/&lt;REALM&gt;\n    - --oidc-username-prefix=-\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/guides/set-kubeconfig-with-oidc-parameters/#user-kubeconfig-configuration","title":"User Kubeconfig configuration","text":"<p>Add the following to the Kubernetes client configuration file (./kube/config). For the full command reference, see kubeconfig set.  </p> <ul> <li>Make sure to replace values with the actual cluster information and user credentials.  </li> <li>There can be multiple contexts in the kubeconfig file. The command will configure the current context.</li> </ul> <pre><code>apiVersion: v1\nkind: Config\npreferences:\n  colors: true\ncurrent-context: &lt;CONTEXT_NAME&gt;\ncontexts:\n- context:\n    cluster: &lt;CLUSTER_NAME&gt;\n    user: &lt;USER_NAME&gt;\n  name: &lt;CONTEXT_NAME&gt;\nclusters:\n- cluster:\n    server: &lt;CLUSTER_URL&gt;\n    certificate-authority-data: &lt;CLUSTER_CERT&gt;\n  name: &lt;CLUSTER_NAME&gt;\nusers:\n- name: &lt;USER_NAME&gt;\n</code></pre>"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/","title":"GPU Time Slicing Scheduler","text":""},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#new-time-slicing-scheduler-by-runai","title":"New Time-slicing scheduler by Run:ai","text":"<p>To provide customers with predictable and accurate GPU compute resources scheduling, Run:ai is introducing a new feature called Time-slicing GPU scheduler which adds fractional compute capabilities on top of other existing Run:ai memory fractions capabilities. Unlike the default NVIDIA GPU orchestrator which doesn\u2019t provide the ability to split or limit the runtime of each workload, Run:ai created a new mechanism that gives each workload exclusive access to the full GPU for a limited amount of time (lease time) in each scheduling cycle (plan time). This cycle repeats itself for the lifetime of the workload.</p> <p>Using the GPU runtime this way guarantees a workload is granted its requested GPU compute resources proportionally to its requested GPU fraction.</p> <p>Run:ai offers two new Time-slicing modes:</p> <ol> <li>Strict\u2014each workload gets its precise GPU compute fraction, which equals to its requested GPU (memory) fraction. In terms of official Kubernetes resource specification, this means:</li> </ol> <pre><code>gpu-compute-request = gpu-compute-limit = gpu-(memory-)fraction\n</code></pre> <ol> <li>Fair\u2014each workload is guaranteed at least its GPU compute fraction, but at the same time can also use additional GPU runtime compute slices that are not used by other idle workloads. Those excess time slices are divided equally between all workloads running on that GPU (after each got at least its requested GPU compute fraction). In terms of official Kubernetes resource specification, this means:</li> </ol> <pre><code>gpu-compute-request = gpu-(memory-)fraction\n\ngpu-compute-limit = 1.0\n</code></pre> <p>The figure below illustrates how Strict time-slicing mode is using the GPU from Lease (slice) and Plan (cycle) perspective:</p> <p></p> <p>The figure below illustrates how Fair time-slicing mode is using the GPU from Lease (slice) and Plan (cycle) perspective:</p> <p></p>"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#setting-the-time-slicing-scheduler-policy","title":"Setting the Time-slicing scheduler policy","text":"<p>Time-slicing is a cluster flag which changes the default behavior of Run:ai GPU fractions feature.</p> <p>Enable time-slicing by setting the following cluster flag in the <code>runaiconfig</code> file:</p> <pre><code>global: \n    core: \n        timeSlicing: \n            mode: fair/strict\n</code></pre> <p>If the <code>timeSlicing</code> flag is not set, the system continues to use the default NVidia GPU orchestrator to maintain backward compatability.</p>"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#time-slicing-plan-and-lease-times","title":"Time-slicing Plan and Lease Times","text":"<p>Each GPU scheduling cycle is a plan, the plan time is determined by the lease time and granularity (precision). By default, basic lease time is 250ms with 5% granularity (precision), which means the plan (cycle) time is: 250 / 0.05 = 5000ms (5 Sec). Using these values, a workload that asked to get gpu-fraction=0.5 gets 2.5s runtime out of 5s cycle time.</p> <p>Different workloads requires different SLA and precision, so it also possible to tune the lease time and precision for customizing the time-slicing capabilities to your cluster.</p> <p>Note</p> <p>Decreasing the lease time makes time-slicing less accurate. Increasing the lease time make the system more accurate, but each workload is less responsive.</p> <p>Once timeSlicing is enabled, all submitted GPU fraction or GPU memory workloads will have their gpu-compute-request\\limit set automatically by the system, depending on the annotation used on the timeSlicing mode:</p>"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#strict-compute-resources","title":"Strict Compute Resources","text":"Annotation Value GPU Compute Request GPU Compute Limit <code>gpu-fraction</code> x x x <code>gpu-memory</code> x 0 1.0"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#fair-compute-resources","title":"Fair Compute Resources","text":"Annotation Value GPU Compute Request GPU Compute Limit <code>gpu-fraction</code> x x 1.0 <code>gpu-memory</code> x 0 1.0 <p>Note</p> <p>The above tables show that when submitting a workload using gpu-memory annotation, the system will split the GPU compute time between the different workloads running on that GPU. This means the workload can get anything from very little compute time (&gt;0) to full GPU compute time (1.0).</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/","title":"Introduction","text":"<p>When we discuss the allocation of deep learning compute resources, the discussion tends to focus on GPUs as the most critical resource. But two additional resources are no less important:</p> <ul> <li>CPUs. Mostly needed for preprocessing and postprocessing tasks during a deep learning training run.</li> <li>Memory. Has a direct influence on the quantities of data a training run can process in batches.</li> </ul> <p>GPU servers tend to come installed with a significant amount of memory and CPUs.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#requesting-cpu-memory","title":"Requesting CPU &amp; Memory","text":"<p>When submitting a Job, you can request a guaranteed amount of CPUs and memory by using the --cpu and --memory flags in the runai submit command. For example:</p> <pre><code>runai submit job1 -i ubuntu --gpu 2 --cpu 12 --memory 1G\n</code></pre> <p>The system guarantees that if the Job is scheduled, you will be able to receive this amount of CPU and memory.</p> <p>For further details on these flags see: runai submit</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#cpu-over-allocation","title":"CPU over allocation","text":"<p>The number of CPUs your Job will receive is guaranteed to be the number defined using the <code>--cpu</code> flag. In practice, however, you may receive more CPUs than you have asked for:</p> <ul> <li>If you are currently alone on a node, you will receive all the node CPUs until such time when another workload has joined.</li> <li>However, when a second workload joins, each workload will receive a number of CPUs proportional to the number requested via the <code>--cpu</code> flag. For example, if the first workload asked for 1 CPU and the second for 3 CPUs, then on a node with 40 cpus, the workloads will receive 10 and 30 CPUs respectively. If the flag <code>--cpu</code> is not specified, it will be taken from the cluster default (see the section below)</li> </ul>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#memory-over-allocation","title":"Memory over allocation","text":"<p>The amount of Memory your Job will receive is guaranteed to be the number defined using the --memory flag. In practice, however, you may receive more memory than you have asked for. This is along the same lines as described with CPU over allocation above.</p> <p>It is important to note, however, that if you have used this memory over-allocation, and new workloads have joined, your Job may receive an out-of-memory exception and terminate.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#cpu-and-memory-limits","title":"CPU and Memory limits","text":"<p>You can limit your Job's allocation of CPU and memory by using the --cpu-limit and --memory-limit flags in the runai submit command. For example:</p> <pre><code>runai submit job1 -i ubuntu --gpu 2 --cpu 12 --cpu-limit 24 \\\n    --memory 1G --memory-limit 4G\n</code></pre> <p>The limit behavior is different for CPUs and memory.</p> <ul> <li>Your Job will never be allocated with more than the amount stated in the <code>--cpu-limit</code> flag</li> <li>If your Job tries to allocate more than the amount stated in the <code>--memory-limit</code> flag it will receive an out-of-memory exception.</li> </ul> <p>The limit (for both CPU and memory) overrides the cluster default described in the section below</p> <p>For further details on these flags see: runai submit</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#flag-defaults","title":"Flag Defaults","text":""},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-cpu-flag","title":"Defaults for --cpu flag","text":"<p>If your Job has not specified <code>--cpu</code>, the system will use a default. The default is cluster-wide and is defined as a ratio of GPUs to CPUs.</p> <p>If, for example, the default has been defined as 1:6 and your Job has specified <code>--gpu 2</code> and has not specified <code>--cpu</code>, then the implied <code>--cpu</code> flag value is 12 CPUs.</p> <p>The system comes with a cluster-wide default of 1:1. To change the ratio see below.</p> <p>If you didn't request any GPUs for your job and has not specified <code>--cpu</code>, the default is defined as a ratio of CPU limit to CPUs.</p> <p>If, for example, the default has been defined as 1:0.2 and your Job has specified <code>--cpu-limit 10</code> and has not specified <code>--cpu</code>, then the implied <code>--cpu</code> flag value is 2 CPUs.</p> <p>The system comes with a cluster-wide default of 1:0.1. To change the ratio see below.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-memory-flag","title":"Defaults for --memory flag","text":"<p>If your Job has not specified <code>--memory</code>, the system will use a default. The default is cluster-wide and is proportional to the number of requested GPUs.</p> <p>The system comes with a cluster-wide default of 100MiB of allocated CPU memory per GPU. To change the ratio see below.</p> <p>If you didn't request any GPUs for your job and has not specified <code>--memory</code>, the default is defined as a ratio of CPU Memory limit to CPU Memory Request.</p> <p>The system comes with a cluster-wide default of 1:0.1. To change the ratio see below.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-cpu-limit-flag","title":"Defaults for --cpu-limit flag","text":"<p>If your Job has not specified <code>--cpu-limit</code>, then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to CPUs. See below on how to change the ratio.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-memory-limit-flag","title":"Defaults for --memory-limit flag","text":"<p>If your Job has not specified <code>--memory-limit</code>, then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to Memory. See below on how to change the ratio.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#changing-the-ratios","title":"Changing the ratios","text":"<p>To change the cluster wide-ratio use the following process. The example shows:</p> <ul> <li>a CPU request with a default ratio of 2:1 CPUs to GPUs.</li> <li>a CPU Memory request with a default ratio of 200MB per GPU.</li> <li>a CPU limit with a default ratio of 4:1 CPU to GPU.</li> <li>a Memory limit with a default ratio of 2GB per GPU.</li> <li>a CPU request with a default ratio of 0.1 CPUs per 1 CPU limit.</li> <li>a CPU Memory request with a default ratio of 0.1:1 request per CPU Memory limit.</li> </ul> <p>You must edit the cluster installation values file:</p> <ul> <li>When installing the Run:ai cluster, edit the values file.</li> <li>On an existing installation, use the upgrade cluster instructions to modify the values file.</li> <li>You must specify at least the first 4 values as follows:</li> </ul> <pre><code>runai-operator:\n  config:\n    limitRange:\n      cpuDefaultRequestGpuFactor: 2\n      memoryDefaultRequestGpuFactor: 200Mi\n      cpuDefaultLimitGpuFactor: 4\n      memoryDefaultLimitGpuFactor: 2Gi\n      cpuDefaultRequestCpuLimitFactorNoGpu: 0.1\n      memoryDefaultRequestMemoryLimitFactorNoGpu: 0.1\n</code></pre>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#validating-cpu-memory-allocations","title":"Validating CPU &amp; Memory Allocations","text":"<p>To review CPU &amp; Memory allocations you need to look into Kubernetes. A Run:ai Job creates a Kubernetes pod. The pod declares its resource requests and limits. To see the memory and CPU consumption in Kubernetes:</p> <ul> <li>Get the pod name for the Job by running:</li> </ul> <p><code>runai describe job &lt;JOB_NAME&gt;</code></p> <p>the pod will appear under the <code>PODS</code> category.</p> <ul> <li>Run:</li> </ul> <p><code>kubectl describe pod &lt;POD_NAME&gt;</code></p> <p>The information will appear under <code>Requests</code> and <code>Limits</code>. For example:</p> <pre><code>Limits:\n    nvidia.com/gpu:  2\nRequests:\n    cpu:             1\n    memory:          104857600\n    nvidia.com/gpu:  2\n</code></pre>"},{"location":"Researcher/scheduling/dynamic-gpu-fractions/","title":"Dynamic GPU Fractions","text":""},{"location":"Researcher/scheduling/dynamic-gpu-fractions/#introduction","title":"Introduction","text":"<p>Many AI workloads are using GPU resources intermittently and sometimes these resources are not used at all. These AI workloads need these resources when they are running AI applications, or debugging a model in development. Other workloads such as Inference, might be using GPU resources at a lower utilization rate than requested, and may suddenly ask for higher guaranteed resources at peak utilization times.</p> <p>This pattern of resource request vs. actual resource utilization causes lower utilization of GPUs. This mainly happens if there are many workloads requesting resources to match their peak demand, even though the majority of the time they operate far below that peak.</p> <p>Run:ai has introduced Dynamic GPU fractions in v2.15 to cope with resource request vs. actual resource utilization which enables users to optimize GPU resource usage.</p> <p>Dynamic GPU fractions is part of Run:ai's core capabilities to enable workloads to optimize the use of GPU resources. This works by providing the ability to specify and consume GPU memory and compute resources dynamically by leveraging Kubernetes Request and Limit notations.</p> <p>Dynamic GPU fractions allow a workload to request a guaranteed fraction of GPU memory or GPU compute resource (similar to a Kubernetes request), and at the same time also request the ability to grow beyond that guaranteed request up to a specific limit (similar to a Kubernetes limit), if the resources are available.</p> <p>For example, with Dynamic GPU Fractions, a user can specify a workload with a GPU fraction Request of 0.25 GPU, and add the parameter <code>gpu-fraction-limit</code> of up to 0.80 GPU. The cluster/node-pool scheduler schedules the workload to a node that can provide the GPU fraction request (0.25), and then assigns the workload to a GPU. The GPU scheduler monitors the workload and allows it to occupy memory between 0 to 0.80 of the GPU memory (based on the parameter <code>gpu-fraction-limit</code>), where only 0.25 of the GPU memory is guaranteed to that workload. The rest of the memory (from 0.25 to 0.8) is \u201cloaned\u201d to the workload, as long as it is not needed by other workloads.</p> <p>Run:ai automatically manages the state changes between <code>request</code> and <code>Limit</code> as well as the reverse (when the balance need to be \"returned\"), updating the metrics and workloads' states and graphs.</p>"},{"location":"Researcher/scheduling/dynamic-gpu-fractions/#setting-fractional-gpu-memory-limit","title":"Setting Fractional GPU Memory Limit","text":"<p>With the fractional GPU memory limit, users can submit workloads using GPU fraction <code>Request</code> and <code>Limit</code>.</p> <p>You can either:</p> <ol> <li> <p>Use a GPU Fraction parameter (use the <code>gpu-fraction</code> annotation)</p> <p>or</p> </li> <li> <p>Use an absolute GPU Memory parameter (<code>gpu-memory</code> annotation)</p> </li> </ol> <p>When setting a GPU memory limit either as GPU fraction, or GPU memory size, the <code>Limit</code> must be equal or greater than the GPU fraction memory request.</p> <p>Both GPU fraction and GPU memory are translated into the actual requested memory size of the Request (guaranteed resources) and the Limit (burstable resources).</p> <p>To guarantee fair quality of service between different workloads using the same GPU, Run:ai developed an extendable GPU <code>OOMKiller</code> (Out Of Memory Killer) component that guarantees the quality of service using Kubernetes semantics for resources Request and Limit.</p> <p>The <code>OOMKiller</code> capability requires adding <code>CAP_KILL</code> capabilities to the Dynamic GPU fraction and to the Run:ai core scheduling module (toolkit daemon). This capability is disabled by default.</p> <p>To change the state of Dynamic GPU Fraction in the cluster, edit the <code>runaiconfig</code> file and set:</p> <pre><code>spec: \n  global: \n    core: \n      dynamicFraction: \n        enabled: true # Boolean field default is true.\n</code></pre> <p>To set the gpu memory limit per workload, add the <code>RUNAI_GPU_MEMORY_LIMIT</code> environment variable to the first container in the pod. This is the GPU consuming container.</p> <p>To use <code>RUNAI_GPU_MEMORY_LIMIT</code> environment variable:</p> <ol> <li> <p>Submit a workload yaml directly, and set the <code>RUNAI_GPU_MEMORY_LIMIT</code> environment variable.</p> </li> <li> <p>Create a policy, per Project or globally. For example, set all Interactive workloads of <code>Project=research_vision1</code> to always set the environment variable of <code>RUNAI_GPU_MEMORY_LIMIT</code> to 1.</p> </li> <li> <p>Pass the environment variable through the CLI or the UI.</p> </li> </ol> <p>The supported values depend on the label used. You can use them in either the UI or the CLI. Use only one of the variables in the following table (they cannot be mixed):</p> Variable Input format <code>gpu-fraction</code> A fraction value (for example: 0.25, 0.75). <code>gpu-memory</code> Kubernetes resources quantity which must be larger than <code>gpu-memory</code>. For example, 500000000, 2500M, 4G. NOTE: The <code>gpu-memory</code> label values are always in MB, unlike the env variable."},{"location":"Researcher/scheduling/dynamic-gpu-fractions/#compute-resources-ui-with-dynamic-fractions-support","title":"Compute Resources UI with Dynamic Fractions support","text":"<p>To enable the UI elements for Dynamic Fractions, press Settings, General, then open the Resources pane and toggle GPU Resource Optimization. This enables all the UI features related to GPU Resource Optimization for the whole tenant. There are other per cluster or per node-pool configurations that should be configured in order to use the capabilities of \u2018GPU Resource Optimization\u2019 See the documentation for each of these features. Once the \u2018GPU Resource Optimization\u2019 feature is enabled, you will be able to create Compute Resources with the GPU Portion (Fraction) Limit and GPU Memory Limit. In addition, you will be able to view the workloads\u2019 utilization vs. Request and Limit parameters in the Metrics pane for each workload.</p> <p></p> <p>Note</p> <p>To use Dynamic Fractions, GPU devices per pod must be equal to 1. If more than 1 GPU device is used per pod, or if a MIG profile is selected, Dynamic Fractions cannot be used for that Compute Resource (and any related pods).</p> <p>Note</p> <p>When setting a workload with Dynamic Fractions, (for example, when using it with GPU Request or GPU memory Limits), you practically make the workload burstable. This means it can use memory that is not guaranteed for that workload and is susceptible to an \u2018OOM Kill\u2019 signal if the actual owner of that memory requires it back. This applies to non-preemptive workloads as well. For that reason, its recommended that you use Dynamic Fractions with Interactive workloads running Notebooks. Notebook pods are not evicted when their GPU process is OOM Kill\u2019ed. This behavior is the same as standard Kubernetes burstable CPU workloads.</p>"},{"location":"Researcher/scheduling/fractions/","title":"Allocation of GPU Fractions","text":""},{"location":"Researcher/scheduling/fractions/#introduction","title":"Introduction","text":"<p>A single GPU has a significant amount of memory. Ranging from a couple of gigabytes in older generations and up to 80GB per GPU in the later models of the latest NVIDIA GPU technology. A single GPU also has a vast amount of computing power. </p> <p>This amount of memory and computing power is important for processing large amounts of data, such as in training deep learning models. However, there are quite a few applications that do not need this power. Examples can be inference workloads and the model-creation phase. It would thus be convenient if we could divide up a GPU between various workloads, thus achieving better GPU utilization. </p> <p>This article describes two complementary technologies that allow the division of GPUs and how to use them with Run:ai.</p> <ol> <li>Run:ai Fractions. </li> <li>Dynamic allocation using NVIDIA Multi-instance GPU (MIG)</li> </ol>"},{"location":"Researcher/scheduling/fractions/#runai-fractions","title":"Run:ai Fractions","text":"<p>Run:ai provides the capability to allocate a container with a specific amount of GPU RAM. As a researcher, if you know that your code needs 4GB of RAM. You can submit a job using the flag <code>--gpu-memory 4G</code> to specify the exact portion of the GPU memory that you need. Run:ai will allocate your container that specific amount of GPU RAM. Attempting to reach beyond your allotted RAM will result in an out-of-memory exception. </p> <p>You can also use the flag <code>--gpu 0.2</code> to get 20% of the GPU memory on the GPU assigned for you. </p> <p>For more details on Run:ai fractions see the fractions quickstart.</p> <p>Limitation</p> <p>With the fraction technology all running workloads, which utilize the GPU, share the compute in parallel and on average get an even share of the compute. For example, assuming two containers, one with 0.25 GPU workload and the other with 0.75 GPU workload - both will get (on average) an equal part of the computation power. If one of the workloads does not utilize the GPU, the other workload will get the entire GPU's compute power.</p> <p>Info</p> <p>For interoperability with other Kubernetes schedulers, Run:ai creates special reservation pods. Once a workload has been allocated a fraction of a GPU, Run:ai will create a pod in a dedicated <code>runai-reservation</code> namespace with the full GPU as a resource. This would cause other schedulers to understand that the GPU is reserved.    </p>"},{"location":"Researcher/scheduling/fractions/#dynamic-mig","title":"Dynamic MIG","text":"<p>NVIDIA MIG allows GPUs based on the NVIDIA Ampere architecture (such as NVIDIA A100) to be partitioned into separate GPU Instances:</p> <ul> <li>When divided, the portion acts as a fully independent GPU.</li> <li>The division is static, in the sense that you have to call NVIDIA API or the <code>nvidia-smi</code> command to create or remove the MIG partition. </li> <li>The division is both of compute and memory.</li> <li>The division has fixed sizes.  Up to 7 units of compute and memory in fixed sizes. The various MIG profiles can be found in the NVIDIA documentation. A typical profile can be <code>MIG 2g.10gb</code> which provides 2/7 of the compute power and 10GB of RAM</li> <li>Reconfiguration of MIG profiles on the GPU requires administrator permissions and the draining of all running workloads. </li> </ul> <p>Run:ai provides a way to dynamically create a MIG partition:</p> <ul> <li>Using the same experience as the Fractions technology above, if you know that your code needs 4GB of RAM. You can use the flag <code>--gpu-memory 4G</code> to specify the portion of the GPU memory that you need. Run:ai will call the NVIDIA MIG API to generate the smallest possible MIG profile for your request, and allocate it to your container. </li> <li>MIG is configured on the fly according to workload demand, without needing to drain workloads or to involve an IT administrator.</li> <li>Run:ai will automatically deallocate the partition when the workload finishes. This happens in a lazy fashion in the sense that the partition will not be removed until the scheduler decides that it is needed elsewhere. </li> <li>Run:ai provides an additional flag to dynamically create the specific MIG partition in NVIDIA terminology. As such, you can specify <code>--mig-profile 2g.10gb</code>.  </li> <li>In a single GPU cluster you have some MIG nodes that are dynamically allocated and some that are not.</li> </ul> <p>For more details on Run:ai fractions see the dynamic MIG quickstart.</p>"},{"location":"Researcher/scheduling/fractions/#setting-up-dynamic-mig","title":"Setting up Dynamic MIG","text":"<p>As described above, MIG is only available in the latest NVIDIA architecture. </p> <ul> <li>When working with Kubernetes, NVIDIA defines a concept called MIG Strategy. With Run:ai you must set the MIG strategy to <code>mixed</code>. See NVIDIA prerequisites on how to set this flag. </li> <li> <p>The administrator needs to specifically enable dynamic MIG on the node by running: </p> <p><pre><code>runai-adm set node-role --dynamic-mig-enabled &lt;node-name&gt;\n</code></pre> (use <code>runai-adm remove</code> to unset)</p> </li> <li> <p>Make sure that MIG is enabled on the node level by running <code>nvidia-smi</code> on the node and verifying that MIG Mode is enabled (see highlight below):</p> </li> </ul> <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                   On |\n| N/A   32C    P0    42W / 400W |      0MiB / 40536MiB |     N/A      Default |\n|                               |                      |              Enabled |\n+-------------------------------+----------------------+----------------------+\n</code></pre> <ul> <li> <p>To enable MIG Mode see NVIDIA documentation.</p> </li> <li> <p>Set:     <pre><code>kubectl label node &lt;node-name&gt; node-role.kubernetes.io/runai-mig-enabled=true\n</code></pre>    (use <code>kubectl</code> to unset)</p> </li> </ul> <p>Limitations</p> <ul> <li>Once a node has been marked as dynamic MIG enabled, it can only be used via the Run:ai scheduler.</li> <li>Run:ai currently supports H100 or A100 nodes with 40GB/80GB RAM.</li> <li>GPU utilization, shown on the Run:ai dashboards, may not be accurate while MIG jobs are running.</li> </ul>"},{"location":"Researcher/scheduling/fractions/#mixing-fractions-and-dynamic-mig","title":"Mixing Fractions and Dynamic MIG","text":"<p>Given a specific node, the IT administrator can decide whether to use one technology or the other. When the Researcher asks for a specific amount of GPU memory, Run:ai will either provide it on an annotated node by dynamically allocating a MIG partition, or use a different node using the fractions technology.</p>"},{"location":"Researcher/scheduling/fractions/#see-also","title":"See Also","text":"<ul> <li>Fractions quickstart.</li> <li>Dynamic MIG quickstart</li> </ul>"},{"location":"Researcher/scheduling/gpu-memory-swap/","title":"GPU Memory SWAP","text":""},{"location":"Researcher/scheduling/gpu-memory-swap/#introduction","title":"Introduction","text":"<p>To ensure efficient and effective usage of an organization\u2019s resources, Run:ai provides multiple features on multiple layers to help administrators and practitioners maximize their existing GPUs resource utilization.</p> <p>Run:ai\u2019s GPU memory swap feature helps administrators and AI practitioners to further increase the utilization of existing GPU hardware by improving GPU sharing between AI initiatives and stakeholders. This is done by expanding the GPU physical memory to the CPU memory which is typically an order of magnitude larger than that of the GPU.</p> <p>Expanding the GPU physical memory, helps the Run:ai system to put more workloads on the same GPU physical hardware, and to provide a smooth workload context switching between GPU memory and CPU memory, eliminating the need to kill workloads when the memory requirement is larger than what the GPU physical memory can provide.</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#benefits-of-gpu-memory-swap","title":"Benefits of GPU memory swap","text":"<p>There are several use cases where GPU memory swap can benefit and improve the user experience and the system's overall utilization:</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#sharing-a-gpu-between-multiple-interactive-workloads-notebooks","title":"Sharing a GPU between multiple interactive workloads (notebooks)","text":"<p>AI practitioners use notebooks to develop and test new AI models and to improve existing AI models. While developing or testing an AI model, notebooks use GPU resources intermittently, yet, required resources of the GPU\u2019s are pre-allocated by the notebook and cannot be used by other workloads after one notebook has already reserved them. To overcome this inefficiency, Run:ai introduced Dynamic Fractions and Node Level Scheduler.</p> <p>When one or more workloads require more than their requested GPU resources, there\u2019s a high probability not all workloads can run on a single GPU because the total memory required is larger than the physical size of the GPU memory.</p> <p>With GPU memory swap, several workloads can run on the same GPU, even if the sum of their used memory is larger than the size of the physical GPU memory. GPU memory swap can swap in and out workloads interchangeably, allowing multiple workloads to each use the full amount of GPU memory. The most common scenario is for one workload to run on the GPU (for example, an interactive notebook),while other notebooks are either idle or using the CPU to develop new code (while not using the GPU). From a user experience point of view, the swap in and out is a smooth process since the notebooks do not notice that they are being swapped in and out of the GPU memory. On rare occasions, when multiple notebooks need to access the GPU simultaneously, slower workload execution may be experienced.</p> <p>Notebooks typically use the GPU intermittently, therefore with high probability, only one workload (for example, an interactive notebook), will use the GPU at a time. The more notebooks the system puts on a single GPU, the higher the chances are that there will be more than one notebook requiring the GPU resources at the same time. Admins have a significant role here in fine tuning the number of notebooks running on the same GPU, based on specific use patterns and required SLAs. Using \u2018Node Level Scheduler\u2019 reduces GPU access contention between different interactive notebooks running on the same node.</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#sharing-a-gpu-between-inferenceinteractive-workloads-and-training-workloads","title":"Sharing a GPU between inference/interactive workloads and training workloads","text":"<p>A single GPU can be shared between an interactive or inference workload (for example, a Jupyter notebook, image recognition services, or an LLM service), and a training workload that is not time-sensitive or delay-sensitive. At times when the inference/interactive workload uses the GPU, both training and inference/interactive workloads share the GPU resources, each running part of the time swapped-in to the GPU memory, and swapped-out into the CPU memory the rest of the time.</p> <p>Whenever the inference/interactive workload stops using the GPU, the swap mechanism swaps out the inference/interactive workload GPU data to the CPU memory. Kubernetes wise, the POD is still alive and running using the CPU. This allows the training workload to run faster when the inference/interactive workload is not using the GPU, and slower when it does, thus sharing the same resource between multiple workloads, fully utilizing the GPU at all times, and maintaining uninterrupted service for both workloads.</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#serving-inference-warm-models-with-gpu-memory-swap","title":"Serving inference warm models with GPU memory swap","text":"<p>Running multiple inference models is a demanding task and you will need to ensure that your SLA is met. You need to provide high performance and low latency, while maximizing GPU utilization. This becomes even more challenging when the exact model usage patterns are unpredictable. You must plan for the agility of inference services and strive to keep models on standby in a ready state rather than an idle state.</p> <p>Run:ai\u2019s GPU memory swap feature enables you to load multiple models to a single GPU, where each can use up to the full amount GPU memory. Using an application load balancer, the administrator can control to which server each inference request is sent. Then the GPU can be loaded with multiple models, where the model in use is loaded into the GPU memory and the rest of the models are swapped-out to the CPU memory. The swapped models are stored as ready models to be loaded when required. GPU memory swap always maintains the context of the workload (model) on the GPU so it can easily and quickly switch between models. This is unlike industry standard model servers that load models from scratch into the GPU whenever required.</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#configuring-memory-swap","title":"Configuring memory swap","text":"<p>Perquisites\u2014before configuring the GPU Memory Swap the administrator must configure the Dynamic Fractions feature, and optionally configure the Node Level Scheduler feature. </p> <p>The first enables you to make your workloads burstable, and both features will maximize your workloads\u2019 performance and GPU utilization within a single node.</p> <p>To enable GPU memory swap in a Run:aAi cluster, the administrator must update the <code>runaiconfig</code> file with the following parameters:</p> <pre><code>spec: \n global: \n   core: \n     swap:\n       enabled: true\n       limits:\n         cpuRam: 100Gi\n</code></pre> <p>The example above uses <code>100Gi</code> as the size of the swap memory.</p> <p>You can also use the <code>patch</code> command from your terminal:</p> <pre><code>kubectl patch -n runai runaiconfigs.run.ai/runai --type='merge' --patch '{\"spec\":{\"global\":{\"core\":{\"swap\":{\"enabled\": true, \"limits\": {\"cpuRam\": \"100Gi\"}}}}}}'\n</code></pre> <p>To make a workload swappable, a number of conditions must be met:</p> <ol> <li> <p>The workload MUST use Dynamic Fractions. This means the workload\u2019s memory request is less than a full GPU, but it may add a GPU memory limit to allow the workload to effectively use the full GPU memory.</p> </li> <li> <p>The administrator must label each node that they want to provide GPU memory swap with a <code>run.ai/swap-enabled=true</code> this enables the feature on that node. Enabling the feature reserves CPU memory to serve the swapped GPU memory from all GPUs on that node. The administrator sets the size of the CPU reserved RAM memory using the runaiconfigs file.</p> </li> <li> <p>Optionally, configure Node Level Scheduler. Using node level scheduler can help in the following ways:</p> <ul> <li>The Node Level Scheduler automatically spreads workloads between the different GPUs on a node, ensuring maximum workload performance and GPU utilization.</li> <li>In scenarios where Interactive notebooks are involved, if the CPU reserved memory for the GPU swap is full, the Node Level Scheduler preempts the GPU process of that workload and potentially routes the workload to another GPU to run.</li> </ul> </li> </ol>"},{"location":"Researcher/scheduling/gpu-memory-swap/#configure-system-reserved-gpu-resources","title":"Configure <code>system reserved</code> GPU Resources","text":"<p>Swappable workloads require reserving a small part of the GPU for non-swappable allocations like binaries and GPU context. To avoid getting out-of-memory (OOM) errors due to non-swappable memory regions, the system reserves a 2GiB of GPU RAM memory by default, effectively truncating the total size of the GPU memory. For example, a 16GiB T4 will appear as 14GiB on a swap-enabled node. The exact reserved size is application-dependent, and 2GiB is a safe assumption for 2-3 applications sharing and swapping on a GPU. This value can be changed by editing the <code>runaiconfig</code> specification as follows:</p> <pre><code>spec: \n global: \n   core: \n     swap:\n       limits:\n         reservedGpuRam: 2Gi\n</code></pre> <p>You can also use the <code>patch</code> command from your terminal:</p> <pre><code>kubectl patch -n runai runaiconfigs.run.ai/runai --type='merge' --patch '{\"spec\":{\"global\":{\"core\":{\"swap\":{\"limits\":{\"reservedGpuRam\": &lt;quantity&gt;}}}}}}'\n</code></pre> <p>This configuration is in addition to the Dynamic Fractions configuration, and optional Node Level Scheduler configuration.</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#preventing-your-workloads-from-getting-swapped","title":"Preventing your workloads from getting swapped","text":"<p>If you prefer your workloads not to be swapped into CPU memory, you can specify on the pod an anti-affinity to <code>run.ai/swap-enabled=true</code> node label when submitting your workloads and the Scheduler will ensure not to use swap-enabled nodes.</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#known-limitations","title":"Known Limitations","text":"<ul> <li>A pod created before the GPU memory swap feature was eneabled in that cluster, cannot be scheduled to a swap-enabled node. A proper event is generated in case no matching node is found. Users must re-submit those pods to make them swap-enabled.</li> <li>GPU memory swap cannot be enabled if fairshare time-slicing or strict time-slicing is used, GPU memory swap can only be used with the default time-slicing mechanism.</li> <li>CPU RAM size cannot be decreased once GPU memory swap is enabled.</li> </ul>"},{"location":"Researcher/scheduling/gpu-memory-swap/#what-happens-when-the-cpu-reserved-memory-for-gpu-swap-is-exhausted","title":"What happens when the CPU reserved memory for GPU swap is exhausted?","text":"<p>CPU memory is limited, and since a single CPU serves multiple GPUs on a node, this number is usually between 2 to 8. For example, when using 80GB of GPU memory, each swapped workload consumes up to 80GB (but may use less) assuming each GPU is shared between 2-4 workloads. In this example, you can see how the swap memory can become very large. Therefore, we give administrators a way to limit the size of the CPU reserved memory for swapped GPU memory on each swap enabled node.</p> <p>Limiting the CPU reserved memory means that there may be scenarios where the GPU memory cannot be swapped out to the CPU reserved RAM. Whenever the CPU reserved memory for swapped GPU memory is exhausted, the workloads currently running will not be swapped out to the CPU reserved RAM, instead, Node Level Scheduler and Dynamic Fractions logic takes over and provides GPU resource optimization.see Dynamic Fractions and Node Level Scheduler.</p>"},{"location":"Researcher/scheduling/job-statuses/","title":"Job Statuses","text":""},{"location":"Researcher/scheduling/job-statuses/#introduction","title":"Introduction","text":"<p>The runai submit function and its sibling the runai submit-dist mpi function submit Run:ai Jobs for execution.</p> <p>A Job has a status. Once a Job is submitted it goes through several statuses before ending in an End State. Most of these statuses originate in the underlying Kubernetes infrastructure, but some are Run:ai-specific. </p> <p>The purpose of this document is to explain these statuses as well as the lifecycle of a Job. </p>"},{"location":"Researcher/scheduling/job-statuses/#successful-flow","title":"Successful Flow","text":"<p>A regular, training Job that has no errors and executes without preemption would go through the following statuses:</p> <p></p> <ul> <li>Pending - the Job is waiting to be scheduled.</li> <li>ContainerCreating - the Job has been scheduled, the Job docker image is now downloading.</li> <li>Running - the Job is now executing.</li> <li>Succeeded - the Job has finished with exit code 0 (success).</li> </ul> <p>The Job can be preempted, in which case it can go through other statuses:</p> <ul> <li>Terminating - the Job is now being preempted.</li> <li>Pending - the Job is waiting in queue again to receive resources.</li> </ul> <p>An interactive Job, by definition, needs to be closed by the Researcher and will thus never reach the Succeeded status. Rather, it would be moved by the Researcher to status Deleted.</p> <p>For a further explanation of the additional statuses, see the table below.</p>"},{"location":"Researcher/scheduling/job-statuses/#error-flow","title":"Error flow","text":"<p>A regular, training Job may encounter an error inside the running process (exit code is non-zero). In which case the following will happen:</p> <p></p> <p>The Job enters an Error status and then immediately tries to reschedule itself for another attempted run. The reschedule can happen on another node in the system. After a specified number of retries, the Job will enter a final status of Fail</p> <p>An interactive Job, enters an Error status and then moves immediately to CrashLoopBackOff trying to reschedule itself. The reschedule attempt has no 'back-off' limit and will continue to retry indefinitely </p> <p></p> <p>Jobs may be submitted with an image that cannot be downloaded. There are special statuses for such Jobs. See table below </p>"},{"location":"Researcher/scheduling/job-statuses/#status-table","title":"Status Table","text":"<p>Below is a list of statuses. For each status the list shows:</p> <ul> <li> <p>Name</p> </li> <li> <p>End State - this status is the final status in the lifecycle of the Job</p> </li> <li> <p>Resource Allocation - when the Job is in this status, does the system allocate resources to it</p> </li> <li> <p>Description</p> </li> <li> <p>Color - Status color as can be seen in the Run:ai User Interface Job list</p> </li> </ul> <p>Status</p> <p>End State</p> <p>Resource Allocation</p> <p>Description</p> <p>Color</p> <p>Running</p> <p></p> <p>Yes</p> <p>Job is running successfully</p> <p></p> <p>Terminating</p> <p></p> <p>Yes</p> <p>Pod is being evicted at the moment (e.g. due to an over-quota allocation, the reason will be written once eviction finishes). A new pod will be created shortly</p> <p></p> <p>ContainerCreating</p> <p></p> <p>Yes</p> <p>Image is being pulled from registry.</p> <p></p> <p>Pending</p> <p></p> <p>-</p> <p>Job is pending. Possible reasons:</p> <p>- Not enough resources</p> <p>- Waiting in Queue (over-quota etc).</p> <p></p> <p>Succeeded</p> <p>Yes</p> <p>-</p> <p>An Unattended (training) Job has ran and finished successfully.</p> <p></p> <p>Deleted</p> <p>Yes</p> <p>-</p> <p>Job has been deleted.</p> <p></p> <p>TimedOut</p> <p>Yes</p> <p>-</p> <p>Interactive Job has reached the defined timeout of the project.</p> <p></p> <p>Preempted</p> <p>Yes</p> <p>-</p> <p>Interactive preemptible Job has been evicted.</p> <p></p> <p>ContainerCannotRun</p> <p>Yes</p> <p>-</p> <p>Container has failed to start running. This is typically a problem within the docker image itself.</p> <p></p> <p>Error</p> <p></p> <p>Yes for interactive only </p> <p>The Job has returned an exit code different than zero. It is now waiting for another run attempt (retry).</p> <p></p> <p>Fail</p> <p>Yes</p> <p>-</p> <p>Job has failed after a number of retries (according to \"--backoffLimit\" field) and will not be trying again.</p> <p></p> <p>CrashLoopBackOff</p> <p></p> <p>Yes</p> <p>Interactive Only: During backoff after Error, before a retry attempt to run pod on the same node.</p> <p></p> <p>ErrImagePull, ImagePullBackOff</p> <p></p> <p>Yes</p> <p>Failing to retrieve docker image</p> <p></p> <p>Unknown</p> <p>Yes</p> <p>-</p> <p>The Run:ai Scheduler wasn't running when the Job has finished.</p> <p></p>"},{"location":"Researcher/scheduling/job-statuses/#how-to-get-more-information","title":"How to get more information","text":"<p>The system stores various events during the Job's lifecycle. These events can be helpful in diagnosing issues around Job scheduling. To view these events run:</p> <pre><code>runai describe job &lt;workload-name&gt;\n</code></pre> <p>Sometimes, useful information can be found by looking at  logs emitted from the process running inside the container. For example, Jobs that have exited with an exit code different than zero may write an exit reason in this log. To see Job logs run:</p> <pre><code>runai logs &lt;job-name&gt;\n</code></pre>"},{"location":"Researcher/scheduling/job-statuses/#distributed-training-mpi-jobs","title":"Distributed Training (mpi) Jobs","text":"<p>A distributed (mpi) Job, which has no errors will be slightly more complicated and has additional statuses associated with it. </p> <ul> <li> <p>Distributed Jobs start with an \"init container\" which sets the stage for a distributed run.</p> </li> <li> <p>When the init container finishes, the main \"launcher\" container is created. The launcher is responsible for coordinating between the different workers</p> </li> <li> <p>Workers run and do the actual work.</p> </li> </ul> <p>A successful flow of distribute training would look as:</p> <p></p> <p>Additional Statuses:</p> <p>Status</p> <p>End State</p> <p>Resource Allocation</p> <p>Description</p> <p>Color</p> <p>Init:&lt;number A&gt;/&lt;number B&gt;</p> <p></p> <p>Yes</p> <p>The Pod has B Init Containers, and A have completed so far.</p> <p></p> <p>PodInitializing</p> <p></p> <p>Yes</p> <p>The pod has finished executing Init Containers. The system is creating the main 'launcher' container</p> <p></p> <p>Init:Error</p> <p></p> <p></p> <p>An Init Container has failed to execute.</p> <p></p> <p>Init:CrashLoopBackOff</p> <p></p> <p></p> <p>An Init Container has failed repeatedly to execute</p> <p></p>"},{"location":"Researcher/scheduling/node-level-scheduler/","title":"Optimize performance with Node Level Scheduler","text":"<p>The Node Level Scheduler optimizes the performance of your pods and maximizes the utilization of GPUs by making optimal local decisions on GPU allocation to your pods. While the Cluster Scheduler chooses the specific node for a POD, but has no visibility to node\u2019s GPUs internal state, the Node Level Scheduler is aware of the local GPUs states and makes optimal local decisions such that it can optimize both the GPU utilization and pods\u2019 performance running on the node\u2019s GPUs.</p> <p>Node Level Scheduler applies to all workload types, but will best optimize the performance of burstable workloads, giving those more GPU memory than requested and up to the limit specified. Be aware, burstable workloads are always susceptible to an OOM Kill signal if the owner of the excess memory requires it back. This means that using the Node Level Scheduler with Inference or Training workloads may cause pod preemption. Interactive workloads that are using notebooks behave differently since the OOM Kill signal will cause the Notebooks' GPU process to exit but not the notebook itself. This keeps the Interactive pod running and retrying to attach a GPU again. This makes Interactive workloads with notebooks a great use case for burstable workloads and Node Level Scheduler.</p>"},{"location":"Researcher/scheduling/node-level-scheduler/#interactive-notebooks-use-case","title":"Interactive Notebooks Use Case","text":"<p>Consider the following example of a node with 2 GPUs and 2 interactive pods that are submitted and want GPU resources.</p> <p></p> <p>The Scheduler instructs the node to put the two pods on a single GPU, bin packing a single GPU and leaving the other free for a workload that might want a full GPU or more than half GPU. However that would mean GPU#2 is idle while the two notebooks can only use up to half a GPU, even if they temporarily need more.</p> <p></p> <p>However, with Node Level Scheduler enabled, the local decision will be to spread those two pods on two GPUs and allow them to maximize bot pods\u2019 performance and GPUs\u2019 utilization by bursting out up to the full GPU memory and GPU compute resources.</p> <p></p> <p>The Cluster Scheduler still sees a node with a full empty GPU. When a 3rd pod is scheduled, and it requires a full GPU (or more than 0.5 GPU), the scheduler will send it to that node, and Node Level Scheduler will move one of the Interactive workloads to run with the other pod in GPU#1, as was the Cluster Scheduler initial plan.</p> <p></p> <p>This is an example of one scenario that shows how Node Level Scheduler locally optimizes and maximizes GPU utilization and pods\u2019 performance.</p>"},{"location":"Researcher/scheduling/node-level-scheduler/#how-to-configure-node-level-scheduler","title":"How to configure Node Level Scheduler","text":"<p>Node Level Scheduler can be enabled per Node-Pool, giving the Administrator the option to decide which Node-Pools will be used with this new feature.</p> <p>To use Node Level Scheduler the Administrator should follow the steps:</p> <ol> <li> <p>Enable Node Level Scheduler at the cluster level (per cluster), edit the <code>runaiconfig</code> file and set:</p> <pre><code>spec: \n  global: \n      core: \n        nodeScheduler:\n          enabled: true\n</code></pre> <p>The Administrator can also use this patch command to perform the change:</p> <pre><code>kubectl patch -n runai runaiconfigs.run.ai/runai --type='merge' --patch '{\"spec\":{\"global\":{\"core\":{\"nodeScheduler\":{\"enabled\": true}}}}}'\n</code></pre> </li> <li> <p>To enable \u2018GPU resource optimization\u2019 on your tenant\u2019s, go to your tenant\u2019s UI and press Tools &amp; Settings, General, the open the Resources pane and toggle Resource Optimization to on.</p> </li> <li> <p>To enable \u2018Node Level Scheduler\u2019 on any of the Node Pools you want to use this feature, go to the tenant\u2019s UI \u2018Node Pools\u2019 tab (under \u2018Nodes\u2019), and either create a new Node-Pool or edit an existing Node-Pool. In the Node-Pool\u2019s form, under the \u2018Resource Utilization Optimization\u2019 tab, change the \u2018Number of workloads on each GPU\u2019 to any value other than \u2018Not Enforced\u2019 (i.e. 2, 3, 4, 5).</p> </li> </ol> <p>The Node Level Scheduler is now ready to be used on that Node-Pool.</p>"},{"location":"Researcher/scheduling/schedule-to-aws-groups/","title":"Scheduling workloads to AWS placement groups","text":"<p>Run:ai supports AWS placement groups when building and submitting a job. AWS Placement Groups are used to maximize throughput and performance of distributed training workloads.</p> <p>To enable and configure this feature:</p> <ol> <li>Press <code>Jobs | New job</code>.</li> <li>In <code>Scheduling and lifecycle</code> enable the <code>Topology aware scheduling</code>.</li> <li>In <code>Topology key</code>, enter the label of the topology of the node.</li> <li> <p>In <code>Scheduling rule</code> choose <code>Required</code> or <code>Preferred</code> from the drop down.</p> <ul> <li><code>Required</code>\u2014when enabled, all PODs must be scheduled to the same placement group.</li> <li><code>Preferred</code>\u2014when enabled, this is a best-effort, to place as many PODs on the same placement group.</li> </ul> </li> </ol>"},{"location":"Researcher/scheduling/strategies/","title":"Introduction","text":"<p>When the Run:ai scheduler schedules Jobs, it can use two alternate placement strategies:</p> Strategy Description Bin Packing Fill up a GPU or CPU and/or a node before moving on to the next one Spreading Equally spread Jobs amongst GPUs, CPUs and nodes"},{"location":"Researcher/scheduling/strategies/#bin-packing","title":"Bin Packing","text":"<p>Bin packing is the default strategy. With bin packing, the scheduler tries to:</p> <ul> <li>Fill up a node (GPUSs or CPUs) with Jobs before allocating Jobs to second and third nodes.</li> <li>In a multi GPU node, when using fractions, fill up a GPU before allocating Jobs to a second GPU.</li> </ul> <p>The advantage of this strategy is that the scheduler, over time, can package more Jobs into the cluster. As the strategy minimizes fragmentation.</p> <p>In a GPU node, for example, if we have 2 GPUs in a single node on the cluster, and 2 tasks requiring 0.5 GPUs each, using bin-packing, we would place both Jobs on the same GPU and remain with a full GPU ready for the next Job.</p> <p>In a CPU node, for example, if we have 4 CPUs in a single node on the cluster, and 2 tasks requiring 1 CPU each, using bin-packing, we would place both Jobs on the same node and still have more capacity for the next Job.</p>"},{"location":"Researcher/scheduling/strategies/#spreading","title":"Spreading","text":"<p>There are disadvantages to bin-packing:</p> <ul> <li>Within a single GPU, two fractional Jobs compete for the same onboard compute power.</li> <li>Within a single node, two Jobs (even on separate GPUs) compete for networking resources, compute power and memory.</li> </ul> <p>When there are more resources available than requested, it sometimes makes sense to spread Jobs amongst nodes and GPUs, to allow higher utilization of computing resources and network resources.</p> <p>Returning to the example above, if we have 2 GPUs in a single node on the cluster, and 2 Jobs requiring 0.5 GPUs each, using spread scheduling we would place each Job on a separate GPU, allowing both to benefit from the computing power of a full GPU.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/","title":"The Run:ai Scheduler","text":"<p>Each time a user submits a workload via the Run:ai platform, through a 3rd party framework, or directly to Kubernetes APIs, the submitted workload goes to the selected Kubernetes cluster, and is handled by the Run:ai Scheduler.</p> <p>The Scheduler\u2019s main role is to find the best-suited node or nodes for each submitted workload. The nodes must match the resources and other characteristics requested by the workload, while adhering to the quota and fairness principles of the Run:ai platform. A workload can be a single pod running on a single node, or a distributed workload using multiple pods, each running on a node (or part of a node). It is not rare to find large training workloads using 128 nodes and even more, or inference workloads using many pods (replicas) and nodes. There are numerous types of workloads, some are Kubernetes native and some are 3rd party extensions on top of Kubernetes native pods. The Run:ai Scheduler schedules any Kubernetes native workloads, Run:ai workloads, or any other type of 3rd party workload.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#scheduler-basics","title":"Scheduler basics","text":"<p>Set out below are some basic terms and information regarding the Run:ai Scheduler.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#terminology","title":"Terminology","text":"<p>This section describes the terminology and building blocks of the Run:ai scheduler, it also explains some of the scheduling principles used by the Run:ai scheduler.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#workloads-and-pod-groups","title":"Workloads and Pod-Groups","text":"<p>The Run:ai scheduler attaches any newly created pod to a pod-group. A pod-group may contain one or more pods representing a workload. For example, if the submitted workload is a PyTorch distributed training with 32 workers, a single pod-group is created for the entire workload, and all pods are then attached to the pod-group with certain rules that may apply to the pod-group itself, for example, gang scheduling.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#scheduling-queue","title":"Scheduling queue","text":"<p>A scheduling queue (or simply a queue) represents a scheduler primitive that manages the scheduling of workloads based on different parameters. A queue is created for each project/node pool pair and department/node pool pair. The Run:ai scheduler supports hierarchical queueing, project queues are bound to department queues, per node pool. This allows an organization to manage quota, over-quota, and other characteristics for projects and their associated departments.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#priority-and-preemption","title":"Priority and Preemption","text":"<p>Run:ai supports scheduling workloads using different priorities and preemption policies. In the Run:ai scheduling system, higher priority workloads (pods) may preempt lower priority workloads (pods) within the same scheduling queue (project), according to their Preemption policy. Run:ai Scheduler implicitly assumes any PriorityClass &gt;= 100 is non-preemptible and any PriorityClass &lt; 100 is preemptible.</p> <p>Cross project and cross department workload preemptions are referred to as Resource reclaim and are based on fairness between queues rather than the priority of the workloads.</p> <p>To make it easier for users to submit AI workloads, Run:ai preconfigured several Kubernetes PriorityClass objects, the Run:ai preset PriorityClass objects have their preemptionPolicy always set to PreemptLowerPriority, regardless of their actual Run:ai preemption policy within the Run:ai platform.</p> PriorityClass Name PriorityClass Run:ai preemption policy K8S preemption policy Inference 125 Non-preemptible PreemptLowerPriority Build 100 Non-preemptible PreemptLowerPriority Interactive-preemptible 75 Preemptible PreemptLowerPriority Train 50 Preemptible PreemptLowerPriority"},{"location":"Researcher/scheduling/the-runai-scheduler/#quota","title":"Quota","text":"<p>Each project and department includes a set of guaranteed resource quotas per node pool per resource type. For example, Project LLM-Train/Node Pool NV-H100 quota parameters specify the number of GPUs, CPUs(cores), and the amount of CPU memory that this project guarantees for that node pool.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#over-quota","title":"Over-quota","text":"<p>Projects and departments can have a share in the unused resources of any node pool, beyond their quota of resources. We name these resources as over quota resources. The admin configures the over-quota parameters per node pool for each project and department.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#over-quota-priority","title":"Over quota priority","text":"<p>Projects can receive a share of the cluster/node pool unused resources when the over-quota priority setting is enabled, the part each Project receives depends on its over-quota priority value, and the total weights of all other projects\u2019 over-quota priorities. The admin configures the over-quota priority parameters per node pool for each project and department.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#fairshare-and-fairshare-balancing","title":"Fairshare and fairshare balancing","text":"<p>Run:ai Scheduler calculates a numerical value per project (or department) for each node-pool, representing the project\u2019s (department\u2019s) sum of guaranteed resources plus the portion of non-guaranteed resources in that node pool. We name this value fairshare.</p> <p>The scheduler strives to provide each project (or department) the resources they deserve using two main parameters - deserved quota and deserved fairshare (i.e. quota + over quota resources), this is done per node pool. If one project\u2019s node pool queue is below fairshare and another project\u2019s node pool queue is above fairshare, the scheduler shifts resources between queues to balance fairness; this may result in the preemption of some over-quota preemptible workloads.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#over-subscription","title":"Over-subscription","text":"<p>Over-subscription is a scenario where the sum of all guaranteed resource quotas surpasses the physical resources of the cluster or node pool. In this case, there may be scenarios in which the scheduler cannot find matching nodes to all workload requests, even if those requests were within the resource quota of their associated projects.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#gang-scheduling","title":"Gang scheduling","text":"<p>Gang scheduling describes a scheduling principle where a workload composed of multiple pods is either fully scheduled (i.e. all pods are scheduled and running) or fully pending (i.e. all pods are not running). Gang scheduling refers to a single pod group.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#fairness-fair-resource-distribution","title":"Fairness (fair resource distribution)","text":"<p>Fairness is a major principle within the Run:ai scheduling system. In essence, it means that the Run:ai Scheduler always respects certain resource splitting rules (fairness) between projects and between departments.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#preemption-of-lower-priority-workloads-within-a-project","title":"Preemption of lower priority workloads within a project","text":"<p>Workload priority is always respected within a project. This means higher priority workloads are scheduled before lower priority workloads, it also means that higher priority workloads may preempt lower priority workloads within the same project if the lower priority workloads are preemptible.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#reclaim-of-resources-between-projects-and-departments","title":"Reclaim of resources between projects and departments","text":"<p>Reclaim is an inter-project (and inter-department) scheduling action that takes back resources from one project (or department) that has used them as over-quota, back to a project (or department) that deserves those resources as part of its guaranteed quota, or to balance fairness between projects, each to its fairshare (i.e. sharing fairly the portion of the unused resources).</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#multi-level-quota-system","title":"Multi-Level quota system","text":"<p>Each project has a set of guaranteed resource quotas (GPUs, CPUs, and CPU memory) per node pool. Projects can go over-quota and get a share of the unused resources (over-quota) in a node pool beyond their guaranteed quota in that node pool. The same applies to Departments. The Scheduler balances the amount of over quota between departments, and then between projects.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#placement-strategy-bin-pack-and-spread","title":"Placement strategy - bin-pack and spread","text":"<p>The admin can set per node pool placement strategy of the scheduler for GPU based workloads and for CPU-only based workloads.</p> <p>Each type\u2019s strategy can be either bin-pack or spread.</p> <p>GPU workloads:</p> <ul> <li>Bin-pack means the Scheduler places as many workloads as possible in each GPU and each node to use fewer resources and maximize GPU and node vacancy.  </li> <li>Spread means the Scheduler spreads workloads across as many GPUs and nodes as possible to minimize the load and maximize the available resources per workload.  </li> <li>GPU workloads are workloads that request both GPU and CPU resources.</li> </ul> <p>CPU workloads:</p> <ul> <li>Bin-pack means the scheduler places as many workloads as possible in each CPU and node to use fewer resources and maximize CPU and node vacancy.  </li> <li>Spread means the scheduler spreads workloads across as many CPUs and nodes as possible to minimize the load and maximize the available resources per workload.  </li> <li>CPU workloads are workloads that request only CPU resources</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#scheduler-deep-dive","title":"Scheduler deep dive","text":""},{"location":"Researcher/scheduling/the-runai-scheduler/#allocation","title":"Allocation","text":"<p>When a user submits a workload, the workload controller creates a pod or pods (for distributed training workloads or a deployment based Inference). When the scheduler gets a submit request with the first pod, it creates a pod group and allocates all the relevant building blocks of that workload. The next pods of the same workload are attached to the same pod group.</p> <p>A workload, with its associated pod group, is queued in the appropriate queue. In every scheduling cycle, the Scheduler ranks the order of queues by calculating their precedence for scheduling.</p> <p>The next step is for the scheduler to find nodes for those pods, assign the pods to their nodes (bind operation), and bind other building blocks of the pods such as storage, ingress etc.</p> <p>If the pod-group has a gang scheduling rule attached to it, the scheduler either allocates and binds all pods together, or puts all of them into the pending state. It retries to schedule them all together in the next scheduling cycle.</p> <p>The scheduler also updates the status of the pods and their associate pod group, users are able to track the workload submission process both in the CLI or Run:ai UI.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#preemption","title":"Preemption","text":"<p>If the scheduler cannot find resources for the submitted workloads (and all of its associated pods), and the workload deserves resources either because it is under its queue quota or under its queue fairshare, the scheduler tries to reclaim resources from other queues; if this doesn\u2019t solve the resources issue, the scheduler tries to preempt lower priority preemptible workloads within the same queue.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#reclaim-preemption-between-projects-and-departments","title":"Reclaim preemption between projects (and departments)","text":"<p>Reclaim is an inter-project (and inter-department) resource balancing action that takes back resources from one project (or department) that has used them as an over-quota, back to a project (or department) that deserves those resources as part of its deserved quota, or to balance fairness between projects (or departments), so a project (or department) doesn\u2019t exceed its fairshare (portion of the unused resources).</p> <p>This mode of operation means that a lower priority workload submitted in one project (e.g. training) can reclaim resources from a project that runs a higher priority workload (e.g. preemptive workspace) if fairness balancing is required.</p> <p>Note</p> <p>Only preemptive workloads can go over-quota as they are susceptible to reclaim (cross-projects preemption) of the over-quota resources they are using. The amount of over-quota resources a project can gain depends on the over-quota priority or quota (if over-quota priority is disabled). Departments\u2019 over-quota is always proportional to its quota.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#priority-preemption-within-a-project","title":"Priority preemption within a project","text":"<p>Higher priority workloads may preempt lower priority preemptible workloads within the same project/node pool queue. For example, in a project that runs a training workload that exceeds the project quota for a certain node pool, a newly submitted workspace within the same project/node pool may stop (preempt) the training workload if there are not enough over-quota resources for the project within that node pool to run both workloads (e.g. workspace using in-quota resources and training using over-quota resources).</p> <p>There is no priority notion between workloads of different projects.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#quota-over-quota-and-fairshare","title":"Quota, over-quota, and fairshare","text":"<p>Run:ai scheduler strives to ensure fairness between projects and between departments, this means each department and project always strive to get their deserved quota, and unused resources are split between projects according to known rules (e.g. over-quota weights).</p> <p>If a project needs more resources even beyond its fairshare, and the scheduler finds unused resources that no other project needs, this project can consume resources even beyond its fairshare.</p> <p>Some scenarios can prevent the scheduler from fully providing the deserved quota and fairness promise, such as fragmentation or other scheduling constraints like affinities, taints etc.</p> <p>The example below illustrates a split of quota between different projects and departments, using several node pools:</p> <p>Legend:</p> <ul> <li>OQP = Over-quota priority</li> <li>OQ = Over-quota</li> </ul> <p></p> <p>The example below illustrates how fairshare is calculated per project/node pool and per department/node pool for the above example:</p> <p></p> <p>The Over quota (OQ) portion of each Project (per node pool) is calculated as:</p> <pre><code>[(OQ-Priority) / (\u03a3 Projects OQ-Priorities)] x (Unused Resource per node pool)\n</code></pre> <p>Fairshare(FS) is calculated as: the sum of Quota + Over-Quota</p> <p>Let\u2019s see how Project 2 over quota and fairshare are calculated:</p> <p>For this example, we assume that out of the 40 available GPUs in node pool A, 20 GPUs are currently unused (unused means either not part of any project\u2019s quota, or part of a project\u2019s quota but not used by any workloads of that project).</p> <p>Project 2 over quota share:</p> <pre><code>[(Project 2 OQ-Priority) / (\u03a3 all Projects OQ-Priorities)] x (Unused Resource within node pool A)\n\n[(3) / (2 + 3 + 1)] x (20) = (3/6) x 20 = 10 GPUs\n</code></pre> <p>Fairshare = deserved quota + over quota = 6 +10 = 16 GPUs</p> <p>Similarly, fairshare is also calculated for CPU and CPU memory.</p> <p>The scheduler can grant a project more resources than its fairshare if the scheduler finds resources not required by other projects that may deserve those resources.</p> <p>One can also see in the above illustration that Project 3 has no guaranteed quota, but it still has a share of the excess resources in node pool A. Run:ai Scheduler ensures that Project 3 receives its part of the unused resources for over quota, even if this results in reclaiming resources from other projects and preempting preemptible workloads.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#fairshare-balancing","title":"Fairshare balancing","text":"<p>The Scheduler constantly re-calculates the fairshare of each project and department (per node pool, represented in the scheduler as queues), resulting in the re-balancing of resources between projects and between departments. This means that a preemptible workload that was granted resources to run in one scheduling cycle, can find itself preempted and go back to the pending state waiting for resources on the next cycle.</p> <p>A queue, representing a scheduler-managed object for each Project or Department per node pool, can be in one of 3 states:</p> <ul> <li>__In-quota __    The queue\u2019s allocated resources \u2264 queue deserved quota  </li> <li>__Over-quota (but below fairshare) __    The queue\u2019s deserved quota &lt; queue\u2019s allocates resources &lt;= queue\u2019s fairshare  </li> <li>Over-Fairshare (and over-quota)   The queue\u2019s fairshare &lt; queue\u2019s allocated resources</li> </ul> <p></p> <p>The scheduler\u2019s first priority is to ensure each queue (representing a project/node pool or department/node pool scheduler object) receives its deserved quota. Then the scheduler tries to find and allocate more resources to queues that need resources beyond their deserved quota and up to their fairshare, finally, the scheduler tries to allocate resources to queues that need even more resources - beyond their fairshare.</p> <p>When re-balancing resources between queues of different projects and departments, the scheduler goes in the opposite direction, i.e. first take resources from over-fairshare queues, then from over-quota queues, and finally, in some scenarios, even from queues that are below their deserved quota.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#summary","title":"Summary","text":"<p>The scheduler\u2019s role is to bind any submitted pod to a node that satisfies the pod\u2019s requirements and constraints while adhering to the Run:ai quota and fairness system. In some scenarios, the scheduler finds a node for a pod (or nodes for a group of pods) immediately. In other scenarios, the scheduler has to preempt an already running workload to \u201cmake room\u201d, while sometimes a workload becomes pending until resources are released by other workloads (e.g. wait for other workloads to terminate), and only then it is scheduled and run.</p> <p>Other than scenarios where the requested resources or other constraints cannot be met within the cluster, either because the resources physically don\u2019t exist (e.g. a node with 16 GPUs, or a GPU with 200GB of memory), or a combination of constraints cannot be matched (e.g. a GPU with 80GB of memory together with a node with specific label or storage type), the scheduler eventually finds any workload its matching nodes to use, but this process may take some time.</p> <p>The Run:ai scheduler adheres to Kubernetes standard rules, but it also adds a layer of fairness between queues, queue hierarchy, node pools, and many more features, making the scheduling and Quota management more sophisticated, granular, and robust. The combination of these scheduler capabilities results in higher efficiency, scale, and maximization of cluster utilization.</p>"},{"location":"Researcher/tools/dev-jupyter/","title":"Use a Jupyter Notebook with a Run:ai Job","text":"<p>See the Jupyter Notebook Quickstart here.</p>"},{"location":"Researcher/tools/dev-pycharm/","title":"Use PyCharm with a Run:ai Job","text":"<p>Once you launch a workload using Run:ai, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook</p> <p>This document is about accessing the remote container created by Run:ai, from JetBrain's PyCharm. </p>"},{"location":"Researcher/tools/dev-pycharm/#submit-a-workload","title":"Submit a Workload","text":"<p>You will need your image to run an SSH server  (e.g OpenSSH). For the purposes of this document, we have created an image named <code>runai.jfrog.io/demo/pycharm-demo</code>. The image runs both python and ssh. Details on how to create the image are here. The image is configured to use the <code>root</code> user and password for SSH.</p> <p>Run the following command to connect to the container as if it were running locally:</p> <pre><code>runai submit build-remote -i runai.jfrog.io/demo/pycharm-demo --interactive  \\\n        --service-type=portforward --port 2222:22\n</code></pre> <p>The terminal will show the connection: </p> <pre><code>The job 'build-remote' has been submitted successfully\nYou can run `runai describe job build-remote -p team-a` to check the job status\nINFO[0007] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0045] Job started\nOpen access point(s) to service from localhost:2222\nForwarding from [::1]:2222 -&gt; 22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 2222</li> </ul> <p>Note</p> <pre><code>It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run:\n\n```\nrunai submit build-remote -i runai.jfrog.io/demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22\n```\n\n* The Job starts an sshd server on port 22.\n* The Job redirects the external port 30022 to port 22 and uses a [Node Port](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types){target=_blank} service type.\n* Run: `runai list worklaods`\n\n* Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222\n</code></pre>"},{"location":"Researcher/tools/dev-pycharm/#pycharm","title":"PyCharm","text":"<ul> <li>Under PyCharm | Preferences go to: Project | Python Interpreter </li> <li>Add a new SSH Interpreter. </li> <li>As Host, use the IP address above. Change the port to the above and use the Username <code>root</code></li> <li>You will be prompted for a password. Enter <code>root</code></li> <li>Apply settings and run the code via this interpreter. You will see your project uploaded to the container and running remotely. </li> </ul>"},{"location":"Researcher/tools/dev-tensorboard/","title":"Connecting to TensorBoard","text":"<p>Once you launch a Deep Learning workload using Run:ai, you may want to view its progress. A popular tool for viewing progress is TensorBoard.</p> <p>The document below explains how to use TensorBoard to view the progress or a Run:ai Job.</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-a-workload","title":"Submit a Workload","text":"<p>When you submit a workload, your workload must save TensorBoard logs which can later be viewed. Follow this document on how to do this. You can also view the Run:ai sample code here.</p> <p>The code shows:</p> <ul> <li>A reference to a log directory:</li> </ul> <pre><code>log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n</code></pre> <ul> <li>A registered Keras callback for TensorBoard:</li> </ul> <pre><code>tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n\nmodel.fit(x_train, y_train,\n        ....\n        callbacks=[..., tensorboard_callback])\n</code></pre> <p>The <code>logs</code> directory must be saved on a Network File Server such that it can be accessed by the TensorBoard Job. For example, by running the Job as follows:</p> <pre><code>runai submit train-with-logs -i tensorflow/tensorflow:1.14.0-gpu-py3 \\\n  -v /mnt/nfs_share/john:/mydir -g 1  --working-dir /mydir --command -- ./startup.sh\n</code></pre> <p>Note the volume flag (<code>-v</code>) and working directory flag (<code>--working-dir</code>). The logs directory will be created on <code>/mnt/nfs_share/john/logs/fit</code>.</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-a-tensorboard-workload","title":"Submit a TensorBoard Workload","text":"<p>There are two ways to submit a TensorBoard Workload: via the Command-line interface or the user interface</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-via-the-user-interface","title":"Submit via the User interface","text":"<ul> <li>Within the user interface go to the Job list.</li> <li>Select <code>New Job</code> on the top right.</li> <li>Select <code>Interactive</code> at the top. </li> <li>Add an image that supports TensorBoard. For example: <code>tensorflow/tensorflow:latest</code>.</li> <li>Select the <code>TensorBoard</code> button.</li> <li>Add a mounted volume on which TensorBoard logs exist. The example above uses <code>/mnt/nfs_share/john</code>. Map to <code>/mydir</code></li> <li>Add <code>/mydir</code> to the <code>TensorBoard Logs Directory</code>. </li> </ul> <p>Submit the Job. When running, select the job and press <code>Connect</code> on the top right.</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-via-the-command-line-interface","title":"Submit via the Command-line interface","text":"<p>Run the following:</p> <pre><code>runai submit tb -i tensorflow/tensorflow:latest --interactive --service-type=portforward --port 8888:8888  --working-dir /mydir  -v /mnt/nfs_share/john:/mydir  -- tensorboard --logdir logs/fit --port 8888 --host 0.0.0.0\n</code></pre> <p>The terminal will show the following: </p> <pre><code>The job 'tb' has been submitted successfully\nYou can run `runai describe job tb -p team-a` to check the job status\nINFO[0006] Waiting for job to start\nWaiting for job to start\nINFO[0014] Job started\nOpen access point(s) to service from localhost:8888\nForwarding from 127.0.0.1:8888 -&gt; 8888\nForwarding from [::1]:8888 -&gt; 8888\n</code></pre> <p>Browse to http://localhost:8888/ to view TensorBoard.</p> <p>Note</p> <p>A single TensorBoard Job can be used to view multiple deep learning Jobs, provided it has access to the logs directory for these Jobs. </p> <p>You can also submit a TensorBoard Job via the user interface. In which case, instead of <code>portforward</code> you will need to select a different service type. If the URL to the TensorBoard job includes a path, you may need to use the TensorBoard flag <code>--path_prefix</code>. For example, if your access point is acme.com/tensorboard1 add  <code>--path_prefix /tensorboard1</code>.</p>"},{"location":"Researcher/tools/dev-vscode/","title":"Use Visual Studio Code with a Run:ai Job","text":"<p>Once you launch a workload using Run:ai, you will want to connect to it. You can do so via command line or via other tools such as a Jupyter Notebook</p> <p>Important</p> <p>This document is about accessing the remote container created by Run:ai, from the installed version of Visual Studio Code. If you want to use Visual Studio Code for web, please see Visual Studio Code Web Quickstart.</p>"},{"location":"Researcher/tools/dev-vscode/#submit-a-workload","title":"Submit a Workload","text":"<p>You will need your image to run an SSH server  (e.g OpenSSH). For this document, we have created an image named <code>runai.jfrog.io/demo/pycharm-demo</code>. The image runs both python and ssh. Details on how to create the image are here. The image is configured to use the <code>root</code> user and password for SSH.</p> <p>Run the following command to connect to the container as if it were running locally:</p> <pre><code>runai submit build-remote -i runai.jfrog.io/demo/pycharm-demo --interactive  \\\n        --service-type=portforward --port 2222:22\n</code></pre> <p>The terminal will show the connection: </p> <pre><code>The job 'build-remote' has been submitted successfully\nYou can run `runai describe job build-remote -p team-a` to check the job status\nINFO[0007] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0045] Job started\nOpen access point(s) to service from localhost:2222\nForwarding from [::1]:2222 -&gt; 22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 2222</li> </ul> <p>Note</p> <p>It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run:</p> <pre><code>runai submit build-remote -i runai.jfrog.io/demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The Job redirects the external port 30022 to port 22 and uses a Node Port service type.</li> <li> <p>Run: <code>runai list jobs</code></p> </li> <li> <p>Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222 </p> </li> </ul>"},{"location":"Researcher/tools/dev-vscode/#visual-studio-code","title":"Visual Studio Code","text":"<ul> <li>Under Visual Studio code install the Remote SSH extension.</li> <li>Create an ssh entry to the service by editing .ssh/config file or use the command Remote-SSH: Connect to Host... from the Command Palette.  Enter the IP address and port from above (e.g. ssh root@35.34.212.12 -p 30022 or ssh root@127.0.0.1 -p 2222). User and password are <code>root</code> </li> <li>Using VS Code, install the Python extension on the remote machine </li> <li>Write your first Python code and run it remotely.</li> </ul>"},{"location":"Researcher/tools/dev-x11forward-pycharm/","title":"Use PyCharm with X11 Forwarding and Run:ai","text":"<p>X11 is a window system for the Unix operating systems. X11 forwarding allows executing a program remotely through an SSH connection. Meaning, the executable file itself is hosted on a different machine than where the graphical interface is being displayed. The graphical windows are forwarded to your local machine through the SSH connection.</p> <p>This section is about setting up X11 forwarding from a Run:ai-based container to a PyCharm IDE on a remote machine.</p>"},{"location":"Researcher/tools/dev-x11forward-pycharm/#submit-a-workload","title":"Submit a Workload","text":"<p>You will need your image to run an SSH server  (e.g OpenSSH). For the purposes of this document, we have created an image named <code>runai.jfrog.io/demo/quickstart-x-forwarding</code>. The image runs:</p> <ul> <li>Python</li> <li>SSH Daemon configured for X11Forwarding </li> <li>OpenCV python library for image handling</li> </ul> <p>Details on how to create the image are here. The image is configured to use the <code>root</code> user and password for SSH.</p> <p>Run the following command to connect to the container as if it were running locally:</p> <pre><code>runai submit xforward-remote -i runai.jfrog.io/demo/quickstart-x-forwarding --interactive  \\\n        --service-type=portforward --port 2222:22\n</code></pre> <p>The terminal will show the connection:</p> <pre><code>The job 'xforward-remote' has been submitted successfully\nYou can run `runai describe job xforward-remote -p team-a` to check the job status\nINFO[0007] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0045] Job started\nOpen access point(s) to service from localhost:2222\nForwarding from [::1]:2222 -&gt; 22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 2222</li> </ul>"},{"location":"Researcher/tools/dev-x11forward-pycharm/#setup-the-x11-forwarding-tunnel","title":"Setup the X11 Forwarding Tunnel","text":"<p>Connect to the new Job by running:</p> <pre><code>ssh -X root@127.0.0.1 -p 2222\n</code></pre> <p>Note the <code>-X</code> flag. </p> <p>Run:</p> <p><pre><code>echo $DISPLAY\n</code></pre> Copy the value. It will be used as a PyCharm environment variable.</p> <p>Important</p> <p>The ssh terminal should remain active throughout the session.</p>"},{"location":"Researcher/tools/dev-x11forward-pycharm/#pycharm","title":"PyCharm","text":"<ul> <li>Under PyCharm | Preferences go to: Project | Python Interpreter</li> <li>Add a new SSH Interpreter.</li> <li>As Host, use <code>localhost</code>. Change the port to the above (<code>2222</code>) and use the Username <code>root</code>.</li> <li>You will be prompted for a password. Enter <code>root</code>.</li> <li>Make sure to set the correct path of the Python binary. In our case it's <code>/usr/local/bin/python</code>.</li> <li> <p>Apply your settings.</p> </li> <li> <p>Under PyCharm configuration set the following environment variables:</p> <ol> <li><code>DISPLAY</code> - set environment variable you copied before</li> <li><code>HOME</code> - In our case it's <code>/root</code>. This is required for the X11 authentication to work.</li> </ol> </li> </ul> <p>Run your code. You can use our sample code here.</p>"},{"location":"Researcher/workloads/inference-overview/","title":"Inference overview","text":""},{"location":"Researcher/workloads/inference-overview/#what-is-inference","title":"What is Inference","text":"<p>Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output.</p> <p>With Inference workloads, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time.</p>"},{"location":"Researcher/workloads/inference-overview/#inference-and-gpus","title":"Inference and GPUs","text":"<p>The Inference process is a subset of the original Training algorithm on a single datum (e.g. one sentence or one image), or a small batch. As such, GPU memory requirements are typically smaller than a full-blown Training process.</p> <p>Given that, Inference lends itself nicely to the usage of Run:ai Fractions. You can, for example, run 4 instances of an Inference server on a single GPU, each employing a fourth of the memory.</p>"},{"location":"Researcher/workloads/inference-overview/#inference-runai","title":"Inference @Run:ai","text":"<p>Run:ai provides Inference services as an equal part together with the other two Workload types: Train and Build.</p> <ul> <li> <p>Inference is considered a high-priority workload as it is customer-facing. Running an Inference workload (within the Project's quota) will preempt any Run:ai Workload marked as Training.</p> </li> <li> <p>Inference workloads will receive priority over Train and Build workloads during scheduling.</p> </li> <li> <p>Inference is implemented as a Kubernetes Deployment object with a defined number of replicas. The replicas are load-balanced by Kubernetes so adding more replicas will improve the overall throughput of the system.</p> </li> <li> <p>Multiple replicas will appear in Run:ai as a single Inference workload. The workload will appear in all Run:ai dashboards and views as well as the Command-line interface.</p> </li> <li> <p>Inference workloads can be submitted via Run:ai user interface as well as Run:ai API. Internally, spawning an Inference workload also creates a Kubernetes Service. The service is an end-point to which clients can connect.</p> </li> </ul>"},{"location":"Researcher/workloads/inference-overview/#autoscaling","title":"Autoscaling","text":"<p>To withstand SLA, Inference workloads are typically set with auto scaling. Auto-scaling is the ability to add more computing power (Kubernetes pods) when the load increases and shrink allocated resources when the system is idle. There are several ways to trigger autoscaling. Run:ai supports the following:</p> Metric Units Run:ai name Throughput requests/second throughput Concurrency concurrency <p>The Minimum and Maximum number of replicas can be configured as part of the autoscaling configuration.</p> <p>Autoscaling also supports a scale-to-zero policy with Throughput and Concurrency metrics, meaning that given enough time under the target threshold, the number of replicas will be scaled down to 0.</p> <p>This has the benefit of conserving resources at the risk of a delay from \"cold starting\" the model when traffic resumes.</p>"},{"location":"Researcher/workloads/inference-overview/#see-also","title":"See Also","text":"<ul> <li>To set up Inference, see Cluster installation prerequisites.</li> <li>For running Inference see Inference quick-start.</li> <li>To run Inference using API see Workload overview.</li> </ul>"},{"location":"Researcher/workloads/managing-workloads/","title":"Managing Workloads","text":"<p>This article explains the procedure for managing workloads.</p>"},{"location":"Researcher/workloads/managing-workloads/#workloads-table","title":"Workloads table","text":"<p>The Workloads table can be found under Workloads in the Run:ai platform.</p> <p>The workloads table provides a list of all the workloads scheduled on the run:ai Scheduler, and allows you to manage them.</p> <p></p> <p>The Workloads table consists of the following columns:</p> Column Description Workload The name of the workload Type The workload type Preemptible Is the workload preemptible Status The different phases in a workload life cycle. Project The project in which the workload runs. Department The department that the workload is associated with. this column is visible only if the department toggle is enabled by your administrator. Created by The user who created the workload Running/requested pods The number of running pods out of the requested Creation time The timestamp for when the workload was created Completion time The timestamp the workload reached a terminal state (failed/completed) Connection(s) The method by which you can access and interact with the running workload. It's essentially the \"doorway\" through which you can reach and use the tools the workload provide. (E.g node port, external URL, etc). Click one of the values in the column to view the list of connections and their parameters Data source(s) Data resources used by the workload Environment The environment used by the workload Workload architecture Standard or distributed. A standard workload consists of a single process. A distributed workload consists of multiple processes working together. These processes can run on different nodes. GPU compute request Amount of GPU devices Requested GPU compute allocation Amount of GPU devices allocated GPU memory request Amount of GPU memory Requested GPU memory allocation Amount of GPU memory allocated CPU compute request Amount of CPU cores requested CPU compute allocation Amount of CPU cores allocated CPU memory request Amount of CPU memory requested CPU memory allocation Amount of CPU memory allocated Cluster The cluster that the workload is associated with"},{"location":"Researcher/workloads/managing-workloads/#workload-status","title":"Workload status","text":"<p>The following table describes the different phases in a workload life cycle.</p> Status Description Entry Condition Exit Condition Creating Workload setup is initiated in the cluster. Resources and pods are now provisioning. A workload is submitted. A multi-pod group is created. Pending Workload is queued and awaiting resource allocation. A pod group exists. All pods are scheduled. Initializing Workload is retrieving images, starting containers, and preparing pods. All pods are scheduled. All pods are initialized or a failure to initialize is detected. Running Workload is currently in progress with all pods operational. All pods initialized (all containers in pods are ready). Workload completion or failure. Degraded Pods may not align with specifications, network services might be incomplete, or persistent volumes may be detached. Check your logs for specific details. Pending - All pods are running but have issues. Running - All pods are running with no issues. Running - All resources are OK. Completed - Workload finished with fewer resources. Failed - Workload failure or user-defined rules. Deleting Workload and its associated resources are being decommissioned from the cluster. Deleting the workload. Resources are fully deleted. Stopped Workload is on hold and resources are intact but inactive. Stopping the workload without deleting resources. Transitioning back to the initializing phase or proceeding to deleting the workload. Failed Image retrieval failed or containers experienced a crash. Check your logs for specific details. An error occurs preventing the successful completion of the workload. Terminal state. Completed Workload has successfully finished its execution. The workload has finished processing without errors. Terminal state."},{"location":"Researcher/workloads/managing-workloads/#pods-associated-with-workload","title":"Pods Associated with Workload","text":"<p>Click one of the values in the Running/requested pods column, to view the list of pods and their parameters.</p> Column Description Pod Pod name Status Pod lifecycle stages Node The node on which the pod resides Node pool The node pool in which the pod resides (applicable if node pools are enabled) Image The pod\u2019s main image GPU compute allocation Amount of GPU devices allocated for the pod GPU memory allocation Amount of GPU memory allocated for the pod"},{"location":"Researcher/workloads/managing-workloads/#connections-associated-with-workload","title":"Connections Associated with Workload","text":"<p>A connection refers to the method by which you can access and interact with the running workloads. It is essentially the \"doorway\" through which you can reach and use the applications (tools) these workloads provide.</p> <p>Click one of the values in the Connection(s) column, to view the list of connections and their parameters. Connections are network interfaces that communicate with the application running in the workload. Connections are either the URL the application exposes or the IP and the port of the node that the workload is running on.</p> Column Description Name The name of the application running on the workload Connection type The network connection type selected for the workload Access Who is authorized to use this connection (everyone, specific groups/users) Address The connection URL Copy button Copy URL to clipboard Connect button Enabled only for supported tools"},{"location":"Researcher/workloads/managing-workloads/#data-sources-associated-with-workload","title":"Data Sources Associated with Workload","text":"<p>Click one of the values in the Data source(s) column, to view the list of data sources and their parameters.</p> Column Description Data source The name of the data source mounted to the workload Type The data source type"},{"location":"Researcher/workloads/managing-workloads/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV  </li> <li>Refresh - Click REFRESH to update the table with the latest data  </li> <li>Show/Hide details - Click to view additional information on the selected row</li> </ul>"},{"location":"Researcher/workloads/managing-workloads/#showhide-details","title":"Show/Hide details","text":"<p>Click a row in the Workloads table and then click the SHOW DETAILS button at the upper-right side of the action bar. The details pane appears, presenting the following tabs:</p>"},{"location":"Researcher/workloads/managing-workloads/#event-history","title":"Event History","text":"<p>Displays the workload status over time. It displays events describing the workload lifecycle and alerts on notable events. Use the filter to search through the history for specific events.</p>"},{"location":"Researcher/workloads/managing-workloads/#metrics","title":"Metrics","text":"<ul> <li>GPU utilization   Per GPU graph and an average of all GPUs graph, all on the same chart, along an adjustable period allows you to see the trends of all GPUs compute utilization (percentage of GPU compute) in this node.  </li> <li>GPU memory utilization   Per GPU graph and an average of all GPUs graph, all on the same chart, along an adjustable period allows you to see the trends of all GPUs memory usage (percentage of the GPU memory) in this node.  </li> <li>CPU compute utilization   The average of all CPUs\u2019 cores compute utilization graph, along an adjustable period allows you to see the trends of CPU compute utilization (percentage of CPU compute) in this node.  </li> <li>CPU memory utilization   The utilization of all CPUs memory in a single graph, along an adjustable period allows you to see the trends of CPU memory utilization (percentage of CPU memory) in this node.  </li> <li> <p>CPU memory usage   The usage of all CPUs memory in a single graph, along an adjustable period allows you to see the trends of CPU memory usage (in GB or MB of CPU memory) in this node.</p> </li> <li> <p>For GPUs charts - Click the GPU legend on the right-hand side of the chart, to activate or deactivate any of the GPU lines.  </p> </li> <li>You can click the date picker to change the presented period  </li> <li>You can use your mouse to mark a sub-period in the graph for zooming in, and use Reset zoom to go back to the preset period  </li> <li>Changes in the period affect all graphs on this screen.</li> </ul>"},{"location":"Researcher/workloads/managing-workloads/#logs","title":"Logs","text":"<p>Workload events are ordered in chronological order. The logs contain events from the workload\u2019s lifecycle to help monitor and debug issues.</p>"},{"location":"Researcher/workloads/managing-workloads/#adding-new-workload","title":"Adding new workload","text":"<p>Before starting, make sure you have created a project or have one created for you to work with workloads.</p> <p>To create a new workload:</p> <ol> <li>Click +NEW WORKLOAD </li> <li>Select a workload type - Follow the links below to view the step-by-step guide for each workload type:  <ul> <li>Workspace Used for data preparation and model-building tasks.  </li> <li>Training Used for training tasks of all sorts  </li> <li>Inference Used for inference and serving tasks  </li> <li>Job (legacy) This type is displayed only if enabled by your Administrator, under General Settings \u2192 Workloads \u2192 Workload policies  </li> </ul> </li> <li>Click CREATE WORKLOAD</li> </ol>"},{"location":"Researcher/workloads/managing-workloads/#stopping-a-workload","title":"Stopping a workload","text":"<p>Stopping a workload kills the workload pods and releases the workload resources.</p> <ol> <li>Select the workload you want to stop  </li> <li>Click STOP</li> </ol>"},{"location":"Researcher/workloads/managing-workloads/#running-a-workload","title":"Running a workload","text":"<p>Running a workload spins up new pods and resumes the workload work after it was stopped.</p> <ol> <li>Select the workload you want to run again  </li> <li>Click RUN</li> </ol>"},{"location":"Researcher/workloads/managing-workloads/#connecting-to-a-workload","title":"Connecting to a workload","text":"<p>To connect to an application running in the workload (for example, Jupyter Notebook)</p> <ol> <li>Select the workload you want to connect  </li> <li>Click CONNECT </li> <li>Select the tool from the drop-down list  </li> <li>The selected tool is opened in a new tab on your browser</li> </ol>"},{"location":"Researcher/workloads/managing-workloads/#deleting-a-workload","title":"Deleting a workload","text":"<ol> <li>Select the workload you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion  </li> </ol> <p>Note</p> <p>Once a workload is deleted you can view it in the Deleted tab in the workloads view. This tab is displayed only if enabled by your Administrator, under General Settings \u2192 Workloads \u2192 Deleted workloads</p> <ol> <li>Select the workload you want to copy and edit  </li> <li>Click COPY &amp; EDIT </li> <li>Update the workload and click CREATE WORKLOAD</li> </ol>"},{"location":"Researcher/workloads/managing-workloads/#using-api","title":"Using API","text":"<p>Go to the Workloads API reference to view the available actions</p>"},{"location":"Researcher/workloads/managing-workloads/#troubleshooting","title":"Troubleshooting","text":"<p>To understand the condition of the workload, review the workload status in the Workload table. For more information, see check the workload\u2019s event history.</p> <p>Listed below are a number of known issues when working with workloads and how to fix them:</p> Issue Mediation Cluster connectivity issues (there are issues with your connection to the cluster error message) Verify that you are on a network that has been granted access to the cluster.  Reach out to your cluster admin for instructions on verifying this.  If you are an admin, see the troubleshooting section in the cluster documentation Workload in \u201cInitializing\u201d status for some time Check that you have access to the Container image registry.  Check the statuses of the pods in the pods\u2019 modal.  Check the event history for more details Workload has been pending for some time Check that you have the required quota.  Check the project\u2019s available quota in the project dialog.  Check that all services needed to run are bound to the workload.  Check the event history for more details. PVCs created using the K8s API or <code>kubectl</code> are not visible or mountable in Run:ai. This is by design.  - Create a new data source of type PVC in the Run:ai UI  - In the Data mount section, select Existing PVC  - Select the PVC you created via the K8S API  You are now able to select and mount this PVC in your Run:ai submitted workloads. Workload is not visible in the UI. Check that the workload hasn\u2019t been deleted.  See the \u201cDeleted\u201d tab in the workloads view"},{"location":"Researcher/workloads/trainings/","title":"Trainings","text":"<p>The Trainings interface provides a wizard to make submitting workloads easy.</p>"},{"location":"Researcher/workloads/trainings/#prerequisites","title":"Prerequisites","text":"<p>You must have:</p> <ul> <li>Workspaces enabled.</li> <li>At least one Project configured.</li> </ul> <p>Note</p> <p>See your system administrator to ensure the prerequisites are enabled and configured.</p>"},{"location":"Researcher/workloads/trainings/#adding-trainings","title":"Adding Trainings","text":"<p>Note</p> <p>Where there is a card gallery, use the search bar to find specific cards based on title or field values.</p> <p>To add a training:</p> <ol> <li>Press Tranings in the menu.</li> <li>In the Projects pane, select the destination project. Use the search box to find projects that are not listed. If you can't find the project, you can create your own, or see your system administrator.</li> <li>In the Multi-node pane, choose <code>Single node</code> for a single node training, or <code>Multi-node (distributed)</code> for distributed training. When you choose <code>Multi-node</code>, select a framework that is listed, then select the <code>multi-node</code> training configuration by selecting either <code>Workers &amp; master</code> or <code>Workers only</code>.</li> <li>In the Templates pane, select a template from the list. Use the search box to find templates that are not listed. If you can't find the specific template you need, see Creating a new template, or see your system administrator.</li> <li>In the Training name pane, enter a name for the Training, then press continue.</li> <li> <p>Select an environment from the tiles. If your environment is not listed, use the Search environments box to find it or press New environment to create a new environment. Press  to create an environment if needed. In the Set the connection for your tool(s), enter the URL of the tool if a custom URL has been enabled in the selected environment. Use the Private toggle to lock access to the tool to only the creator of the environment.</p> <p>In the Runtime Settings:</p> <ol> <li>Press Commands and Arguments to add special commands and arguments to your environment selection.</li> <li>Press Environment variable to add an environment variable. Press again if you need more environment variables.</li> </ol> </li> <li> <p>In the Compute resource pane:</p> <ol> <li>Select the number of workers for your training.</li> <li>Select Compute resources for your training or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> </ol> <p>Note</p> <p>The number of compute resources for the workers is based on the number of workers selected.</p> </li> <li> <p>(Optional) Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a *Volume persistency.</li> </ol> </li> <li> <p>(Optional) In the Data sources pane, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> </li> <li> <p>(Optional) In the General pane, add special settings for your training (optional):</p> <ol> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minutes, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails. (default = 30 days)</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>If you if selected  <code>Workers &amp; master</code> Press Continue to <code>Configure the master</code> and go to the next step. If not, then press Create training.</p> </li> <li> <p>If you do not want a different setup for the master, press Create training. If you would like to have a different setup for the master, toggle the switch to enable to enable a different setup.</p> <ol> <li>In the Environment pane select or create a new environment. Use the search box to find environments that are not listed. Press More settings to add an <code>Environment variable</code> or to edit the Command and Arguments field for the environment you selected.</li> <li>In the Compute resource pane, select a Compute resources for your training or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> <li> <p>(Optional) Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a *Volume persistency.</li> </ol> </li> <li> <p>(Optional) In the Data sources pane, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> </li> <li> <p>(Optional) In the General pane, add special settings for your training (optional):</p> <ol> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minutes, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails. (default = 30 days)</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> </ol> </li> <li> <p>When your training configuration is complete. press Create training.</p> </li> </ol>"},{"location":"Researcher/workloads/trainings/#managing-trainings","title":"Managing Trainings","text":"<p>The Trainings list contains a list of training jobs that you have created or have access to.</p> <p>To manage your trainings:</p> <ol> <li>Press Tranings in the left menu.</li> <li>Select a Training from the list.</li> <li>Choose from the following actions:<ul> <li>Activate\u2014activates the selected training job.</li> <li>Stop\u2014stops the selected training job.</li> <li>Connect\u2014connects to the training job's configured environment.</li> <li>Copy &amp; edit\u2014copies the details of the selected training job to a new training job.</li> <li>Delete\u2014deletes the current training session.</li> <li>Show details\u2014displays details about the training job.</li> </ul> </li> </ol>"},{"location":"Researcher/workloads/trainings/#training-details","title":"Training details","text":"<p>Training details are displayed using the Show details action. The details available per training job include;</p> <ul> <li>Event history\u2014a graph of the job's status over time along with a list of events found in the log.</li> <li> <p>Metrics\u2014a graph of available metrics for the job. Use the drop down select a date and a time slice. Metrics include:</p> </li> <li> <p>GPU utilization</p> </li> <li>GPU memory usage</li> <li>CPU usage</li> <li> <p>CPU memory usage</p> </li> <li> <p>Logs\u2014a log file of the current status. Use the download button to save the logs.</p> </li> </ul> <p>To hide the training details, press Hide details.</p>"},{"location":"Researcher/workloads/trainings/#download-trainings-table","title":"Download Trainings Table","text":"<p>You can download the Trainings table to a CSV file. Downloading a CSV can provide a snapshot history of your Trainings over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>To download the Trainings table to a CSV: 1. Open Trainings. 2. From the Columns icon, select the columns you would like to have displayed in the table. 3. Click on the ellipsis labeled More, and download the CSV.</p>"},{"location":"Researcher/workloads/workload-support/","title":"Workload Support","text":"<p>Workloads are the basic unit of work in Run:ai. Researchers and Engineers use workloads for every stage in their AI Project lifecycle. Workloads can be used to build, train, or deploy a model. Run:ai supports all types of Kubernetes workloads. Researchers can work with any workload in their organization but will get the largest value working with Run:ai native workloads.</p> <p>Run:ai offers three native types of workloads:</p> <ul> <li>Workspaces For building the model  </li> <li>Training For training tasks of the model and data preparation  </li> <li>Inference For deploying and serving the model</li> </ul> <p>Run:ai native workloads can be created via the Run:ai User interface, API or Command-line interface.</p>"},{"location":"Researcher/workloads/workload-support/#levels-of-support","title":"Levels of support","text":"<p>Different types of workloads have different levels of support. Understanding what capabilities are needed before selecting the workload type to work with is important. The table below details the level of support for each workload type in Run:ai. The Run:ai native workloads are fully supported with all of Run:ai advanced features and capabilities. While third-party workloads are partially supported. The list of capabilities can change between different Run:ai versions.</p> Functionality Workload Type Run:ai workloads Third-party workloads Training - Standard Workspace Inference Training - distributed All K8s workloads Fairness v v v v v Priority and preemption v v v v v Over quota v v v v v Node pools v v v v v Bin packing / Spread v v v v v Fractions v v v v v Dynamic fractions v v v v v Node level scheduler v v v v v GPU swap v v v v v Elastic scaling NA NA v v v Gang scheduling v v v v v Monitoring v v v v v RBAC v v v v Workload awareness v v v v Workload submission v v v v Workload actions (stop/run) v v v Policies v v v v Scheduling rules v v v <p>Note</p> <p>Workload awareness</p> <p>Specific workload-aware visibility, so that different pods are identified and treated as a single workload (for example GPU utilization, workload view, dashboards).</p>"},{"location":"Researcher/workloads/workload-support/#workload-scopes","title":"Workload scopes","text":"<p>Workloads must be created under a project. A project is the fundamental organization unit in the Run:ai account. To manage workloads, it\u2019s required to first create a project or have one created by the administrator.</p>"},{"location":"Researcher/workloads/workload-support/#policies-and-rules","title":"Policies and rules","text":"<p>Policies and rules empower administrators to establish default values and implement restrictions on workloads allowing enhanced control, assuring compatibility with organizational policies, and optimizing resource usage and utilization.</p>"},{"location":"Researcher/workloads/workload-support/#workload-statuses","title":"Workload statuses","text":"<p>The following table describes the different phases in a workload life cycle.</p> Phase Description Entry condition Exit condition Creating Workload setup is initiated in the Cluster. Resources and pods are now provisioning A workload is submitted A multi-pod group is created Pending Workload is queued and awaiting resource allocation. A pod group exists All pods are scheduled Initializing Workload is retrieving images, starting containers, and preparing pods All pods are scheduled All pods are initialized or a failure to initialize is detected Running Workload is currently in progress with all pods operational All pods initialized (all containers in pods are ready) workload completion or failure Degraded Pods may not align with specifications, network services might be incomplete, or persistent volumes may be detached. Check your logs for specific details. Pending: All pods are running but with issues Running: All pods are running with no issues. Running: All resources are OK Completed: Workload finished with fewer resources Failed: Workload failure or user-defined rules Deleting Workload and its associated resources are being decommissioned from the cluster Deleting the workload. Resources are fully deleted Stopped The workload is on hold and resources are intact but inactive Stopping the workload without deleting resources Transitioning back to the initializing phase or proceeding to deleting the workload Failed Image retrieval failed or containers experienced a crash. Check your logs for specific details. An error occurs preventing the successful completion of the workload Terminal State Completed Workload has successfully finished its execution The workload has finished processing without errors Terminal State"},{"location":"Researcher/workloads/assets/compute/","title":"Compute Resources","text":"<p>This article explains what compute resources are and how to create and use them.</p> <p>Compute resources are one type of workload asset. A compute resource is a template that simplifies how workloads are submitted and can be used by AI practitioners when they submit their workloads.</p> <p>A compute resource asset is a preconfigured building block that encapsulates all the specifications of compute requirements for the workload including:</p> <ul> <li>GPU devices and GPU memory  </li> <li>CPU memory and CPU compute</li> </ul>"},{"location":"Researcher/workloads/assets/compute/#compute-resource-table","title":"Compute resource table","text":"<p>The Compute resource table can be found under Compute resources in the Run:ai UI.</p> <p>The Compute resource table provides a list of all the compute resources defined in the platform and allows you to manage them.</p> <p></p> <p>The Compute resource table consists of the following columns:</p> Column Description Compute resource The name of the compute resource Description A description of the essence of the compute resource GPU devices request per pod The number of requested physical devices per pod of the workload that uses this compute resource GPU memory request per device The amount of GPU memory per requested device that is granted to each pod of the workload that uses this compute resource CPU memory request The minimum amount of CPU memory per pod of the workload that uses this compute resource CPU memory limit The maximum amount of CPU memory per pod of the workload that uses this compute resource CPU compute request The minimum number of CPU cores per pod of the workload that uses this compute resource CPU compute limit The maximum number of CPU cores per pod of the workload that uses this compute resource Scope The scope of this compute resource within the organizational tree. Click the name of the scope to view the organizational tree diagram Workload(s) The list of workloads associated with the compute resource Template(s) The list of workload templates that use this compute resource Created by The name of the user who created the compute resource Creation time The timestamp for when the rule was created Cluster The cluster that the compute resource is associated with"},{"location":"Researcher/workloads/assets/compute/#workloads-associated-with-the-compute-resource","title":"Workloads associated with the compute resource","text":"<p>Click one of the values in the Workload(s) column to view the list of workloads and their parameters.</p> Column Description Workload The workload that uses the compute resource Type (Workspace/Training/Inference) Status Represents the workload lifecycle. see the full list of workload status"},{"location":"Researcher/workloads/assets/compute/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table</li> </ul>"},{"location":"Researcher/workloads/assets/compute/#adding-new-compute-resource","title":"Adding new compute resource","text":"<p>To add a new compute resource:</p> <ol> <li>Go to the Compute resource table  </li> <li>Click +NEW COMPUTE RESOURCE </li> <li>Select under which cluster to create the compute resource  </li> <li>Select a scope </li> <li>Enter a name for the compute resource. The name must be unique.  </li> <li>Optional: Provide a description of the essence of the compute resource  </li> <li> <p>Set the resource types needed within a single node    (The Run:ai scheduler tries to match a single node that complies with the compute resource for each of the workload\u2019s pods)  </p> <ul> <li> <p>GPU </p> <ul> <li>GPU devices per pod The number of devices (physical GPUs) per pod    (for example, if you requested 3 devices per pod and the running workload using this compute resource consists of 3 pods, there are 9 physical GPU devices used in total)  </li> </ul> <p>Note</p> <ul> <li>You can insert a whole number of devices (0; 1; 2; 3; \u2026)  </li> <li>When setting it to zero, the workload using this computer resource neither requests or uses GPU resources while running  </li> <li>Only when setting it to 1, a fraction of a GPU memory can be requested  </li> <li>When setting a number higher than 1, the entire GPU memory of the devices is used by the running workloads  </li> </ul> <ul> <li>GPU memory per device <ul> <li>Select the memory request format  <ul> <li>% (of device) - Fraction of a GPU device\u2019s memory  </li> <li>MB (memory size) - An explicit GPU memory unit  </li> <li>GB (memory size) - An explicit GPU memory unit  </li> <li>Multi-instance GPU (MIG) - MIG profile  </li> </ul> </li> <li>Set the memory Request - The minimum amount of GPU memory that is provisioned per device. This means that any pod of a running workload that uses this compute resource, receives this amount of GPU memory for each device(s) the pod utilizes  </li> <li>Optional: Set the memory Limit - The maximum amount of GPU memory that is provisioned per device. This means that any pod of a running workload that uses this compute resource, receives at most this amount of GPU memory for each device(s) the pod utilizes. To set a Limit, first enable the limit toggle. The limit value must be equal to or higher than the request.  </li> </ul> </li> </ul> <p>Note</p> <ul> <li>GPU memory limit is disabled by default. If you cannot see the Limit toggle in the compute resource form, then it must be enabled by your Administrator, under General Settings \u2192 Resources \u2192 GPU resource optimization  </li> <li>When a Limit is set and is bigger than the Request, the scheduler allows each pod to reach the maximum amount of GPU memory in an opportunistic manner (only upon availability).  </li> <li>If the GPU Memory Limit is bigger that the Request the pod is prone to be killed by the Run:ai toolkit (out of memory signal). The greater the difference between the GPU memory used and the request, the higher the risk of being killed  </li> <li>If GPU resource optimization is turned off, the minimum and maximum are in fact equal  </li> </ul> </li> <li> <p>CPU </p> <ul> <li>CPU compute per pod <ul> <li>Select the units for the CPU compute (Cores / Millicores)  </li> <li>Set the CPU compute Request - the minimum amount of CPU compute that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives this amount of CPU compute for each pod.  </li> <li>Optional: Set the CPU compute Limit - The maximum amount of CPU compute that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives at most this amount of CPU compute. To set a Limit, first enable the limit toggle. The limit value must be equal to or higher than the request. By default, the limit is set to \u201cUnlimited\u201d - which means that the pod may consume all the node's free CPU compute resources.  </li> </ul> </li> <li>CPU memory per pod <ul> <li>Select the units for the CPU memory (MB / GB)  </li> <li>Set the CPU memory Request - The minimum amount of CPU memory that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives this amount of CPU memory for each pod.  </li> <li>Optional: Set the CPU memory Limit - The maximum amount of CPU memory that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives at most this amount of CPU memory. To set a Limit, first enable the limit toggle. The limit value must be equal to or higher than the request. By default, the limit is set to \u201cUnlimited\u201d - Meaning that the pod may consume all the node's free CPU memory resources.  </li> </ul> </li> </ul> <p>Note</p> <p>If the CPU Memory Limit is bigger that the Request the pod is prone to be killed by the operating system (out of memory signal). The greater the difference between the CPU memory used and the request, the higher the risk of being killed.  </p> </li> </ul> </li> <li> <p>Optional: More settings  </p> <ul> <li>Increase shared memory size When enabled, the shared memory size available to the pod is increased from the default 64MB to the node's total available memory or the CPU memory limit, if set above.  </li> <li>Set extended resource(s) Click +EXTENDED RESOURCES to add resource/quantity pairs. For more information on how to set extended resources, see the Extended resources and Quantity guides  </li> </ul> </li> <li> <p>Click CREATE COMPUTE RESOURCE</p> <p>Note</p> <p>It is also possible to add compute resources directly when creating a specific Workspace, training or inference workload.</p> </li> </ol>"},{"location":"Researcher/workloads/assets/compute/#editing-a-compute-resource","title":"Editing a compute resource","text":"<p>To edit a compute resource:</p> <ol> <li>Select the compute resource from the table  </li> <li>Click RENAME to edit its name and description</li> </ol> <p>Note</p> <p>Additional fields can be edited using the API.</p>"},{"location":"Researcher/workloads/assets/compute/#copying-editing-a-compute-resource","title":"Copying &amp; editing a compute resource","text":"<p>To copy &amp; edit a compute resource:</p> <ol> <li>Select the compute resource you want to duplicate  </li> <li>Click COPY &amp; EDIT </li> <li>Update the compute resource and click CREATE COMPUTE RESOURCE</li> </ol>"},{"location":"Researcher/workloads/assets/compute/#deleting-a-compute-resource","title":"Deleting a compute resource","text":"<ol> <li>Select the compute resource you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion  </li> </ol> <p>Note</p> <p>It is not possible to delete a compute resource being used by an existing workload and template.</p>"},{"location":"Researcher/workloads/assets/compute/#using-api","title":"Using API","text":"<p>Go to the Compute resources API reference to view the available actions</p>"},{"location":"Researcher/workloads/assets/credentials/","title":"Credentials","text":"<p>Credentials are used to unlock protected resources such as applications, containers, and other assets.</p> <p>The Credentials manager in the Run:ai environment supports the following types of credentials:</p> <p>Docker registry</p> <p>Access key</p> <p>Username and password</p> <p>Generic Secret</p>"},{"location":"Researcher/workloads/assets/credentials/#secrets","title":"Secrets","text":"<p>Credentials are built using <code>Secrets</code>. A <code>Secret</code> is an object that contains a small amount of sensitive data so that you don't need to include confidential data in your application code. When creating a credential you can either create a new secret or use an existing secret.</p>"},{"location":"Researcher/workloads/assets/credentials/#existing-secrets","title":"Existing secrets","text":"<p>An existing secret is a secret that you have created before creating the credential. One way to create a secret is to use the Kubernetes Secrets creation tool to create a pre-existing secret for the credential. You must <code>label</code> these secrets so that they are registered in the Run:ai environment.</p> <p>The following command makes the secret available to all projects in the cluster.</p> <pre><code>kubectl label secret -n runai &lt;SECRET_NAME&gt; run.ai/resource=&lt;credential_type&gt; run.ai/cluster-wide=true\n</code></pre> <p>The following command makes the secret available to the entire scope of a department.</p> <pre><code>kubectl label secret -n runai &lt;SECRET_NAME&gt; run.ai/resource=&lt;credential_type&gt; run.ai/department=&lt;department-id&gt;\n</code></pre> <p><code>credential_type</code> is one of the following: <code>password</code> / <code>access-key</code> / <code>docker-registry</code></p> <p>The following command makes the secret available to a specific project in the cluster.</p> <pre><code>kubectl label secret -n &lt;NAMESPACE_OF_PROJECT&gt; &lt;SECRET_NAME&gt; run.ai/credentials=true\n</code></pre>"},{"location":"Researcher/workloads/assets/credentials/#user-id-and-password","title":"User-id and password","text":"<p>You can create a credential using a user-id and password. Use the user-id and password of the target resource.</p>"},{"location":"Researcher/workloads/assets/credentials/#configuring-credentials","title":"Configuring Credentials","text":"<p>Important</p> <p>To configure Credentials you need to make sure <code>Workspaces</code> are enabled.</p> <p>To configure Credentials:</p> <ol> <li>Press <code>Credentials</code> in the left menu.</li> <li>Press <code>New Credential</code> and select one from the list.</li> </ol>"},{"location":"Researcher/workloads/assets/credentials/#docker-registry","title":"<code>Docker registry</code>","text":"<ol> <li>Select a <code>Scope</code> (cluster, department, or project) for the credential.</li> <li>In the <code>Credential name</code> field, enter a name for the credential.</li> <li> <p>In the <code>Secret</code> field, choose from <code>Existing secret</code> or <code>New secret</code>.</p> <ul> <li> <p>If you select <code>Existing secret</code>, select an unused secret from the drop down.</p> <p>Note</p> <p>Existing secrets can't be used more than once.</p> </li> <li> <p>If you choose <code>New secret</code>, enter a username and password.</p> </li> </ul> </li> <li> <p>Enter a URL for the docker registry, then press <code>Create credentials</code> to create the credential.</p> </li> </ol>"},{"location":"Researcher/workloads/assets/credentials/#access-key","title":"<code>Access key</code>","text":"<ol> <li>Select a <code>Scope</code> (cluster, department, or project) for the credential.</li> <li>In the <code>Credential name</code> field, enter a name for the credential.</li> <li> <p>In the <code>Secret</code> field, choose from <code>Existing secret</code> or <code>New secret</code>.</p> <ul> <li> <p>If you select <code>Existing secret</code>, select an unused secret from the drop down.</p> <p>Note</p> <p>Existing secrets can't be used more than once.  </p> </li> <li> <p>If you choose <code>New secret</code>, enter an access key and access secret.</p> </li> </ul> </li> <li> <p>Press <code>Create credentials</code> to create the credential.</p> </li> </ol>"},{"location":"Researcher/workloads/assets/credentials/#username-and-password","title":"<code>Username and password</code>","text":"<ol> <li>Select a <code>Scope</code> (cluster, department, or project) for the credential.</li> <li>In the <code>Credential name</code> field, enter a name for the credential.</li> <li> <p>In the <code>Secret</code> field, choose from <code>Existing secret</code> or <code>New secret</code>.</p> <ul> <li> <p>If you select <code>Existing secret</code>, select an unused secret from the drop down.</p> <p>Note</p> <p>Existing secrets can't be used more than once.</p> </li> <li> <p>If you choose <code>New secret</code>, enter a username and password.</p> </li> </ul> </li> <li> <p>Press <code>Create credentials</code> to create the credential.</p> </li> </ol>"},{"location":"Researcher/workloads/assets/credentials/#generic-secret","title":"Generic Secret","text":"<ol> <li>Select a <code>Scope</code> (cluster, department, or project) for the credential.</li> <li>In the <code>Credential name</code> field, enter a name for the credential, and the in the Description field, enter a description..</li> <li> <p>In the <code>Secret</code> field, choose from <code>Existing secret</code> or <code>New secret</code>.</p> <ul> <li> <p>If you select <code>Existing secret</code>, select an unused secret from the drop down.</p> <p>Note</p> <p>Existing secrets can't be used more than once.</p> </li> <li> <p>If you choose <code>New secret</code>, enter a key, valuer pair.</p> </li> </ul> </li> <li> <p>Press <code>Create credentials</code> to create the credential.</p> </li> </ol>"},{"location":"Researcher/workloads/assets/credentials/#credentials-table","title":"Credentials Table","text":"<p>The Credentials table contains a column that shows the status of the credential. The following statuses are supported:</p> Status Description No issues found No issues were found when propagating the credential to the configured scope. Issues found Issues were found while propagating the credentials to the configured scope. Issues found The credential could not be created in the cluster. No status Status could not be displayed because the credentials scope is an account. No Status Status could not be displayed because the current version of the cluster is not up to date. <p>You can download the Credentials table to a CSV file. Downloading a CSV can provide a snapshot history of your credentials over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>Use the Cluster filter at the top of the table to see credentials that are assigned to specific clusters.</p> <p>Note</p> <p>The cluster filter will be in the top bar when there are clusters that are installed with version 2.16 or lower.</p> <p>Use the Add filter to add additional filters to the table.</p> <p>To download the Credentials table to a CSV:</p> <ol> <li>Open Credentials.</li> <li>From the Columns icon, select the columns you would like to have displayed in the table.</li> <li>Click on the ellipsis labeled More, and download the CSV.</li> </ol>"},{"location":"Researcher/workloads/assets/data-volumes/","title":"Data Volumes","text":"<p>Data Volumes offer a powerful solution for storing, managing, and sharing AI training data within your Run.ai environment. This functionality promotes collaboration, simplifies data access control, and streamlines the AI development lifecycle.</p>"},{"location":"Researcher/workloads/assets/data-volumes/#what-are-data-volumes","title":"What are Data Volumes","text":"<p>Data Volumes are snapshots of datasets stored in Kubernetes Persistent Volume Claims (PVCs). They act as a central repository for training data, and offer several key benefits.</p> <ul> <li>Managed with dedicated permissions\u2014Data admins, a new role within Run.ai, have exclusive control over data volume creation, data population, and sharing.</li> <li>Shared between multiple scopes\u2014Unlike other Run:ai data sources, data volumes can be shared across projects, departments, or clusters. This promotes data reuse and collaboration within your organization.</li> <li>Coupled to workloads in the submission process\u2014 Similar to other Run:ai data sources, Data volumes can be easily attached to AI workloads during submission, specifying the data path within the workload environment.</li> </ul> <p>Note</p> <p>Data volumes are not versioned.</p> <p></p>"},{"location":"Researcher/workloads/assets/data-volumes/#data-volumes-use-cases","title":"Data volumes use cases","text":"<p>The following are typical use cases for Data Volumes:</p> <ul> <li>Sharing large data sets with multiple researchers in my organization\u2014Sometimes we have data located in a remote location. After moving it inside the cluster, sharing it easily with multiple users is still hard. Data volumes can help you do that seamlessly and with maximum security and control</li> <li>Sharing data created during the AI work cycle\u2014When it is needed to share training results, generated data sets or other artifacts with our team members. Data volume helps you take your data and share it with your colleagues.</li> </ul>"},{"location":"Researcher/workloads/assets/data-volumes/#data-volumes-authorization","title":"Data volumes authorization","text":"<p>There is now a new role called <code>Data Volumes Administrator</code> which contains the following two sets of permissions and allows you to manage your Data Volumes easily.</p> <p>Note</p> <p>CRUD = Create, Read, Update, and Delete.</p> <p>Data Volumes administrator contains two permission entities:</p> <ul> <li>Data volumes - CRUD</li> <li>Data volumes - sharing list - CRUD</li> </ul> <p>Data volumes (should have the origin project in the scope)</p> <ul> <li>Can create DV in the scope</li> <li>Can read DV in the scope</li> <li>Can update DV in the scope</li> <li>Can delete DV in the scope</li> </ul> <p>Data volumes - sharing list</p> <ul> <li>Can Share DV in the scope</li> <li>Can unshare DV from the scope</li> </ul>"},{"location":"Researcher/workloads/assets/data-volumes/#data-volume-administrator-permissions","title":"Data volume administrator permissions","text":"Entity Permissions Data volumes\u00a0 CRUD Data volumes - sharing list CRUD Account R Department R Project R Jobs R Workloads R Cluster R Overview dashboard R Consumption dashboard R Analytics dashboard R Policies R workloads R Workspaces R Trainings R Environments R Compute resources R Templates R Data source R Inferences R"},{"location":"Researcher/workloads/assets/data-volumes/#data-volume-permissions-for-each-role","title":"Data volume permissions for each role","text":"Role DV permissions Data volume administrator DV CRUD, Sharing CRUD System administrator DV CRUD, Sharing CRUD Department admin DV CRUD, Sharing CRUD Department viewer DV R Researcher manager DV CRUD, Sharing CRUD Editor DV CRUD, Sharing CRUD L1 DV CRUD L2 DV R ML engineer DV R Assets admins\u00a0 DV R Application admin DV R Viewer DV R"},{"location":"Researcher/workloads/assets/data-volumes/#using-data-volumes","title":"Using Data volumes","text":"<p>This section outlines the procedure for creating, sharing, and submitting (Researcher) data volumes.</p>"},{"location":"Researcher/workloads/assets/data-volumes/#creating-data-volumes","title":"Creating Data Volumes","text":"<p>Note</p> <p>Data volume admins can create data volumes within specific projects. Since data volumes are created from PVCs, there has to be a PVC in the namespace of a run:ai project, and a PV bound to it, for Run:Ai to have access to it and create the Data volume from it. Once the DV is created, the admin manages its sharing configurations.</p> <p>Data Volumes are created using the API endpoint. For more information, see Data Volumes</p>"},{"location":"Researcher/workloads/assets/data-volumes/#sharing-data-volumes","title":"Sharing Data volumes","text":"<p>Sharing permissions is a sub-entity of the Data volume management permissions. Meaning they can be assigned independently. A user can have permission to create a DV but not to share it. A data volume can be shared with one or multiple scopes. In all the scopes that the DV is shared, it can be used by the users in their workloads.</p> <p>Data Volumes are shared using the API endpoint. For more information, see Data Volumes.</p>"},{"location":"Researcher/workloads/assets/data-volumes/#using-data-volumes-in-workloads","title":"Using Data Volumes in Workloads","text":"<p>You can attach a data volume to a workload during submission in the same way other data sources are used. You need to specify the desired data path within the data source parameters.</p> <p>Researchers can list available data volumes within their permitted scopes for easy selection.</p> <p>For more information on using a data volume when submitting a workload, see Submitting Workloads.</p> <p>You can also add a data volumes to your workload when submitting a workload via the API. For more information, see Workloads.</p>"},{"location":"Researcher/workloads/assets/datasources/","title":"Overview","text":""},{"location":"Researcher/workloads/assets/datasources/#introduction","title":"Introduction","text":"<p>A data source is a location where data sets relevant to the research are stored. Workspaces can be attached to several data sources for reading and writing. The data can be located locally or in the cloud. Run:ai data sources can use a variety of storage technologies such as Git, S3, NFS, PVC, and more.  </p> <p>The data source is an optional building block for the creation of a workspace.</p> <p></p>"},{"location":"Researcher/workloads/assets/datasources/#create-a-new-data-source","title":"Create a new data source","text":"<p>When you select <code>New Compute Resource</code> you will be presented with various data source options described below.</p>"},{"location":"Researcher/workloads/assets/datasources/#create-an-nfs-data-source","title":"Create an NFS data source","text":"<p>To create an NFS data source, provide:</p> <ul> <li>A data source name.</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li>An NFS server.</li> <li>The path to the data within the server.</li> <li>The path within the container where the data will be mounted.</li> </ul> <p>The data can be set as read-write or limited to read-only permission regardless of any other user privileges.</p>"},{"location":"Researcher/workloads/assets/datasources/#create-a-pvc-data-source","title":"Create a PVC data source","text":"<p>To create an PVC data source, provide:</p> <ul> <li>A data source name</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li> <p>Select an existing PVC or create a new one by providing:</p> </li> <li> <p>A claim name</p> </li> <li>A storage class</li> <li>Access mode</li> <li>Required storage size</li> <li> <p>Volume system mode</p> </li> <li> <p>The path within the container where the data will be mounted.</p> </li> </ul> <p>You can see the status of the resources created in the Data sources table.</p>"},{"location":"Researcher/workloads/assets/datasources/#create-an-s3-data-source","title":"Create an S3 data source","text":"<p>S3 storage saves data in buckets. S3 is typically attributed to AWS cloud service but can also be used as a separate service unrelated to Amazon.</p> <p>To create an S3 data source, provide</p> <ul> <li>A data source name</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li>The relevant S3 service URL server</li> <li>The bucket name of the data.</li> <li>The path within the container where the data will be mounted.</li> </ul> <p>An S3 data source can be public or private. For the latter option, please select the relevant credentials associated with the project to allow access to the data. S3 buckets that use credentials will have a status associated with it. For more information, see Data sources table.</p>"},{"location":"Researcher/workloads/assets/datasources/#create-a-git-data-source","title":"Create a Git data source","text":"<p>To create a Git data source, provide:</p> <ul> <li>A data source name.</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li>The relevant repository URL.</li> <li>The path within the container where the data will be mounted.</li> </ul> <p>The Git data source can be public or private. To allow access to a private Git data source, you must select the relevant credentials associated with the project. Git data sources that use credentials will have a status associated with it. For more information, see Data sources table.</p>"},{"location":"Researcher/workloads/assets/datasources/#create-a-host-path-data-source","title":"Create a host path data source","text":"<p>To create a host path data source, provide:</p> <ul> <li>A data source name.</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li>The relevant path on the host.</li> <li>The path within the container where the data will be mounted.</li> </ul> <p>Note</p> <p>The data can be limited to read-only permission regardless of any other user privileges.</p>"},{"location":"Researcher/workloads/assets/datasources/#create-a-configmap-data-source","title":"Create a ConfigMap data source","text":"<ul> <li> <p>A Run:ai project scope which is assigned to that item and all its subsidiaries.</p> <p>Note</p> <p>You can only choose a project as a scope.</p> </li> </ul> <p>ConfigMaps must be created on the cluster before being used within Run:ai. When created, the ConfigMap must have a label of <code>run.ai/resource: &lt;resource-name&gt;</code>. The resource name specified must be unique to that created resource. </p> <ul> <li>A data source name.</li> <li> <p>A data mount consisting of:</p> </li> <li> <p>A ConfigMap name\u2014select from the drop down.</p> </li> <li>A target location\u2014the path to the container.</li> </ul>"},{"location":"Researcher/workloads/assets/datasources/#create-a-secret-as-data-source","title":"Create a Secret as data source","text":"<ul> <li>A Run:ai project scope which is assigned to that item and all its subsidiaries.</li> <li> <p>A Credentials. To create a new Credentials, see Configuring Credentials</p> <p>Note</p> <p>You can only choose a project as a scope.</p> </li> <li> <p>A data source name and description.</p> </li> <li> <p>A data mount consisting of:</p> </li> <li> <p>A Credentials\u2014select from the drop down.</p> </li> <li>A target location\u2014the path to the container.</li> </ul>"},{"location":"Researcher/workloads/assets/datasources/#data-sources-table","title":"Data sources table","text":"<p>The Data sources table contains a column for the status of the data source. The following statuses are supported:</p> Status Description No issues found No issues were found when propagating the data source to the PROJECTS. Issues found Failed to create the data source for some or all of the PROJECTS. Issues found Failed to access the cluster. Deleting The data source is being removed. <p>Note</p> <ul> <li>The Status column in the table shows statuses based on your level of permissions. For example, a user that has create permissions for the scope, will see statuses that are calculated from the entire scope, while users who have only view and use permissions, will only be able to see statuses from a subset of the scope (assets that they have permissions to).</li> <li>The status of \u201c-\u201d indicates that there is no status because this asset is not cluster-syncing.</li> </ul> <p>You can download the Data Sources table to a CSV file. Downloading a CSV can provide a snapshot history of your Data Sources over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>Use the Cluster filter at the top of the table to see data sources that are assigned to specific clusters.</p> <p>Note</p> <p>The cluster filter will be in the top bar when there are clusters that are installed with version 2.16 or lower.</p> <p>Use the Add filter to add additional filters to the table.</p> <p>To download the Data Sources table to a CSV:</p> <ol> <li>Open Data Sources.</li> <li>From the Columns icon, select the columns you would like to have displayed in the table.</li> <li>Click on the ellipsis labeled More, and download the CSV.</li> </ol>"},{"location":"Researcher/workloads/assets/environments/","title":"Environments","text":"<p>This article explains what environments are and how to create and use them.</p> <p>Environments are one type of workload asset. An environment consists of a configuration that simplifies how workloads are submitted and can be used by AI practitioners when they submit their workloads.</p> <p>An environment asset is a preconfigured building block that encapsulates aspects for the workload such as:</p> <ul> <li>Container image and container configuration  </li> <li>Tools and connections  </li> <li>The type of workload it serves</li> </ul>"},{"location":"Researcher/workloads/assets/environments/#environments-table","title":"Environments table","text":"<p>The Environments table can be found under Environments in the Run:ai platform.</p> <p>The Environment table provides a list of all the environment defined in the platform and allows you to manage them.</p> <p></p> <p>The Environments table consists of the following columns:</p> Column Description Environment The name of the environment Description A description of the essence of the environment Scope The scope of this environment within the organizational tree. Click the name of the scope to view the organizational tree diagram Image The application or service to be run by the workload Workload Architecture This can be either standard for running workloads on a single node or distributed for running distributed workloads on a multiple nodes Tool(s) The tools and connection types the environment exposes Workload(s) The list of existing workloads that use the environment Workload types The workload types that can use the environment Template(s) The list of workload templates that use this environment Created by The user who created the environment. By default Run:ai UI comes with preinstalled environments created by Run:ai Creation time The timestamp for when the environment was created Cluster The cluster that the environment is associated with"},{"location":"Researcher/workloads/assets/environments/#tools-associated-with-the-environment","title":"Tools associated with the environment","text":"<p>Click one of the values in the tools column to view the list of tools and their connection type.</p> Column Description Tool name The name of the tool or application AI practitioner can set up within the environment. Connection type The method by which you can access and interact with the running workload. It's essentially the \"doorway\" through which you can reach and use the tools the workload provide. (E.g node port, external URL, etc)"},{"location":"Researcher/workloads/assets/environments/#workloads-associated-with-the-environment","title":"Workloads associated with the environment","text":"<p>Click one of the values in the Workload(s) column to view the list of workloads and their parameters.</p> Column Description Workload The workload that uses the environment Type The workload type (Workspace/Training/Inference) Status Represents the workload lifecycle. see the full list of workload status"},{"location":"Researcher/workloads/assets/environments/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"Researcher/workloads/assets/environments/#environments-created-by-runai","title":"Environments created by Run:ai","text":"<p>When installing Run:ai, you automatically get the environment created by Run:ai to ease up the onboarding process and support different use cases out of the box. These environments are created at the scope of the account.</p> Environment Image Jupiter-lab jupyter/scipy-notebook jupyter-tensorboard gcr.io/run-ai-demo/jupyter-tensorboard tensorboard tensorflow/tensorflow:latest llm-server runai.jfrog.io/core-llm/runai-vllm:v0.5.5-0.5.0 chatbot-ui runai.jfrog.io/core-llm/llm-app gpt2 runai.jfrog.io/core-llm/quickstart-inference:gpt2-cpu"},{"location":"Researcher/workloads/assets/environments/#adding-a-new-environment","title":"Adding a new environment","text":"<p>Environment creation is limited to specific roles</p> <p>To add a new environment:</p> <ol> <li>Go to the Environments table  </li> <li>Click +NEW ENVIRONMENT </li> <li>Select under which cluster to create the environment  </li> <li>Select a scope </li> <li>Enter a name for the environment. The name must be unique.  </li> <li>Optional: Provide a description of the essence of the environment  </li> <li>Enter the Image URL    If a token or secret is required to pull the image, it is possible to create it via credentials of type docker registry. These credentials are automatically used once the image is pulled (which happens when the workload is submitted)  </li> <li>Set the image pull policy - the condition for when to pull the image from the registry  </li> <li>Set the workload architecture:  <ul> <li>Standard Only standard workloads can use the environment. A standard workload consists of a single process.  </li> <li>Distributed Only distributed workloads can use the environment. A distributed workload consists of multiple processes working together. These processes can run on different nodes.  </li> <li>Select a framework from the list.  </li> </ul> </li> <li>Set the workload type:  <ul> <li>Workspace </li> <li>Training </li> <li>Inference </li> <li>When inference is selected, define the endpoint of the model by providing both the protocol and the container\u2019s serving port  </li> </ul> </li> <li>Optional: Set the connection for your tool(s). The tools must be configured in the image. When submitting a workload using the environment, it is possible to connect to these tools  <ul> <li>Select the tool from the list (the available tools varies from IDE, experiment tracking, and more, including a custom tool for your choice)  </li> <li>Select the connection type  <ul> <li>External URL <ul> <li>Auto generate   A unique URL is automatically created for each workload using the environment  </li> <li>Custom URL   The URL is set manually  </li> </ul> </li> <li>Node port <ul> <li>Auto generate   A unique port is automatically exposed for each workload using the environment  </li> <li>Custom URL   Set the port manually  </li> </ul> </li> <li>Set the container port </li> </ul> </li> </ul> </li> <li>Optional: Set a command and arguments for the container running the pod  <ul> <li>When no command is added, the default command of the image is used (the image entrypoint)  </li> <li>The command can be modified while submitting a workload using the environment  </li> <li>The argument(s) can be modified while submitting a workload using the environment  </li> </ul> </li> <li>Optional: Set the environment variable(s) <ul> <li>The environment variable(s) are added to the default environment variables that are already set within the image  </li> <li>The environment variables can be modified and new variables can be added while submitting a workload using the environment  </li> </ul> </li> <li>Optional: Set the container\u2019s working directory to define where the container\u2019s process starts running. When left empty, the default directory is used.  </li> <li>Optional: Set where the UID, GID and supplementary groups are taken from, this can be:  <ul> <li>From the image </li> <li>From the IdP token (only available in an SSO installations)  </li> <li>Custom (manually set) - decide whether the submitter can modify these value upon submission.  </li> </ul> </li> <li>Optional: Select Linux capabilities - Grant certain privileges to a container without granting all the privileges of the root user. </li> <li>Click CREATE ENVIRONMENT</li> </ol> <p>Note</p> <p>It is also possible to add environments directly when creating a specific workspace, training or inference workload</p>"},{"location":"Researcher/workloads/assets/environments/#editing-an-environment","title":"Editing an environment","text":"<p>To edit an environment:</p> <ol> <li>Select the environment from the table  </li> <li>Click Rename to edit its name and description</li> </ol> <p>Note</p> <p>Additional fields can be edited using the API</p>"},{"location":"Researcher/workloads/assets/environments/#copying-editing-an-environment","title":"Copying &amp; Editing an environment","text":"<p>To copy &amp; edit an environment:</p> <ol> <li>Select the project you want to duplicate  </li> <li>Click COPY &amp; EDIT. </li> <li>Update the environment and click SAVE.</li> </ol>"},{"location":"Researcher/workloads/assets/environments/#deleting-an-environment","title":"Deleting an environment","text":"<p>To delete an environment:</p> <ol> <li>Select the environment you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>It is not possible to delete an environment being used by an existing workload and template.</p>"},{"location":"Researcher/workloads/assets/environments/#using-api","title":"Using API","text":"<p>Go to the Environment API reference to view the available actions</p>"},{"location":"Researcher/workloads/assets/existing-PVC/","title":"Persistent Volumes (PVs) &amp; Persistent Volume Claims (PVCs)","text":"<p>Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) are concepts in Kubernetes for managing storage. A PV is a piece of storage in the cluster, provisioned by an administrator or dynamically by Kubernetes using a StorageClass. It is a resource in the cluster, just like a node is a cluster resource.</p> <p>PVCs are requests for storage by a user. They are similar to pods, in that pods consume node resources and PVCs consume PV resources. PVCs allow users to request specific sizes and access modes (for example, read/write once, read-only many) without needing to know the details of the underlying storage infrastructure.</p> <p>Using PVs and PVCs in Kubernetes is essential for AI workloads as they provide a reliable and consistent way to manage storage that persists beyond the lifecycle of individual pods. This ensures that data generated by AI workloads is not lost when pods are rescheduled or updated, providing a seamless and efficient storage solution that can handle the large datasets typically associated with AI projects.</p>"},{"location":"Researcher/workloads/assets/existing-PVC/#data-source-of-type-persistent-volume-claim-pvc","title":"Data source of type Persistent Volume Claim (PVC)","text":"<p>At Run:ai, a data source of type PVC is an abstraction, mapping directly to a Kubernetes PVC. This type of integration allows you to specify and manage your data storage requirements within the Run:ai platform, while using familiar Kubernetes storage concepts.</p> <p>By leveraging PVCs as data sources, Run:ai enables access to persistent storage for workloads, ensuring that data remains consistent and accessible across different compute resources and workload executions.</p>"},{"location":"Researcher/workloads/assets/existing-PVC/#creating-a-data-source-of-type-pvc-via-the-ui","title":"Creating a data source of type PVC via the UI","text":"<p>Like any other asset, when creating a data source, the user can select the scope of the data source, based on their permissions set in Run:ai\u2019s Role Based Access Control (RBAC) system.</p> <p>For example: By selecting Department B as the scope of the asset, any user with a role which allows them to view the data source in Department A or any of its subordinate units (current and future) can view this PVC.  </p> <p>There are two different ways of creating data source of type PVC:</p> <ol> <li>Existing PVC - Data source of type PVC using an existing PVC in the cluster.</li> <li>New PVC - Data source of type PVC by creating a new PVC in the cluster. </li> </ol> <p>Note</p> <p>If there are no existing PVCs that Run:ai has visibility or authorization to use, this option is disabled in the Run:ai platform. For details on providing visibility and authorization, see below Existing PVC.</p>"},{"location":"Researcher/workloads/assets/existing-PVC/#existing-pvc","title":"Existing PVC","text":"<p>To select an existing PVC in the Run:ai platform, the admin is responsible for performing a number of actions prior to creating the data source via the Run:ai UI (or API). These actions provide Run:ai with access to the existing PVC, authorization to share across the selected scope and eventually result in exposing the existing PVC in the UI for the user to select.</p> <p>Click the link for more information on creating a data source of type PVC via API.</p> <p>The actions taken by the admin are based on the scope (cluster, department or project) that the admin wants for data source of type PVC.</p>"},{"location":"Researcher/workloads/assets/existing-PVC/#for-a-cluster-scope","title":"For a cluster scope","text":"<ol> <li>Locate the PVC in the runai namespace  </li> <li>Provide Run:ai with visibility and authorization to share the PVC to your selected scope by implementing the following label: run.ai/cluster-wide: \"true\"</li> </ol> <p>Note</p> <p>This step is also relevant for creating the data source of type PVC via API.</p> <p>In the Run:ai platform finish creating the data source of type PVC:</p> <ol> <li>Select your cluster as a scope  </li> <li>Select the existing PVC  </li> <li>Complete all mandatory fields  </li> <li>Click Create</li> </ol>"},{"location":"Researcher/workloads/assets/existing-PVC/#for-a-department-scope","title":"For a department scope","text":"<ol> <li>Locate the PVC in the runai namespace  </li> <li>Provide Run:ai with visibility and authorization to share the PVC to your selected scope by implementing the following label: run.ai/department: \"id\"  </li> <li>In the Run:ai platform finish creating the data source of type PVC:  </li> <li>Select you department as a scope (the same one as in the label)  </li> <li>Select the existing PVC  </li> <li>Complete all mandatory fields  </li> <li>Click Create</li> </ol>"},{"location":"Researcher/workloads/assets/existing-PVC/#for-a-project-scope","title":"For a project scope","text":"<p>Note</p> <p>For project scope, no labels are required.</p> <ol> <li>In the Run:ai platform finish creating the data source of type PVC:  </li> <li>Select your project as a scope  </li> <li>Select the existing PVC  </li> <li>Complete all mandatory fields  </li> <li>Click Create</li> </ol>"},{"location":"Researcher/workloads/assets/existing-PVC/#creating-a-new-pvc","title":"Creating a new PVC","text":"<p>When creating a data source of type PVC using a new PVC, Run:ai creates the PVC for you in the cluster.</p> <ol> <li>Select your scope of choice  </li> <li>Select new PVC  </li> <li>Complete all mandatory fields  </li> <li>Click Create</li> </ol> <p>Notes</p> <p>When creating data source of type PVC using a new PVC, the PVC is immediately created in the cluster (even if no workload has requested to use this PVC). PVCs created in the cluster using the 'New PVC' option will not appear in the 'Existing PVC' selection.</p>"},{"location":"Researcher/workloads/assets/overview/","title":"Overview","text":"<p>Workload assets enable organizations to:</p> <ul> <li>Create and reuse preconfigured setup for code, data, storage and resources to be used by AI practitioners to simplify the process of submitting workloads  </li> <li>Share the preconfigured setup with a wide audience of AI practitioners with similar needs</li> </ul> <p>Note</p> <ul> <li>The creation of assets is possible only via API and the Run:ai UI  </li> <li>The submission of workloads using assets, is possible only via the Run:ai UI</li> </ul>"},{"location":"Researcher/workloads/assets/overview/#workload-asset-types","title":"Workload asset types","text":"<p>There are four workload asset types used by the workload:</p> <ul> <li>Environments   The container image, tools and connections for the workload  </li> <li>Data sources   The type of data, its origin and the target storage location such as PVCs or cloud storage buckets where datasets are stored  </li> <li>Compute resources   The compute specification, including GPU and CPU compute and memory  </li> <li>Credentials   The secrets to be used to access sensitive data, services, and applications such as docker registry or S3 buckets</li> </ul>"},{"location":"Researcher/workloads/assets/overview/#asset-scope","title":"Asset scope","text":"<p>When a workload asset is created, a scope is required. The scope defines who in the organization can view and/or use the asset.</p> <p>Note</p> <p>When an asset is created via API, the scope can be the entire account, this is currently an experimental feature.</p>"},{"location":"Researcher/workloads/assets/overview/#who-can-create-an-asset","title":"Who can create an asset?","text":"<p>Any subject (user, application, or SSO group) with a role that has permissions to Create an asset, can do so within their scope.</p>"},{"location":"Researcher/workloads/assets/overview/#who-can-use-an-asset","title":"Who can use an asset?","text":"<p>Assets are used when submitting workloads. Any subject (user, application or SSO group) with a role that has permissions to Create workloads, can also use assets.</p>"},{"location":"Researcher/workloads/assets/overview/#who-can-view-an-asset","title":"Who can view an asset?","text":"<p>Any subject (user, application, or SSO group) with a role that has permission to View an asset, can do so within their scope.  </p>"},{"location":"Researcher/workloads/assets/secrets/","title":"Secrets in Workloads","text":""},{"location":"Researcher/workloads/assets/secrets/#kubernetes-secrets","title":"Kubernetes Secrets","text":"<p>Sometimes you want to use sensitive information within your code. For example passwords, OAuth tokens, or ssh keys. The best practice for saving such information in Kubernetes is via Kubernetes Secrets. Kubernetes Secrets let you store and manage sensitive information. Access to secrets is limited via configuration.</p> <p>A Kubernetes secret may hold multiple key - value pairs.</p>"},{"location":"Researcher/workloads/assets/secrets/#using-secrets-in-runai-workloads","title":"Using Secrets in Run:ai Workloads","text":"<p>Our goal is to provide Run:ai Workloads with secrets as input in a secure way. Using the Run:ai command line, you will be able to pass a reference to a secret that already exists in Kubernetes.</p>"},{"location":"Researcher/workloads/assets/secrets/#creating-a-secret","title":"Creating a secret","text":"<p>For details on how to create a Kubernetes secret see: https://kubernetes.io/docs/concepts/configuration/secret/. Here is an example:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\n  namespace: runai-&lt;project-name&gt;\ndata:\n  username: am9obgo=\n  password: bXktcGFzc3dvcmQK\n</code></pre> <p>Then run: <pre><code>kubectl apply -f &lt;file-name&gt;\n</code></pre></p> <p>Notes</p> <ul> <li>Secrets are base64 encoded</li> <li>Secrets are stored in the scope of a namespace and will not be accessible from other namespaces. Hence the reference to the Run:ai Project name above. Run:ai provides the ability to propagate secrets throughout all Run:ai Projects. See below.</li> </ul>"},{"location":"Researcher/workloads/assets/secrets/#attaching-a-secret-to-a-workload-on-submit","title":"Attaching a secret to a Workload on Submit","text":"<p>When you submit a new Workload, you will want to connect the secret to the new Workload. To do that, run:</p> <pre><code>runai submit -e &lt;ENV-VARIABLE&gt;=SECRET:&lt;secret-name&gt;,&lt;secret-key&gt; ....\n</code></pre> <p>For example:</p> <pre><code>runai submit -i ubuntu -e MYUSERNAME=SECRET:my-secret,username\n</code></pre>"},{"location":"Researcher/workloads/assets/secrets/#secrets-and-projects","title":"Secrets and Projects","text":"<p>As per the note above, secrets are namespace-specific. If your secret relates to all Run:ai Projects, do the following to propagate the secret to all Projects:</p> <ul> <li>Create a secret within the <code>runai</code> namespace.</li> <li>Run the following once to allow Run:ai to propagate the secret to all Run:ai Projects:</li> </ul> <pre><code>runai-adm set secret &lt;secret name&gt; --cluster-wide\n</code></pre> <p>Reminder</p> <p>The Run:ai Administrator CLI can be obtained here.</p> <p>To delete a secret from all Run:ai Projects, run:</p> <pre><code>runai-adm remove secret &lt;secret name&gt; --cluster-wide\n</code></pre>"},{"location":"Researcher/workloads/assets/secrets/#secrets-and-policies","title":"Secrets and Policies","text":"<p>A Secret can be set at the policy level. For additional information see policies guide.</p>"},{"location":"Researcher/workloads/assets/templates/","title":"Templates","text":"<p>This article explains the procedure to manage templates.</p> <p>A template is a pre-set configuration that is used to quickly configure and submit workloads using existing assets. A template consists of all the assets a workload needs, allowing researchers to submit a workload in a single click, or make subtle adjustments to differentiate them from each other.</p>"},{"location":"Researcher/workloads/assets/templates/#workspace-templates-table","title":"Workspace templates table","text":"<p>Access to the Templates table can be found on the left-hand menu in the Run:ai platform.</p> <p>The Templates table provides a list of all the templates defined in the platform, and allows you to manage them.</p> <p>Flexible Management</p> <p>It is also possible to manage templates directly for a specific user, application, project, or department.</p> <p></p> <p>The Templates table consists of the following columns:</p> Column Description Scope The scope to which the subject has access. Click the name of the scope to see the scope and its subordinates Environment The name of the environment related to the workspace template Compute resource The name of the compute resource connected to the workspace template Data source(s) The name of the data source(s) connected to the workspace template Created by The subject that created the template Creation time The timestamp for when the template was created Cluster The cluster name containing the template"},{"location":"Researcher/workloads/assets/templates/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV  </li> <li>Refresh (optional) - Click REFRESH to update the table with the latest data  </li> <li>Show/Hide details (optional) - Click to view additional information on the selected row</li> </ul>"},{"location":"Researcher/workloads/assets/templates/#adding-a-new-workspace-template","title":"Adding a new workspace template","text":"<p>To add a new template:</p> <ol> <li>Click +NEW TEMPLATE </li> <li>Set the scope for the template  </li> <li>Enter a name for the template  </li> <li>Select the environment for your workload  </li> <li> <p>Select the node resources needed to run your workload     - or -    Click +NEW COMPUTE RESOURCE</p> </li> <li> <p>Set the volume needed for your workload  </p> </li> <li>Create a new data source  </li> <li>Set auto-deletion, annotations and labels, as required  </li> <li>Click CREATE TEMPLATE</li> </ol>"},{"location":"Researcher/workloads/assets/templates/#editing-a-template","title":"Editing a template","text":"<p>To edit a template:</p> <ol> <li>Select the template from the table  </li> <li>Click Rename to provide it with a new name  </li> <li>Click Copy &amp; Edit to make any changes to the template</li> </ol>"},{"location":"Researcher/workloads/assets/templates/#deleting-a-template","title":"Deleting a template","text":"<p>To delete a template:</p> <ol> <li>Select the template you want to delete  </li> <li>Click DELETE </li> <li>Confirm you want to delete the template</li> </ol>"},{"location":"Researcher/workloads/assets/templates/#using-api","title":"Using API**","text":"<p>Go to the Workload template API reference to view the available actions  </p>"},{"location":"Researcher/workloads/workspaces/overview/","title":"Getting familiar with workspaces","text":"<p>Workspace is a simplified tool for researchers to conduct experiments, build AI models, access standard MLOps tools, and collaborate with their peers.</p> <p>Run:ai workspaces abstract complex concepts related to running containerized workloads in a Kubernetes environment. Aspects such as networking, storage, and secrets, are built from predefined abstracted setups, that ease and streamline the researcher's AI model development.</p> <p>A workspace consists of all the setup and configuration needed for the research, including container images, data sets, resource requests, as well as all required tools for the research, in a single place.  This setup is set to facilitate the research needs and yet to ensure infrastructure owners keep control and efficiency when supporting the various needs.</p> <p>A workspace is associated with a specific Run:ai project (internally: a Kubernetes namespace). A researcher can create multiple workspaces under a specific project.</p> <p>Researchers can only view and use workspaces that are created under projects they are assigned to.</p> <p></p> <p>Workspaces can be created with just a few clicks of a button. See Workspace creation.  </p> <p>Workspaces can be stopped and started to save expensive resources without losing complex environment configurations.</p> <p>Only when a workspace is in status active (see also Workspace Statuses) does it consume resources. </p> <p>When the workspace is active it exposes the connections to the tools (for example, a Jupyter notebook) within the workspace. </p> <p></p> <p>An active workspace is a Run:ai interactive workload. The interactive workload starts when the workspace is started and stops when the workspace is stopped. </p> <p>Workspaces can be used via the user interface or programmatically via the Run:ai Admin API. Workspaces are not supported via the command line interface. You can still run an interactive workload via the command line. </p>"},{"location":"Researcher/workloads/workspaces/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Workspaces are made from building blocks. Read about the various building blocks</li> <li>See how to create a Workspace.  </li> </ul>"},{"location":"Researcher/workloads/workspaces/workspace-v2/","title":"Create a new workspace","text":"<p>A Workspace is assigned to a project and is affected by the project\u2019s quota just like any other workload. A workspace is shared with all project members for collaboration.</p> <p>Note</p> <ul> <li>You must have at least one project configured in the system. To configure a project, see Creating a project.</li> <li>You must have at least 1 researcher assigned to the project.</li> </ul> <p>Use the Jobs form below if you have not enabled the Workloads feature.</p> Jobs enabledWorkloads enabled <p>To create a new workspace:</p> <ol> <li>Press Workspaces on left menu, then press New workspace.</li> <li>Select a project from the project tiles. If your project is not listed, use the Search projects box to find a project.</li> <li> <p>Select a template from the template tiles. If your template is not listed, use the Search templates box to find a template. Choose Start from scratch if you do not have, or want to use a template.</p> <p>A template contains a set of predefined building blocks as well as additional configurations which allow the user to immediately create a templated-based workspace.</p> </li> <li> <p>Enter a name for your workspace and press Continue.</p> </li> <li> <p>Select an environment from the tiles. If your environment is not listed, use the Search environments box to find it or press New environment to create a new environment. Press  to create an environment if needed. In the Set the connection for your tool(s), enter the URL of the tool if a custom URL has been enabled in the selected environment. Use the Private toggle to lock access to the tool to only the creator of the environment.</p> <p>In the Runtime Settings:</p> <ol> <li>Press Commands and Arguments to add special commands and arguments to your environment selection.</li> <li>Press Environment variable to add an environment variable. Press again if you need more environment variables.</li> </ol> </li> <li> <p>Select a compute resource from the tiles. If your compute resource is not listed, use the Search compute resources box to find it. Press New compute resource to create a compute resource if needed.</p> </li> <li> <p>Open the Volume pane, and press Volume to add a volume to your workspace.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a Volume persistency. Choose from Persistent or Ephemeral.</li> </ol> </li> <li> <p>Select a data source from the tiles. If your data source is not listed, use the Search compute resources box to find it. Press New data source to create a new data source if needed.</p> </li> <li> <p>In the General pane, add special settings for your workspace:</p> <ol> <li>Press Auto-deletion to delete the workspace automatically when it either completes or fails. You can configure the timeframe in days, hours, minutes, and seconds. If the timeframe is set to 0, the workspace will be deleted immediately after it completes or fails. (default = 30 days)</li> <li>Press Annotation to a name and value to annotate the workspace. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the workspace. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>Press Create workspace</p> </li> </ol> <p>To create a new workspace:</p> <ol> <li>Press Workloads on left menu, then press New workload, then choose Workspace.</li> <li>Select a project from the project tiles. If your project is not listed, use the Search projects box to find a project.</li> <li> <p>Select a template from the template tiles. If your template is not listed, use the Search templates box to find a template. Choose Start from scratch if you do not have, or want to use a template.</p> <p>A template contains a set of predefined building blocks as well as additional configurations which allow the user to immediately create a templated-based workspace.</p> </li> <li> <p>Enter a name for your workspace and press Continue.</p> </li> <li> <p>Select an environment from the tiles. If your environment is not listed, use the Search environments box to find it. Press New environment to create an environment if needed. In the Set the connection for your tool(s), enter the URL of the tool if a custom URL has been enabled in the selected environment. Use the Private toggle to lock access to the tool to only the creator of the environment.</p> <p>In the Runtime Settings:</p> <ol> <li>Press Commands and Arguments to add special commands and arguments to your environment selection.</li> <li>Press Environment variable to add an environment variable. Press again if you need more environment variables.</li> </ol> </li> <li> <p>Select a compute resource from the tiles. If your compute resource is not listed, use the Search compute resources box to find it. Press New compute resource to create a compute resource if needed.</p> </li> <li> <p>Open the Volume pane, and press Volume to add a volume to your workspace.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a Volume persistency. Choose from Persistent or Ephemeral.</li> </ol> </li> </ol> <ol> <li>Select a data source from the tiles. If your data source is not listed, use the Search data resources box to find it. Press New data source to create a new data source if needed.</li> <li> <p>In the General pane, add special settings for your workspace:</p> <ol> <li>Press Auto-deletion to delete the workspace automatically when it either completes or fails. You can configure the timeframe in days, hours, minuets, and seconds. If the timeframe is set to 0, the workspace will be deleted immediately after it completes or fails. (default = 30 days)</li> <li>Press Annotation to a name and value to annotate the workspace. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the workspace. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>Press Create workspace</p> </li> </ol>"},{"location":"admin/overview-administrator/","title":"Overview: Infrastructure Administrator","text":"<p>The Infrastructure Administrator is an IT person, responsible for the installation, setup and IT maintenance of the Run:ai product. </p> <p>As part of the Infrastructure Administrator documentation you will find:</p> <ul> <li>Install Run:ai <ul> <li>Understand the Run:ai installation</li> <li>Set up a Run:ai Cluster.</li> <li>Set up Researchers to work with Run:ai.</li> </ul> </li> <li>IT Configuration of the Run:ai system</li> <li>Connect Run:ai to an identity provider.</li> <li>Maintenance &amp; monitoring of the Run:ai system</li> <li>Troubleshooting.</li> </ul>"},{"location":"admin/authentication/accessrules/","title":"Access Rules","text":"<p>This article explains the procedure to manage Access rules.</p> <p>Access rules provide users, groups, or applications privileges to system entities.</p> <p>An access rule is the assignment of a role to a subject in a scope: <code>&lt;Subject&gt;</code> is a <code>&lt;Role&gt;</code> in a <code>&lt;Scope&gt;</code>.</p> <p>For example, user user@domain.com is a department admin in department A.</p>"},{"location":"admin/authentication/accessrules/#access-rules-table","title":"Access rules table","text":"<p>The Access rules table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The Access rules table provides a list of all the access rules defined in the platform and allows you to manage them.</p> <p>Note</p> <p>Flexible management</p> <p>It is also possible to manage access rules directly for a specific user, application, project, or department.</p> <p></p> <p>The Access rules table consists of the following columns:</p> Column Description Type The type of subject assigned to the access rule (user, SSO group, or application). Subject The user, SSO group, or application assigned with the role Role The role assigned to the subject Scope The scope to which the subject has access. Click the name of the scope to see the scope and its subordinates Authorized by The user who granted the access rule Creation time The timestamp for when the rule was created Last updated The last time the access rule was updated"},{"location":"admin/authentication/accessrules/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"admin/authentication/accessrules/#adding-new-access-rules","title":"Adding new access rules","text":"<p>To add a new access rule:</p> <ol> <li>Click +NEW ACCESS RULE </li> <li>Select a subject User, SSO Group, or Application </li> <li>Select or enter the subject identifier:  <ul> <li>User Email for a local user created in Run:ai or for SSO user as recognized by the IDP  </li> <li>Group name as recognized by the IDP  </li> <li>Application name as created in Run:ai  </li> </ul> </li> <li>Select a role  </li> <li>Select a scope  </li> <li>Click SAVE RULE</li> </ol> <p>Note</p> <p>An access rule consists of a single subject with a single role in a single scope. To assign multiple roles or multiple scopes to the same subject, multiple access rules must be added.</p>"},{"location":"admin/authentication/accessrules/#editing-an-access-rule","title":"Editing an access rule","text":"<p>Access rules cannot be edited. To change an access rule, you must delete the rule, and then create a new rule to replace it.</p>"},{"location":"admin/authentication/accessrules/#deleting-an-access-rule","title":"Deleting an access rule","text":"<ol> <li>Select the access rule you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol>"},{"location":"admin/authentication/accessrules/#using-api","title":"Using API","text":"<p>Go to the Access rules API reference to view the available actions</p>"},{"location":"admin/authentication/applications/","title":"Applications","text":"<p>This article explains the procedure to manage applications and it\u2019s permissions.</p> <p>Applications are used for API integrations with Run:ai. An application contains a secret key. Using the secret key you can obtain a token and use it within subsequent API calls.</p> <p>Applications are managed locally and assigned with Access Rules to manage its permissions.</p> <p>For example, application ci-pipeline-prod assigned with a Researcher role in Cluster: A.</p>"},{"location":"admin/authentication/applications/#applications-table","title":"Applications table","text":"<p>The Applications table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The Applications table provides a list of all the applications defined in the platform, and allows you to manage them.</p> <p></p> <p>The Applications table consists of the following columns:</p> Column Description Application The name of the application Status The status of the application Access rule(s) The access rules assigned to the application Last login The timestamp for the last time the user signed in Created by The user who created the application Creation time The timestamp for when the application was created Last updated The last time the application was updated"},{"location":"admin/authentication/applications/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"admin/authentication/applications/#creating-an-application","title":"Creating an application","text":"<p>To create an application:</p> <ol> <li>Click +NEW APPLICATION </li> <li>Enter the application\u2019s Name </li> <li>Click CREATE </li> <li>Copy the credentials and store it securely:  <ul> <li>Application name </li> <li>Secret key </li> </ul> </li> <li>Click DONE</li> </ol> <p>Note</p> <p>The secret key is visible only at the time of creation, it cannot be recovered but can be regenerated.</p>"},{"location":"admin/authentication/applications/#adding-an-access-rule-to-an-application","title":"Adding an access rule to an application","text":"<p>To create an access rule:</p> <ol> <li>Select the application you want to add an access rule for  </li> <li>Click ACCESS RULES </li> <li>Click +ACCESS RULE </li> <li>Select a role  </li> <li>Select a scope  </li> <li>Click SAVE RULE </li> <li>Click CLOSE</li> </ol>"},{"location":"admin/authentication/applications/#deleting-an-access-rule-from-an-application","title":"Deleting an access rule from an application","text":"<p>To delete an access rule:</p> <ol> <li>Select the application you want to remove an access rule from  </li> <li>Click ACCESS RULES </li> <li>Find the access rule assigned to the user you would like to delete  </li> <li>Click on the trash icon  </li> <li>Click CLOSE</li> </ol>"},{"location":"admin/authentication/applications/#regenerating-key","title":"Regenerating key","text":"<p>To regenerate an application\u2019s key:</p> <ol> <li>Select the application you want to regenerate it\u2019s secret key  </li> <li>Click REGENERATE KEY </li> <li>Click REGENERATE </li> <li>Review the user\u2019s credentials and store it securely:  <ul> <li>Application name  </li> <li>Secret key </li> </ul> </li> <li>Click DONE</li> </ol> <p>Warning</p> <p>Regenerating an application key revokes its previous key.</p>"},{"location":"admin/authentication/applications/#deleting-an-application","title":"Deleting an application","text":"<ol> <li>Select the application you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol>"},{"location":"admin/authentication/applications/#using-api","title":"Using API","text":"<p>Go to the Applications, Access rules API reference to view the available actions</p>"},{"location":"admin/authentication/authentication-overview/","title":"Authentication &amp; Authorization","text":"<p>Run:ai Authentication &amp; Authorization enables a streamlined experience for the user with precise controls covering the data each user can see and the actions each user can perform in the Run:ai platform.</p> <p>Authentication verifies user identity during login, and Authorization assigns the user with specific permissions according to the assigned access rules.</p> <p>Authenticated access is required to use all aspects of the Run:ai interfaces, including the Run:ai platform, the Run:ai Command Line Interface (CLI) and APIs.</p>"},{"location":"admin/authentication/authentication-overview/#authentication","title":"Authentication","text":"<p>There are multiple methods to authenticate and access Run:ai.</p>"},{"location":"admin/authentication/authentication-overview/#single-sign-on-sso","title":"Single Sign-On (SSO)","text":"<p>Single Sign-On (SSO) is the preferred authentication method by large organizations, as it avoids the need to manage duplicate sets of user identities.</p> <p>Run:ai offers SSO integration, enabling users to utilize existing organizational credentials to access Run:ai without requiring dedicated credentials.</p> <p>Run:ai supports three methods to set up SSO:</p> <ul> <li>SAML </li> <li>OpenID Connect (OIDC) </li> <li>OpenShift</li> </ul> <p>When using SSO, it is highly recommended to manage at least one local user, as a breakglass account (an emergency account), in case access to SSO is not possible.</p>"},{"location":"admin/authentication/authentication-overview/#username-and-password","title":"Username and password","text":"<p>Username and password access can be used when SSO integration is not possible.</p>"},{"location":"admin/authentication/authentication-overview/#secret-key-for-application-programmatic-access","title":"Secret key (for Application programmatic access)","text":"<p>A Secret is the authentication method for Applications. Applications use the Run:ai APIs to perform automated tasks including scripts and pipelines based on their assigned access rules](accessrules.md).</p>"},{"location":"admin/authentication/authentication-overview/#authorization","title":"Authorization","text":"<p>The Run:ai platform uses Role Base Access Control (RBAC) to manage authorization.</p> <p>Once a user or an application is authenticated, they can perform actions according to their assigned access rules.</p>"},{"location":"admin/authentication/authentication-overview/#role-based-access-control-rbac-in-runai","title":"Role Based Access Control (RBAC) in Run:ai","text":"<p>While Kubernetes RBAC is limited to a single cluster, Run:ai expands the scope of Kubernetes RBAC, making it easy for administrators to manage access rules across multiple clusters.</p> <p>RBAC at Run:ai is configured using access rules.</p> <p>An access rule is the assignment of a role to a subject in a scope: <code>&lt;Subject&gt;</code> is a <code>&lt;Role&gt;</code> in a <code>&lt;Scope&gt;</code>.</p> <ul> <li>Subject </li> <li>A user, a group, or an application assigned with the role  </li> <li>Role </li> <li>A set of permissions that can be assigned to subjects  </li> <li>A permission is a set of actions (view, edit, create and delete) over a Run:ai entity (e.g. projects, workloads, users)  <ul> <li>For example, a role might allow a user to create and read Projects, but not update or delete them  </li> <li>Roles at Run:ai are system defined and cannot be created, edited or deleted  </li> </ul> </li> <li>Scope </li> <li>A scope is part of an organization in which a set of permissions (roles) is effective. Scopes include Projects, Departments, Clusters, Account (all clusters).</li> </ul> <p>Below is an example of an access rule: username@company.com is a Department admin in Department: A</p> <p></p>"},{"location":"admin/authentication/non-root-containers/","title":"User Identity in Container","text":"<p>The identity of the user in the container determines its access to resources. For example, network file storage solutions typically use this identity to determine the container's access to network volumes. This document explains multiple ways for propagating the user identity into the container.</p>"},{"location":"admin/authentication/non-root-containers/#the-default-root-access","title":"The Default: Root Access","text":"<p>In docker, as well as in Kubernetes, the default for running containers is running as root. The implication of running as root is that processes running within the container have enough permissions to change anything in the container, and if propagated to network resources - can have permissions outside the container as well.</p> <p>This gives a lot of power to the Researcher but does not sit well with modern security standards of enterprise security.</p> <p>By default, if you run:</p> <p><pre><code>runai submit -i ubuntu --attach --interactive -- bash\n</code></pre> then run <code>id</code>, you will see the root user.</p>"},{"location":"admin/authentication/non-root-containers/#use-runai-flags-to-limit-root-access","title":"Use Run:ai flags to limit root access","text":"<p>There are two [runai submit flags that control user identity at the Researcher level:</p> <ul> <li>The flag <code>--run-as-user</code> starts the container with a specific user. The user is the current Linux user (see below for other behaviors if used in conjunction with Single sign-on).</li> <li>The flag <code>--prevent-privilege-escalation</code> prevents the container from elevating its own privileges into <code>root</code> (e.g. running <code>sudo</code> or changing system files.).</li> </ul> <p>Equivalent flags exist in the Researcher User Interface.</p>"},{"location":"admin/authentication/non-root-containers/#run-as-current-user","title":"Run as Current User","text":"<p>From a Linux/Mac box, run:</p> <pre><code>runai submit -i ubuntu --attach --interactive --run-as-user -- bash\n</code></pre> <p>then run <code>id</code>, you will see the users and groups of the box you have been using to launch the Job.</p>"},{"location":"admin/authentication/non-root-containers/#prevent-escalation","title":"Prevent Escalation","text":"<p>From a Linux/Mac box, run:</p> <pre><code>runai submit -i ubuntu --attach --interactive --run-as-user \\\n  --prevent-privilege-escalation  -- bash\n</code></pre> <p>then verify that you cannot run <code>su</code> to become root within the container.</p>"},{"location":"admin/authentication/non-root-containers/#setting-a-cluster-wide-default","title":"Setting a Cluster-Wide Default","text":"<p>The two flags are voluntary. They are not enforced by the system. It is however possible to enforce them using Policies. Policies allow an Administrator to force compliance on both the User Interface and Command-line interface.</p>"},{"location":"admin/authentication/non-root-containers/#passing-user-identity","title":"Passing user identity","text":""},{"location":"admin/authentication/non-root-containers/#passing-user-identity-from-identity-provider","title":"Passing user identity from Identity Provider","text":"<p>A best practice is to store the user identifier (UID) and the group identifier (GID) in the organization's directory. Run:ai allows you to pass these values to the container and use them as the container identity.</p> <p>To perform this, you must:</p> <ul> <li>Set up single sign-on. Perform the steps for UID/GID integration.</li> <li>Run: <code>runai login</code> and enter your credentials</li> <li>Use the flag --run-as-user</li> </ul> <p>Running <code>id</code> should show the identifier from the directory.</p>"},{"location":"admin/authentication/non-root-containers/#passing-user-identity-explicitly-via-the-researcher-ui","title":"Passing user identity explicitly via the Researcher UI","text":"<p>Via the Researcher User Interface, it is possible to explicitly provide the user id and group id:</p> <p></p>"},{"location":"admin/authentication/non-root-containers/#using-openshift-or-gatekeeper-to-provide-cluster-level-controls","title":"Using OpenShift or Gatekeeper to provide Cluster Level Controls","text":"<p>Run:ai supports OpenShift as a Kubernetes platform. In OpenShift the system will provide a random UID to containers. The flags <code>--run-as-user</code> and <code>--prevent-privilege-escalation</code> are disabled on OpenShift. It is possible to achieve a similar effect on Kubernetes systems that are not OpenShift. A leading tool is Gatekeeper. Gatekeeper similarly enforces non-root on containers at the system level.</p>"},{"location":"admin/authentication/non-root-containers/#creating-a-temporary-home-directory","title":"Creating a Temporary Home Directory","text":"<p>When containers run as a specific user, the user needs to have a pre-created home directory within the image. Otherwise, when running a shell, you will not have a home directory:</p> <pre><code>runai submit -i ubuntu --attach --interactive --run-as-user -- bash\nThe job 'job-0' has been submitted successfully\nYou can run `runai describe job job-0 -p team-a` to check the job status\nWaiting for pod to start running...\nINFO[0007] Job started\nConnecting to pod job-0-0-0\nIf you don't see a command prompt, try pressing enter.\nI have no name!@job-0-0-0:/$ \n</code></pre> <p>Adding home directories to an image per user is not a viable solution. To overcome this, Run:ai provides an additional flag <code>--create-home-dir</code>. Adding this flag creates a temporary home directory for the user within the container.  </p> <p>Notes</p> <ul> <li>Data saved in this directory will not be saved when the container exits.</li> <li>This flag is set by default to true when the <code>--run-as-user</code> flag is used, and false if not.</li> </ul>"},{"location":"admin/authentication/researcher-authentication/","title":"Setup Researcher Access Control","text":""},{"location":"admin/authentication/researcher-authentication/#introduction","title":"Introduction","text":"<p>The following instructions explain how to complete the configuration of access control for Researchers. This requires several steps:</p> <ul> <li>(Mandatory) Modify the Kubernetes entry point (called the <code>Kubernetes API server</code>) to validate the credentials of incoming requests against the Run:ai Authentication authority.</li> <li>(Command-line Interface usage only) Modify the Kubernetes profile to prompt the Researcher for credentials when running <code>runai login</code> (or <code>oc login</code> for OpenShift).</li> </ul> <p>Important</p> <ul> <li>As of Run:ai version 2.16, you only need to perform these steps when accessing Run:ai from the command-line interface or sending YAMLs directly to Kubernetes</li> <li>As of Run:ai version 2.18, you only need to perform these steps when if using the older command-line interface or sending YAMLs directly to Kubernetes.</li> </ul>"},{"location":"admin/authentication/researcher-authentication/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<p>You must direct the Kubernetes API server to authenticate via Run:ai. This requires adding flags to the Kubernetes API Server. The flags show in the Run:ai user interface under <code>Settings</code> | <code>General</code> | <code>Researcher Authentication</code> | <code>Server configuration</code>.</p> <p>Modifying the API Server configuration differs between Kubernetes distributions:</p> Vanilla KubernetesOpenShiftRKERKE2GKEEKSBCMAKSOther <ul> <li>Locate the Kubernetes API Server configuration file. The file's location may differ between different Kubernetes distributions. The location for vanilla Kubernetes is <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code></li> <li>Edit the document, under the <code>command</code> tag, add the server configuration text described above.</li> <li>Verify that the <code>kube-apiserver-&lt;master-node-name&gt;</code> pod in the <code>kube-system</code> namespace has been restarted and that changes have been incorporated. Run the below and verify that the oidc flags you have added:</li> </ul> <pre><code>kubectl get pods -n kube-system kube-apiserver-&lt;master-node-name&gt; -o yaml\n</code></pre> <p>No configuration is needed. Instead, Run:ai assumes that an Identity Provider has been defined at the OpenShift level and that the Run:ai Cluster installation has set the <code>OpenshiftIdp</code> flag to true. For more information see the Run:ai OpenShift control-plane setup.</p> <p>Edit Rancher <code>cluster.yml</code> (with Rancher UI, follow this). Add the following:</p> cluster.yml<pre><code>kube-api:\n    always_pull_images: false\n    extra_args:\n        oidc-client-id: runai  # (1)\n        ...\n</code></pre> <ol> <li>These are example parameters. Copy the actual parameters from <code>Settings | General | Researcher Authentication</code> as described above.</li> </ol> <p>You can verify that the flags have been incorporated into the RKE cluster by following the instructions here and running <code>docker inspect &lt;kube-api-server-container-id&gt;</code>, where <code>&lt;kube-api-server-container-id&gt;</code> is the container ID of api-server via obtained in the Rancher document. </p> <p>If working via the RKE2 Quickstart, edit <code>/etc/rancher/rke2/config.yaml</code>. Add the parameters provided in the server configuration section as described above in the following fashion:</p> /etc/rancher/rke2/config.yaml<pre><code>kube-apiserver-arg:\n- \"oidc-client-id=runai\" # (1)\n...\n</code></pre> <ol> <li>These are example parameters. Copy the actual parameters from <code>Settings | General | Researcher Authentication</code> as described above.</li> </ol> <p>If working via Rancher UI, need to add the flag as part of the cluster provisioning. </p> <p>Under <code>Cluster Management | Create</code>, turn on RKE2 and select a platform. Under <code>Cluster Configuration | Advanced | Additional API Server Args</code>. Add the Run:ai flags as <code>&lt;key&gt;=&lt;value&gt;</code> (e.g. <code>oidc-username-prefix=-</code>).</p> <p>Install Anthos identity service by running:</p> <pre><code>gcloud container clusters update &lt;gke-cluster-name&gt; \\\n    --enable-identity-service --project=&lt;gcp-project-name&gt; --zone=&lt;gcp-zone-name&gt;\n</code></pre> <p>Install the yq utility and run:</p> <p>For username-password authentication, run:</p> <pre><code>kubectl get clientconfig default -n kube-public -o yaml &gt; login-config.yaml\nyq -i e \".spec +={\\\"authentication\\\":[{\\\"name\\\":\\\"oidc\\\",\\\"oidc\\\":{\\\"clientID\\\":\\\"runai\\\",\\\"issuerURI\\\":\\\"$OIDC_ISSUER_URL\\\",\\\"kubectlRedirectURI\\\":\\\"http://localhost:8000/callback\\\",\\\"userClaim\\\":\\\"sub\\\",\\\"userPrefix\\\":\\\"-\\\"}}]}\" login-config.yaml\nkubectl apply -f login-config.yaml\n</code></pre> <p>For single-sign-on, run:</p> <pre><code>kubectl get clientconfig default -n kube-public -o yaml &gt; login-config.yaml\nyq -i e \".spec +={\\\"authentication\\\":[{\\\"name\\\":\\\"oidc\\\",\\\"oidc\\\":{\\\"clientID\\\":\\\"runai\\\",\\\"issuerURI\\\":\\\"$OIDC_ISSUER_URL\\\",\\\"groupsClaim\\\":\\\"groups\\\",\\\"kubectlRedirectURI\\\":\\\"http://localhost:8000/callback\\\",\\\"userClaim\\\":\\\"email\\\",\\\"userPrefix\\\":\\\"-\\\"}}]}\" login-config.yaml\nkubectl apply -f login-config.yaml\n</code></pre> <p>Where the <code>OIDC</code> flags are provided in the Run:ai server configuration section as described above. </p> <p>Then update runaiconfig with  the Anthos endpoint - gke-oidc-envoy. Get the externel IP of the service in the Anthos namespace.</p> <pre><code>kubectl get svc -n anthos-identity-service\nNAME               TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)              AGE\ngke-oidc-envoy     LoadBalancer   10.37.3.111   39.201.319.10   443:31545/TCP        12h\n</code></pre> <p>Add the IP to runaiconfig </p> <pre><code>kubectl -n runai patch runaiconfig runai -p '{\"spec\": {\"researcher-service\": {\"args\": {\"gkeOidcEnvoyHost\": \"35.236.229.19\"}}}}'  --type=\"merge\"\n</code></pre> <p>To create a kubeconfig profile for Researchers run:</p> <pre><code>kubectl oidc login --cluster=CLUSTER_NAME --login-config=login-config.yaml \\\n    --kubeconfig=developer-kubeconfig\n</code></pre> <p>(this will require installing the kubectl oidc plug-in as described in the Anthos document above <code>gcloud components install kubectl-oidc</code>)</p> <p>Then modify the <code>developer-kubeconfig</code> file as described in the Command-line Inteface Access section below.</p> <ul> <li>In the AWS Console, under EKS, find your cluster.</li> <li>Go to <code>Configuration</code> and then to <code>Authentication</code>.</li> <li>Associate a new <code>identity provider</code>. Use the parameters provided in the server configuration section as described above. The process can take up to 30 minutes.</li> </ul> <p>Please follow the \"Vanilla Kubernetes\" instructions</p> <p>Please contact Run:ai customer support.</p> <p>See specific instructions in the documentation of the Kubernetes distribution.  </p>"},{"location":"admin/authentication/researcher-authentication/#command-line-interface-access","title":"Command-line Interface Access","text":"<p>To control access to Run:ai (and Kubernetes) resources, you must modify the Kubernetes configuration file. The file is distributed to users as part of the Command-line interface installation.</p> <p>When making changes to the file, keep a copy of the original file to be used for cluster administration. After making the modifications, distribute the modified file to Researchers.</p> <ul> <li>Under the <code>~/.kube</code> directory edit the <code>config</code> file, remove the administrative user, and replace it with text from <code>Settings | General | Researcher Authentication</code> | <code>Client Configuration</code>.</li> <li>Under <code>contexts | context | user</code> change the user to <code>runai-authenticated-user</code>.</li> </ul> <p>Important</p> <ul> <li>After adding the new user, ensure to delete the following fields from the kubeconfig file to prevent unauthorized access: - Delete: <code>client-certificate-data</code>- Delete: <code>client-key-data</code>- Remove: Any references to the <code>admin</code> user.</li> </ul>"},{"location":"admin/authentication/researcher-authentication/#test-via-command-line-interface","title":"Test via Command-line interface","text":"<ul> <li>Run: <code>runai login</code> (in OpenShift environments use <code>oc login</code> rather than <code>runai login</code>).</li> <li>You will be prompted for a username and password. In a single sign-on flow, you will be asked to copy a link to a browser, log in and return a code.</li> <li>Once login is successful, submit a Job.</li> <li>If the Job was submitted with a Project to which you have no access, your access will be denied.</li> <li>If the Job was submitted with a Project to which you have access, your access will be granted.</li> </ul> <p>You can also submit a Job from the Run:ai User interface and verify that the new job shows on the job list with your user name.</p>"},{"location":"admin/authentication/researcher-authentication/#test-via-user-interface","title":"Test via User Interface","text":"<ul> <li>Open the Run:ai user interface, go to <code>Workloads</code>.</li> <li>On the top-right, select <code>Submit Workload</code>.</li> </ul>"},{"location":"admin/authentication/roles/","title":"Roles","text":"<p>This article explains the available roles in the Run:ai platform.</p> <p>A role is a set of permissions that can be assigned to a subject in a scope.</p> <p>A permission is a set of actions (View, Edit, Create and Delete) over a Run:ai entity (e.g. projects, workloads, users).Roles table</p> <p>The Roles table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The Roles table displays a list of predefined roles available to users in the Run:ai platform. It is not possible to create additional rules or edit or delete existing rules.</p> <p></p> <p>The Roles table consists of the following columns:</p> Column Description Role The name of the role Created by The name of the role creator Creation time The timestamp when the role was created"},{"location":"admin/authentication/roles/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"admin/authentication/roles/#reviewing-a-role","title":"Reviewing a role","text":"<ol> <li>To review a role click the role name on the table  </li> <li>In the role form review the following:  <ul> <li>Role name   The name of the role  </li> <li>Entity  A system-managed object that can be viewed, edited, created or deleted by a user based on their assigned role and scope  </li> <li>Actions  The actions that the role assignee is authorized to perform for each entity  <ul> <li>View If checked, an assigned user with this role can view instances of this type of entity within their defined scope  </li> <li>Edit If checked, an assigned user with this role can change the settings of an instance of this type of entity within their defined scope  </li> <li>Create If checked, an assigned user with this role can create new instances of this type of entity within their defined scope  </li> <li>Delete If checked, an assigned user with this role can delete instances of this type of entity within their defined scope</li> </ul> </li> </ul> </li> </ol>"},{"location":"admin/authentication/roles/#roles-in-runai","title":"Roles in Run:ai","text":"<p>Run:ai supports the following roles and their permissions:  Under each role is a detailed list of the actions that the role assignee is authorized to perform for each entity.</p> Compute resource administrator <p></p> Data source administrator <p></p> Data volume administrator <p></p> Department administrator <p></p> Department viewer <p></p> Editor <p></p> Environment administrator <p></p> L1 researcher <p></p> L2 researcher <p></p> ML engineer <p></p> Research manager <p></p> System administrator <p></p> Template administrator <p></p> Viewer <p></p>"},{"location":"admin/authentication/roles/#permitted-workloads","title":"Permitted workloads","text":"<p>When assigning a role with either one, all or any combination of the View, Edit, Create and Delete permissions for workloads, the subject has permissions to manage not only Run:ai native workloads (Workspace, Training, Inference), but also a list of 3rd party workloads:</p> <ul> <li>k8s: StatefulSet</li> <li>k8s: ReplicaSet</li> <li>k8s: Pod</li> <li>k8s: Deployment</li> <li>batch: Job</li> <li>batch: CronJob</li> <li>machinelearning.seldon.io: SeldonDeployment</li> <li>kubevirt.io: VirtualMachineInstance</li> <li>kubeflow.org: TFJob</li> <li>kubeflow.org: PyTorchJob</li> <li>kubeflow.org: XGBoostJob</li> <li>kubeflow.org: MPIJob</li> <li>kubeflow.org: MPIJob</li> <li>kubeflow.org: Notebook</li> <li>kubeflow.org: ScheduledWorkflow</li> <li>amlarc.azureml.com: AmlJob</li> <li>serving.knative.dev: Service</li> <li>workspace.devfile.io: DevWorkspace</li> <li>ray.io: RayCluster</li> <li>ray.io: RayJob</li> <li>ray.io: RayService</li> <li>ray.io: RayCluster</li> <li>ray.io: RayJob</li> <li>ray.io: RayService</li> <li>tekton.dev: TaskRun</li> <li>tekton.dev: PipelineRun</li> <li>argoproj.io: Workflow</li> </ul>"},{"location":"admin/authentication/roles/#using-api","title":"Using API","text":"<p>Go to the Roles API reference to view the available actions.</p>"},{"location":"admin/authentication/users/","title":"Users","text":"<p>This article explains the procedure to manage users and their permissions.</p> <p>Users can be managed locally, or via the Identity provider, while assigned with Access Rules to manage its permissions.</p> <p>For example, user user@domain.com is a department admin in department A.</p>"},{"location":"admin/authentication/users/#users-table","title":"Users table","text":"<p>The Users table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The users table provides a list of all the users in the platform. You can manage local users and manage user permissions (access rules) for both local and SSO users.</p> <p>Note</p> <p>Single Sign-On users</p> <p>SSO users are managed by the identity provider and appear once they have signed in to Run:ai</p> <p></p> <p>The Users table consists of the following columns:</p> Column Description User The unique identity of the user (email address) Type The type of the user - SSO / local Last login The timestamp for the last time the user signed in Access rule(s) The access rules assigned to the user Created By The user who created the user Creation time The timestamp for when the user was created Last updated The last time the user was updated"},{"location":"admin/authentication/users/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"admin/authentication/users/#creating-a-local-user","title":"Creating a local user","text":"<p>To create a local user:</p> <ol> <li>Click +NEW LOCAL USER </li> <li>Enter the user\u2019s Email address </li> <li>Click CREATE </li> <li>Review and copy the user\u2019s credentials:  <ul> <li>User Email </li> <li>Temporary password to be used on first sign-in  </li> </ul> </li> <li>Click DONE</li> </ol> <p>Note</p> <p>The temporary password is visible only at the time of user\u2019s creation, and must be changed after the first sign-in</p>"},{"location":"admin/authentication/users/#adding-an-access-rule-to-a-user","title":"Adding an access rule to a user","text":"<p>To create an access rule:</p> <ol> <li>Select the user you want to add an access rule for  </li> <li>Click ACCESS RULES </li> <li>Click +ACCESS RULE </li> <li>Select a role  </li> <li>Select a scope  </li> <li>Click SAVE RULE </li> <li>Click CLOSE</li> </ol>"},{"location":"admin/authentication/users/#deleting-users-access-rule","title":"Deleting user\u2019s access rule","text":"<p>To delete an access rule:</p> <ol> <li>Select the user you want to remove an access rule from  </li> <li>Click ACCESS RULES </li> <li>Find the access rule assigned to the user you would like to delete  </li> <li>Click on the trash icon  </li> <li>Click CLOSE</li> </ol>"},{"location":"admin/authentication/users/#resetting-a-user-password","title":"Resetting a user password","text":"<p>To reset a user\u2019s password:</p> <ol> <li>Select the user you want to reset it\u2019s password  </li> <li>Click RESET PASSWORD </li> <li>Click RESET </li> <li>Review and copy the user\u2019s credentials:  <ul> <li>User Email </li> <li>Temporary password to be used on next sign-in  </li> </ul> </li> <li>Click DONE</li> </ol>"},{"location":"admin/authentication/users/#deleting-a-user","title":"Deleting a user","text":"<ol> <li>Select the user you want to delete  </li> <li>Click DELETE </li> <li>In the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>To ensure administrative operations are always available, at least one local user with System Administrator role should exist.</p>"},{"location":"admin/authentication/users/#using-api","title":"Using API","text":"<p>Go to the Users, Access rules API reference to view the available actions</p>"},{"location":"admin/authentication/sso/openidconnect/","title":"Setup SSO with OpenID Connect","text":"<p>Single Sign-On (SSO) is an authentication scheme, allowing users to log-in with a single pair of credentials to multiple, independent software systems.</p> <p>This article explains the procedure to configure single sign-on to Run:ai using the OpenID Connect protocol.</p>"},{"location":"admin/authentication/sso/openidconnect/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have the following available from your identity provider:</p> <ul> <li>Discovery URL - the OpenID server where the content discovery information is published.  </li> <li>ClientID - the ID used to identify the client with the Authorization Server.  </li> <li>Client Secret - a secret password that only the Client and Authorization server know.  </li> <li>Optional: Scopes - a set of user attributes to be used during authentication to authorize access to a user's details.</li> </ul>"},{"location":"admin/authentication/sso/openidconnect/#setup","title":"Setup","text":"<p>Follow the steps below to setup SSO with OpenID Connect.</p>"},{"location":"admin/authentication/sso/openidconnect/#adding-the-identity-provider","title":"Adding the identity provider","text":"<ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section and click +IDENTITY PROVIDER </li> <li>Select Custom OpenID Connect </li> <li>Enter the Discovery URL, Client ID, and Client Secret </li> <li>Copy the Redirect URL to be used in your identity provider  </li> <li>Optional: Add the OIDC scopes  </li> <li>Optional: Enter the user attributes and their value in the identity provider (see the user attributes table below)  </li> <li>Click SAVE    User attributes</li> </ol> Attribute Default value in Run:ai Description User role groups GROUPS If it exists in the IDP, it allows you to assign Run:ai role groups via the IDP. The IDP attribute must be a list of strings. Linux User ID UID If it exists in the IDP, it allows Researcher containers to start with the Linux User UID. Used to map access to network resources such as file systems to users. The IDP attribute must be of type integer. Linux Group ID GID If it exists in the IDP, it allows Researcher containers to start with the Linux Group GID. The IDP attribute must be of type integer. Supplementary Groups SUPPLEMENTARYGROUPS If it exists in the IDP, it allows Researcher containers to start with the relevant Linux supplementary groups. The IDP attribute must be a list of integers. Email email Defines the user attribute in the IDP holding the user's email address, which is the user identifier in Run:ai User first name firstName Used as the user\u2019s first name appearing in the Run:ai user interface User last name lastName Used as the user\u2019s last name appearing in the Run:ai user interface"},{"location":"admin/authentication/sso/openidconnect/#testing-the-setup","title":"Testing the setup","text":"<ol> <li>Log-in to the Run:ai platform as an admin  </li> <li>Add Access Rules to an SSO user defined in the IDP  </li> <li>Open the Run:ai platform in an incognito browser tab  </li> <li>On the sign-in page click CONTINUE WITH SSO    You are redirected to the identity provider sign in page  </li> <li>In the identity provider sign-in page, log in with the SSO user who you granted with access rules  </li> <li>If you are unsuccessful signing-in to the identity provider, follow the Troubleshooting section below</li> </ol>"},{"location":"admin/authentication/sso/openidconnect/#editing-the-identity-provider","title":"Editing the identity provider","text":"<p>You can view the identity provider details and edit its configuration:</p> <ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider box, click Edit identity provider </li> <li>You can edit either the Discovery URL, Client ID, Client Secret, OIDC scopes, or the User attributes</li> </ol> <p>### Removing the identity provider</p> <p>You can remove the identity provider configuration:</p> <ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider card, click Remove identity provider </li> <li>In the dialog, click REMOVE to confirm the action</li> </ol> <p>Note</p> <p>To avoid losing access, removing the identity provider must be carried out by a local user.</p>"},{"location":"admin/authentication/sso/openidconnect/#troubleshooting","title":"Troubleshooting","text":"<p>If testing the setup was unsuccessful, try the different troubleshooting scenarios according to the error you received.</p>"},{"location":"admin/authentication/sso/openidconnect/#troubleshooting-scenarios","title":"Troubleshooting scenarios","text":"403 - Sorry, we can\u2019t let you see this page. Something about permissions\u2026 <p>Description: The authenticated user is missing permissions</p> <p>Mitigation:</p> <ol> <li>Validate either the user or its related group/s are assigned with access rules </li> <li>Validate groups attribute is available in the configured OIDC Scopes  </li> <li>Validate the user\u2019s groups attribute is mapped correctly</li> </ol> <p>Advanced:</p> <ol> <li>Open the Chrome DevTools: Right-click on page \u2192 Inspect \u2192 Console tab  </li> <li>Run the following command to retrieve and paste the user\u2019s token: <code>localStorage.token;</code> </li> <li>Paste in https://jwt.io </li> <li>Under the Payload section validate the values of the user\u2019s attributes</li> </ol> 401 - We\u2019re having trouble identifying your account because your email is incorrect or can\u2019t be found. <p>Description: Authentication failed because email attribute was not found.</p> <p>Mitigation:</p> <ol> <li>Validate email attribute is available in the configured OIDC Scopes  </li> <li>Validate the user\u2019s email attribute is mapped correctly</li> </ol> Unexpected error when authenticating with identity provider <p></p> <p>Description: User authentication failed</p> <p>Mitigation:</p> <ol> <li>Validate that the configured OIDC Scopes exist and match the Identity Provider\u2019s available scopes</li> </ol> <p>Advanced:</p> <ol> <li>Look for the specific error message in the URL address</li> </ol> Unexpected error when authenticating with identity provider (SSO sign-in is not available) <p></p> <p>Description: User authentication failed</p> <p>Mitigation:</p> <ol> <li>Validate that the configured OIDC scope exists in the Identity Provider  </li> <li>Validate the configured Client Secret match the Client Secret in the Identity Provider</li> </ol> <p>Advanced:</p> <ol> <li>Look for the specific error message in the URL address</li> </ol> Client not found <p>Description: OIDC Client ID was not found in the Identity Provider</p> <p>Mitigation:</p> <ol> <li>Validate that the configured Client ID matches the Identity Provider Client ID  </li> </ol>"},{"location":"admin/authentication/sso/openshift/","title":"Setup SSO with OpenShift","text":"<p>Single Sign-On (SSO) is an authentication scheme, allowing users to log-in with a single pair of credentials to multiple, independent software systems.</p> <p>This article explains the procedure to configure single sign-on to Run:ai using the OpenID Connect protocol in OpenShift V4.</p>"},{"location":"admin/authentication/sso/openshift/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have the following available from your OpenShift cluster:</p> <ul> <li>OpenShift OAuth client - see Registering an additional OAuth client </li> <li>ClientID - the ID used to identify the client with the Authorization Server.  </li> <li>Client Secret - a secret password that only the Client and Authorization Server know.  </li> <li>Base URL - the OpenShift API Server endpoint (example: https://api.&lt;cluster-url&gt;:6443)</li> </ul>"},{"location":"admin/authentication/sso/openshift/#setup","title":"Setup","text":"<p>Follow the steps below to setup SSO with OpenShift.</p>"},{"location":"admin/authentication/sso/openshift/#adding-the-identity-provider","title":"Adding the identity provider","text":"<ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section and click +IDENTITY PROVIDER </li> <li>Select OpenShift V4 </li> <li>Enter the Base URL, Client ID, and Client Secret from your OpenShift OAuth client.  </li> <li>Copy the Redirect URL to be used in your OpenShift OAuth client  </li> <li>Optional: Enter the user attributes and their value in the identity provider (see the user attributes table below)  </li> <li>Click SAVE    User attributes</li> </ol> Attribute Default value in Run:ai Description User role groups GROUPS If it exists in the IDP, it allows you to assign Run:ai role groups via the IDP. The IDP attribute must be a list of strings. Linux User ID UID If it exists in the IDP, it allows researcher containers to start with the Linux User UID. Used to map access to network resources such as file systems to users. The IDP attribute must be of type integer. Linux Group ID GID If it exists in the IDP, it allows researcher containers to start with the Linux Group GID. The IDP attribute must be of type integer. Supplementary Groups SUPPLEMENTARYGROUPS If it exists in the IDP, it allows researcher containers to start with the relevant Linux supplementary groups. The IDP attribute must be a list of integers. Email email Defines the user attribute in the IDP holding the user's email address, which is the user identifier in Run:ai User first name firstName Used as the user\u2019s first name appearing in the Run:ai platform User last name lastName Used as the user\u2019s last name appearing in the Run:ai platform"},{"location":"admin/authentication/sso/openshift/#testing-the-setup","title":"Testing the setup","text":"<ol> <li>Open the Run:ai platform as an admin  </li> <li>Add Access Rules to an SSO user defined in the IDP  </li> <li>Open the Run:ai platform in an incognito browser tab  </li> <li>On the sign-in page click CONTINUE WITH SSO    You are redirected to the OpenShift IDP sign-in page  </li> <li>In the identity provider sign-in page, log-in with the SSO user who you granted with access rules  </li> <li>If you are unsuccessful signing-in to the identity provider, follow the Troubleshooting section below</li> </ol>"},{"location":"admin/authentication/sso/openshift/#editing-the-identity-provider","title":"Editing the identity provider","text":"<p>You can view the identity provider details and edit its configuration:</p> <ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider box, click Edit identity provider </li> <li>You can edit either the Base URL, Client ID, Client Secret, or the User attributes</li> </ol>"},{"location":"admin/authentication/sso/openshift/#removing-the-identity-provider","title":"Removing the identity provider","text":"<p>You can remove the identity provider configuration:</p> <ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider card, click Remove identity provider </li> <li>In the dialog, click REMOVE to confirm the action</li> </ol> <p>Note</p> <p>To avoid losing access, removing the identity provider must be carried out by a local user.</p>"},{"location":"admin/authentication/sso/openshift/#troubleshooting","title":"Troubleshooting","text":"<p>If testing the setup was unsuccessful, try the different troubleshooting scenarios according to the error you received.</p>"},{"location":"admin/authentication/sso/openshift/#troubleshooting-scenarios","title":"Troubleshooting scenarios","text":"403 - Sorry, we can\u2019t let you see this page. Something about permissions\u2026 <p>Description: The authenticated user is missing permissions</p> <p>Mitigation:</p> <ol> <li>Validate either the user or its related group/s are assigned with access rules </li> <li>Validate groups attribute is available in the configured OIDC Scopes  </li> <li>Validate the user\u2019s groups attribute is mapped correctly</li> </ol> <p>Advanced:</p> <ol> <li>Open the Chrome DevTools: Right-click on page \u2192 Inspect \u2192 Console tab  </li> <li>Run the following command to retrieve and copy the user\u2019s token: <code>localStorage.token;</code> </li> <li>Paste in https://jwt.io </li> <li>Under the Payload section validate the value of the user\u2019s attributes</li> </ol> 401 - We\u2019re having trouble identifying your account because your email is incorrect or can\u2019t be found. <p>Description: Authentication failed because e-mail attribute was not found.</p> <p>Mitigation:</p> <ol> <li>Validate email attribute is available in the configured OIDC Scopes  </li> <li>Validate the user\u2019s email attribute is mapped correctly</li> </ol> Unexpected error when authenticating with identity provider <p></p> <p>Description: User authentication failed</p> <p>Mitigation:</p> <ol> <li>Validate the the configured OIDC Scopes exist and match the Identity Provider\u2019s available scopes</li> </ol> <p>Advanced:</p> <ol> <li>Look for the specific error message in the URL address</li> </ol> Unexpected error when authenticating with identity provider (SSO sign-in is not available) <p></p> <p>Description: User authentication failed</p> <p>Mitigation:</p> <ol> <li>Validate that the configured OIDC scope exists in the Identity Provider  </li> <li>Validate that the configured Client Secret matches the Client Secret value in the OAuthclient Kubernetes object.</li> </ol> <p>Advanced:</p> <ol> <li>Look for the specific error message in the URL address</li> </ol> unauthorized_client <p></p> <p>Description: OIDC Client ID was not found in the OpenShift IDP</p> <p>Mitigation:</p> <ol> <li>Validate that the configured Client ID matches the value in the OAuthclient Kubernetes object.  </li> </ol>"},{"location":"admin/authentication/sso/saml/","title":"Setup SSO with SAML","text":"<p>Single Sign-On (SSO) is an authentication scheme, allowing users to log-in with a single pair of credentials to multiple, independent software systems.</p> <p>This article explains the procedure to configure SSO to Run:ai using the SAML 2.0 protocol.</p>"},{"location":"admin/authentication/sso/saml/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following available from your identity provider:</p> <ul> <li>SAML XML Metadata</li> </ul>"},{"location":"admin/authentication/sso/saml/#setup","title":"Setup","text":"<p>Follow the steps below to setup SSO with SAML.</p>"},{"location":"admin/authentication/sso/saml/#adding-the-identity-provider","title":"Adding the identity provider","text":"<ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section and click +IDENTITY PROVIDER </li> <li>Select Custom SAML 2.0 </li> <li>Select either From computer or From URL <ul> <li>From computer - click the Metadata XML file field, then select your file for upload  </li> <li>From URL - in the Metadata XML URL field, enter the URL to the XML Metadata file  </li> </ul> </li> <li>Copy the Redirect URL and Entity ID to be used in your identity provider  </li> <li>Optional: Enter the user attributes and their value in the identity provider (see the user attributes table below)  </li> </ol> Attribute Default value in Run:ai Description User role groups GROUPS If it exists in the IDP, it allows you to assign Run:ai role groups via the IDP. The IDP attribute must be a list of strings. Linux User ID UID If it exists in the IDP, it allows Researcher containers to start with the Linux User UID. Used to map access to network resources such as file systems to users. The IDP attribute must be of type integer. Linux Group ID GID If it exists in the IDP, it allows Researcher containers to start with the Linux Group GID. The IDP attribute must be of type integer. Supplementary Groups SUPPLEMENTARYGROUPS If it exists in the IDP, it allows Researcher containers to start with the relevant Linux supplementary groups. The IDP attribute must be a list of integers. Email email Defines the user attribute in the IDP holding the user's email address, which is the user identifier in Run:ai. User first name firstName Used as the user\u2019s first name appearing in the Run:ai platform. User last name lastName Used as the user\u2019s last name appearing in the Run:ai platform. <ol> <li>Click SAVE </li> </ol>"},{"location":"admin/authentication/sso/saml/#testing-the-setup","title":"Testing the setup","text":"<ol> <li>Open the Run:ai platform as an admin  </li> <li>Add Access Rules to an SSO user defined in the IDP  </li> <li>Open the Run:ai platform in an incognito browser tab  </li> <li>On the sign-in page click CONTINUE WITH SSO.     You are redirected to the identity provider sign in page  </li> <li>In the identity provider sign-in page, log-in with the SSO user who you granted with access rules  </li> <li>If you are unsuccessful signing-in to the identity provider, follow the Troubleshooting section below</li> </ol>"},{"location":"admin/authentication/sso/saml/#editing-the-identity-provider","title":"Editing the identity provider","text":"<p>You can view the identity provider details and edit its configuration:</p> <ol> <li>Go Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider box, click Edit identity provider </li> <li>You can edit either the metadata file or the user attributes  </li> <li>You can view the identity provider URL, identity provider entity ID, and the certificate expiration date</li> </ol>"},{"location":"admin/authentication/sso/saml/#removing-the-identity-provider","title":"Removing the identity provider","text":"<p>You can remove the identity provider configuration:</p> <ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider card, click Remove identity provider </li> <li>In the dialog, click REMOVE to confirm the action</li> </ol> <p>Note</p> <p>To avoid losing access, removing the identity provider must be carried out by a local user.</p>"},{"location":"admin/authentication/sso/saml/#downloading-the-xml-metadata-file","title":"Downloading the XML metadata file","text":"<p>You can download the XML file to view the identity provider settings:</p> <ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider card, click Download metadata XML file</li> </ol>"},{"location":"admin/authentication/sso/saml/#troubleshooting","title":"Troubleshooting","text":"<p>If testing the setup was unsuccessful, try the different troubleshooting scenarios according to the error you received. If an error still occurs, check the advanced troubleshooting section.</p>"},{"location":"admin/authentication/sso/saml/#troubleshooting-scenarios","title":"Troubleshooting scenarios","text":"Invalid signature in response from identity provider <p>Description: After trying to log-in, the following message is received in the RunLai log-in page.   Mitigation:   1. Go to the Tools &amp; Settings menu   2. Click General   3. Open the Security section   4. In the identity provider box, check for a \"Certificate expired\u201d error   5. If it is expired, update the SAML metadata file to include a valid certificate</p> 401 - We\u2019re having trouble identifying your account because your email is incorrect or can\u2019t be found. <p>Description: Authentication failed because email attribute was not found.</p> <p>Mitigation:</p> <ol> <li>Validate the user\u2019s email attribute is mapped correctly</li> </ol> 403 - Sorry, we can\u2019t let you see this page. Something about permissions\u2026 <p>Description: The authenticated user is missing permissions</p> <p>Mitigation:</p> <ol> <li>Validate either the user or its related group/s are assigned with access rules </li> <li>Validate the user\u2019s groups attribute is mapped correctly</li> </ol> <p>Advanced:</p> <ol> <li>Open the Chrome DevTools: Right-click on page \u2192 Inspect \u2192 Console tab  </li> <li>Run the following command to retrieve and paste the user\u2019s token: <code>localStorage.token;</code> </li> <li>Paste in https://jwt.io </li> <li>Under the Payload section validate the values of the user\u2019s attributes</li> </ol>"},{"location":"admin/authentication/sso/saml/#advanced-troubleshooting","title":"Advanced Troubleshooting","text":"Validating the SAML request <p>The SAML login flow can be separated into two parts:</p> <ul> <li>Run:ai redirects to the IDP for log-ins using a SAML Request  </li> <li>On successful log-in, the IDP redirects back to Run:ai with a SAML Response</li> </ul> <p>Validate the SAML Request to ensure the SAML flow works as expected:</p> <ol> <li>Go to the Run:ai login screen  </li> <li>Open the Chrome Network inspector: Right-click \u2192 Inspect on the page \u2192 Network tab  </li> <li>On the sign-in page click CONTINUE WITH SSO.  </li> <li>Once redirected to the Identity Provider, search in the Chrome network inspector for an HTTP request showing the SAML Request. Depending on the IDP url, this would be a request to the IDP domain name. For example, <code>accounts.google.com/idp?1234</code>.  </li> <li>When found, go to the Payload tab and copy the value of the SAML Request  </li> <li>Paste the value into a SAML decoder (e.g. https://www.samltool.com/decode.php)  </li> <li>Validate the request:  <ul> <li>The content of the <code>&lt;saml:Issuer&gt;</code> tag is the same as <code>Entity ID</code> given when adding the identity provider  </li> <li>The content of the <code>AssertionConsumerServiceURL</code> is the same as the <code>Redirect URI</code> given when adding the identity provider  </li> </ul> </li> <li>Validate the response:  <ul> <li>The user email under the <code>&lt;saml2:Subject&gt;</code> tag is the same as the logged-in user  </li> <li>Make sure that under the <code>&lt;saml2:AttributeStatement&gt;</code> tag, there is an Attribute named <code>email</code> (lowercase). This attribute is mandatory.  </li> <li>If other, optional user attributes (<code>groups</code>, <code>firstName</code>, <code>lastName</code>, <code>uid</code>, <code>gid</code>) are mapped make sure they also exist under <code>&lt;saml2:AttributeStatement&gt;</code> along with their respective values.</li> </ul> </li> </ol>"},{"location":"admin/config/access-roles/","title":"Understand the Kubernetes Cluster Access provided to Run:ai","text":"<p>Run:ai has configuration flags that control specific behavioral aspects of Run:ai. Specifically, those which require additional permissions. Such as automatic namespace/project creation, secret propagation, and more.</p> <p>The purpose of this document is to provide security officers with the ability to review what cluster-wide access Run:ai requires, and verify that it is in line with organizational policy, before installing the Run:ai cluster. </p>"},{"location":"admin/config/access-roles/#review-cluster-access-roles","title":"Review Cluster Access Roles","text":"<p>Run the following:</p> <pre><code>helm repo add runai https://run-ai-charts.storage.googleapis.com\nhelm repo update\nhelm install runai-cluster runai/runai-cluster -n runai -f runai-&lt;cluster-name&gt;.yaml \\\n        --dry-run &gt; cluster-all.yaml\n</code></pre> <p>The file <code>cluster-all.yaml</code> can be then be reviewed. You can use the internal filenames (provided in comments within the file) to gain more understanding according to the table below:</p> Folder File Purpose <code>clusterroles</code> <code>base.yaml</code> Mandatory Kubernetes Cluster Roles and Cluster Role Bindings <code>clusterroles</code> <code>project-controller-ns-creation.yaml</code> Automatic Project Creation and Maintenance. Provides Run:ai with the ability to create Kubernetes namespaces when the Run:ai administrator creates new Projects. Can be turned on/off via flag <code>clusterroles</code> <code>project-controller-rb-creation.yaml</code> Automatically assign Users to Projects. Can be turned on/off via flag <code>clusterroles</code> <code>project-controller-cluster-wide-secrets.yaml</code> Allow the propagation of Secrets. See Secrets in Jobs. Can be turned on/off via flag <code>clusterroles</code> <code>project-controller-limit-range.yaml</code> Disables the usage of the Kubernetes Limit Range feature. Can be turned on/off via flag <code>ocp</code> <code>scc.yaml</code> OpenShift-specific Security Contexts <code>priorityclasses</code> 4 files Folder contains a list of Priority Classes used by Run:ai"},{"location":"admin/config/admin-messages/","title":"Administrator Messages","text":"<p>System administrators can use Administrator messages to make announcements to users once they have logged in. These messages typically are used to keep user informed about different aspects of the platform.</p> <p>To configure an Administrator message:</p> <ol> <li>Press <code>Settings | General</code>.</li> <li>Expand the Message from administrator pane.</li> <li>Press Message.</li> <li>Enter your message in the text box. Use the formatting tools in the toolbar to add special formatting or links to the message.</li> <li>Enable the <code>Display \"Don't show this again\" checkbox on message to users</code> to allow the users to see the message only once.</li> <li>Press Publish when complete.</li> </ol>"},{"location":"admin/config/allow-external-access-to-containers/","title":"External access to Containers","text":""},{"location":"admin/config/allow-external-access-to-containers/#introduction","title":"Introduction","text":"<p>Researchers working with containers may at times need to remotely access the container. Some examples:</p> <ul> <li>Using a Jupyter notebook that runs within the container</li> <li>Using PyCharm to run python commands remotely.</li> <li>Using TensorBoard to view machine learning visualizations</li> </ul> <p>This requires exposing container ports. When using docker, the way Researchers expose ports is by declaring them when starting the container. Run:ai has similar syntax.</p> <p>Run:ai is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers several options:</p> Method Description Prerequisites Port Forwarding Simple port forwarding allows access to the container via local and/or remote port. None NodePort Exposes the service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service from outside the cluster by requesting <code>&lt;NODE-IP&gt;:&lt;NODE-PORT&gt;</code> regardless of which node the container actually resides in. None LoadBalancer Exposes the service externally using a cloud provider\u2019s load balancer. Only available with cloud providers <p>See https://kubernetes.io/docs/concepts/services-networking/service for further details on these options.</p>"},{"location":"admin/config/allow-external-access-to-containers/#workspaces-configuration","title":"Workspaces configuration","text":"<p>Workspaces allow the Researcher to build AI models interactively. </p> <p>Workspaces allow the Researcher to launch tools such as Visual Studio code, TensorFlow, TensorBoard etc. These tools require access to the container. Access is provided via URLs. </p> <p>Run:ai uses the Cluster URL provided to dynamically create SSL-secured URLs for researchers\u2019 workspaces in the format of <code>https://&lt;CLUSTER_URL&gt;/project-name/workspace-name</code>.</p> <p>While this form of path-based routing conveniently works with applications like Jupyter Notebooks, it may often not be compatible with other applications. These applications assume running at the root file system, so hardcoded file paths and settings within the container may become invalid when running at a path other than the root. For instance, if the container is expecting to find a file at <code>/etc/config.json</code> but is running at <code>/project-name/workspace-name</code>, the file will not be found. This can cause the container to fail or not function as intended.</p> <p>To address this issue, Run:ai provides support for host-based routing. When enabled, Run:ai creates workspace URLs in a subdomain format (<code>https://project-name-workspace-name.&lt;CLUSTER_URL&gt;/</code>), which allows all workspaces to run at the root path and function properly. </p> <p>To enable host-based routing you must perform the following steps:</p> <p>Note</p> <p>For OpenShift, editing the Runaiconfig command is the only step required to generate workspace URLs. Refer to the last step below.</p> <ol> <li>Create a second DNS entry (A record) for <code>*.&lt;CLUSTER_URL&gt;</code>, pointing to the same IP as the cluster Fully Qualified Domain Name (FQDN)</li> <li> <p>Obtain a wildcard SSL certificate for this DNS.</p> </li> <li> <p>Add the certificate as a secret:</p> </li> </ol> <pre><code>kubectl create secret tls runai-cluster-domain-star-tls-secret -n runai \\ \n    --cert /path/to/fullchain.pem --key /path/to/private.pem\n</code></pre> <ol> <li>Create the following ingress rule:</li> </ol> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: runai-cluster-domain-star-ingress\n  namespace: runai\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: '*.&lt;CLUSTER_URL&gt;'\n  tls:\n  - hosts:\n    - '*.&lt;CLUSTER_URL&gt;'\n    secretName: runai-cluster-domain-star-tls-secret\n</code></pre> <p>Replace <code>&lt;CLUSTER_URL&gt;</code> as described above and run: <code>kubectl apply -f &lt;filename&gt;</code>.</p> <ol> <li>Edit Runaiconfig to generate the URLs correctly:</li> </ol> <pre><code>kubectl patch RunaiConfig runai -n runai --type=\"merge\" \\\n    -p '{\"spec\":{\"global\":{\"subdomainSupport\": true}}}' \n</code></pre> <p>Once these requirements have been met, all workspaces will automatically be assigned a secured URL with a subdomain, ensuring full functionality for all researcher applications.</p>"},{"location":"admin/config/allow-external-access-to-containers/#see-also","title":"See Also","text":"<ul> <li>To learn how to use port forwarding see the Quickstart document:  Launch an Interactive Build Workload with Connected Ports.</li> <li>See CLI command runai submit.</li> </ul>"},{"location":"admin/config/cli-admin-install/","title":"Install the Run:ai Administrator Command-line Interface","text":"<p>The Run:ai Administrator Command-line Interface (Administrator CLI) allows performing administrative tasks on the Run:ai Cluster.  </p> <p>The instructions below will guide you through the process of installing the Administrator CLI.</p> <p>Tip</p> <p>Most of the functionality of the Run:ai Administrator CLI has been removed. It now serves the limited purpose of collecting logs and setting node roles. </p>"},{"location":"admin/config/cli-admin-install/#prerequisites","title":"Prerequisites","text":"<ul> <li>Run:ai Administrator CLI runs on Mac and Linux.   </li> <li>Kubectl (Kubernetes command-line interface) is installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/</li> <li>A Kubernetes configuration file obtained from a computer previously connected to the Kubernetes cluster</li> </ul>"},{"location":"admin/config/cli-admin-install/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<p>The Run:ai Administrator CLI requires a Kubernetes profile with cluster administrative rights. </p>"},{"location":"admin/config/cli-admin-install/#installation","title":"Installation","text":"<p>Download the Run:ai Administrator Command-line Interface by running:</p> MacLinux <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/darwin  # (1) \nchmod +x runai-adm\nsudo mv runai-adm /usr/local/bin/runai-adm\n</code></pre> <ol> <li>In self-hosted environment, use the control-plane URL instead of <code>app.run.ai</code> </li> </ol> <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/linux  # (1)\nchmod +x runai-adm\nsudo mv runai-adm /usr/local/bin/runai-adm\n</code></pre> <ol> <li>In self-hosted environment, use the control-plane URL instead of <code>app.run.ai</code> </li> </ol> <p>To verify the installation run:</p> <pre><code>runai-adm version\n</code></pre>"},{"location":"admin/config/cli-admin-install/#download-a-specific-version","title":"Download a specific version","text":"<p>To download a specific version of <code>runai-adm</code> add the version number to URL. For example:</p> <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/v2.7.22/darwin\n</code></pre>"},{"location":"admin/config/cli-admin-install/#updating-the-runai-administrator-cli","title":"Updating the Run:ai Administrator CLI","text":"<p>To update the CLI to the latest version perform the same install process again. The command <code>runai-adm update</code> is no longer supported.</p>"},{"location":"admin/config/cluster-wide-pvc/","title":"Cluster wide PVCs","text":"<p>A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes. For more information about PVCs, see Persistent Volumes.</p> <p>PVCs are namespace-specific. If your PVC relates to all run:ai Projects, do the following to propagate the PVC to all Projects:</p> <p>Create a PVC within the run:ai namespace, then run the following once to propagate the PVC to all run:ai Projects:</p> <pre><code>kubectl label persistentvolumeclaims -n runai &lt;PVC_NAME&gt; runai/cluster-wide=true\n</code></pre> <p>To delete a PVC from all run:ai Projects, run:</p> <pre><code>kubectl label persistentvolumeclaims -n runai &lt;PVC_NAME&gt; runai/cluster-wide-\n</code></pre> <p>You can add a PVC to a job using the <code>New job</code> form.</p> <p>To add a PVC to a new job:</p> <ol> <li>On the <code>New job</code> form, press <code>Storage</code>.</li> <li>In <code>Persistent Volume Claims</code> press <code>Add</code>.</li> <li>Enable <code>Existing PVC</code>.</li> <li>Enter the name (claim name) of the PVC.</li> <li>Enter the storage class. (Optional)</li> <li>Enter the size.</li> <li>Enable / disable access modes.</li> </ol>"},{"location":"admin/config/clusters/","title":"Clusters","text":"<p>This article explains the procedure to view and manage Clusters.</p> <p>The Cluster table provides a quick and easy way to see the status of your cluster.</p> <p></p>"},{"location":"admin/config/clusters/#clusters-table","title":"Clusters table","text":"<p>The Clusters table can be found under Clusters in the Run:ai platform.</p> <p>The clusters table provides a list of the clusters added to Run:ai platform, along with their status.</p> <p>The clusters table consists of the following columns:</p> Column Description Cluster The name of the cluster Status The status of the cluster. For more information see the table below. Hover over the information icon for a short description and links to troubleshooting Creation time The timestamp when the cluster was created URL The URL that was given to the cluster Run:ai cluster version The Run:ai version installed on the cluster Kubernetes distribution The flavor of Kubernetes distribution Kubernetes version The version of Kubernetes installed Run:ai cluster UUID The unique ID of the cluster"},{"location":"admin/config/clusters/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"admin/config/clusters/#cluster-status","title":"Cluster status","text":"Status Description Waiting to connect The cluster has never been connected. Disconnected There is no communication from the cluster to the {{glossary.Control plane}}. This may be due to a network issue. See the troubleshooting scenarios. Missing prerequisites Some prerequisites are missing from the cluster. As a result, some features may be impacted. See the troubleshooting scenarios. Service issues At least one of the services is not working properly. You can view the list of nonfunctioning services for more information. See the troubleshooting scenarios. Connected The Run:ai cluster is connected, and all Run:ai services are running."},{"location":"admin/config/clusters/#adding-a-new-cluster","title":"Adding a new cluster","text":"<p>To add a new cluster see the installation guide.</p>"},{"location":"admin/config/clusters/#removing-a-cluster","title":"Removing a cluster","text":"<ol> <li>Select the cluster you want to remove  </li> <li>Click REMOVE</li> <li>A dialog appears: Make sure to carefully read the message before removing  </li> <li>Click REMOVE to confirm the removal.</li> </ol>"},{"location":"admin/config/clusters/#using-the-api","title":"Using the API","text":"<p>Go to the Clusters API reference to view the available actions</p>"},{"location":"admin/config/clusters/#troubleshooting","title":"Troubleshooting","text":"<p>Before starting, make sure you have the following:</p> <ul> <li>Access to the Kubernetes cluster where Run:ai is deployed with the necessary permissions  </li> <li>Access to the Run:ai Platform</li> </ul>"},{"location":"admin/config/clusters/#troubleshooting-scenarios","title":"Troubleshooting scenarios","text":"Cluster disconnected <p>Description: When the cluster's status is \u2018disconnected\u2019, there is no communication from the cluster services reaching the Run:ai Platform. This may be due to networking issues or issues with Run:ai services.</p> <p>Mitigation:</p> <ol> <li> <p>Check Run:ai\u2019s services status: </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permission to view pods  </li> <li>Copy and paste the following command to verify that Run:ai\u2019s services are running:  </li> </ul> <p><pre><code>kubectl get pods -n runai | grep -E 'runai-agent|cluster-sync|assets-sync'\n</code></pre> * If any of the services are not running, see the \u2018cluster has service issues\u2019 scenario.  </p> </li> <li> <p>Check the network connection  </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to create pods  </li> <li>Copy and paste the following command to create a connectivity check pod:  </li> </ul> <pre><code>kubectl run control-plane-connectivity-check -n runai --image=wbitt/network-multitool \\\n    --command -- /bin/sh -c 'curl -sSf &lt;control-plane-endpoint&gt; &gt; /dev/null &amp;&amp; echo \"Connection Successful\" \\\n    || echo \"Failed connecting to the Control Plane\"'\n</code></pre> <ul> <li>Replace <code>&lt;control-plane-endpoint&gt;</code> with the URL of the Control Plane in your environment. If the pod fails to connect to the Control Plane, check for potential network policies </li> </ul> </li> <li> <p>Check and modify the network policies</p> <ul> <li>Open your terminal  </li> <li> <p>Copy and paste the following command to check the existence of network policies: <pre><code>kubectl get networkpolicies -n runai\n</code></pre></p> </li> <li> <p>Review the policies to ensure that they allow traffic from the Run:ai namespace to the Control Plane. If necessary, update the policies to allow the required traffic. Example of allowing traffic:</p> </li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: allow-control-plane-traffic\nnamespace: runai\nspec:\npodSelector:\n    matchLabels:\n    app: runai\npolicyTypes:\n    - Ingress\n    - Egress\negress:\n    - to:\n        - ipBlock:\n            cidr: &lt;control-plane-ip-range&gt;\n    ports:\n        - protocol: TCP\n        port: &lt;control-plane-port&gt;\ningress:\n    - from:\n        - ipBlock:\n            cidr: &lt;control-plane-ip-range&gt;\n    ports:\n        - protocol: TCP\n        port: &lt;control-plane-port&gt;\n</code></pre> <ul> <li> <p>Check infrastructure-level configurations: </p> <ul> <li>Ensure that firewall rules and security groups allow traffic between your Kubernetes cluster and the Control Plane  </li> <li>Verify required ports and protocols:  <ul> <li>Ensure that the necessary ports and protocols for Run:ai\u2019s services are not blocked by any firewalls or security groups  </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Check Run:ai services logs  </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to view logs  </li> <li>Copy and paste the following commands to view the logs of the Run:ai services: </li> </ul> <pre><code>kubectl logs deployment/runai-agent -n runai\nkubectl logs deployment/cluster-sync -n runai\nkubectl logs deployment/assets-sync -n runai\n</code></pre> <ul> <li>Try to identify the problem from the logs. If you cannot resolve the issue, continue to the next step. </li> </ul> </li> <li> <p>Contact Run:ai\u2019s support  </p> <ul> <li>If the issue persists, contact Run:ai\u2019s support for assistance.</li> </ul> </li> </ol> Cluster has service issues <p>Description: When a cluster's status is Has service issues, it means that one or more Run:ai services running in the cluster are not available.</p> <p>Mitigation:</p> <ol> <li> <p>Verify non-functioning services  </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to view the <code>runaiconfig</code> resource  </li> <li>Copy and paste the following command to determine which services are not functioning:  </li> </ul> <pre><code>kubectl get runaiconfig -n runai runai -ojson | jq -r '.status.conditions | map(select(.type == \"Available\"))'\n</code></pre> </li> <li> <p>Check for Kubernetes events </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to view events  </li> <li>Copy and paste the following command to get all Kubernetes events:  </li> </ul> <pre><code>kubectl get events  -A\n</code></pre> </li> <li> <p>Inspect resource details  </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to describe resources  </li> <li>Copy and paste the following command to check the details of the required resource:  </li> </ul> <pre><code>kubectl describe &lt;resource_type&gt; &lt;name&gt;\n</code></pre> </li> <li> <p>Contact Run:ai\u2019s Support  </p> <ul> <li>If the issue persists, contact contact Run:ai\u2019s support for assistance.</li> </ul> </li> </ol> Cluster is waiting to connect <p>Description: When the cluster's status is \u2018waiting to connect\u2019, it means that no communication from the cluster services reaches the Run:ai Platform. This may be due to networking issues or issues with Run:ai services.</p> <p>Mitigation:</p> <ol> <li> <p>Check Run:ai\u2019s services status </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to view pods  </li> <li>Copy and paste the following command to verify that Run:ai\u2019s services are running:  </li> </ul> <pre><code>kubectl get pods -n runai | grep -E 'runai-agent|cluster-sync|assets-sync'\n</code></pre> <ul> <li>If any of the services are not running, see the \u2018cluster has service issues\u2019 scenario. </li> </ul> </li> <li> <p>Check the network connection </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to create pods  </li> <li>Copy and paste the following command to create a connectivity check pod:  </li> </ul> <pre><code>kubectl run control-plane-connectivity-check -n runai --image=wbitt/network-multitool --command -- /bin/sh -c 'curl -sSf &lt;control-plane-endpoint&gt; &gt; /dev/null &amp;&amp; echo \"Connection Successful\" || echo \"Failed connecting to the Control Plane\"'\n</code></pre> <ul> <li>Replace <code>&lt;control-plane-endpoint&gt;</code> with the URL of the Control Plane in your environment. If the pod fails to connect to the Control Plane, check for potential network policies:  </li> </ul> </li> <li> <p>Check and modify the network policies  </p> <ul> <li>Open your terminal  </li> <li>Copy and paste the following command to check the existence of network policies:  </li> </ul> <pre><code>kubectl get networkpolicies -n runai\n</code></pre> <ul> <li>Review the policies to ensure that they allow traffic from the Run:ai namespace to the Control Plane. If necessary, update the policies to allow the required traffic.  Example of allowing traffic:  </li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: allow-control-plane-traffic\nnamespace: runai\nspec:\n  podSelector:\n    matchLabels:\n    app: runai\n  policyTypes:\n    - Ingress\n    - Egress\n  egress:\n    - to:\n        - ipBlock:\n            cidr: &lt;control-plane-ip-range&gt;\n    ports:\n        - protocol: TCP\n        port: &lt;control-plane-port&gt;\n  ingress:\n    - from:\n        - ipBlock:\n            cidr: &lt;control-plane-ip-range&gt;\n    ports:\n        - protocol: TCP\n        port: &lt;control-plane-port&gt;\n</code></pre> <ul> <li>Check infrastructure-level configurations:  </li> <li>Ensure that firewall rules and security groups allow traffic between your Kubernetes cluster and the Control Plane  </li> <li>Verify required ports and protocols:  <ul> <li>Ensure that the necessary ports and protocols for Run:ai\u2019s services are not blocked by any firewalls or security groups </li> </ul> </li> </ul> </li> <li> <p>Check Run:ai services logs  </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permission to view logs  </li> <li>Copy and paste the following commands to view the logs of the Run:ai services:  </li> </ul> <pre><code>kubectl logs deployment/runai-agent -n runai\nkubectl logs deployment/cluster-sync -n runai\nkubectl logs deployment/assets-sync -n runai\n</code></pre> <ul> <li>Try to identify the problem from the logs. If you cannot resolve the issue, continue to the next step  </li> </ul> </li> <li> <p>Contact Run:ai\u2019s support  </p> <ul> <li>If the issue persists, contact Run:ai\u2019s support for assistance.</li> </ul> </li> </ol> Cluster is missing prerequisites <p>Description: When a cluster's status displays Missing prerequisites, it indicates that at least one of the Mandatory Prerequisites has not been fulfilled. In such cases, Run:ai services may not function properly.</p> <p>Mitigation:</p> <p>If you have ensured that all prerequisites are installed and the status still shows missing prerequisites, follow these steps:</p> <ol> <li>Check the message in the Run:ai platform for further details regarding the missing prerequisites.  </li> <li> <p>Inspect the <code>runai-public</code> ConfigMap:  </p> <ul> <li>Open your terminal. In the terminal, type the following command to list all ConfigMaps in the <code>runai</code> namespace: </li> </ul> <pre><code>kubectl get configmap -n runai\n</code></pre> </li> <li> <p>Describe the ConfigMap  </p> <ul> <li>Locate the ConfigMap named <code>runai-public</code> from the list  </li> <li>To view the detailed contents of this ConfigMap, type the following command:  </li> </ul> <pre><code>kubectl describe configmap runai-public -n runai\n</code></pre> </li> <li> <p>Find Missing Prerequisites  </p> <ul> <li>In the output displayed, look for a section labeled <code>dependencies.required</code> </li> <li>This section provides detailed information about any missing resources or prerequisites. Review this information to identify what is needed  </li> </ul> </li> <li> <p>Contact Run:ai\u2019s support  </p> <ul> <li>If the issue persists, contact Run:ai\u2019s support for assistance.</li> </ul> </li> </ol>"},{"location":"admin/config/default-scheduler/","title":"Setting Run:ai as the default scheduler per Namespace (Project)","text":""},{"location":"admin/config/default-scheduler/#introduction","title":"Introduction","text":"<p>Kubernetes has a default scheduler that makes decisions on where to place Kubernetes Pods. Run:ai has implemented a different scheduler called the <code>runai-scheduler</code>. By default, Run:ai uses its own scheduler</p> <p>You can decide to use the Run:ai scheduler for other, non-Run:ai, workloads by adding the following to the workload's YAML file:</p> <pre><code>schedulerName: runai-scheduler\n</code></pre>"},{"location":"admin/config/default-scheduler/#making-runai-the-default-scheduler","title":"Making Run:ai the default scheduler","text":"<p>There may be cases where you cannot change the YAML file but still want to use the Run:ai Scheduler to schedule those workloads. </p> <p>For such cases, another option is to configure the Run:ai Scheduler as the default scheduler for a specific namespace. This will now make any workload type that is submitted to that namespace (equivalent to a Run:ai Project) use the Run:ai scheduler. </p> <p>To configure this, add the following annotation to the namespace itself:</p> <p><code>runai/enforce-scheduler-name: true</code></p>"},{"location":"admin/config/default-scheduler/#example","title":"Example","text":"<p>To annotate a project named <code>proj-a</code>, use the following command:</p> <pre><code>kubectl annotate ns runai-proj-a runai/enforce-scheduler-name=true\n</code></pre> <p>Verify the namespace in YAML format to see the annotation:</p> <pre><code>kubectl get ns runai-proj-a -o yaml\n</code></pre> <p>Output: </p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  annotations:\n    runai/enforce-scheduler-name: \"true\"\n  creationTimestamp: \"2024-04-09T08:15:50Z\"\n  labels:\n    kubernetes.io/metadata.name: runai-proj-a\n    runai/namespace-version: v2\n    runai/queue: proj-a\n  name: runai-proj-a\n  resourceVersion: \"388336\"\n  uid: c53af666-7989-43df-9804-42bf8965ce83\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Active\n</code></pre>"},{"location":"admin/config/dr/","title":"Backup &amp; Restore","text":""},{"location":"admin/config/dr/#runai-cluster-restore","title":"Run:ai Cluster Restore","text":"<p>This article explains how to restore a Run:ai cluster on a different Kubernetes environment.</p> <p>In the event of a critical Kubernetes failure or alternatively, if you want to migrate a Run:ai cluster to a new Kubernetes environment, simply reinstall the Run:ai cluster. Once you have reinstalled and reconnected the cluster - projects, workloads and other cluster data is synced automatically.</p> <p>The restoration or back-up of Run:ai cluster Advanced features and Customized deployment configurations which are stored locally on the Kubernetes cluster is optional and they can be restored and backed-up separately.</p>"},{"location":"admin/config/dr/#backup","title":"Backup","text":"<p>As back-up of data is not required, the backup procedure is optional for advanced deployments, as explained above.</p>"},{"location":"admin/config/dr/#backup-cluster-configurations","title":"Backup cluster configurations","text":"<p>To backup Run:ai cluster configurations:</p> <ol> <li>Run the following command in your terminal: <pre><code>kubectl get runaiconfig runai -n runai -o yaml -o=jsonpath='{.spec}' &gt; runaiconfig_backup.yaml\n</code></pre></li> <li>Once the <code>runaiconfig_back.yaml</code> back-up file is created, save the file externally, so that it can be retrieved later.</li> </ol>"},{"location":"admin/config/dr/#restore","title":"Restore","text":"<p>Follow the steps below to restore the Run:ai cluster on a new Kubernetes environment.</p>"},{"location":"admin/config/dr/#prerequisites","title":"Prerequisites","text":"<p>Before restoring the Run:ai cluster, it is essential to validate that it is both disconnected and uninstalled.</p> <ol> <li>If the Kubernetes cluster is still available, uninstall the Run:ai cluster - make sure not to remove the cluster from the Control Plane  </li> <li>Navigate to the Cluster page in the Run:ai platform  </li> <li>Search for the cluster, and make sure its status is Disconnected</li> </ol>"},{"location":"admin/config/dr/#re-installing-runai-cluster","title":"Re-installing Run:ai Cluster","text":"<ol> <li>Follow the Run:ai cluster installation instructions and ensure all prerequisites are met  </li> <li>If you have a back-up of the cluster configurations, reload it once the installation is complete <pre><code>kubectl apply -f runaiconfig_backup.yaml -n runai\n</code></pre></li> <li>Navigate to the Cluster page in the Run:ai platform  </li> <li>Search for the cluster, and make sure its status is Connected</li> </ol>"},{"location":"admin/config/dr/#runai-control-plane","title":"Run:ai Control Plane","text":"<p>The self-hosted variant of Run:ai also installs the control-plane at the customer site. As such, it becomes the responsibility of the IT organization to verify that the system is configured for proper backup and learn how to recover the data when needed.</p>"},{"location":"admin/config/dr/#database-storage","title":"Database Storage","text":"<p>Run:ai uses an internal PostgreSQL database. The database is stored on a Kubernetes Persistent Volume (PV). You must provide a backup solution for the database. Some options:</p> <ul> <li>Backing up of PostgreSQL itself. Example: <code>kubectl -n runai-backend exec -it runai-backend-postgresql-0 -- env  PGPASSWORD=password pg_dump -U postgres   backend   &gt; cluster_name_db_backup.sql</code></li> <li>Backing up the persistent volume holding the database storage.</li> <li>Using third-party backup solutions.</li> </ul> <p>Run:ai also supports an external PostgreSQL database. For details see external PostgreSQL database</p>"},{"location":"admin/config/dr/#metrics-storage","title":"Metrics Storage","text":"<p>Run:ai stores metric history using Thanos. Thanos is configured to store data on a persistent volume. The recommendation is to back up the PV.</p>"},{"location":"admin/config/dr/#backing-up-control-plane-configuration","title":"Backing up Control-Plane Configuration","text":"<p>The installation of the Run:ai control plane can be configured. The configuration is provided as <code>--set</code> command in the helm installation. These changes will be preserved on upgrade, but will not be preserved on uninstall or upon damage to Kubernetes. Thus, it is best to back up these customizations. For a list of customizations used during the installation, run:</p> <p><code>helm get values runai-backend -n runai-backend</code></p>"},{"location":"admin/config/dr/#recovery","title":"Recovery","text":"<p>To recover Run:ai</p> <ul> <li>Re-create the Kubernetes/OpenShift cluster.</li> <li>Recover the persistent volumes for metrics and database.</li> <li>Re-install the Run:ai control plane. Use the additional configuration previously saved and connect to the restored PostgreSQL PV. Connect Prometheus to the stored metrics PV.</li> <li>Re-install the cluster. Add additional configuration post-install.  </li> <li>If the cluster is configured such that Projects do not create a namespace automatically, you will need to re-create namespaces and apply role bindings as discussed in Kubernetes or OpenShift.</li> </ul>"},{"location":"admin/config/ha/","title":"High Availability","text":"<p>The purpose of this document is to configure Run:ai such that it will continue to provide service even if parts of the system are down.</p> <p>A frequent fail scenario is a physical node in the system becoming non-responsive due to physical problems or lack of resources. In such a case, Kubernetes will attempt to relocate the running pods, but the process may take time, during which Run:ai will be down.</p> <p>A different scenario is a high transaction load, leading to system overload. To address such a scenario, please review the article: scaling the Run:ai system.</p>"},{"location":"admin/config/ha/#runai-control-plane","title":"Run:ai Control Plane","text":""},{"location":"admin/config/ha/#runai-system-workers","title":"Run:ai system workers","text":"<p>The Run:ai control plane allows the optional gathering of Run:ai pods into specific nodes. When this feature is used, it is important to set more than one node as a Run:ai system worker. Otherwise, the horizontal scaling described below will not span multiple nodes, and the system will remain with a single point of failure.  </p>"},{"location":"admin/config/ha/#horizontal-scalability-of-runai-services","title":"Horizontal Scalability of Run:ai services","text":"<p>Horizontal scalability is about instructing the system to create more pods to dynamically scale according to incoming load and downsize when the load subsides.</p> <p>The Run:ai control plane is running on a single Kubernetes namespace named <code>runai-backend</code>. The namespace contains various Kubernetes Deployments and StatefulSets. Each of these services can be scaled horizontally.</p>"},{"location":"admin/config/ha/#deployments","title":"Deployments","text":"<p>Each of the Run:ai deployments can be set to scale up, by adding a helm settings on install/upgrade. E.g. <code>--set frontend.autoscaling.enabled=true</code>. For a full list of settings, please contact Run:ai customer support.</p>"},{"location":"admin/config/ha/#statefulsets","title":"StatefulSets","text":"<p>Run:ai uses three third parties which are managed as Kubernetes StatefulSets:</p> <ul> <li>Keycloak\u2014Stores the Run:ai authentication configuration as well as user identities. To scale Keycloak, use the Run:ai control-plane helm flag <code>--set keycloakx.autoscaling.enabled=true</code>. By default, Keycloak sets a minimum of 3 pods and will scale to more on transaction load.</li> <li>PostgreSQL\u2014It is not possible to configure an internal PostgreSQL to scale horizontally. If this is of importance, please contact Customer Support to understand how to connect Run:ai to an external PostgreSQL service which can be configured for high availability.</li> <li>Thanos\u2014To enable Thanos autoscaling, use the following Run:ai control-plane helm flags:</li> </ul> <pre><code>--set thanos.query.autoscaling.enabled=true  \n--set thanos.query.autoscaling.maxReplicas=2\n--set thanos.query.autoscaling.minReplicas=2 \n</code></pre>"},{"location":"admin/config/ha/#runai-cluster","title":"Run:ai Cluster","text":""},{"location":"admin/config/ha/#runai-system-workers_1","title":"Run:ai system workers","text":"<p>The Run:ai cluster allows the mandatory gathering of Run:ai pods into specific nodes. When this feature is used, it is important to set more than one node as a Run:ai system worker. Otherwise, the horizontal scaling described below may not span multiple nodes, and the system will remain with a single point of failure.  </p>"},{"location":"admin/config/ha/#prometheus","title":"Prometheus","text":"<p>The default Prometheus installation uses a single pod replica. If the node running the pod is unresponsive, metrics will not be scraped from the cluster and will not be sent to the Run:ai control-plane.</p> <p>Prometheus supports high availability by allowing to run multiple instances. The tradeoff of this approach is that all instances will scrape and send the same data. The Run:ai control plane will identify duplicate metric series and ignore them. This approach will thus increase network, CPU and memory consumption.</p> <p>To change the number of Prometheus instances, edit the <code>runaiconfig</code> as described under customizing the Run:ai cluster. Under <code>prometheus.spec</code>, set <code>replicas</code> to 2.</p>"},{"location":"admin/config/large-clusters/","title":"Scaling the Run:ai system","text":"<p>The purpose of this document is to provide information on how to scale the Run:ai cluster and the Run:ai control-plane to withstand large transaction loads</p>"},{"location":"admin/config/large-clusters/#scaling-the-runai-control-plane","title":"Scaling the Run:ai Control Plane","text":"<p>The Control plane deployments which may encounter load are:</p> Name Kubernetes Deployment name Purpose Backend runai-backend-backend Main control-plane service Frontend runai-backend-frontend Serving of the Run:ai console Grafana runai-backend-grafana Serving of the Run:ai metrics inside the Run:ai console <p>To increase the number of replicas, run:</p> <p>To increase the number of replicas, use the following Run:ai control-plane helm flags</p> <pre><code>--set backend.autoscaling.enabled=true \n--set frontend.autoscaling.enabled=true\n--set grafana.autoscaling.enabled=true --set grafana.autoscaling.minReplicas=2\n</code></pre> <p>Important</p> <p>If you have chosen to mark some of the nodes as Run:ai System Workers, the new replicas will attempt to use these nodes first. Thus, for high availability purposes, you will want to mark more than one node as a Run:ai System Worker.  </p>"},{"location":"admin/config/large-clusters/#thanos","title":"Thanos","text":"<p>Thanos is the 3rd party used by Run:ai to store metrics Under a significant user load, we would also need to increase resources for the Thanos query function. Use the following Run:ai control-plane helm flags:</p> <pre><code>--set thanos.query.resources.limits.memory=3G\n--set thanos.query.resources.requests.memory=3G\n--set thanos.query.resources.limits.cpu=1\n--set thanos.query.resources.requests.cpu=1\n\n--set thanos.receive.resources.limits.memory=6G \n--set thanos.receive.resources.requests.memory=6G\n--set thanos.receive.resources.limits.cpu=1 \n--set thanos.receive.resources.requests.cpu=1\n</code></pre>"},{"location":"admin/config/large-clusters/#scaling-the-runai-cluster","title":"Scaling the Run:ai Cluster","text":""},{"location":"admin/config/large-clusters/#cpu-memory-resources","title":"CPU &amp; Memory Resources","text":"<p>Under Kubernetes, each of the Run:ai containers, has default resource requirements that reflect an average customer load. With significantly larger cluster loads, certain Run:ai services will require more CPU and memory resources. Run:ai now supports the ability to configure these resources and to do so for each Run:ai service group separately.</p>"},{"location":"admin/config/large-clusters/#service-groups","title":"Service Groups","text":"<p>Run:ai supports setting requests and limits configurations for CPU and memory for Run:ai containers. The configuration is set per service group. Each service group reflects a certain load type:</p> Service Group Description Run:ai containers SchedulingServices Containers associated with the Run:ai scheduler Scheduler, StatusUpdater, MetricsExporter, PodGrouper, PodGroupAssigner, Binder SyncServices Containers associated with syncing updates between the Run:ai cluster and the Run:ai control plane Agent, ClusterSync, AssetsSync WorkloadServices Containers associated with submitting Run:ai Workloads WorkloadController, JobController"},{"location":"admin/config/large-clusters/#configuration-steps","title":"Configuration Steps","text":"<p>To configure resource requirements for a group of services, update the RunaiConfig. Set the <code>spec.global.&lt;service-group&gt;.</code> resources section. The following example shows the configuration of scheduling services resource requirements:</p> <pre><code>apiVersion: run.ai/v1\nkind: RunaiConfig\nmetadata:\nspec:\n global:\n   schedulingServices:\n     resources:\n       limits:\n         cpu: 1000m\n         memory: 1Gi\n       requests:\n         cpu: 100m\n         memory: 512Mi\n</code></pre> <p>Use <code>syncServices</code> and <code>workloadServices</code> for the other two service groups.</p>"},{"location":"admin/config/large-clusters/#recommended-resource-specifications-for-large-clusters","title":"Recommended Resource Specifications For Large Clusters","text":"<p>In large clusters (100 nodes or 1500 GPUs or more), we recommend the following configuration for SchedulingServices and SyncServices groups:</p> <pre><code>resources:\n requests:\n   cpu: 1\n   memory: 1Gi\n limits:\n   cpu: 2\n   memory: 2Gi\n</code></pre>"},{"location":"admin/config/large-clusters/#sending-metrics","title":"Sending Metrics","text":"<p>Run:ai uses Prometheus to scrape metrics from the Run:ai cluster and to send them to the Run:ai control plane. The number of metrics is a function of the number of Nodes, Jobs and Projects which the system contains. When reaching hundreds of Nodes and Projects, the system will be sending large quantities of metrics which, in turn, will create a strain on the network as well as the receiving side in the control plane (SaaS or self-hosted).</p> <p>To reduce this strain, we suggest to configure Prometheus to send information in larger bulks and reduce the number of network connections:</p> <ul> <li>Edit the <code>runaiconfig</code> as described under customizing the cluster.</li> <li>Under <code>prometheus.remoteWrite</code> add the following:</li> </ul> <pre><code>queueConfig:\n  capacity: 5000\n  maxSamplesPerSend: 1000\n  maxShards: 100\n</code></pre> <p>This article provides additional details and insight.</p> <p>Also, note that this configuration enlarges the Prometheus queues and thus increases the required memory. It is hence suggested to reduce the metrics retention period as described here</p>"},{"location":"admin/config/limit-to-node-group/","title":"Group Nodes","text":""},{"location":"admin/config/limit-to-node-group/#why","title":"Why?","text":"<p>In some business scenarios, you may want to direct the Run:ai scheduler to schedule a Workload to a specific node or a node group. For example, in some academic institutions, Hardware is bought using a specific grant and thus \"belongs\" to a specific research group. Another example is an inference workload that is optimized to a specific GPU type and must have dedicated resources reserved to ensure enough capacity.</p> <p>Run:ai provides two methods to designate, and group, specific resources:</p> <ul> <li>Node Pools: Run:ai allows administrators to group specific nodes into a node pool. A node pool is a group of nodes identified by a given name (node pool name) and grouped by any label (key and value combination). The label can be chosen by the administrator or can be an existing, pre-set, label (such as an NVIDIA GPU type label).</li> <li>Node Affinity: Run:ai allows this \"taint\" by labeling a node, or a set of nodes and then during scheduling, using the flag <code>--node-type &lt;label&gt;</code> to force this allocation.</li> </ul> <p>Important</p> <p>One can set and use both node pool and node affinity combined as a prerequisite to the scheduler, for example, if a researcher wants to use a T4 node with an Infiniband card - he or she can use a node pool of T4 and from that group, choose only the nodes with Infiniband card (node-type = infiniband).</p> <p>There is a tradeoff in place when allowing Researchers to designate specific nodes. Overuse of this feature limits the scheduler in finding an optimal resource and thus reduces overall cluster utilization.</p>"},{"location":"admin/config/limit-to-node-group/#configuring-node-groups","title":"Configuring Node Groups","text":"<p>To configure a node pool:</p> <ul> <li>Find the label key &amp; value you want to use for Run:ai to create the node pool.</li> <li>Check that the nodes you want to group as a pool have a unique label to use, otherwise you should mark those nodes with your own uniquely identifiable label.</li> <li>Get the names of the nodes you want Run:ai to group together. To get a list of nodes, run:</li> </ul> <pre><code>kubectl get nodes\nKubectl get nodes --show-labels\n</code></pre> <ul> <li>If you chose to set your own label, run the following:</li> </ul> <pre><code>kubectl label node &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;\n</code></pre> <p>The same value can be set to a single node or multiple nodes. Node Pool can only use one label (key &amp; value) at a time.</p> <ul> <li>To create a node pool use the create node pool Run:ai API.</li> </ul> <p>To configure a node affinity:</p> <ul> <li>Get the names of the nodes where you want to limit Run:ai. To get a list of nodes, run:</li> </ul> <pre><code>kubectl get nodes\n</code></pre> <ul> <li>For each node run the following:</li> </ul> <pre><code>kubectl label node &lt;node-name&gt; run.ai/type=&lt;label&gt;\n</code></pre> <p>The same value can be set to a single node, or for multiple nodes. A node can only be set with a single value.</p>"},{"location":"admin/config/limit-to-node-group/#using-node-groups-via-the-cli","title":"Using Node Groups via the CLI","text":"<p>To use Run:ai node pool with a workload, use Run:ai CLI command \u2018node-pool\u2019: </p> <pre><code>runai submit job1 ... --node-pools \"my-pool\" ...\n</code></pre> <p>To use multiple node pools with a workload, use the Run:ai CLI command:</p> <pre><code>runai submit job1 ... --node-pools \"my-pool my-pool2 my-pool3\" ...\n</code></pre> <p>With multiple node pools, the researcher creates a list of prioritized node pools and lets the scheduler try and choose from any of the node pools in the list, according to the given priority. </p> <p>To use node affinity, use the node type label with the <code>--node-type</code> flag:</p> <pre><code>runai submit job1 ... --node-type \"my-nodes\"\n</code></pre> <p>A researcher may combine the two flags to select both a node pool and a specific set of nodes out of that node pool (e.g. gpu-type=t4 and node-type=infiniband):</p> <pre><code>runai submit job1 ... --node-pool-name \u201cmy pool\u201d --node-type \"my-nodes\"\n</code></pre> <p>Note</p> <p>When submitting a workload, if you choose a node pool label and a node affinity (node type) label which does not intersect, the Run:ai scheduler will not be able to schedule that workload as it represents an empty nodes group.</p> <p>See the runai submit documentation for further information.</p>"},{"location":"admin/config/limit-to-node-group/#assigning-node-groups-to-a-project","title":"Assigning Node Groups to a Project","text":"<p>Node Pools are automatically assigned to all Projects and Departments with zero resource allocation as default. Allocating resources to a node pool can be done for each Project and Department. Submitting a workload to a node pool that has zero allocation for a specific project (or department) results in that workload running as an over-quota workload.</p> <p>To assign and configure specific node affinity groups or node pools to a Project see working with Projects.</p> <p>When the command-line interface flag is used in conjunction with Project-based affinity, the flag is used to refine the list of allowable node groups set in the Project.</p>"},{"location":"admin/config/node-affinity-with-cloud-node-pools/","title":"Node affinity with cloud node pools","text":"<p>Run:ai allows for node affinity. Node affinity is the ability to assign a Project to run on specific nodes. To use the node affinity feature, You will need to label the target nodes with the label  <code>run.ai/type</code>. Most cloud clusters allow configuring node labels for the node pools in the cluster. This guide shows how to apply this configuration to different cloud providers.</p> <p>To make the node affinity work with node pools on various cloud providers, we need to make sure the node pools are configured with the appropriate Kubernetes label (<code>run.ai/type=&lt;TYPE_VALUE&gt;</code>).</p>"},{"location":"admin/config/node-affinity-with-cloud-node-pools/#setting-node-labels-while-creating-a-new-cluster","title":"Setting node labels while creating a new cluster","text":"<p>You can configure node-pool labels at cluster creation time</p> GKEAKSEKS <ul> <li>At the first creation screen, you will see a menu on the left side named <code>node-pools</code>.</li> <li>Expand the node pool you want to label.</li> <li>Click on <code>Metadata</code>.</li> <li>Near the bottom, you will find the Kubernetes <code>label</code> section. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>When creating AKS cluster at the node-pools page click on create new node-pool.</li> <li>Go to the <code>labels</code> section and add key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>Create a regular EKS cluster.</li> <li>Click on <code>compute</code>.</li> <li>Click on <code>Add node group</code>.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul>"},{"location":"admin/config/node-affinity-with-cloud-node-pools/#setting-node-labels-for-a-new-node-pool","title":"Setting node labels for a new node pool","text":"GKEAKSEKS <ul> <li>At the node pool creation screen, go to the <code>metadata</code> section.</li> <li>Near the bottom, you will find the Kubernetes <code>label</code> section. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>Go to your AKS page at Azure.</li> <li>On the left menu click the <code>node-pools</code> button.</li> <li>Click on <code>Add Node Pool</code>.</li> <li>In the new Node Pool page go to <code>Optional settings</code>.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>Go to <code>Add node group</code> screen.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul>"},{"location":"admin/config/node-affinity-with-cloud-node-pools/#editing-node-labels-for-an-existing-node-pool","title":"Editing node labels for an existing node pool","text":"GKEAKSEKS <ul> <li>Go to the <code>Google Kubernetes Engine</code> page in the Google Cloud console.</li> <li>Go to <code>Google Kubernetes Engine</code>.</li> <li>In the cluster list, click the name of the cluster you want to modify.</li> <li>Click the <code>Nodes</code> tab</li> <li>Under <code>Node Pools</code>, click the name of the node pool you want to modify, then click <code>Edit</code>.</li> <li>Near the bottom, you will find the Kubernetes <code>label</code> section. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <p>To update an existing node pool label you must use the azure cli. Run the following command:</p> <pre><code>az aks nodepool update \\\n    --resource-group [RESOURCE GROUP] \\\n    --cluster-name [CLUSTER NAME] \\\n    --name labelnp \\\n    --labels run.ai/type=[TYPE_VALUE] \\\n    --no-wait\n</code></pre> <ul> <li>Go to the <code>node group</code> page and click on <code>Edit</code>.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul>"},{"location":"admin/config/node-roles/","title":"Designating Specific Role Nodes","text":"<p>When installing a production cluster you may want to:</p> <ul> <li>Set one or more Run:ai system nodes. These are nodes dedicated to Run:ai software. </li> <li>Machine learning frequently requires jobs that require CPU but not GPU. You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines. </li> <li>Limit Run:ai monitoring and scheduling to specific nodes in the cluster. </li> </ul> <p>To perform these tasks you will need the Run:ai Administrator CLI. See Installing the Run:ai Administrator Command-line Interface.</p>"},{"location":"admin/config/node-roles/#dedicated-runai-system-nodes","title":"Dedicated Run:ai System Nodes","text":"<p>Find out the names of the nodes designated for the Run:ai system by running <code>kubectl get nodes</code>. For each such node run:</p> <pre><code>runai-adm set node-role --runai-system-worker &lt;node-name&gt;\n</code></pre> <p>If you re-run <code>kubectl get nodes</code> you will see the node role of these nodes changed to <code>runai-system</code></p> <p>To remove the runai-system node role run:</p> <pre><code>runai-adm remove node-role --runai-system-worker &lt;node-name&gt;\n</code></pre> <p>Warning</p> <p>Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).</p>"},{"location":"admin/config/node-roles/#dedicated-gpu-and-cpu-nodes","title":"Dedicated GPU and CPU Nodes","text":"<p>Important</p> <p>To enable this feature, you must set the cluster configuration flag <code>global.nodeAffinity.restrictScheduling</code> to <code>true</code>. For more information see customize cluster.</p> <p>Separate nodes into those that:</p> <ul> <li>Run GPU workloads</li> <li>Run CPU workloads</li> <li>Do not run Run:ai at all. These jobs will not be monitored using the Run:ai Administration User interface. </li> </ul> <p>Review nodes names using <code>kubectl get nodes</code>. For each such node run:</p> <pre><code>runai-adm set node-role --gpu-worker &lt;node-name&gt;\n</code></pre> <p>or </p> <pre><code>runai-adm set node-role --cpu-worker &lt;node-name&gt;\n</code></pre> <p>Nodes not marked as GPU worker or CPU worker will not run Run:ai at all.</p> <p>To set all workers not running runai-system as GPU only or CPU only workers run:</p> <pre><code>runai-adm set node-role [--gpu-worker | --cpu-worker] --all\n</code></pre> <p>To remove the CPU or GPU worker node role run:</p> <pre><code>runai-adm remove node-role --cpu-worker &lt;node-name&gt;\n</code></pre> <p>or </p> <pre><code>runai-adm remove node-role --gpu-worker &lt;node-name&gt;\n</code></pre>"},{"location":"admin/config/notifications/","title":"Notifications System","text":""},{"location":"admin/config/notifications/#email-notifications-for-data-scientists","title":"Email Notifications for Data Scientists","text":"<p>Managing numerous data science workloads requires monitoring various stages, including submission, scheduling, initialization, execution, and completion. Additionally, handling suspensions and failures is crucial for ensuring timely workload completion. Email Notifications address this need by sending alerts for critical workload life cycle changes. This empowers data scientists to take necessary actions and prevent delays.</p>"},{"location":"admin/config/notifications/#setting-up-email-notifications","title":"Setting Up Email Notifications","text":"<p>Important</p> <p>The system administrator needs to enable and setup email notifications so that users are kept informed about different system statuses.</p> <p>To enable email notifications for the system:</p> <ol> <li> <p>Press Tools &amp; Settings, then select Notifications.</p> <p>Note</p> <p>For SaaS deployments, use the Enable email notifications toggle.</p> </li> <li> <p>In the SMTP Host field, enter the SMTP server address and in the SMTP port field the port number.</p> </li> <li>Select an Authentication type Plain or Login. Enter a username and password to be used for authentication.</li> <li>Enter the From email address and the Display name.</li> <li>Press Verify to ensure that the email configuration is working.</li> <li>Press Save when complete.</li> </ol>"},{"location":"admin/config/notifications/#system-notifications","title":"System Notifications","text":"<p>Administrators can set system wide notifications for all the users in order to alert them of important information. System notifications allows administrators the ability to update users with events that may be occurring within the Run:ai platform. The system notification will appear at each login or after the message has changed for users who are already logged in.</p> <p>To configure system notifications:</p> <ol> <li>Press Tools &amp; Settings, then select Notifications.</li> <li>In the System notification pane, press +MESSAGE.</li> <li>Enter your message in the text box. Use the formatting tool bar to add special formats to your message text.</li> <li>Enable the \"Don't show this again\" option to allow users to opt out of seeing the message multiple times.</li> <li>When complete, press Save &amp; Publish.</li> </ol>"},{"location":"admin/config/org-cert/","title":"None","text":"<p>In the context of Run:ai, the cluster and control-plane need to be aware of this certificate for consumers to be able to connect to the system.</p>"},{"location":"admin/config/org-cert/#preparation","title":"Preparation","text":"<p>You will need to have the public key of the local certificate authority. </p>"},{"location":"admin/config/org-cert/#control-plane-installation","title":"Control-Plane Installation","text":"<ul> <li>Create the <code>runai-backend</code> namespace if it does not exist. </li> <li> <p>Add the public key to the <code>runai-backend</code> namespace: <pre><code>kubectl -n runai-backend create secret generic runai-ca-cert \\ \n    --from-file=runai-ca.pem=&lt;ca_bundle_path&gt;\n</code></pre></p> </li> <li> <p>As part of the installation instructions, you need to create a secret for runai-backend-tls. Use the local certificate authority instead.</p> </li> <li>Install the control plane, add the following flag to the helm command <code>--set global.customCA.enabled=true</code></li> </ul>"},{"location":"admin/config/org-cert/#cluster-installation","title":"Cluster Installation","text":"<ul> <li>Create the <code>runai</code> namespace if it does not exist. </li> <li>Add the public key to the <code>runai</code> namespace: <pre><code>kubectl -n runai create secret generic runai-ca-cert \\\n    --from-file=runai-ca.pem=&lt;ca_bundle_path&gt;\n</code></pre></li> <li>In case you're using Openshift, add the public key to the <code>openshift-monitoring</code> namespace: <pre><code>kubectl -n openshift-monitoring create secret generic runai-ca-cert \\\n    --from-file=runai-ca.pem=&lt;ca_bundle_path&gt;\n</code></pre></li> <li>Install the Run:ai operator, add the following flag to the helm command <code>--set global.customCA.enabled=true</code></li> </ul>"},{"location":"admin/config/overview/","title":"Run:ai Configuration Articles","text":"<p>This section provides a list of installation-related articles dealing with a wide range of subjects:</p> Article Purpose Designating Specific Role Nodes Set one or more designated Run:ai system nodes or limit Run:ai monitoring and scheduling to specific nodes in the cluster. Create and Troubleshoot Clusters Create new clusters, view properties and status, and troubleshoot cluster connectivity related issues. Set Default Scheduler Set the default scheduler for a specific namespace Review Kubernetes Access provided to Run:ai In Restrictive Kubernetes environments such as when using OpenShift, understand and control what Kubernetes roles are provided to Run:ai External access to Containers Understand the available options for Researchers to access containers from the outside Install the Run:ai Administrator Command-line Interface The Administrator command-line is useful in a variety of flows such as cluster upgrade, node setup etc. Set Node affinity with cloud node pools Set node affinity when using a cloud provider for your cluster Local Certificate Authority For self-hosted Run:ai environments, specifically air-gapped installation, setup a local certificate authority to allow customers to safely connect to Run:ai Backup &amp; Restore For self-hosted Run:ai environments, set up a scheduled backup of Run:ai data High Availability Configure Run:ai such that it will continue to provide service even if parts of the system are down. Scaling Scale the Run:ai cluster and the Run:ai control-plane to withstand large transaction loads Emails and system notification Configure e-mail notification"},{"location":"admin/config/secure-cluster/","title":"Secure your cluster","text":"<p>This article details the security considerations for deploying Run:ai. It is intended to help administrators and security officers understand the specific permissions required by Run:ai.</p>"},{"location":"admin/config/secure-cluster/#access-to-the-kubernetes-cluster","title":"Access to the Kubernetes cluster","text":"<p>Run:ai integrates with Kubernetes clusters and requires specific permissions to successfully operate. These are permissions are controlled with configuration flags that dictate how Run:ai interacts with cluster resources. Prior to installation, security teams can review the permissions and ensure it aligns with their organization\u2019s policies.</p>"},{"location":"admin/config/secure-cluster/#permissions-and-their-related-use-case","title":"Permissions and their related use-case","text":"<p>Run:ai provides various security-related permissions that can be customized to fit specific organizational needs. Below are brief descriptions of the key use cases for these customizations:</p> Permission Use case Automatic Namespace creation Controls whether Run:ai automatically creates Kubernetes namespaces when new projects are created. Useful in environments where namespace creation must be strictly managed. Automatic user assignment Decides if users are automatically assigned to projects within Run:ai. Helps manage user access more tightly in certain compliance-driven environments. Secret propagation Determines whether Run:ai should propagate secrets across the cluster. Relevant for organizations with specific security protocols for managing sensitive data. Disabling Kubernetes limit range Chooses whether to disable the Kubernetes Limit Range feature. May be adjusted in environments with specific resource management needs. <p>Note</p> <p>These security customizations allow organizations to tailor Run:ai to their specific needs. All changes should be modified cautiously and only when necessary to meet particular security, compliance or operational requirements.</p>"},{"location":"admin/config/secure-cluster/#secure-installation","title":"Secure installation","text":"<p>Many organizations enforce IT compliance rules for Kubernetes, with strict access control for installing and running workloads. OpenShift uses Security Context Constraints (SCC) for this purpose. Run:ai fully supports SCC, ensuring integration with OpenShift's security requirements.</p>"},{"location":"admin/config/secure-cluster/#security-vulnerabilities","title":"Security vulnerabilities","text":"<p>The platform is actively monitored for security vulnerabilities, with regular scans conducted to identify and address potential issues. Necessary fixes are applied to ensure that the software remains secure and resilient against emerging threats, providing a safe and reliable experience.</p>"},{"location":"admin/config/shared-storage/","title":"Shared Storage","text":"<p>Shared storage is a critical component in AI and machine learning workflows, particularly in scenarios involving distributed training and shared datasets. In AI and ML environments, data must be readily accessible across multiple nodes, especially when training large models or working with vast datasets. Shared storage enable seamless access to data, ensuring that all nodes in a distributed training setup can read and write to the same datasets simultaneously. This setup not only enhances efficiency but is also crucial for maintaining consistency and speed in high-performance computing environments.</p> <p>While Run:ai Platform supports a variety of remote data sources, such as Git and S3, it is often more efficient to keep data close to the compute resources. This proximity is typically achieved through the use of shared storage, accessible to multiple nodes in your Kubernetes cluster.</p>"},{"location":"admin/config/shared-storage/#shared-storage","title":"Shared storage","text":"<p>When implementing shared storage in Kubernetes, there are two primary approaches:</p> <ul> <li>Utilizing the Kubernetes Storage Classes of your storage provider; or  </li> <li>Using a direct NFS (Network File System) mount</li> </ul> <p>Storage Classes being the recommended option.</p> <p>Run:ai Data Sources support both direct NFS mount and Kubernetes Storage Classes.</p>"},{"location":"admin/config/shared-storage/#kubernetes-storage-classes","title":"Kubernetes storage classes","text":"<p>Storage classes in Kubernetes defines how storage is provisioned and managed. This allows you to select storage types optimized for AI workloads. For example, you can choose storage with high IOPS (Input/Output Operations Per Second) for rapid data access during intensive training sessions, or tiered storage options to balance cost and performance-based on your organization\u2019s requirements. This approach supports dynamic provisioning, enabling storage to be allocated on-demand as required by your applications.</p> <p>Run:ai data sources such as Persistent Volume Claims (PVC) and Data Volumes leverage storage class to manage and allocate storage efficiently. This ensures that the most suitable storage option is always accessible, contributing to the efficiency and performance of AI workloads.</p> <p>Note</p> <p>Run:ai lists all available storage classes in the Kubernetes cluster, making it easy for users to select the appropriate storage. Additionally, policies can be set to restrict or enforce the use of specific storage classes, to helpl maintain compliance with organizational standards and optimize resource utilization.</p> Kubernetes 1.23 (old) <p>When using Kubernetes 1.23, Data Source of PVC type does not work using a Storage Class with the property <code>volumeBindingMode</code> equals to <code>WaitForFirstConsumer</code></p>"},{"location":"admin/config/shared-storage/#direct-nfs-mount","title":"Direct NFS mount","text":"<p>Direct NFS allows you to mount a shared file system directly across multiple nodes in your Kubernetes cluster. This method provides a straightforward way to share data among nodes and is often used for simple setups or when a dedicated NFS server is available.</p> <p>However, using NFS can present challenges related to security and control. Direct NFS setups might lack the fine-grained control and security features available with storage class.</p>"},{"location":"admin/maintenance/alert-monitoring/","title":"System Monitoring","text":"<p>This article explains how to configure Run:ai to generate health alerts and to connect these alerts to alert-management systems within your organization. Alerts are generated for Run:ai clusters.</p>"},{"location":"admin/maintenance/alert-monitoring/#alert-infrastructure","title":"Alert infrastructure","text":"<p>Run:ai uses Prometheus for externalizing metrics and providing visibility to end-users. The Run:ai Cluster installation includes Prometheus or can connect to an existing Prometheus instance used in your organization. The alerts are based on the Prometheus AlertManager. Once installed, it is enabled by default.</p> <p>This document explains how to:</p> <ul> <li>Configure alert destinations - triggered alerts send data to specified destinations  </li> <li>Understand the out-of-the-box cluster alerts, provided by Run:ai  </li> <li>Add additional custom alerts</li> </ul>"},{"location":"admin/maintenance/alert-monitoring/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster with the necessary permissions  </li> <li>Up and running Run:ai environment, including Prometheus Operator  </li> <li>kubectl command-line tool installed and configured to interact with the cluster</li> </ul>"},{"location":"admin/maintenance/alert-monitoring/#set-up","title":"Set-up","text":"<p>Use the steps below to set up monitoring alerts.</p>"},{"location":"admin/maintenance/alert-monitoring/#validating-prometheus-operator-installed","title":"Validating Prometheus operator installed","text":"<ol> <li>Verify that the Prometheus Operator Deployment is running    Copy the following command and paste it in your terminal, where you have access to the Kubernetes cluster: <code>kubectl get deployment kube-prometheus-stack-operator -n monitoring</code>    In your terminal, you can see an output indicating the deployment's status, including the number of replicas and their current state.  </li> <li>Verify that Prometheus instances are running    Copy the following command and paste it in your terminal: <code>kubectl get prometheus -n runai</code>    You can see the Prometheus instance(s) listed along with their status.</li> </ol>"},{"location":"admin/maintenance/alert-monitoring/#enabling-prometheus-alertmanager","title":"Enabling Prometheus AlertManager","text":"<p>In each of the steps in this section, copy the content of the code snippet to a new YAML file (e.g., <code>step1.yaml</code>).</p> <ul> <li>Copy the following command to your terminal, to apply the YAML file to the cluster:</li> </ul> <p>kubectl apply -f step1.yaml Copy the following command to your terminal to create the AlertManager CustomResource, to enable AlertManager: </p> <pre><code>apiVersion: monitoring.coreos.com/v1  \nkind: Alertmanager  \nmetadata:  \n   name: runai  \n   namespace: runai  \nspec:  \n   replicas: 1  \n   alertmanagerConfigSelector:  \n      matchLabels:\n         alertmanagerConfig: runai \n</code></pre> <ul> <li>Copy the following command to your terminal to validate that the AlertManager instance has started: <code>kubectl get alertmanager -n runai</code> </li> <li>Copy the following command to your terminal to validate that the Prometheus operator has created a Service for AlertManager: <code>kubectl get svc alertmanager-operated -n runai</code></li> </ul>"},{"location":"admin/maintenance/alert-monitoring/#configuring-prometheus-to-send-alerts","title":"Configuring Prometheus to send alerts","text":"<ol> <li>Open the terminal on your local machine or another machine that has access to your Kubernetes cluster  </li> <li> <p>Copy and paste the following command in your terminal to edit the Prometheus configuration for the <code>runai</code> Namespace: <pre><code>kubectl edit prometheus runai -n runai\n</code></pre> This command opens the Prometheus configuration file in your default text editor (usually <code>vi</code> or <code>nano</code>).</p> </li> <li> <p>Copy and paste the following text to your terminal to change the configuration file: <pre><code>alerting:  \n   alertmanagers:  \n      - namespace: runai  \n        name: alertmanager-operated  \n        port: web\n</code></pre></p> </li> <li>Save the changes and exit the text editor.  </li> </ol> <p>Note</p> <p>To save changes using <code>vi</code>, type <code>:wq</code> and press Enter.   The changes are applied to the Prometheus configuration in the cluster.</p>"},{"location":"admin/maintenance/alert-monitoring/#alert-destinations","title":"Alert destinations","text":"<p>Set out below are the various alert destinations.</p>"},{"location":"admin/maintenance/alert-monitoring/#configuring-alertmanager-for-custom-email-alerts","title":"Configuring AlertManager for custom email alerts","text":"<p>In each step, copy the contents of the code snippets to a new file and apply it to the cluster using <code>kubectl apply -f</code>.</p> <p>Add your smtp password as a secret: </p> <pre><code>apiVersion: v1  \nkind: Secret  \nmetadata:  \n   name: alertmanager-smtp-password  \n   namespace: runai  \nstringData:\n   password: \"your_smtp_password\"\n</code></pre> <p>Replace the relevant smtp details with your own, then apply the <code>alertmanagerconfig</code> using <code>kubectl apply</code>.  </p> <pre><code> apiVersion: monitoring.coreos.com/v1alpha1  \n kind: AlertmanagerConfig  \n metadata:  \n   name: runai  \n   namespace: runai  \n labels:  \n    alertmanagerConfig: runai  \n spec:  \n    route:  \n       continue: true  \n       groupBy:   \n       - alertname\n\n       groupWait: 30s  \n       groupInterval: 5m  \n       repeatInterval: 1h\n\n    matchers:  \n    - matchType: =~  \n      name: alertname  \n      value: Runai.*\n\n    receiver: email\n\n receivers:  \n - name: 'email'  \n   emailConfigs:  \n   - to: '&lt;destination_email_address&gt;'  \n     from: '&lt;from_email_address&gt;'  \n     smarthost: 'smtp.gmail.com:587'  \n     authUsername: '&lt;smtp_server_user_name&gt;'  \n     authPassword:  \n       name: alertmanager-smtp-password\n         key: password  \n</code></pre> <p>Save and exit the editor. The configuration is automatically reloaded.</p>"},{"location":"admin/maintenance/alert-monitoring/#third-party-alert-destinations","title":"Third-party alert destinations","text":"<p>Prometheus AlertManager provides a structured way to connect to alert-management systems. There are built-in plugins for popular systems such as PagerDuty and OpsGenie, including a generic Webhook.</p>"},{"location":"admin/maintenance/alert-monitoring/#example-integrating-runai-with-a-webhook","title":"Example: Integrating Run:ai with a Webhook","text":"<ol> <li>Use webhook.site to get a unique URL.  </li> <li>Use the upgrade cluster instructions to modify the values file:     Edit the values file to add the following, and replace <code>&lt;WEB-HOOK-URL&gt;</code> with the URL from webhook.site.</li> </ol> <p><pre><code>codekube-prometheus-stack:  \n  ...  \n  alertmanager:  \n    enabled: true  \n    config:  \n      global:  \n        resolve_timeout: 5m  \n      receivers:  \n      - name: \"null\"  \n      - name: webhook-notifications  \n        webhook_configs:  \n          - url: &lt;WEB-HOOK-URL&gt;  \n            send_resolved: true  \n      route:  \n        group_by:  \n        - alertname  \n        group_interval: 5m  \n        group_wait: 30s  \n        receiver: 'null'  \n        repeat_interval: 10m  \n        routes:  \n        - receiver: webhook-notifications\n</code></pre> 3. Verify that you are receiving alerts on the webhook.site, in the left pane:  </p> <p></p>"},{"location":"admin/maintenance/alert-monitoring/#built-in-alerts","title":"Built-in alerts","text":"<p>A Run:ai cluster comes with several built-in alerts. Each alert notifies on a specific functionality of a Run:ai\u2019s entity. There is also a single, inclusive alert: <code>Run:ai Critical Problems</code>, which aggregates all component-based alerts into a single cluster health test.</p> <p>Runai agent cluster info push rate low</p> Meaning The <code>cluster-sync</code> Pod in the <code>runai</code> namespace might not be functioning properly Impact Possible impact - no info/partial info from the cluster is being synced back to the control-plane Severity Critical Diagnosis <code>kubectl get pod -n runai</code> to see if the <code>cluster-sync</code> pod is running Troubleshooting/Mitigation To diagnose issues with the <code>cluster-sync</code> pod, follow these steps: Paste the following command to your terminal, to receive detailed information about the <code>cluster-sync</code> deployment:<code>kubectl describe deployment cluster-sync -n runai</code> Check the Logs: Use the following command to view the logs of the <code>cluster-sync</code> deployment:<code>kubectl logs deployment/cluster-sync -n runai</code> Analyze the Logs and Pod Details: From the information provided by the logs and the deployment details, attempt to identify the reason why the <code>cluster-sync</code> pod is not functioning correctly Check Connectivity: Ensure there is a stable network connection between the cluster and the Run:ai Control Plane. A connectivity issue may be the root cause of the problem. Contact Support: If the network connection is stable and you are still unable to resolve the issue, contact Run:ai support for further assistance <p>Runai agent pull rate low</p> Meaning The <code>runai-agent</code> pod may be too loaded, is slow in processing data (possible in very big clusters), or the <code>runai-agent</code> pod itself in the <code>runai</code> namespace may not be functioning properly. Impact Possible impact - no info/partial info from the control-plane is being synced in the cluster Severity Critical Diagnosis Run: <code>kubectl get pod -n runai</code> And see if the <code>runai-agent</code> pod is running. Troubleshooting/Mitigation To diagnose issues with the <code>runai-agent</code> pod, follow these steps: Describe the Deployment: Run the following command to get detailed information about the <code>runai-agent</code> deployment:<code>kubectl describe deployment runai-agent -n runai</code> Check the Logs: Use the following command to view the logs of the <code>runai-agent</code> deployment:<code>kubectl logs deployment/runai-agent -n runai</code> Analyze the Logs and Pod Details: From the information provided by the logs and the deployment details, attempt to identify the reason why the <code>runai-agent</code> pod is not functioning correctly. There may be a connectivity issue with the control plane. Check Connectivity: Ensure there is a stable network connection between the <code>runai-agent</code> and the control plane. A connectivity issue may be the root cause of the problem. Consider Cluster Load: If the <code>runai-agent</code> appears to be functioning properly but the cluster is very large and heavily loaded, it may take more time for the agent to process data from the control plane. Adjust Alert Threshold: If the cluster load is causing the alert to fire, you can adjust the threshold at which the alert triggers. The default value is 0.05. You can try changing it to a lower value (e.g., 0.045 or 0.04).To edit the value, paste the following in your terminal:<code>kubectl edit runaiconfig -n runai</code>In the editor, navigate to:spec:   prometheus:     agentPullPushRateMinForAlert: If the <code>agentPullPushRateMinForAlert</code> value does not exist, add it under <code>spec -&gt; prometheus</code> <p>Runai container memory usage critical</p> Meaning <code>Runai</code> container is using more than 90% of its Memory limit Impact The container might run out of memory and crash. Severity Critical Diagnosis Calculate the memory usage, this is performed by pasting the following to your terminal: <code>container_memory_usage_bytes{namespace=~\"runai|runai-backend\"}</code> Troubleshooting/Mitigation Add more memory resources to the container. If the issue persists, contact Run:ai <p>Runai container memory usage warning</p> Meaning Runai container is using more than 80% of its memory limit Impact The container might run out of memory and crash Severity Warning Diagnosis Calculate the memory usage, this can be done by pasting the following to your terminal: <code>container_memory_usage_bytes{namespace=~\"runai|runai-backend\"}</code> Troubleshooting/Mitigation Add more memory resources to the container. If the issue persists, contact Run:ai <p>Runai container restarting</p> Meaning <code>Runai</code> container has restarted more than twice in the last 10 min Impact The container might become unavailable and impact the Run:ai system Severity Warning Diagnosis To diagnose the issue and identify the problematic pods, paste this into your terminal: <code>kubectl get pods -n runai kubectl get pods -n runai-backend</code>One or more of the pods have a restart count &gt;= 2. Troubleshooting/Mitigation Paste this into your terminal:<code>kubectl logs -n NAMESPACE POD_NAME</code>Replace <code>NAMESPACE</code> and <code>POD_NAME</code> with the relevant pod information from the previous step. Check the logs for any standout issues and verify that the container has sufficient resources. If you need further assistance, contact Run:ai <p>Runai CPU usage warning</p> Meaning <code>runai</code> container is using more than 80% of its CPU limit Impact This might cause slowness in the operation of certain Run:ai features. Severity Warning Diagnosis Paste the following query to your terminal in order to calculate the CPU usage: <code>rate(container_cpu_usage_seconds_total{namespace=~\"runai|runai-backend\"}[2m])</code> Troubleshooting/Mitigation Add more CPU resources to the container. If the issue persists, please contact Run:ai. <p>Runai critical problem</p> Meaning One of the critical Run:ai alerts is currently active Impact Impact is based on the active alert Severity Critical Diagnosis Check Run:ai alerts in Prometheus to identify any active critical alerts <p>Runai daemonSet rollout stuck / Runai DaemonSet unavailable on nodes</p> Meaning There are currently 0 available pods for the <code>runai</code> daemonset on the relevant node Impact No fractional GPU workloads support Severity Critical Diagnosis Paste the following command to your terminal: <code>kubectl get daemonset -n runai-backend</code> In the result of this command, identify the daemonset(s) that don\u2019t have any running pods Troubleshooting/Mitigation Paste the following command to your terminal, where <code>daemonsetX</code> is the problematic daemonset from the pervious step: <code>kubectl describe daemonsetX -n runai</code> on the relevant deamonset(s) from the previous step. The next step is to look for the specific error which prevents it from creating pods. Possible reasons might be:Node Resource Constraints: The nodes in the cluster may lack sufficient resources (CPU, memory, etc.) to accommodate new pods from the daemonset. Node Selector or Affinity Rules: The daemonset may have node selector or affinity rules that are not matching with any nodes currently available in the cluster, thus preventing pod creation. <p>Runai deployment insufficient replicas / Runai deployment no available replicas /RunaiDeploymentUnavailableReplicas</p> Meaning <code>Runai</code> deployment has one or more unavailable pods Impact When this happens, there may be scale issues. Additionally, new versions cannot be deployed, potentially resulting in missing features. Severity Critical Diagnosis Paste the following commands to your terminal, in order to get the status of the deployments in the <code>runai</code> and <code>runai-backend</code> namespaces:<code>kubectl get deployment -n runai kubectl get deployment -n runai-backend</code>Identify any deployments that have missing pods. Look for discrepancies in the <code>DESIRED</code> and <code>AVAILABLE</code> columns. If the number of <code>AVAILABLE</code> pods is less than the <code>DESIRED</code> pods, it indicates that there are missing pods. Troubleshooting/Mitigation Paste the following commands to your terminal, to receive detailed information about the problematic deployment:<code>kubectl describe deployment &lt;DEPLOYMENT_NAME&gt; -n runai kubectl describe deployment &lt;DEPLOYMENT_NAME&gt; -n runai-backend</code> Paste the following commands to your terminal, to check the replicaset details associated with the deployment:<code>kubectl describe replicaset &lt;REPLICASET_NAME&gt; -n runai kubectl describe replicaset &lt;REPLICASET_NAME&gt; -n runai-backend</code> Paste the following commands to your terminal to retrieve the logs for the deployment to identify any errors or issues:<code>kubectl logs deployment/&lt;DEPLOYMENT_NAME&gt; -n runai kubectl logs deployment/&lt;DEPLOYMENT_NAME&gt; -n runai-backend</code> From the logs and the detailed information provided by the <code>describe</code> commands, analyze the reasons why the deployment is unable to create pods. Look for common issues such as: Resource constraints (CPU, memory) Misconfigured deployment settings or replicasets Node selector or affinity rules preventing pod schedulingIf the issue persists, contact Run:ai. <p>Runai project controller reconcile failure</p> Meaning The <code>project-controller</code> in <code>runai</code> namespace had errors while reconciling projects Impact Some projects might not be in the \u201cReady\u201d state. This means that they are not fully operational and may not have all the necessary components running or configured correctly. Severity Critical Diagnosis Retrieve the logs for the <code>project-controller</code> deployment by pasting the following command in your terminal:<code>kubectl logs deployment/project-controller -n runai</code> Carefully examine the logs for any errors or warning messages. These logs help you understand what might be going wrong with the project controller. Troubleshooting/Mitigation Once errors in the log have been identified, follow these steps to mitigate the issue: The error messages in the logs should provide detailed information about the problem. Read through them to understand the nature of the issue. If the logs indicate which project failed to reconcile, you can further investigate by checking the status of that specific project. Run the following command, replacing <code>&lt;PROJECT_NAME&gt;</code> with the name of the problematic project:<code>kubectl get project &lt;PROJECT_NAME&gt; -o yaml</code> Review the status section in the YAML output. This section describes the current state of the project and provide insights into what might be causing the failure.If the issue persists, contact Run:ai. <p>Runai StatefulSet insufficient replicas / Runai StatefulSet no available replicas</p> Meaning <code>Runai</code> statefulset has no available pods Impact Absence of Metrics Database Unavailability Severity Critical Diagnosis To diagnose the issue, follow these steps: Check the status of the stateful sets in the <code>runai-backend</code> namespace by running the following command:<code>kubectl get statefulset -n runai-backend</code> Identify any stateful sets that have no running pods. These are the ones that might be causing the problem. Troubleshooting/Mitigation Once you've identified the problematic stateful sets, follow these steps to mitigate the issue: Describe the stateful set to get detailed information on why it cannot create pods. Replace <code>X</code> with the name of the stateful set:<code>kubectl describe statefulset X -n runai-backend</code> Review the description output to understand the root cause of the issue. Look for events or error messages that explain why the pods are not being created. If you're unable to resolve the issue based on the information gathered, contact Run:ai support for further assistance."},{"location":"admin/maintenance/alert-monitoring/#adding-a-custom-alert","title":"Adding a custom alert","text":"<p>You can add additional alerts from Run:ai. Alerts are triggered by using the Prometheus query language with any Run:ai metric.</p> <p>To create an alert, follow these steps using Prometheus query language with Run:ai Metrics:</p> <ul> <li>Modify Values File: Use the upgrade cluster instructions to modify the values file.  </li> <li>Add Alert Structure: Incorporate alerts according to the structure outlined below. Replace placeholders <code>&lt;ALERT-NAME&gt;</code>, <code>&lt;ALERT-SUMMARY-TEXT&gt;</code>, <code>&lt;PROMQL-EXPRESSION&gt;</code>, <code>&lt;optional: duration s/m/h&gt;</code>, and <code>&lt;critical/warning&gt;</code> with appropriate values for your alert, as described below.</li> </ul> <p><pre><code>kube-prometheus-stack:  \n   additionalPrometheusRulesMap:  \n     custom-runai:  \n       groups:  \n       - name: custom-runai-rules  \n         rules:  \n         - alert: &lt;ALERT-NAME&gt;  \n           annotations:  \n             summary: &lt;ALERT-SUMMARY-TEXT&gt;  \n           expr:  &lt;PROMQL-EXPRESSION&gt;  \n           for: &lt;optional: duration s/m/h&gt;  \n           labels:  \n             severity: &lt;critical/warning&gt;\n</code></pre> * <code>&lt;ALERT-NAME&gt;</code>: Choose a descriptive name for your alert, such as <code>HighCPUUsage</code> or <code>LowMemory</code>. <code>&lt;ALERT-SUMMARY-TEXT&gt;</code>: Provide a brief summary of what the alert signifies, for example, <code>High CPU usage detected</code> or <code>Memory usage below threshold</code>. <code>&lt;PROMQL-EXPRESSION&gt;</code>: Construct a Prometheus query (PROMQL) that defines the conditions under which the alert should trigger. This query should evaluate to a boolean value (<code>1</code> for alert, <code>0</code> for no alert). <code>&lt;optional: duration s/m/h&gt;</code>: Optionally, specify a duration in seconds (<code>s</code>), minutes (<code>m</code>), or hours (<code>h</code>) that the alert condition should persist before triggering an alert. If not specified, the alert triggers as soon as the condition is met. <code>&lt;critical/warning&gt;</code>: Assign a severity level to the alert, indicating its importance. Choose between <code>critical</code> for severe issues requiring immediate attention, or <code>warning</code> for less critical issues that still need monitoring.</p> <p>You can find an example in the Prometheus documentation.</p>"},{"location":"admin/maintenance/audit-log/","title":"Audit Log","text":""},{"location":"admin/maintenance/audit-log/#introduction","title":"Introduction","text":"<p>The Run:ai control plane provides audit log API and audit log user interface table. Both reflect the same information:</p> <ul> <li>All changes to business objects</li> <li>All logins to the control plane.</li> </ul>"},{"location":"admin/maintenance/audit-log/#event-history-audit-log-user-interface","title":"Event History - Audit Log User Interface","text":"<p>The Administrators of the system can view the audit log using the user interface. The audit log screen is under the 'Event History' section:</p> <p></p>"},{"location":"admin/maintenance/audit-log/#event-history-audit-log-information-fields","title":"Event History (audit log) information fields","text":"<p>The Administrator can choose what information fields to view within the audit log table, this is done by clicking the 'Columns' button and checking the required fields to be presented:</p> <p></p> <p></p> <p>Here's the list of available information fields in the Event History (audit log) table:</p> Field Type Description User/App user id The identity of the User or Application that executed this operation. Data &amp; Time date The exact timestamp at which the event occured.  Format <code>dd/mm/yyyy</code> for date and <code>hh:mm am/pm</code> for time. Event event type The type of the logged operation. Possible values: <code>Create</code>, <code>Update</code>, <code>Delete</code>, <code>Login</code>. Event ID integer Sequanicialy incrmental number of the logged operation, lower number means older event, higher means newer event. Status string The outcome of the logged operation. Possible values: <code>Succeeded</code>, <code>Failed</code>. Entity type string The type of the logged business object. Possible values: <code>Project</code>, <code>Department</code>, <code>User</code>, <code>Group</code>, <code>Login</code>, <code>Settings</code>, <code>Applications</code>, <code>Node Pool</code>. Entity name string The name of logged business object. Entity ID string The system's internal id of the logged business object. Cluster Name string The name of the cluster that the loged operation relates to. If the operation is not cluster specific - cluster name remains empty. Cluster ID string The system internal identifier of the cluster that the loged operation relates to. If the operation is not cluster specific - cluster id remains empty."},{"location":"admin/maintenance/audit-log/#event-history-date-selector","title":"Event History - Date Selector","text":"<p>The Event History table saves logged operations for the last 90 days. However, the table itself presents up to the last 30 days of information due to the potentially very high number of operations that might be logged during this period. To view older logged operations, or if you wish to refine your search and get more specific results or fewer results, you should use the time selector and change the period you search for. You can also refine your search by using filters as explained below.  </p> <p></p>"},{"location":"admin/maintenance/audit-log/#event-history-filters","title":"Event History - Filters","text":"<p>The administrator can choose to filter the table using a list of predefined filters. The filter's value is a free text keyword entered by the administrator and must be fully matched to the requested field's actual value, otherwise, the filter will not find the requested keyword. Multiple filters can be set in parallel.</p> <p></p> <p></p>"},{"location":"admin/maintenance/audit-log/#event-history-download-the-audit-log-file","title":"Event History - Download the Audit Log file","text":"<p>The event history table allows you to download the logged information in text form formatted as CSV or JSON files. The scope of the downloaded information is set by the scope of the table filters, i.e. if no filters or date selectors are used, the downloaded file includes the full scope of the information that the table holds - i.e. up to 30 days of logged information. To view older logged information (up to 90 days older, but no more than 30 days at a time), shorter periods, or narrower (filtered) scopes - use the date selector and filters.</p> <p></p>"},{"location":"admin/maintenance/audit-log/#audit-log-api","title":"Audit log API","text":"<p>Since the amount of data is not trivial, the API is based on paging in the sense that it will retrieve a specified number of items for each API call. You can get more data by using subsequent calls. </p>"},{"location":"admin/maintenance/audit-log/#retrieve-audit-log-data-via-api","title":"Retrieve Audit Log data via API","text":"<p>To retrieve the Audit log you need to call an API. You can do this via code or by using the Audit function via a user interface for calling APIs.</p>"},{"location":"admin/maintenance/audit-log/#retrieve-via-code","title":"Retrieve via Code","text":"<p>Create an Application and generate a bearer token by following the API Authentication document.  </p> <p>To get the first 40 records of the audit log starting January 1st, 2022, run:</p> <pre><code>curl -X 'GET' \\\n  'https://&lt;COMPANY-URL&gt;/v1/k8s/audit?start=2022-1-1' \\  # (1)\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer &lt;ACCESS-TOKEN&gt;' # (2)\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is <code>app.run.ai</code> for SaaS installations (not <code>&lt;company&gt;.run.ai</code>) or the Run:ai user interface URL for Self-hosted installations.</li> <li>To obtain a Bearer token see API authentication.</li> </ol> <p>Sample result:</p> <pre><code>[\n    {\n        \"id\": 3,\n        \"tenantId\": 1,\n        \"happenedAt\": \"2022-07-07T09:45:32.069Z\",\n        \"action\": \"Update\",\n        \"version\": \"1.0\",\n        \"entityId\": \"1\",\n        \"entityType\": \"Project\",\n        \"entityName\": \"team-a\",\n        \"sourceType\": \"User\",\n        \"sourceId\": \"a79500fb-c452-471f-adc0-b65c972bd5c2\",\n        \"sourceName\": \"test@run.ai\",\n        \"context\": {\n            \"user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\",\n            \"ip_address\": \"10.244.0.0\"\n        }\n    },\n    {\n        \"id\": 2,\n        \"tenantId\": 1,\n        \"happenedAt\": \"2022-07-07T08:27:39.649Z\",\n        \"action\": \"Create\",\n        \"version\": \"1.0\",\n        \"entityId\": \"fdc90aab-b183-4856-8337-14039063b876\",\n        \"entityType\": \"App\",\n        \"entityName\": \"admin\",\n        \"sourceType\": \"User\",\n        \"sourceId\": \"a79500fb-c452-471f-adc0-b65c972bd5c2\",\n        \"sourceName\": \"test@run.ai\",\n        \"context\": {\n            \"user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\",\n            \"ip_address\": \"10.244.0.0\"\n        }\n    },\n...\n]\n</code></pre>"},{"location":"admin/maintenance/audit-log/#paging","title":"Paging","text":"<p>Use the <code>limit</code> and <code>offset</code> properties to retrieve all audit log entries.</p>"},{"location":"admin/maintenance/audit-log/#additional-filter","title":"Additional filter","text":"<p>You can add additional filters to the query as follows:</p> Field Type Description start date Start date for audit logs retrieval.  Format <code>yyyy-MM-dd</code> for date or <code>yyyy-MM-ddThh:mm:ss</code> for date-time. end date End date for audit logs retrieval.  Format <code>yyyy-MM-dd</code> for date or <code>yyyy-MM-ddThh:mm:ss</code> for date-time. action string The action of the logged operation. Possible values: <code>Create</code>, <code>Update</code>, <code>Delete</code>, <code>Login</code> source_type string The initiator of the action (user or machine to machine key). Possible values: <code>User</code>, <code>Application</code> source_id string The id of the source of the action. For <code>User</code>, this is the internal user id. For an <code>Application</code>, this is the internal id of the Application source_name string The name of the source of the action. For a <code>User</code>, this is the user's email, for an <code>Application</code>, this is the Application name. entity_type string The type of business object. Possible values: <code>Project</code>, <code>Department</code>, <code>User</code>, <code>Group</code>, <code>Login</code>, <code>Settings</code>, <code>Applications</code> entity_id string The id of the business object limit integer Paging: the number of records to fetch at once (default is 40 record) offset integer Paging: The offset from which to start fetching records. success string enter true for successful audit log records and false for failures (default is all records) download string enter true to download the logs into a file <p></p>"},{"location":"admin/maintenance/node-downtime/","title":"Node Maintenance","text":"<p>This article provides detailed instructions on how to manage both planned and unplanned node downtime in a Kubernetes cluster that is running Run:ai. It covers all the steps to maintain service continuity and ensure the proper handling of workloads during these events.</p>"},{"location":"admin/maintenance/node-downtime/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to Kubernetes cluster   Administrative access to the Kubernetes cluster, including permissions to run <code>kubectl</code> commands  </li> <li>Basic knowledge of Kubernetes   Familiarity with Kubernetes concepts such as nodes, taints, and workloads  </li> <li>Run:ai installation   The Run:ai software installed and configured within your Kubernetes cluster   </li> <li>Node naming conventions   Know the names of the nodes within your cluster, as these are required when executing the commands</li> </ul>"},{"location":"admin/maintenance/node-downtime/#node-types","title":"Node types","text":"<p>This article distinguishes between two types of nodes within a Run:ai installation:</p> <ul> <li>Worker nodes. Nodes on which AI practitioners can submit and run workloads</li> <li>Run:ai system nodes. Nodes on which the Run:ai software runs, managing the cluster's operations</li> </ul>"},{"location":"admin/maintenance/node-downtime/#worker-nodes","title":"Worker nodes","text":"<p>Worker Nodes are responsible for running workloads. When a worker node goes down, either due to planned maintenance or unexpected failure, workloads ideally migrate to other available nodes or wait in the queue to be executed when possible.</p>"},{"location":"admin/maintenance/node-downtime/#training-vs-interactive-workloads","title":"Training vs. Interactive workloads","text":"<p>The following workload types can run on worker nodes: </p> <ul> <li> <p>Training workloads. These are long-running processes that, in case of node downtime, can automatically move to another node.</p> </li> <li> <p>Interactive workloads. These are short-lived, interactive processes that require manual intervention to be relocated to another node.</p> </li> </ul> <p>Note</p> <p>While training workloads can be automatically migrated, it is recommended to plan maintenance and manually manage this process for a faster response, as it may take time for Kubernetes to detect a node failure,</p>"},{"location":"admin/maintenance/node-downtime/#planned-maintenance","title":"Planned maintenance","text":"<p>Before stopping a worker node for maintenance, perform the following steps:</p> <ol> <li> <p>Prevent new workloads on the node    To stop the Kubernetes Scheduler from assigning new workloads to the node and to safely remove all existing workloads, copy the following command to your terminal:  </p> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute\n</code></pre> <p>Explanation: </p> <ul> <li><code>&lt;node-name&gt;</code>     Replace this placeholder with the actual name of the node you want to drain  </li> <li><code>kubectl taint nodes</code>     This command is used to add a taint to the node, which prevents any new pods from being scheduled on it  </li> <li><code>runai=drain:NoExecute</code>     This specific taint ensures that all existing pods on the node are evicted and rescheduled on other available nodes, if possible. </li> </ul> <p>Result: The node stops accepting new workloads, and existing workloads either migrate to other nodes or are placed in a queue for later execution. </p> </li> <li> <p>Shut down and perform maintenance    After draining the node, you can safely shut it down and perform the necessary maintenance tasks. </p> </li> <li> <p>Restart the node     Once maintenance is complete and the node is back online, remove the taint to allow the node to resume normal operations. Copy the following command to your terminal:  </p> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute-\n</code></pre> <p>Explanation: </p> <ul> <li><code>runai=drain:NoExecute-</code>   The <code>-</code> at the end of the command indicates the removal of the taint. This allows the node to start accepting new workloads again.</li> </ul> <p>Result: The node rejoins the cluster's pool of available resources, and workloads can be scheduled on it as usual</p> </li> </ol>"},{"location":"admin/maintenance/node-downtime/#unplanned-downtime","title":"Unplanned downtime","text":"<p>In the event of unplanned downtime:</p> <ol> <li>Automatic Restart     If a node fails but immediately restarts, all services and workloads automatically resume.  </li> <li> <p>Extended Downtime    If the node remains down for an extended period, drain the node to migrate workloads to other nodes. Copy the following command to your terminal:  </p> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute\n</code></pre> <p>Explanation: The command works the same as in the planned maintenance section, ensuring that no workloads remain scheduled on the node while it is down.  </p> </li> <li> <p>Reintegrate the Node     Once the node is back online, remove the taint to allow it to rejoin the cluster's operations. Copy the following command to your terminal:  </p> <p><pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute-\n</code></pre> Result: This action reintegrates the node into the cluster, allowing it to accept new workloads.  </p> </li> <li> <p>Permanent Shutdown     If the node is to be permanently decommissioned, remove it from Kubernetes with the following command:  </p> <p><pre><code>kubectl delete node &lt;node-name&gt;\n</code></pre> Explanation: </p> <ul> <li><code>kubectl delete node</code>   This command completely removes the node from the cluster  </li> <li><code>&lt;node-name&gt;</code>   Replace this placeholder with the actual name of the node  </li> </ul> <p>Result: The node is no longer part of the Kubernetes cluster. If you plan to bring the node back later, it must be rejoined to the cluster using the steps outlined in the next section.</p> </li> </ol>"},{"location":"admin/maintenance/node-downtime/#runai-system-nodes","title":"Run:ai System nodes","text":"<p>In a production environment, the services responsible for scheduling, submitting and managing Run:ai workloads operate on one or more Run:ai system nodes. It is recommended to have more than one system node to ensure high availability. If one system node goes down, another can take over, maintaining continuity. If a second system node does not exist, you must designate another node in the cluster as a temporary Run:ai system node to maintain operations.</p> <p>The protocols for handling planned maintenance and unplanned downtime are identical to those for worker nodes. Refer to the above section for detailed instructions. </p>"},{"location":"admin/maintenance/node-downtime/#rejoining-a-node-into-the-kubernetes-cluster","title":"Rejoining a node into the Kubernetes cluster","text":"<p>To rejoin a node to the Kubernetes cluster, follow these steps:</p> <ol> <li> <p>Generate a join command on the master node    On the master node, copy the following command to your terminal:  </p> <pre><code>kubeadm token create --print-join-command\n</code></pre> <p>Explanation: </p> <ul> <li><code>kubeadm token create</code>     This command generates a token that can be used to join a node to the Kubernetes cluster.  </li> <li><code>--print-join-command</code>     This option outputs the full command that needs to be run on the worker node to rejoin it to the cluster.</li> </ul> <p>Result: The command outputs a <code>kubeadm join</code> command. </p> </li> <li> <p>Run the Join Command on the Worker Node    Copy the <code>kubeadm join</code> command generated from the previous step and run it on the worker node that needs to rejoin the cluster.</p> <p>Explanation: </p> <ul> <li>The <code>kubeadm join</code> command re-enrolls the node into the cluster, allowing it to start participating in the cluster's workload scheduling. </li> </ul> </li> <li> <p>Verify Node Rejoining     Verify that the node has successfully rejoined the cluster by running:  </p> <pre><code>kubectl get nodes\n</code></pre> <p>Explanation:  </p> <p>This command lists all nodes currently part of the Kubernetes cluster, along with their status  </p> <p>Result: The rejoined node should appear in the list with a status of Ready </p> </li> <li> <p>Re-label Nodes     Once the node is back online, ensure it is labeled according to its role within the cluster</p> </li> </ol>"},{"location":"admin/researcher-setup/cli-install/","title":"Install the Run:ai V1 Command-line Interface","text":"<p>The Run:ai Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc.</p> <p>The instructions below will guide you through the process of installing the CLI. The Run:ai CLI runs on Mac, Linux and Windows. </p>"},{"location":"admin/researcher-setup/cli-install/#researcher-authentication","title":"Researcher Authentication","text":"<p>When enabled, Researcher authentication requires additional setup when installing the CLI. To configure authentication see Setup Project-based Researcher Access Control. Use the modified Kubernetes configuration file described in the article.</p>"},{"location":"admin/researcher-setup/cli-install/#prerequisites","title":"Prerequisites","text":"<ul> <li>When installing the command-line interface, it is worth considering future upgrades:<ul> <li>Install the CLI on a dedicated Jumpbox machine. Researchers will connect to the Jumpbox from which they can submit Run:ai commands</li> <li>Install the CLI on a shared directory that is mounted on Researchers' machines.  </li> </ul> </li> <li>A Kubernetes configuration file.</li> </ul>"},{"location":"admin/researcher-setup/cli-install/#setup","title":"Setup","text":""},{"location":"admin/researcher-setup/cli-install/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<ul> <li>In the Researcher's root folder, create a directory .kube. Copy the Kubernetes configuration file into the directory. Each Researcher should have a separate copy of the configuration file. The Researcher should have write access to the configuration file as it stores user defaults.</li> <li>If you choose to locate the file at a different location than <code>~/.kube/config</code>, you must create a shell variable to point to the configuration file as follows:</li> </ul> <pre><code>export KUBECONFIG=&lt;Kubernetes-config-file&gt;\n</code></pre> <ul> <li>Test the connection by running:</li> </ul> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#install-runai-cli","title":"Install Run:ai CLI","text":"<ul> <li>Go to the Run:ai user interface. On the top right select <code>Researcher Command Line Interface</code>.</li> <li>Select <code>Mac</code>, <code>Linux</code> or <code>Windows</code>.</li> <li>Download directly using the button or copy the file to run it on a remote machine</li> </ul> Mac or LinuxWindows <p>Run:</p> <pre><code>chmod +x runai\nsudo mv runai /usr/local/bin/runai\n</code></pre> <p>Rename the downloaded file to have a <code>.exe</code> extension and move the file to a folder that is a part of the <code>PATH</code>.</p> <p>Note</p> <p>An alternative way of downloading the CLI is provided under the CLI Troubleshooting section.</p> <p>To verify the installation run:</p> <pre><code>runai list jobs\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#install-command-auto-completion","title":"Install Command Auto-Completion","text":"<p>It is possible to configure your Linux/Mac shell to complete Run:ai CLI commands. This feature works on bash and zsh shells only.</p> ZshBash <p>Edit the file <code>~/.zshrc</code>. Add the lines:</p> <pre><code>autoload -U compinit; compinit -i\nsource &lt;(runai completion zsh)\n</code></pre> <p>Install the bash-completion package:</p> <ul> <li>Mac: <code>brew install bash-completion</code></li> <li>Ubuntu/Debian: <code>sudo apt-get install bash-completion</code></li> <li>Fedora/Centos: <code>sudo yum install bash-completion</code></li> </ul> <p>Edit the file <code>~/.bashrc</code>. Add the lines:</p> <pre><code>[[ -r \u201c/usr/local/etc/profile.d/bash_completion.sh\u201d ]] &amp;&amp; . \u201c/usr/local/etc/profile.d/bash_completion.sh\u201d\nsource &lt;(runai completion bash)\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#troubleshoot-the-cli-installation","title":"Troubleshoot the CLI Installation","text":"<p>See Troubleshooting a CLI installation</p>"},{"location":"admin/researcher-setup/cli-install/#update-the-runai-cli","title":"Update the Run:ai CLI","text":"<p>To update the CLI to the latest version perform the same install process again.</p>"},{"location":"admin/researcher-setup/cli-install/#delete-the-runai-cli","title":"Delete the Run:ai CLI","text":"<p>If you have installed using the default path, run:</p> <pre><code>sudo rm /usr/local/bin/runai\n</code></pre>"},{"location":"admin/researcher-setup/docker-to-runai/","title":"From Docker to Run:ai","text":""},{"location":"admin/researcher-setup/docker-to-runai/#dockers-images-and-kubernetes","title":"Dockers, Images, and Kubernetes","text":"<p>Researchers are typically proficient in working with Docker. Docker is an isolation level above the operating system which allows creating your own bundle of the operating system + deep learning environment and packaging it within a single file. The file is called a docker image.</p> <p>You create a container by starting a docker image on a machine.</p> <p>Run:ai is based on Kubernetes. At its core, Kubernetes is an orchestration software above Docker: Among other things, it allows location abstraction as to where the actual container is running. This calls for some adaptation to the Researcher's workflow as follows.</p>"},{"location":"admin/researcher-setup/docker-to-runai/#image-repository","title":"Image Repository","text":"<p>If your Kubernetes cluster contains a single GPU node (machine), then your image can reside on the node itself (in which case, when runai submit workloads, the Researcher must use the flag <code>--local-image</code>).</p> <p>If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the image can no longer reside on the node itself.  It must be relocated to an image repository. There are quite a few repository-as-a-service, most notably Docker hub. Alternatively, the organization can install a private repository on-prem.</p> <p>Day-to-day work with the image located remotely is almost identical to local work. The image name now contains its location. For example, <code>nvcr.io/nvidia/pytorch:19.12-py_3</code> is a PyTorch image that is located in nvcr.io. This is the Nvidia image repository as found on the web. </p>"},{"location":"admin/researcher-setup/docker-to-runai/#data","title":"Data","text":"<p>Deep learning is about data. It can be your code, the training data, saved checkpoints, etc.</p> <p>If your Kubernetes cluster contains a single GPU node (machine), then your data can reside on the node itself.</p> <p>If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the data must sit outside the machine, typically on network storage. The storage must be uniformly mapped to your container when it starts (using the -v command).</p>"},{"location":"admin/researcher-setup/docker-to-runai/#working-with-containers","title":"Working with Containers","text":"<p>Starting a container using docker usually involves a single command-line with multiple flags. A typical example: </p> <pre><code>docker run --runtime=nvidia --shm-size 16G -it --rm -e HOSTNAME='hostname' \\\n    -v /raid/public/my_datasets:/root/dataset:ro   -i  nvcr.io/nvidia/pytorch:19.12-py3\n</code></pre> <p>The docker command <code>docker run</code> should be replaced with a Run:ai command <code>runai submit</code>. The flags are usually the same but some adaptation is required. A complete list of flags can be found here: runai submit. </p> <p>There are similar commands to get a shell into the container (runai bash), get the container logs (runai logs), and more. For a complete list see the Run:ai CLI reference. </p>"},{"location":"admin/researcher-setup/docker-to-runai/#schedule-an-onboarding-session","title":"Schedule an Onboarding Session","text":"<p>It is highly recommended to schedule an onboarding session for Researchers with a Run:ai customer success professional. Run:ai can help with the above transition, but adding to that, we at Run:ai have also acquired a large body of knowledge on data science best practices which can help streamline  Researchers' work as well as save money for the organization.</p>"},{"location":"admin/researcher-setup/new-cli-install/","title":"Installing the V2 Command-line interface","text":"<p>This article explains the procedure for installing and configuring the new researcher Command Line Interface (CLI). </p> <p>Important</p> <p>This document refers to the new CLI which only works with clusters of version 2.18 and up.    The installation instructions for the older CLI are here.</p>"},{"location":"admin/researcher-setup/new-cli-install/#enabling-the-v2-cli","title":"Enabling the V2 CLI","text":"<p>Under Tools &amp; Settings \u2192 General settings \u2192 Workloads, enable the flag <code>Improved command line interface</code></p>"},{"location":"admin/researcher-setup/new-cli-install/#installing-the-cli","title":"Installing the CLI","text":"<ol> <li>Click the Help (?) icon in the top right corner  </li> <li>Select Researcher Command Line Interface </li> <li>Select the cluster you want the CLI to communicate with  </li> <li>Select your computer\u2019s operating system </li> <li>Copy the installer command and run it in the terminal or download the binary file for Windows OS</li> <li>Follow the installation process instructions  </li> <li>Click <code>Enter</code> to use the default values (recommended)</li> </ol>"},{"location":"admin/researcher-setup/new-cli-install/#testing-the-installation","title":"Testing the installation","text":"<p>To verify the CLI client was installed properly</p> <ol> <li>Open the terminal  </li> <li>Run the command <code>runai version</code></li> </ol>"},{"location":"admin/researcher-setup/new-cli-install/#configuring-the-cli","title":"Configuring the CLI","text":"<p>Follow the steps below to configure the CLI.</p>"},{"location":"admin/researcher-setup/new-cli-install/#setting-the-control-plane-url","title":"Setting the Control plane URL","text":"<p>The following step is required for Windows users only. Linux and Mac clients are configured via the installation script automatically</p> <p>Run the command <code>runai config set --cp-url &lt;CONTROL_PLANE_URL&gt;</code>.  This will also create the <code>config.json</code> file in the default path.</p>"},{"location":"admin/researcher-setup/new-cli-install/#authenticating-the-cli","title":"Authenticating the CLI","text":"<p>After installation, sign in to the Run:ai platform to authenticate the CLI:</p> <ol> <li>Open the terminal on your local machine. </li> <li>Run <code>runai login</code>.</li> <li>Enter your username and password on the Run:ai platform's sign-in page. </li> <li>Return to the terminal window to use the CLI.</li> </ol>"},{"location":"admin/researcher-setup/new-cli-install/#setting-the-default-cluster","title":"Setting the default cluster","text":"<p>If only one cluster is connected to the account, it is set as the default cluster when you first sign in.  If there are multiple clusters, you must follow the steps below to set your preferred cluster for workload submission:</p> <ol> <li>Open the terminal on your local machine.  </li> <li>Run <code>runai cluster</code> and select the desired cluster from the interactive menu.</li> </ol> <p>Alternatively:  </p> <ol> <li>Open the terminal on your local machine.  </li> <li>Run <code>runai cluster list</code> to find the desired cluster name.  </li> <li>Run the following command <code>runai cluster set &lt;CLUSTER_NAME&gt;</code>.</li> </ol>"},{"location":"admin/researcher-setup/new-cli-install/#setting-a-default-project","title":"Setting a default project","text":"<p>Set a default working project, to easily submit workloads without mentioning the project name in every command.</p> <ol> <li>Open the terminal on your local machine.  </li> <li>Run <code>runai project</code> and select the desired cluster from the interactive menu.</li> </ol> <p>alernativly  </p> <ol> <li>Open the terminal on your local machine.  </li> <li>Run <code>runai cluster list</code> to find the desired project name.  </li> <li>Run the following command <code>runai project set &lt;PROJECT_NAME&gt;</code> </li> <li>If successful, the following message is returned <code>project &lt;PROJECT_NAME&gt; configured successfully</code> </li> </ol>"},{"location":"admin/researcher-setup/new-cli-install/#validating-the-configuration","title":"Validating the configuration","text":"<p>To view the current configuration run <code>runai config generate --json</code></p>"},{"location":"admin/researcher-setup/new-cli-install/#installing-command-auto-completion","title":"Installing command auto-completion","text":"<p>Auto-completion assists with completing the command syntax automatically for ease of use. Auto-completion is installed automatically.  The interfaces below require manual installation:</p> ZshBashWindows <ol> <li>Edit the file <code>~/.zshrc</code> </li> <li>Add the following code:</li> </ol> <pre><code>autoload -U compinit; compinit -i\nsource &lt;(runai completion zsh)\n</code></pre> <ol> <li>Install the bash-completion package  </li> <li>Choose your operating system:      Mac: <code>brew install bash-completion</code>      Ubuntu/Debian: <code>sudo apt-get install bash-completion</code>      Fedora/Centos: <code>sudo yum install bash-completion</code> </li> <li>Edit the file <code>~/.bashrc</code> and add the following lines:</li> </ol> <pre><code>[[ $PS1 &amp;&amp; -f /usr/share/bash-completion/bash_completion ]] &amp;&amp; . /usr/share/bash-completion/bash_completion\nsource &lt;(runai completion bash)\n</code></pre> <p>Add the following code in the powershell profile: <pre><code>runai.exe completion powershell | Out-String | Invoke-Expression\nSet-PSReadLineKeyHandler -Key Tab -Function MenuComplete\n</code></pre> For more completion modes options, see Powershell completions.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/","title":"Researcher Setup Overview","text":"<p>Following is a step-by-step guide for getting a new Researcher up to speed with Run:ai and Kubernetes.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#change-of-paradigms-from-docker-to-kubernetes","title":"Change of Paradigms: from Docker to Kubernetes","text":"<p>As part of Run:ai, the organization is typically moving from Docker-based workflows to Kubernetes. This document is an attempt to help the Researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:ai CLI.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#setup-the-runai-command-line-interface","title":"Setup the Run:ai Command-Line Interface","text":"<p>Run:ai CLI needs to be installed on the Researcher's machine. This document provides step by step instructions.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#provide-the-researcher-with-a-gpu-quota","title":"Provide the Researcher with a GPU Quota","text":"<p>To submit workloads with Run:ai, the Researcher must be provided with a Project that contains a GPU quota. Please see Working with Projects document on how to create Projects and set a quota.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#provide-access-to-the-runai-user-interface","title":"Provide access to the Run:ai User Interface","text":"<p>See Setting up users for further information on how to provide access to users.  </p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#schedule-an-onboarding-session","title":"Schedule an Onboarding Session","text":"<p>It is highly recommended to schedule an onboarding session for Researchers with a Run:ai customer success professional. Run:ai can help with the above transition, but adding to that, we at Run:ai have also acquired a large body of knowledge on data science best practices which can help streamline the Researchers' work as well as save money for the organization. </p>"},{"location":"admin/runai-setup/installation-types/","title":"Installation Types","text":"<p>Run:ai consists of two components:</p> <ul> <li>The Run:ai Cluster. One or more data-science GPU clusters hosted by the customer (on-prem or cloud).</li> <li>The Run:ai Control plane. A single entity that monitors clusters, sets priorities, and business policies.</li> </ul> <p>There are two main installation options:</p> Installation Type Description Classic (SaaS) Run:ai is installed on the customer's data science GPU clusters. The cluster connects to the Run:ai control plane on the cloud (https://<code>&lt;tenant-name&gt;</code>.run.ai).  With this installation, the cluster requires an outbound connection to the Run:ai cloud. Self-hosted The Run:ai control plane is also installed in the customer's data center <p>The self-hosted option is for organizations that cannot use a SaaS solution due to data leakage concerns. The self-hosted installation is priced differently. For further information please talk to Run:ai sales.</p> <p></p>"},{"location":"admin/runai-setup/installation-types/#self-hosted-installation","title":"Self-hosted Installation","text":"<p>Run:ai self-hosting comes with two variants:</p> Self-hosting Type Description Connected The organization can freely download from the internet (though upload is not allowed) Air-gapped The organization has no connection to the internet"},{"location":"admin/runai-setup/installation-types/#self-hosting-with-kubernetes-vs-openshift","title":"Self-hosting with Kubernetes vs OpenShift","text":"<p>Kubernetes has many Certified Kubernetes Providers. Run:ai has been certified with several of them (see the Kubernetes distribution section). The OpenShift installation is different from the rest. As such, the Run:ai self-hosted installation instructions are divided into two separate sections:</p> <ul> <li>OpenShift-based installation. See Run:ai OpenShift installation.</li> <li>Kubernetes-based installation. See Run:ai Kubernetes installation.</li> </ul>"},{"location":"admin/runai-setup/installation-types/#secure-installation","title":"Secure Installation","text":"<p>In many organizations, Kubernetes is governed by IT compliance rules. In this scenario, there are strict access control rules during the installation and running of workloads:</p> <ul> <li>OpenShift is secured using Security Context Constraints (SCC). The Run:ai installation supports SCC.</li> <li>Run:ai provides limited support for Kubernetes Pod Security Admission (PSA). For more information see Kubernetes prerequisites.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-delete/","title":"Cluster Uninstall","text":"<p>This article explains how to uninstall Run:ai Cluster installation from the Kubernetes cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-delete/#unistall-runai-cluster","title":"Unistall Run:ai cluster","text":"<p>Uninstall of Run:ai cluster from the Kubernetes cluster does not delete existing projects, departments or workloads submitted by users.</p> <p>To uninstall the Run:ai cluster, run the following helm command in your terminal:</p> <pre><code>helm uninstall runai-cluster -n runai\n</code></pre> <p>To delete the Run:ai cluster from the Run:ai Platform, see Removing a cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/","title":"Cluster Install","text":"<p>This article explains the steps required to install the Run:ai cluster on a Kubernetes cluster using Helm.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#before-installation","title":"Before installation","text":"<p>There are a number of matters to consider prior to installing using Helm.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#system-and-network-requirements","title":"System and network requirements","text":"<p>Before installing the Run:ai cluster, validate that the system requirements and network requirements are met.</p> <p>Once all the requirements are met, it is highly recommend to use the Run:ai cluster preinstall diagnostics tool to:</p> <ul> <li>Test the below requirements in addition to failure points related to Kubernetes, NVIDIA, storage, and networking  </li> <li>Look at additional components installed and analyze their relevance to a successful installation</li> </ul> <p>To run the preinstall diagnostics tool, download the latest version, and run:</p> SaaSSelf-hostedAirgap <ul> <li>On EKS deployments, run <code>aws configure</code> prior to execution</li> </ul> <pre><code>chmod +x ./preinstall-diagnostics-&lt;platform&gt; &amp;&amp; \\\n./preinstall-diagnostics-&lt;platform&gt; \\\n  --domain ${COMPANY_NAME}.run.ai \\\n  --cluster-domain ${CLUSTER_FQDN}\n</code></pre> <pre><code>chmod +x ./preinstall-diagnostics-&lt;platform&gt; &amp;&amp; \\ \n./preinstall-diagnostics-&lt;platform&gt; \\\n  --domain ${CONTROL_PLANE_FQDN} \\\n  --cluster-domain ${CLUSTER_FQDN} \\\n#if the diagnostics image is hosted in a private registry\n  --image-pull-secret ${IMAGE_PULL_SECRET_NAME} \\\n  --image ${PRIVATE_REGISTRY_IMAGE_URL}    \n</code></pre> <p>In an air-gapped deployment, the diagnostics image is saved, pushed, and pulled manually from the organization's registry.</p> <pre><code>#Save the image locally\ndocker save --output preinstall-diagnostics.tar gcr.io/run-ai-lab/preinstall-diagnostics:${VERSION}\n#Load the image to the organization's registry\ndocker load --input preinstall-diagnostics.tar\ndocker tag gcr.io/run-ai-lab/preinstall-diagnostics:${VERSION} ${CLIENT_IMAGE_AND_TAG} \ndocker push ${CLIENT_IMAGE_AND_TAG}\n</code></pre> <p>Run the binary with the <code>--image</code> parameter to modify the diagnostics image to be used:</p> <pre><code>chmod +x ./preinstall-diagnostics-darwin-arm64 &amp;&amp; \\\n./preinstall-diagnostics-darwin-arm64 \\\n  --domain ${CONTROL_PLANE_FQDN} \\\n  --cluster-domain ${CLUSTER_FQDN} \\\n  --image-pull-secret ${IMAGE_PULL_SECRET_NAME} \\\n  --image ${PRIVATE_REGISTRY_IMAGE_URL}    \n</code></pre> <p>For more information see preinstall diagnostics.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#helm","title":"Helm","text":"<p>Run:ai cluster requires Helm 3.14 or above. To install Helm, see Helm Install.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#permissions","title":"Permissions","text":"<p>A Kubernetes user with the <code>cluster-admin</code> role is required to ensure a successful installation, for more information see Using RBAC authorization.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#runai-namespace","title":"Run:ai namespace","text":"<p>Run:ai cluster must be installed in a namespace named <code>runai</code>. Create the namespace by running:</p> <pre><code>kubectl create ns runai\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#tls-certificates","title":"TLS certificates","text":"<p>A TLS private and public keys are required for HTTP access to the cluster. Create a Kubernetes Secret named <code>runai-cluster-domain-tls-secret</code> in the <code>runai</code> namespace with the cluster\u2019s Fully Qualified Domain Name (FQDN) private and public keys, by running the following:</p> <pre><code>kubectl create secret tls runai-cluster-domain-tls-secret -n runai \\\n    --cert /path/to/fullchain.pem  \\ # Replace /path/to/fullchain.pem with the actual path to your TLS certificate\n    --key /path/to/private.pem # Replace /path/to/private.pem with the actual path to your private key\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#installation","title":"Installation","text":"<p>Follow these instructions to install using Helm.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#adding-a-new-cluster","title":"Adding a new cluster","text":"<p>Follow the steps below to add a new cluster.</p> <p>Note</p> <p>When adding a cluster for the first time, the New Cluster form automatically opens when you log-in to the Run:ai platform. Other actions are prevented, until the cluster is created.</p> <p>If this is your first cluster and you have completed the New Cluster form, start at step 3. Otherwise, start at step 1.</p> <ol> <li>In the Run:ai platform, go to Clusters </li> <li>Click +NEW CLUSTER </li> <li>Enter a unique name for your cluster  </li> <li>Optional: Chose the Run:ai cluster version (latest, by default)  </li> <li>Enter the Cluster URL . For more information see Domain Name Requirement </li> <li>Click Continue</li> </ol>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#installing-runai-cluster","title":"Installing Run:ai cluster","text":"<p>In the next Section, the Run:ai cluster installation steps will be presented.</p> <ol> <li>Follow the installation instructions and run the commands provided on your Kubernetes cluster.  </li> <li>Click DONE</li> </ol> <p>The cluster is displayed in the table with the status Waiting to connect, once installation is complete, the cluster status changes to Connected</p> <p>Note</p> <p>To customize the installation based on your environment, see Customize cluster installation.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter an issue with the installation, try the troubleshooting scenario below.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#installation_1","title":"Installation","text":"<p>If the Run:ai cluster installation failed, check the installation logs to identify the issue. Run the following script to print the installation logs:</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/run-ai/public/main/installation/get-installation-logs.sh\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#cluster-status","title":"Cluster status","text":"<p>If the Run:ai cluster installation completed, but the cluster status did not change its status to Connected, check the cluster troubleshooting scenarios</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/","title":"System Requirements","text":"<p>The Run:ai Cluster is a Kubernetes application.</p> <p>This article explains the required hardware and software system requirements for the Run:ai cluster.</p> <p>Set out below are the system requirements for the Run:ai cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#hardware-requirements","title":"Hardware Requirements","text":"<p>The following hardware requirements are for the Kubernetes Cluster nodes\u2019. By default, all Run:ai cluster services run on all available nodes. For production deployments, you may want to Set Node Roles, to separate between system and worker nodes, reduce downtime and save CPU cycles on expensive GPU Machines.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#runai-cluster-system-nodes","title":"Run:ai Cluster - system nodes","text":"<p>This configuration is the minimum requirement you need to install and use Run:ai Cluster.</p> Component Required Capacity CPU 8 cores Memory 16GB Disk space 50GB"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#runai-cluster-worker-nodes","title":"Run:ai Cluster - Worker nodes","text":"<p>The Run:ai Cluster supports x86 CPUs and NVIDIA GPUs from the T, V, A, L, and H architecture families. For the list of supported GPU models, see Supported NVIDIA Data Center GPUs and Systems.</p> <p>The following configuration represents the minimum hardware requirements for installing and operating the Run:ai cluster on worker nodes. Each node must meet these specifications:</p> Component Required Capacity CPU 2 cores Memory 4GB"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#shared-storage","title":"Shared storage","text":"<p>Run:ai workloads must be able to access data from any worker node in a uniform way, to access training data and code as well as save checkpoints, weights, and other machine-learning-related artifacts.</p> <p>Typical protocols are Network File Storage (NFS) or Network-attached storage (NAS). Run:ai Cluster supports both, for more information see Shared storage.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#software-requirements","title":"Software requirements","text":"<p>The following software requirements must be fulfilled on the Kubernetes cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#operating-system","title":"Operating system","text":"<ul> <li>Any Linux operating system supported by both Kubernetes and NVIDIA GPU Operator  </li> <li>Run:ai cluster on Google Kubernetes Engine (GKE) only supports Ubuntu, as Container-Optimized OS (COS) is not supported by NVIDIA GPU Operator  </li> <li>Internal tests are being performed on Ubuntu 22.04 and CoreOS for OpenShift.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#kubernetes-distribution","title":"Kubernetes distribution","text":"<p>Run:ai Cluster requires Kubernetes. The following Kubernetes distributions are supported:</p> <ul> <li>Vanilla Kubernetes  </li> <li>OpenShift Container Platform (OCP)  </li> <li>NVIDIA Base Command Manager (BCM)  </li> <li>Elastic Kubernetes Engine (EKS)  </li> <li>Google Kubernetes Engine (GKE)  </li> <li>Azure Kubernetes Service (AKS)  </li> <li>Rancher Kubernetes Engine (RKE1)  </li> <li>Rancher Kubernetes Engine 2 (RKE2)</li> </ul> <p>Contact Run:ai customer support for up-to-date support details for:</p> <ul> <li>Ezmeral Runtime Enterprise  </li> <li>Tanzu Platform</li> </ul> <p>Important</p> <p>The latest release of the Run:ai cluster supports Kubernetes 1.28 to 1.30 and OpenShift 4.12 to 4.16</p> <p>For existing Kubernetes clusters, see the following Kubernetes version support matrix for the latest Run:ai cluster releases:</p> Run:ai version Supported Kubernetes versions Supported OpenShift versions v2.13 1.23 to 1.28 4.10 to 4.13 v2.16 1.26 to 1.28 4.11 to 4.14 v2.17 1.27 to 1.29 4.12 to 4.15 v2.18 (latest) 1.28 to 1.30 4.12 to 4.16 <p>For information on supported versions of managed Kubernetes, it's important to consult the release notes provided by your Kubernetes service provider. There, you can confirm the specific version of the underlying Kubernetes platform supported by the provider, ensuring compatibility with Run:ai. For an up-to-date end-of-life statement see Kubernetes Release History or OpenShift Container Platform Life Cycle Policy.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#kubernetes-pod-security-admission","title":"Kubernetes Pod Security Admission","text":"<p>Run:ai v2.15 and above supports <code>restricted</code> policy for Pod Security Admission (PSA) on OpenShift only. Other Kubernetes distributions are only supported with <code>privileged</code> policy.</p> <p>For Run:ai on OpenShift to run with PSA <code>restricted</code> policy:</p> <ul> <li>Label the <code>runai</code> namespace as described in Pod Security Admission with the following labels:</li> </ul> <pre><code>pod-security.kubernetes.io/audit=privileged\npod-security.kubernetes.io/enforce=privileged\npod-security.kubernetes.io/warn=privileged\n</code></pre> <ul> <li>The workloads submitted through Run:ai should comply with the restrictions of PSA restricted policy, This can be enforced using Policies.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#kubernetes-ingress-controller","title":"Kubernetes Ingress Controller","text":"<p>Run:ai cluster requires Kubernetes Ingress Controller to be installed on the Kubernetes cluster.</p> <ul> <li>OpenShift, RKE and RKE2 come pre-installed ingress controller.  </li> <li>Internal tests are being performed on NGINX, Rancher NGINX, OpenShift Router, and Istio.  </li> <li>Make sure that a default ingress controller is set.</li> </ul> <p>There are many ways to install and configure different ingress controllers. A simple example to install and configure NGINX ingress controller using helm:</p> Vanilla KubernetesManaged Kubernetes (EKS, GKE, AKS) <p>Run the following commands:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm upgrade -i nginx-ingress ingress-nginx/ingress-nginx \\\n    --namespace nginx-ingress --create-namespace \\\n    --set controller.kind=DaemonSet \\\n    --set controller.service.externalIPs=\"{&lt;INTERNAL-IP&gt;,&lt;EXTERNAL-IP&gt;}\" # Replace &lt;INTERNAL-IP&gt; and &lt;EXTERNAL-IP&gt; with the internal and external IP addresses of one of the nodes\n</code></pre> <p>Run the following commands:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm install nginx-ingress ingress-nginx/ingress-nginx \\\n    --namespace nginx-ingress --create-namespace\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#nvidia-gpu-operator","title":"NVIDIA GPU Operator","text":"<p>Run:ai Cluster requires NVIDIA GPU Operator to be installed on the Kubernetes Cluster, supports version 22.9 to 24.6</p> <p>See the Installing the NVIDIA GPU Operator, followed by notes below:</p> <ul> <li>Use the default <code>gpu-operator</code> namespace . Otherwise, you must specify the target namespace using the flag <code>runai-operator.config.nvidiaDcgmExporter.namespace</code> as described in customized cluster installation.  </li> <li>NVIDIA drivers may already be installed on the nodes. In such cases, use the NVIDIA GPU Operator flags <code>--set driver.enabled=false</code>. DGX OS is one such example as it comes bundled with NVIDIA Drivers.  </li> <li>To use Dynamic MIG, the GPU Operator must be installed with the flag <code>mig.strategy=mixed</code> as described in customized cluster installation. If the GPU Operator is already installed, edit the <code>clusterPolicy</code> by running</li> </ul> <pre><code>kubectl patch clusterPolicy cluster-policy -n gpu-operator --type=merge -p '{\"spec\":{\"mig\":{\"strategy\": \"mixed\"}}}\n</code></pre> <ul> <li>For distribution-specific additional instructions see below:</li> </ul> OpenShift Container Platform (OCP) <p>The Node Feature Discovery (NFD) Operator is a prerequisite for the NVIDIA GPU Operator in OpenShift. Install the NFD Operator using the Red Hat OperatorHub catalog in the OpenShift Container Platform web console. For more information see Installing the Node Feature Discovery (NFD) Operator</p> Elastic Kubernetes Service (EKS) <ul> <li>When setting-up the cluster, do not install the NVIDIA device plug-in (we want the NVIDIA GPU Operator to install it instead).  </li> <li>When using the eksctl tool to create a cluster, use the flag <code>--install-nvidia-plugin=false</code> to disable the installation.</li> </ul> <p>For GPU nodes, EKS uses an AMI which already contains the NVIDIA drivers. As such, you must use the GPU Operator flags: <code>--set driver.enabled=false</code></p> Google Kubernetes Engine (GKE) <p>Before installing the GPU Operator, create the <code>gpu-operator</code> namespace by running</p> <pre><code>kubectl create ns gpu-operator\n</code></pre> <p>create the following file:</p> resourcequota.yaml<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\nname: gcp-critical-pods\nnamespace: gpu-operator\nspec:\nscopeSelector:\n    matchExpressions:\n    - operator: In\n    scopeName: PriorityClass\n    values:\n    - system-node-critical\n    - system-cluster-critical\n</code></pre> <p>And then run:</p> <pre><code>kubectl apply -f resourcequota.yaml\n</code></pre> Rancher Kubernetes Engine 2 (RKE2) <p>Make sure to specify the <code>CONTAINERD_CONFIG</code> option exactly as outlined in the documentation and custom configuration guide, using the path <code>/var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl</code>. Do not create the file manually if it does not already exist. The GPU Operator will handle this configuration during deployment.</p> <p>For troubleshooting information, see the NVIDIA GPU Operator Troubleshooting Guide.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#prometheus","title":"Prometheus","text":"<p>Run:ai Cluster requires Prometheus to be installed on the Kubernetes cluster.</p> <ul> <li>OpenShift comes pre-installed with prometheus  </li> <li>For RKE2 see Enable Monitoring instructions to install Prometheus</li> </ul> <p>There are many ways to install Prometheus. A simple example to install the community Kube-Prometheus Stack using helm, run the following commands:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n    -n monitoring --create-namespace --set grafana.enabled=false\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#optional-software-requirements","title":"Optional software requirements","text":"<p>Optional Run:ai capabilities, Distributed Training and Inference require additional Kubernetes applications (frameworks) to be installed on the cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#distributed-training","title":"Distributed training","text":"<p>Distributed training enables training of AI models over multiple nodes. This requires distributed training framework to be installed on the cluster. The following frameworks are supported:</p> <ul> <li>TensorFlow </li> <li>PyTorch </li> <li>XGBoost </li> <li>MPI v2</li> </ul> <p>There are many ways to install each framework. A simple example of installation is the Kubeflow Training Operator. Run:ai supports Kubeflow version 1.7 only - which includes TensorFlow, Pytorch, and XGBoost.</p> <p>To install run the following command:</p> <pre><code>kubectl apply -k \"github.com/kubeflow/training-operator.git/manifests/overlays/standalone?ref=v1.7.0\"\n</code></pre> <p>To install MPI v2, which is not included in the Kubeflow Training Operator, run the following command:</p> <pre><code>kubectl apply --server-side -f https://raw.githubusercontent.com/kubeflow/mpi-operator/master/deploy/v2beta1/mpi-operator.yaml\n</code></pre> <p>Note</p> <p>If you need both MPI v2 and Kubeflow Training Operator, follow the steps below:</p> <ul> <li>Install the Kubeflow Training operator as above.  </li> <li>Disable and delete MPI v1 in the Kubeflow Training Operator by running:</li> </ul> <pre><code>kubectl patch deployment training-operator -n kubeflow --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args\", \"value\": [\"--enable-scheme=tfjob\", \"--enable-scheme=pytorchjob\", \"--enable-scheme=xgboostjob\"]}]'\nkubectl delete crd mpijobs.kubeflow.org\n</code></pre> <ul> <li>Install MPI v2 as described above.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#inference","title":"Inference","text":"<p>Inference enables serving of AI models. This requires the Knative Serving framework to be installed on the cluster and supports Knative versions 1.10 to 1.15</p> <p>Follow the Installing Knative instructions. After installation, configure Knative to use the Run:ai scheduler and features, by running:</p> <pre><code>kubectl patch configmap/config-autoscaler \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"enable-scale-to-zero\":\"true\"}}' &amp;&amp; \\\nkubectl patch configmap/config-features \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"kubernetes.podspec-schedulername\":\"enabled\",\"kubernetes.podspec-affinity\":\"enabled\",\"kubernetes.podspec-tolerations\":\"enabled\",\"kubernetes.podspec-volumes-emptydir\":\"enabled\",\"kubernetes.podspec-securitycontext\":\"enabled\",\"kubernetes.podspec-persistent-volume-claim\":\"enabled\",\"kubernetes.podspec-persistent-volume-write\":\"enabled\",\"multi-container\":\"enabled\",\"kubernetes.podspec-init-containers\":\"enabled\"}}'\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#domain-name-requirement","title":"Domain Name Requirement","text":"<p>The following requirement must be followed for naming the domain.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#fully-qualified-domain-name-fqdn","title":"Fully Qualified Domain Name (FQDN)","text":"<p>You must have a Fully Qualified Domain Name (FQDN) to install Run:ai Cluster (ex: <code>runai.mycorp.local</code>). This cannot be an IP. The domain name must be accessible inside the organization only. You also need a TLS certificate (private and public) for HTTPS access.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/","title":"SaaS Cluster Setup Introduction","text":"<p>This section is a step-by-step guide for setting up a Run:ai cluster. </p> <ul> <li>A Run:ai cluster is a Kubernetes application installed on top of a Kubernetes cluster.</li> <li>A Run:ai cluster connects to the Run:ai control plane on the cloud. The control plane provides a control point as well as a monitoring and control user interface for Administrators and Researchers.</li> <li>A customer may have multiple Run:ai Clusters, all connecting to a single control plane.</li> </ul> <p>For additional details see the Run:ai system components</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#documents","title":"Documents","text":"<ul> <li>Review Run:ai cluster System Requirements and Network Requirements.</li> <li>Cluster Install step-by-step guid.</li> <li>Look for troubleshooting tips if required.</li> <li>Cluster Upgrade and Cluster Uninstall instructions. </li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#customization","title":"Customization","text":"<p>For a list of optional customizations see Customize Installation</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#additional-configuration","title":"Additional Configuration","text":"<p>For a list of advanced configuration scenarios such as configuring researcher authentication, Single sign-on limiting the installation to specific nodes, and more, see the Configuration Articles section.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#next-steps","title":"Next Steps","text":"<p>After setting up the cluster, you may want to start setting up Researchers. See: Researcher Setup.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/","title":"Cluster Upgrade","text":"<p>This article explains how to upgrade Run:ai cluster version.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#before-upgrade","title":"Before upgrade","text":"<p>There are a number of matters to consider prior to upgrading the Run:ai cluster version.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#system-and-network-requirements","title":"System and network requirements","text":"<p>Before upgrading the Run:ai cluster, validate that the latest system requirements and network requirements are met, as they can change from time to time.</p> <p>Important</p> <p>It is highly recommended to upgrade the Kubernetes version together with the Run:ai cluster version, to ensure compatibility with latest supported version of your Kubernetes distribution</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#helm","title":"Helm","text":"<p>The latest releases of the Run:ai cluster require Helm 3.14 or above.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade","title":"Upgrade","text":"<p>Follow the instructions to upgrade using Helm. The Helm commands to upgrade the Run:ai cluster version may differ between versions. The steps below describe how to get the instructions from the Run:ai UI.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#getting-the-installation-instructions","title":"Getting the installation instructions","text":"<p>Follow the setup and installation instructions below to get the installation instructions to upgrade the Run:ai cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#setup","title":"Setup","text":"<ol> <li>In the Run:ai UI, go to Clusters </li> <li>Select the cluster you want to upgrade  </li> <li>Click INSTALLATION INSTRUCTIONS </li> <li>Optional: Select the Run:ai cluster version (latest, by default)  </li> <li>Click CONTINUE</li> </ol>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#installation-instructions","title":"Installation instructions","text":"<ol> <li>Follow the installation instructions     run the Helm commands provided on your Kubernetes cluster (see the troubleshooting below if installation fails)  </li> <li>Click DONE </li> <li>Once installation is complete, validate the cluster is Connected and listed with the new cluster version (see the cluster troubleshooting scenarios). Once you have done this, the cluster is upgraded to the latest version.</li> </ol> <p>Note</p> <p>To upgrade to a specific version, modify the <code>--version</code> flag by specifying the desired <code>&lt;version-number&gt;</code>. You can find all available versions by using the <code>helm search repo</code> command.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter an issue with the cluster upgrade, use the troubleshooting scenario below.</p> Installation fails <p>If the Run:ai cluster upgrade fails, check the installation logs to identify the issue.</p> <p>Run the following script to print the installation logs:</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/run-ai/public/main/installation/get-installation-logs.sh\n</code></pre> Cluster status <p>If the Run:ai cluster upgrade completes, but the cluster status does not show as Connected, refer to the cluster troubleshooting scenarios</p> <p>. </p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/","title":"Customize Cluster Installation","text":"<p>This document explains how to customize the Run:ai cluster installation. Customizing the cluster installation is useful if you want to implement specific features.</p> <p>Important</p> <p>Using these instructions to customize your cluster is optional.</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#how-to-customize","title":"How to customize","text":"<p>After the cluster is installed, you can edit the <code>runaiconfig</code> object to add/change configuration. Use the command:</p> <pre><code>kubectl edit runaconfig runai -n runai\n</code></pre> <p>All customizations will be saved when upgrading the cluster to a future version.</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#configurations","title":"Configurations","text":"Key Default Description <code>spec.project-controller.createNamespaces</code> <code>true</code> Set to <code>false</code>if unwilling to provide Run:ai the ability to create namespaces. When set to false, will requires an additional manual step when creating new Run:ai Projects as described below <code>spec.project-controller.clusterWideSecret</code> <code>true</code> Set to <code>false</code> if unwilling to provide Run:ai the ability to create Kubernetes Secrets. When not enabled, automatic secret propagation will not be available <code>spec.mps-server.enabled</code> <code>false</code> Set to <code>true</code> to allow the use of NVIDIA MPS. MPS is useful with Inference workloads <code>spec.global.subdomainSupport</code> <code>false</code> Set to true to allow researcher tools with a sub domain to be spawned from the Run:ai user interface. For more information see External access to containers <code>spec.global.schedulingservices</code> <code>spec.global.syncServices</code> <code>spec.global.workloadServices</code> Set requests and limit configurations for CPU and memory for Run:ai containers. For more information see Large cluster configuration <code>spec.runai-container-toolkit.enabled</code> <code>true</code> Controls the usage of GPU fractions. <code>spec.researcherService.ingress.tlsSecret</code> On Kubernetes distributions other than OpenShift, set a dedicated certificate for the researcher service ingress in the cluster. When not set, the certificate inserted when installing the cluster will be used. The value should be a Kubernetes secret  in the runai namespace <code>spec.researcherService.route.tlsSecret</code> On OpenShift, set a dedicated certificate for the researcher service route. When not set, the OpenShift certificate will be used.  The value should be a Kubernetes secret  in the runai namespace <code>global.image.registry</code> In air-gapped environment, allow cluster images to be pulled from private docker registry. For more information see self-hosted cluster installation <code>prometheus.spec.image</code> <code>quay.io/prometheus/prometheus</code> Due to a known issue In the Prometheus Helm chart, the <code>imageRegistry</code> setting is ignored. To pull the image from a different registry, you can manually specify the Prometheus image reference. <code>global.additionalImagePullSecrets</code> <code>[]</code> Defines a list of secrets to be used to pull images from a private docker registry <code>global.nodeAffinity.restrictScheduling</code> false Restrict scheduling of workloads to specific nodes, based on node labels. For more information see node roles <code>spec.prometheus.spec.retention</code> <code>2h</code> The interval of time where Prometheus will save Run:ai metrics. Promethues is only used as an intermediary to another metrics storage facility and metrics are typically moved within tens of seconds, so changing this setting is mostly for debugging purposes. <code>spec.prometheus.spec.retentionSize</code> Not set The amount of storage allocated for metrics by Prometheus. For more information see Prometheus Storage. <code>spec.prometheus.spec.imagePullSecrets</code> Not set An optional list of references to secrets in the runai namespace to use for pulling Prometheus images (relevant for air-gapped installations). <code>pod-grouper.args.gangScheduleArgoWorkflow</code> true Groups all pods of a single ArgoWorkflow workload into a single Pod-Group for gang scheduling. <code>runai-scheduler.args.defaultStalenessGracePeriod</code> 60s Sets the timeout in seconds before the scheduler evicts a stale pod-group (gang) that went below its min-members in running state: 0s - Immediately (no timeout) -1 - Never"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#understanding-custom-access-roles","title":"Understanding Custom Access Roles","text":"<p>To review the access roles created by the Run:ai Cluster installation, see Understanding Access Roles.</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#manual-creation-of-namespaces","title":"Manual Creation of Namespaces","text":"<p>Run:ai Projects are implemented as Kubernetes namespaces. By default, the administrator creates a new Project via the Administration user interface which then triggers the creation of a Kubernetes namespace named <code>runai-&lt;PROJECT-NAME&gt;</code>. There are a couple of use cases that customers will want to disable this feature:</p> <ul> <li>Some organizations prefer to use their internal naming convention for Kubernetes namespaces, rather than Run:ai's default <code>runai-&lt;PROJECT-NAME&gt;</code> convention.</li> <li>Some organizations will not allow Run:ai to automatically create Kubernetes namespaces.</li> </ul> <p>Follow these steps to achieve this:</p> <ol> <li>Disable the namespace creation functionality. See the  <code>runai-operator.config.project-controller.createNamespaces</code> flag above.</li> <li>Create a Project using the Run:ai User Interface.</li> <li>Create the namespace if needed by running: <code>kubectl create ns &lt;NAMESPACE&gt;</code>. The suggested Run:ai default is <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> <li>Label the namespace to connect it to the Run:ai Project by running <code>kubectl label ns &lt;NAMESPACE&gt;  runai/queue=&lt;PROJECT_NAME&gt;</code>, where <code>&lt;PROJECT_NAME&gt;</code> is the name of the project you have created in the Run:ai user interface above and <code>&lt;NAMESPACE&gt;</code> is the name you chose for your namespace.</li> </ol>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/","title":"NVIDIA DGX Bundle","text":""},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#runai-nvidia-dgx-bundle","title":"Run:ai &amp; NVIDIA DGX Bundle","text":"<p>NVIDIA DGX is a line of NVIDIA-produced servers and workstations which specialize in using GPUs to accelerate deep learning applications.</p> <p>NVIDIA DGX comes bundled out of the box with Run:ai. The purpose of this document is to guide you through the process of installing and configuring Run:ai in this scenario</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#nvidia-base-command-manager","title":"NVIDIA Base Command Manager","text":"<p>NVIDIA Base Command Manager allows the deployment of software on NVIDIA DGX servers. During the installation of the DGX you will select <code>Run:ai</code> as well as Run:ai prerequisites from the Base Command Manager installer.</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#prerequisites","title":"Prerequisites","text":""},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#software-prerequisites","title":"Software Prerequisites","text":"<p>Run:ai assumes the following components to be pre-installed:</p> <ul> <li><code>NVIDIA GPU Operator</code> - available for installation via the Base Command Manager installer</li> <li><code>Prometheus</code> - available for installation via the Base Command Manager installer</li> <li><code>Ingress controller</code> - NGINX is available for installation via the Base Command Manager installer. </li> </ul>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#runai-prerequisites","title":"Run:ai prerequisites","text":"<p>The Run:ai cluster installer will require the following:</p> <ul> <li><code>Run:ai tenant name</code> - provided by Run:ai customer support.</li> <li><code>Run:ai install secret</code> - provided by Run:ai customer support.</li> <li><code>Cluster URL</code> - your organization should provide you with a domain name.</li> <li><code>Private and public keys</code> -your organization should provide a trusted certificate for the above domain name. The Run:ai installer will require both private key and full-chain in PEM format. </li> <li>Post-installation - credentials for the Run:ai user interface. Provided by Run:ai customer support.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#installing-runai-installer","title":"Installing Run:ai installer","text":"<p>Select Run:ai via the Base Command Manager installer. Remember to select all of the above software prerequisites as well. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#using-the-runai-installer","title":"Using the Run:ai installer","text":"<p>Find out the cluster's IP address. Then browse to <code>http://&lt;CLUSTER-IP&gt;:30080/runai-installer</code>. Alternatively use the Base Command Manager landing page at <code>http://&lt;CLUSTER-IP&gt;/#runai</code>.  </p> <p>Note</p> <ul> <li>Use <code>http</code> rather than <code>https</code>.</li> <li>Use the IP and not a domain name.</li> </ul> <p>A wizard would open up containing 3 pages: Prerequisites, setup, and installation. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#prerequisites-page","title":"Prerequisites Page","text":"<p>The first, verification page, verifies that all of the above software prerequisites are met. Press the \"Verify\" button. You will not be able to continue unless all prerequisites are met. When all are met, press the <code>Continue</code> button. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#setup-page","title":"Setup Page","text":"<p>The setup page asks to provide all of the Run:ai prerequisites described above. The page will verify the Run:ai input (tenant name and install secret) but will not verify the validity of the cluster URL and certificate. If those are incorrect, the Run:ai installation will show as successful but certain aspects of Run:ai will not work. </p> <p>After filling up the form, press <code>Continue</code>. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#installation-page","title":"Installation page","text":"<p>The Run:ai installation will start. Depending on your download network speed the installation can take from 2 to 10 minutes. When the installation is successful you will see a <code>START USING RUN:AI</code> button. Press the button and enter your credentials to enter the Run:ai user interface. </p> <p>Save the URL for future use. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#post-installation","title":"Post-installation.","text":"<p>Post installation, you will want to:</p> <ul> <li>(Mandatory) Set up Researcher Access Control. Without this, the Job Submit form will not work.</li> <li>Set up Run:ai Users Working with Users.</li> <li>Set up Projects for Researchers Working with Projects.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#troubleshooting","title":"Troubleshooting","text":"<p>The cluster installer is a pod in Kubernetes. The pod is responsible for the installation preparation and prerequisite gathering phase. In case of an error during this pre-installation, you need to gather the pod's log. </p> <p>Once the Run:ai cluster installation has started, the behavior is identical to any Run:ai cluster installation flavor. See the troubleshooting page.</p>"},{"location":"admin/runai-setup/cluster-setup/network-req/","title":"Network Requirements","text":"<p>The following network requirements are for the Run:ai cluster installation and usage.</p>"},{"location":"admin/runai-setup/cluster-setup/network-req/#external-access","title":"External access","text":"<p>Set out below are the domains to whitelist and ports to open for installation, upgrade, and usage of the application and its management.</p> <p>Ensure the inbound and outbound rules are correctly applied to your firewall.</p>"},{"location":"admin/runai-setup/cluster-setup/network-req/#inbound-rules","title":"Inbound rules","text":"<p>To allow your organization\u2019s Run:ai users to interact with the cluster using the Run:ai Command-line interface, or access specific UI features, certain inbound ports need to be open.</p> Name Description Source Destination Port Run:ai cluster Run:ai cluster HTTPS entrypoint 0.0.0.0 all k8s nodes 443"},{"location":"admin/runai-setup/cluster-setup/network-req/#outbound-rules","title":"Outbound rules","text":"<p>For the Run:ai cluster installation and usage, certain outbound ports must be open.</p> Name Description Source Destination Port Run:ai Platform Run:ai cloud instance Run:ai system nodes app.run.ai 443 Grafana Run:ai cloud metrics store Run:ai system nodes prometheus-us-central1.grafana.net and runailabs.com 443 Google Container Registry Run:ai image repository All K8S nodes gcr.io/run-ai-prod 443 JFrog Artifactory Run:ai Helm repository Helm client machine runai.jfrog.io 443 <p>The Run:ai installation has software requirements that require additional components to be installed on the cluster. This article includes simple installation examples which can be used optionally and require the following cluster outbound ports to be open:</p> Name Description Source Destination Port Kubernetes Registry Ingress Nginx image repository All K8S nodes registry.k8s.io 443 Google Container Registry GPU Operator, and Knative image repository All K8S nodes gcr.io 443 Red Hat Container Registry Prometheus Operator image repository All K8S nodes quay.io 443 Docker Hub Registry Training Operator image repository All K8S nodes docker.io 443 <p>Note</p> <p>If you are using an HTTP proxy, contact Run:ai support for further instructions.</p>"},{"location":"admin/runai-setup/cluster-setup/network-req/#internal-network","title":"Internal network","text":"<p>Ensure that all Kubernetes nodes can communicate with each other across all necessary ports. Kubernetes assumes full interconnectivity between nodes, so you must configure your network to allow this seamless communication. Specific port requirements may vary depending on your network setup.</p>"},{"location":"admin/runai-setup/self-hosted/overview/","title":"Self Hosted Run:ai Installation","text":"<p>The self-hosted option is for organizations that cannot use a SaaS solution due to data leakage concerns.</p> <p>Run:ai self-hosting comes with two variants:</p> Self-hosting Type Description Connected The organization can freely download from the internet (though upload is not allowed) Air-gapped The organization has no connection to the internet  <p>The self-hosted installation is priced differently. For further information please talk to Run:ai sales. </p>"},{"location":"admin/runai-setup/self-hosted/overview/#self-hosting-with-kubernetes-vs-openshift","title":"Self-hosting with Kubernetes vs OpenShift","text":"<p>Run:ai has been certified with a specified set of Kubernetes distributions. The OpenShift installation is different from the rest. As such, the Run:ai self-hosted installation instructions are divided into two separate sections:</p> <ul> <li>OpenShift-based installation. See Run:ai OpenShift installation. The Run:ai operator for OpenShift is certified by Red Hat.</li> <li>Kubernetes-based installation. See Run:ai Kubernetes installation.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/additional-clusters/","title":"Installing additional Clusters","text":"<p>The first Run:ai cluster is typically installed on the same Kubernetes cluster as the Run:ai control plane. Run:ai supports multiple clusters per single control plane. This document is about installing additional clusters on different Kubernetes clusters.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/additional-clusters/#installation","title":"Installation","text":"<p>Follow the Run:ai SaaS installation network instructions as described in Domain name requirement.  Specifically:</p> <ol> <li>Install Run:ai prerequisites. Including ingress controller and Prometheus. </li> <li>The Cluster should have a dedicated URL with a trusted certificate.</li> <li>Create a secret in the Run:ai namespace containing the details of a trusted certificate. </li> <li>Run the <code>helm</code> command as instructed.  </li> </ol>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/","title":"Install the Run:ai Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/k8s/backend/#prerequisites-and-preparations","title":"Prerequisites and preparations","text":"<p>Make sure you have followed the Control Plane prerequisites and preparations.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#helm-install","title":"Helm install","text":"<p>Run the helm command below:</p> ConnectedAirgapped <pre><code>helm repo add runai-backend https://runai.jfrog.io/artifactory/cp-charts-prod\nhelm repo update\nhelm upgrade -i runai-backend -n runai-backend runai-backend/control-plane --version \"~2.18.0\" \\\n    --set global.domain=&lt;DOMAIN&gt;  # (1)\n</code></pre> <ol> <li>Domain name described here. </li> </ol> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-backend</code>.</p> <pre><code>helm upgrade -i runai-backend control-plane-&lt;VERSION&gt;.tgz  \\ # (1)\n    --set global.domain=&lt;DOMAIN&gt;  \\ # (2)\n    --set global.customCA.enabled=true \\  # (3)\n    -n runai-backend -f custom-env.yaml  # (4)\n</code></pre> <ol> <li>Replace <code>&lt;VERSION&gt;</code> with the Run:ai control plane version.</li> <li>Domain name described here. </li> <li>See the Local Certificate Authority instructions below</li> <li><code>custom-env.yaml</code> should have been created by the prepare installation script in the previous section. </li> </ol> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#additional-runai-configurations-optional","title":"Additional Run:ai configurations (optional)","text":"<p>There may be cases where you need to set additional properties, To apply the changes run <code>helm upgrade</code> and use <code>--set</code> to set specific configurations, and restart the relevant Run:ai pods so they can fetch the new configurations.</p> Key Change Description <code>global.ingress.ingressClass</code> Ingress class Run:ai default is using NGINX. If your cluster has a different ingress controller, you can configure the ingress class to be created by Run:ai <code>global.ingress.tlsSecretName</code> TLS secret name Run:ai requires the creation of a secret with domain certificate. If the <code>runai-backend</code> namespace already had such a secret, you can set the secret name here <code>&lt;component&gt;</code> <code>resources:</code> <code>limits:</code> <code>cpu: 500m</code> <code>memory: 512Mi</code> <code>requests:</code> <code>cpu: 250m</code> <code>memory: 256Mi</code> Pod request and limits Set Run:ai and 3rd party services' resources"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#additional-3rd-party-configurations-optional","title":"Additional 3rd party configurations (optional)","text":"<p>The Run:ai Control Plane chart, includes multiple sub-charts of 3rd party components:</p> <ul> <li>PostgreSQL - Data store</li> <li>Thanos - Metrics Store</li> <li>Keycloakx - Identity &amp; Access Management</li> <li>Grafana - Analytics Dashboard</li> <li>Redis - Caching (Disabled, by default)</li> </ul> <p>Tip</p> <p>Click on any component, to view it's chart values and configurations</p> <p>If you have opted to connect to an external PostgreSQL database, refer to the additional configurations table below. Adjust the following parameters based on your connection details:</p> <ol> <li>Disable PostgreSQL deployment - <code>postgresql.enabled</code></li> <li>Run:ai connection details - <code>global.postgresql.auth</code></li> <li>Grafana connection details - <code>grafana.dbUser</code>, <code>grafana.dbPassword</code></li> </ol>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#postgresql","title":"PostgreSQL","text":"Key Change Description <code>postgresql.enabled</code> PostgreSQL installation If set to <code>false</code> the PostgreSQL will not be installed <code>global.postgresql.auth.host</code> PostgreSQL host Hostname or IP address of the PostgreSQL server <code>global.postgresql.auth.port</code> PostgreSQL port Port number on which PostgreSQL is running <code>global.postgresql.auth.username</code> PostgreSQL username Username for connecting to PostgreSQL <code>global.postgresql.auth.password</code> PostgreSQL password Password for the PostgreSQL user specified by <code>global.postgresql.auth.username</code> <code>global.postgresql.auth.postgresPassword</code> PostgreSQL default admin password Password for the built-in PostgreSQL superuser (<code>postgres</code>) <code>global.postgresql.auth.existingSecret</code> Postgres Credentials (secret) Existing secret name with authentication credentials <code>postgresql.primary.initdb.password</code> PostgreSQL default admin password Set the same password as in <code>global.postgresql.auth.postgresPassword</code> (if changed) <code>postgresql.primary.persistence.storageClass</code> Storage class The installation to work with a specific storage class rather than the default one"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#thanos","title":"Thanos","text":"Key Change Description <code>thanos.receive.persistence.storageClass</code> Storage class The installation to work with a specific storage class rather than the default one"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#keycloakx","title":"Keycloakx","text":"Key Change Description <code>keycloakx.adminUser</code> User name of the internal identity provider administrator This user is the administrator of Keycloak <code>keycloakx.adminPassword</code> Password of the internal identity provider administrator This password is for the administrator of Keycloak <code>keycloakx.existingSecret</code> Keycloakx Credentials (secret) Existing secret name with authentication credentials <code>global.keycloakx.host</code> KeyCloak (Run:ai internal identity provider) host path Override the DNS for Keycloak. This can be used to access Keycloak from outside the Run:ai Control Plane cluster via ingress <p>The <code>keycloakx.adminUser</code> can only be set during the initial installation. The admin password, however, can also be changed later through the Keycloak UI, but you must also update the <code>keycloakx.adminPassword</code> value in the Helm chart using helm upgrade. Failing to update the Helm values after changing the password can lead to control plane services encountering errors.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#grafana","title":"Grafana","text":"Key Change Description <code>grafana.db.existingSecret</code> Grafana database connection credentials (secret) Existing secret name with authentication credentials <code>grafana.dbUser</code> Grafana database username Username for accessing the Grafana database <code>grafana.dbPassword</code> Grafana database password Password for the Grafana database user <code>grafana.admin.existingSecret</code> Grafana admin default credentials (secret) Existing secret name with authentication credentials <code>grafana.adminUser</code> Grafana username Override the Run:ai default user name for accessing Grafana <code>grafana.adminPassword</code> Grafana password Override the Run:ai default password for accessing Grafana"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#redis","title":"Redis","text":"Key Change Description <code>redisCache.auth.password</code> Redis (Runai internal cache mechanism) applicative password Override the default password <code>redisCache.auth.existingSecret</code> Redis credentials (secret) Existing secret name with authentication credentials"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#next-steps","title":"Next Steps","text":""},{"location":"admin/runai-setup/self-hosted/k8s/backend/#connect-to-runai-user-interface","title":"Connect to Run:ai User interface","text":"<p>Go to: <code>runai.&lt;domain&gt;</code>. Log in using the default credentials: User: <code>test@run.ai</code>, Password: <code>Abcd!234</code>. Go to the Users area and change the password.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#enable-forgot-password-optional","title":"Enable Forgot Password (optional)","text":"<p>To support the Forgot password functionality, follow the steps below.</p> <ul> <li>Go to <code>runai.&lt;domain&gt;/auth</code> and Log in.</li> <li>Under <code>Realm settings</code>, select the <code>Login</code> tab and enable the <code>Forgot password</code> feature.</li> <li>Under the <code>Email</code> tab, define an SMTP server, as explained here</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#install-runai-cluster","title":"Install Run:ai Cluster","text":"<p>Continue with installing a Run:ai Cluster.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/","title":"Self Hosted installation over Kubernetes - Cluster Setup","text":""},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#prerequisites","title":"Prerequisites","text":"<p>Install prerequisites as per System Requirements document.</p> <p>Note</p> <p>For self-hosted deployments, Kubernetes Ingress Controller and Cluster Fully Qualified Domain Name (FQDN) requirements are only necessary when the Run:ai Control Plane and Run:ai Cluster reside on seperate Kuebrnetes clusters.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#install-cluster","title":"Install Cluster","text":"ConnectedAirgapped <p>Perform the cluster installation instructions explained here.</p> <p>Perform the cluster installation instructions explained here.</p> <p>On the second tab of the cluster wizard, when copying the helm command for installation, you will need to use the pre-provided installation file instead of using helm repositories. As such:</p> <ul> <li>Do not add the helm repository and do not run <code>helm repo update</code>.</li> <li>Instead, edit the <code>helm upgrade</code> command. <ul> <li>Replace <code>runai/runai-cluster</code> with <code>runai-cluster-&lt;version&gt;.tgz</code>. </li> <li>Add  <code>--set global.image.registry=&lt;Docker Registry address&gt;</code> where the registry address is as entered in the preparation section</li> </ul> </li> </ul> <p>The command should look like the following:</p> <pre><code>helm upgrade -i runai-cluster runai-cluster-&lt;version&gt;.tgz \\\n    --set controlPlane.url=... \\\n    --set controlPlane.clientSecret=... \\\n    --set cluster.uid=... \\\n    --set cluster.url=... --create-namespace \\\n    --set global.image.registry=registry.mycompany.local \\\n</code></pre> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation. For more details see Understanding cluster access roles.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#optional-customize-installation","title":"(Optional) Customize Installation","text":"<p>To customize specific aspects of the cluster installation see customize cluster installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/next-steps/","title":"Next Steps","text":"<ul> <li>Create additional I Users.</li> <li>Set up Project-based Researcher Access Control.</li> <li>Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users.</li> <li>Review advanced setup and maintenace scenarios.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/","title":"Preparing for a Run:ai Kubernetes installation","text":"<p>The following section provides IT with the information needed to prepare for a Run:ai installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#prerequisites","title":"Prerequisites","text":"<p>Follow the prerequisites as explained in Self-Hosted installation over Kubernetes.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#software-artifacts","title":"Software artifacts","text":"ConnectedAirgapped <p>You should receive a file: <code>runai-reg-creds.yaml</code> from Run:ai Customer Support. The file provides access to the Run:ai Container registry.</p> <p>SSH into a node with <code>kubectl</code> access to the cluster and <code>Docker</code> installed. Run the following to enable image download from the Run:ai Container Registry on Google cloud:</p> <pre><code>kubectl create namespace runai-backend\nkubectl apply -f runai-reg-creds.yaml\n</code></pre> <p>You should receive a single file <code>runai-air-gapped-&lt;VERSION&gt;.tar.gz</code> from Run:ai customer support</p> <p>SSH into a node with <code>kubectl</code> access to the cluster and <code>Docker</code> installed.</p> <p>Run:ai assumes the existence of a Docker registry for images. Most likely installed within the organization. The installation requires the network address and port for the registry (referenced below as <code>&lt;REGISTRY_URL&gt;</code>). </p> <p>To extract Run:ai files, replace <code>&lt;VERSION&gt;</code> in the command below and run: </p> <pre><code>tar xvf runai-airgapped-package-&lt;VERSION&gt;.tar.gz\n\nkubectl create namespace runai-backend\n</code></pre> <p>Upload images</p> <p>Upload images to a local Docker Registry. Set the Docker Registry address in the form of <code>NAME:PORT</code> (do not add <code>https</code>):</p> <pre><code>export REGISTRY_URL=&lt;Docker Registry address&gt;\n</code></pre> <p>Run the following script (you must dockerd installed and at least 20GB of free disk space to run): </p> <pre><code>sudo -E ./setup.sh\n</code></pre> <p>If Docker is configured to run as non-root then <code>sudo</code> is not required.</p> <p>The script should create a file named <code>custom-env.yaml</code> which will be used by the control-plane installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#private-docker-registry-optional","title":"Private Docker Registry (optional)","text":"<p>To access the organization's docker registry it is required to set the registry's credentials (imagePullSecret)</p> <p>Create the secret named <code>runai-reg-creds</code> based on your existing credentials. For more information, see Pull an Image from a Private Registry.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#configure-your-environment","title":"Configure your environment","text":""},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#domain-certificate","title":"Domain Certificate","text":"<p>The Run:ai control plane requires a domain name (FQDN). You must supply a domain name as well as a trusted certificate for that domain.</p> <ul> <li>When installing the first Run:ai cluster on the same Kubernetes cluster as the control plane, the Run:ai cluster URL will be the same as the control-plane URL.</li> <li>When installing the Run:ai cluster on a separate Kubernetes cluster, follow the Run:ai Domain name requirement.</li> <li>If your network is air-gapped, you will need to provide the Run:ai control-plane and cluster with information about the local certificate authority.</li> </ul> <p>You must provide the domain's private key and crt as a Kubernetes secret in the <code>runai-backend</code> namespace. Run:</p> <pre><code>kubectl create secret tls runai-backend-tls -n runai-backend \\\n    --cert /path/to/fullchain.pem --key /path/to/private.pem\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#local-certificate-authority-air-gapped-only","title":"Local Certificate Authority (air-gapped only)","text":"<p>In air-gapped environments, you must prepare the public key of your local certificate authority as described here. It will need to be installed in Kubernetes for the installation to succeed.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#mark-runai-system-workers-optional","title":"Mark Run:ai system workers (optional)","text":"<p>You can optionally set the Run:ai control plane to run on specific nodes. Kubernetes will attempt to schedule Run:ai pods to these nodes. If lacking resources, the Run:ai nodes will move to another, non-labeled node.  </p> <p>To set system worker nodes run:</p> <pre><code>kubectl label node &lt;NODE-NAME&gt; node-role.kubernetes.io/runai-system=true\n</code></pre> <p>Warning</p> <p>Do not select the Kubernetes master as a <code>runai-system</code> node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#external-postgres-database-optional","title":"External Postgres database (optional)","text":"<p>If you have opted to use an external PostgreSQL database, you need to perform initial setup to ensure successful installation. Follow these steps:</p> <ol> <li> <p>Create a SQL script file, edit the parameters below, and save it locally:</p> <ul> <li>Replace <code>&lt;DATABASE_NAME&gt;</code> with a dedicate database name for RunAi in your PostgreSQL database.</li> <li>Replace <code>&lt;ROLE_NAME&gt;</code> with a dedicated role name (user) for RunAi database.</li> <li>Replace <code>&lt;ROLE_PASSWORD&gt;</code> with a password for the new PostgreSQL role.</li> <li>Replace <code>&lt;GRAFANA_PASSWORD&gt;</code> with the password to be set for Grafana integration.</li> </ul> <pre><code>-- Create a new database for runai\nCREATE DATABASE &lt;DATABASE_NAME&gt;; \n\n-- Create the role with login and password\nCREATE ROLE &lt;ROLE_NAME&gt;  WITH LOGIN PASSWORD '&lt;ROLE_PASSWORD&gt;'; \n\n-- Grant all privileges on the database to the role\nGRANT ALL PRIVILEGES ON DATABASE &lt;DATABASE_NAME&gt; TO &lt;ROLE_NAME&gt;; \n\n-- Connect to the newly created database\n\\c &lt;DATABASE_NAME&gt; \n\n-- grafana\nCREATE ROLE grafana WITH LOGIN PASSWORD '&lt;GRAFANA_PASSWORD&gt;'; \nCREATE SCHEMA grafana authorization grafana;\nALTER USER grafana set search_path='grafana';\n-- Exit psql\n\\q\n</code></pre> </li> <li> <p>Run the following command on a machine where PostgreSQL client (<code>pgsql</code>) is installed:</p> <pre><code>psql --host &lt;POSTGRESQL_HOST&gt; \\ # (1)\n--user &lt;POSTGRESQL_USER&gt; \\ # (2)\n--port &lt;POSTGRESQL_PORT&gt; \\ # (3)\n--dbname &lt;POSTGRESQL_DB&gt; \\ # (4)\n-a -f &lt;SQL_FILE&gt; \\ # (5)\n</code></pre> <ol> <li>Replace <code>&lt;POSTGRESQL_HOST&gt;</code> with the PostgreSQL ip address or hostname.</li> <li>Replace <code>&lt;POSTGRESQL_USER&gt;</code> with the PostgreSQL username.</li> <li>Replace <code>&lt;POSTGRESQL_PORT&gt;</code> with the port number where PostgreSQL is running.</li> <li>Replace <code>&lt;POSTGRESQL_DB&gt;</code> with the name of your PostgreSQL database.</li> <li>Replace <code>&lt;POSTGRESQL_DB&gt;</code> with the name of your PostgreSQL database.</li> <li>Replace <code>&lt;SQL_FILE&gt;</code> with the path to the SQL script created in the previous step.</li> </ol> </li> </ol>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#additional-permissions","title":"Additional permissions","text":"<p>As part of the installation, you will be required to install the Run:ai Control Plane and Cluster Helm Charts. The Helm Charts require Kubernetes administrator permissions. You can review the exact permissions provided by using the <code>--dry-run</code> on both helm charts.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#validate-prerequisites","title":"Validate Prerequisites","text":"<p>Once you believe that the Run:ai prerequisites and preperations are met, we highly recommend installing and running the Run:ai pre-install diagnostics script. The tool:</p> <ul> <li>Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking.</li> <li>Looks at additional components installed and analyze their relevance to a successful Run:ai installation.</li> </ul> <p>To use the script download the latest version of the script and run:</p> <pre><code>chmod +x preinstall-diagnostics-&lt;platform&gt;\n./preinstall-diagnostics-&lt;platform&gt; --domain &lt;dns-entry&gt;\n</code></pre> <p>If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file <code>runai-preinstall-diagnostics.txt</code> in the current directory and send it to Run:ai technical support.</p> <p>For more information on the script including additional command-line flags, see here.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#next-steps","title":"Next steps","text":"<p>Continue with installing the Run:ai Control Plane.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/","title":"Self-Hosted installation over Kubernetes - Prerequisites","text":"<p>Before proceeding with this document, please review the installation types documentation to understand the difference between air-gapped and connected installations. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#runai-components","title":"Run:ai Components","text":"<p>As part of the installation process you will install:</p> <ul> <li>A control-plane managing cluster</li> <li>One or more clusters</li> </ul> <p>Both the control plane and clusters require Kubernetes. Typically the control plane and first cluster are installed on the same Kubernetes cluster but this is not a must. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#installer-machine","title":"Installer machine","text":"<p>The machine running the installation script (typically the Kubernetes master) must have:</p> <ul> <li>At least 50GB of free space.</li> <li>Docker installed.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#helm","title":"Helm","text":"<p>Run:ai requires Helm 3.14 or later. To install Helm, see Installing Helm. If you are installing an air-gapped version of Run:ai, The Run:ai tar file contains the helm binary. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#cluster-hardware-requirements","title":"Cluster hardware requirements","text":"<p>See Cluster prerequisites hardware requirements.</p> <p>In addition, the control plane installation of Run:ai requires the configuration of Kubernetes Persistent Volumes of a total size of 110GB. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#runai-software-requirements","title":"Run:ai software requirements","text":""},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#cluster-nodes","title":"Cluster Nodes","text":"<p>See Run:ai Cluster prerequisites operating system requirements.</p> <p>Nodes are required to be synchronized by time using NTP (Network Time Protocol) for proper system functionality.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#kubernetes","title":"Kubernetes","text":"<p>See Run:ai Cluster prerequisites Kubernetes distribution requirements.</p> <p>The Run:ai control plane operating system prerequisites are identical.</p> <p>The Run:ai control-plane requires a default storage class to create persistent volume claims for Run:ai storage. The storage class, as per Kubernetes standards, controls the reclaim behavior: whether the Run:ai persistent data is saved or deleted when the Run:ai control plane is deleted. </p> <p>Note</p> <p>For a simple (nonproduction) storage class example see Kubernetes Local Storage Class. The storage class will set the directory <code>/opt/local-path-provisioner</code> to be used across all nodes as the path for provisioning persistent volumes.</p> <p>Then set the new storage class as default:</p> <pre><code>kubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#install-prerequisites","title":"Install prerequisites","text":""},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#ingress-controller","title":"Ingress Controller","text":"<p>The Run:ai control plane installation assumes an existing installation of NGINX as the ingress controller. You can follow the Run:ai Cluster prerequisites Kubernetes ingress controller installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#nvidia-gpu-operator","title":"NVIDIA GPU Operator","text":"<p>See Run:ai Cluster prerequisites NVIDIA GPU operator requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the NVIDIA prerequisites.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#prometheus","title":"Prometheus","text":"<p>See Run:ai Cluster prerequisites Prometheus requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the Prometheus prerequisites. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#inference-optional","title":"Inference (optional)","text":"<p>See Run:ai Cluster prerequisites Inference requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the Inference prerequisites. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#external-postgres-database-optional","title":"External Postgres database (optional)","text":"<p>The Run:ai control plane installation includes a default PostgreSQL database. However, you may opt to use an existing PostgreSQL database if you have specific requirements or preferences. Please ensure that your PostgreSQL database is version 16 or higher.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#next-steps","title":"Next steps","text":"<p>Continue to Preparing for a Run:ai Kubernetes Installation .</p>"},{"location":"admin/runai-setup/self-hosted/k8s/project-management/","title":"Self Hosted installation over Kubernetes - Create Projects","text":""},{"location":"admin/runai-setup/self-hosted/k8s/project-management/#introduction","title":"Introduction","text":"<p>The Administrator creates Run:ai Projects via the Run:ai user interface. When enabling Researcher Authentication you also assign users to Projects.</p> <p>Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically:</p> <ol> <li>Creates a namespace by the name of <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> <li>Labels the namespace as managed by Run:ai.</li> <li>Provides access to the namespace for Run:ai services.</li> <li>Associates users with the namespace.</li> </ol> <p>This process may need to be altered if,</p> <ul> <li>Researchers already have existing Kubernetes namespaces</li> <li>The organization's Kubernetes namespace naming convention does not allow the <code>runai-</code> prefix.</li> <li>The organization's policy does not allow the automatic creation of namespaces.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/project-management/#process","title":"Process","text":"<p>Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace:</p> <ul> <li>When setting up a Run:ai cluster, Disable namespace creation by setting the cluster flag <code>createNamespaces</code> to <code>false</code>.</li> <li>Using the Run:ai User Interface, create a new Project <code>&lt;PROJECT-NAME&gt;</code>. A namespace will not be created.</li> <li>Associate and existing namepace <code>&lt;NAMESPACE&gt;</code> with the Run:ai project by running:</li> </ul> <pre><code>kubectl label ns &lt;NAMESPACE&gt;  runai/queue=&lt;PROJECT_NAME&gt;\n</code></pre> <p>Caution</p> <p>Setting the <code>createNamespaces</code> flag to <code>false</code> moves the responsibility of creating namespaces to match Run:ai Projects to the administrator.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/","title":"Uninstall Run:ai","text":""},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/#uninstall-a-runai-cluster","title":"Uninstall a Run:ai Cluster","text":"<p>To uninstall the cluster see: cluster delete </p>"},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/#uninstall-the-runai-control-plane","title":"Uninstall the Run:ai Control Plane","text":"<p>To delete the control plane, run:</p> <pre><code>helm uninstall runai-backend -n runai-backend\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/","title":"Upgrade Run:ai","text":""},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#preparations","title":"Preparations","text":""},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#helm","title":"Helm","text":"<p>Run:ai requires Helm 3.14 or later. Before you continue, validate your installed helm client version. To install or upgrade Helm, see Installing Helm. If you are installing an air-gapped version of Run:ai, The Run:ai tar file contains the helm binary. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#software-files","title":"Software files","text":"ConnectedAirgapped <p>Run the helm command below:</p> <pre><code>helm repo add runai-backend https://runai.jfrog.io/artifactory/cp-charts-prod\nhelm repo update\n</code></pre> <ul> <li>Ask for a tar file <code>runai-air-gapped-&lt;NEW-VERSION&gt;.tar.gz</code> from Run:ai customer support. The file contains the new version you want to upgrade to. <code>&lt;NEW-VERSION&gt;</code> is the updated version of the Run:ai control plane.</li> <li>Upload the images as described here.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#before-upgrade","title":"Before upgrade","text":"<p>Before proceeding with the upgrade, it's crucial to apply the specific prerequisites associated with your current version of Run:ai and every version in between up to the version you are upgrading to.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-from-version-29","title":"Upgrade from version 2.9","text":"<p>Two significant changes to the control-plane installation have happened with version 2.12: PVC ownership and Ingress and installation customization. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#pvc-ownership","title":"PVC ownership","text":"<p>Run:ai will no longer directly create the PVCs that store Run:ai data (metrics and database). Instead, going forward, </p> <ul> <li>Run:ai requires a Kubernetes storage class to be installed.</li> <li>The PVCs are created by the Kubernetes StatefulSets. </li> </ul> <p>The storage class, as per Kubernetes standards, controls the reclaim behavior: whether the data is saved or deleted when the Run:ai control plane is deleted.  </p> <p>To remove the ownership in an older installation, run:</p> <pre><code>kubectl patch pvc -n runai-backend pvc-thanos-receive  -p '{\"metadata\": {\"annotations\":{\"helm.sh/resource-policy\": \"keep\"}}}'\nkubectl patch pvc -n runai-backend pvc-postgresql  -p '{\"metadata\": {\"annotations\":{\"helm.sh/resource-policy\": \"keep\"}}}'\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#ingress","title":"Ingress","text":"<p>Delete the ingress object which will be recreated by the control plane upgrade</p> <pre><code>kubectl delete ing -n runai-backend runai-backend-ingress\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#installation-customization","title":"Installation customization","text":"<p>The Run:ai control-plane installation has been rewritten and is no longer using a backend values file. Instead, to customize the installation use standard <code>--set</code> flags. If you have previously customized the installation, you must now extract these customizations and add them as <code>--set</code> flag to the helm installation:</p> <ul> <li>Find previous customizations to the control plane if such exist. Run:ai provides a utility for that here <code>https://raw.githubusercontent.com/run-ai/docs/v2.13/install/backend/cp-helm-vals-diff.sh</code>. For information on how to use this utility please contact Run:ai customer support. </li> <li>Search for the customizations you found in the optional configurations table and add them in the new format. </li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-control-plane","title":"Upgrade Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-from-version-217-or-later","title":"Upgrade from version 2.17, or later","text":"ConnectedAirgapped <pre><code>helm get values runai-backend -n runai-backend &gt; runai_control_plane_values.yaml\nhelm upgrade runai-backend -n runai-backend runai-backend/control-plane --version \"~2.18.0\" -f runai_control_plane_values.yaml --reset-then-reuse-values\n</code></pre> <pre><code>helm get values runai-backend -n runai-backend &gt; runai_control_plane_values.yaml\nhelm upgrade runai-backend control-plane-&lt;NEW-VERSION&gt;.tgz -n runai-backend  -f runai_control_plane_values.yaml --reset-then-reuse-values\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-from-version-29_1","title":"Upgrade from version 2.9","text":"<ul> <li>Create a <code>tls secret</code> as described in the control plane installation. </li> <li>Upgrade the control plane as described in the control plane installation. During the upgrade, you must tell the installation not to create the two PVCs:</li> </ul> ConnectedAirgapped <pre><code>helm upgrade -i runai-backend -n runai-backend runai-backend/control-plane --version \"~2.18.0\" \\\n--set global.domain=&lt;DOMAIN&gt; \\\n--set postgresql.primary.persistence.existingClaim=pvc-postgresql \\ \n--set thanos.receive.persistence.existingClaim=pvc-thanos-receive \n</code></pre> <p>Note</p> <p>The helm repository name has changed from <code>runai-backend/runai-backend</code> to <code>runai-backend/control-plane</code>.</p> <pre><code>helm upgrade -i runai-backend control-plane-&lt;NEW-VERSION&gt;.tgz -n runai-backend \\\n--set global.domain=&lt;DOMAIN&gt; \\\n--set postgresql.primary.persistence.existingClaim=pvc-postgresql \\ \n--set thanos.receive.persistence.existingClaim=pvc-thanos-receive \n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-cluster","title":"Upgrade Cluster","text":"<p>To upgrade the cluster follow the instructions here.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/additional-clusters/","title":"Installing additional clusters","text":"<p>The first Run:ai cluster is typically installed on the same OpenShift cluster as the Run:ai control plane. Run:ai supports multiple clusters per single control plane. This document is about installing additional clusters on different OpenShift clusters.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/additional-clusters/#additional-cluster-installation","title":"Additional cluster installation","text":"<p>Create a new cluster, then:</p> <ul> <li>Select a target platform <code>OpenShift</code></li> <li>Select a Cluster location <code>Remote to Control Plane</code>.</li> <li>You must enter a specific cluster URL with the format <code>https://runai.apps.&lt;BASE_DOMAIN&gt;</code>. To get the base Domain run <code>oc get dns cluster -oyaml | grep baseDomain</code></li> <li>Ignore the instructions for creating a secret.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/","title":"Install the Run:ai Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/ocp/backend/#prerequisites-and-preparations","title":"Prerequisites and preparations","text":"<p>Make sure you have followed the Control Plane prerequisites and preparations.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#helm-install","title":"Helm Install","text":"<p>Run the helm command below:</p> ConnectedAirgapped <pre><code>helm repo add runai-backend https://runai.jfrog.io/artifactory/cp-charts-prod\nhelm repo update\nhelm upgrade -i runai-backend -n runai-backend runai-backend/control-plane --version \"~2.18.0\" \\\n    --set global.domain=runai.apps.&lt;OPENSHIFT-CLUSTER-DOMAIN&gt; \\ # (1)\n    --set global.config.kubernetesDistribution=openshift\n</code></pre> <ol> <li>The subdomain configured for the OpenShift cluster.</li> </ol> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-backend</code>.</p> <pre><code>helm upgrade -i runai-backend  ./control-plane-&lt;version&gt;.tgz -n runai-backend \\\n    --set global.domain=runai.apps.&lt;OPENSHIFT-CLUSTER-DOMAIN&gt; \\ # (1)\n    --set global.config.kubernetesDistribution=openshift \\\n    --set global.customCA.enabled=true \\ # (2)\n    -f custom-env.yaml  # (3)\n</code></pre> <ol> <li>The domain configured for the OpenShift cluster. To find out the OpenShift cluster domain, run <code>oc get routes -A</code></li> <li>See the Local Certificate Authority instructions below</li> <li><code>custom-env.yaml</code> should have been created by the prepare installation script in the previous section. </li> </ol> <p>(replace <code>&lt;version&gt;</code> with the control plane version)</p> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#additional-runai-configurations-optional","title":"Additional Run:ai configurations (optional)","text":"<p>There may be cases where you need to set additional properties, To apply the changes run <code>helm upgrade</code> and use <code>--set</code> to set specific configurations, and restart the relevant Run:ai pods so they can fetch the new configurations.</p> Key Change Description <code>&lt;component&gt;</code> <code>resources:</code> <code>limits:</code> <code>cpu: 500m</code> <code>memory: 512Mi</code> <code>requests:</code> <code>cpu: 250m</code> <code>memory: 256Mi</code> Pod request and limits Set Run:ai and 3rd party services' resources"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#additional-3rd-party-configurations-optional","title":"Additional 3rd party configurations (optional)","text":"<p>The Run:ai Control Plane chart, includes multiple sub-charts of 3rd party components:</p> <ul> <li>PostgreSQL - Data store</li> <li>Keycloakx - Identity &amp; Access Management</li> <li>Grafana - Analytics Dashboard</li> <li>Redis - Caching (Disabled, by default)</li> </ul> <p>Tip</p> <p>Click on any component, to view it's chart values and configurations</p> <p>If you have opted to connect to an external PostgreSQL database, refer to the additional configurations table below. Adjust the following parameters based on your connection details:</p> <ol> <li>Disable PostgreSQL deployment - <code>postgresql.enabled</code></li> <li>Run:ai connection details - <code>global.postgresql.auth</code></li> <li>Grafana connection details - <code>grafana.dbUser</code>, <code>grafana.dbPassword</code></li> </ol>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#postgresql","title":"PostgreSQL","text":"Key Change Description <code>postgresql.enabled</code> PostgreSQL installation If set to <code>false</code> the PostgreSQL will not be installed <code>global.postgresql.auth.host</code> PostgreSQL host Hostname or IP address of the PostgreSQL server <code>global.postgresql.auth.port</code> PostgreSQL port Port number on which PostgreSQL is running <code>global.postgresql.auth.username</code> PostgreSQL username Username for connecting to PostgreSQL <code>global.postgresql.auth.password</code> PostgreSQL password Password for the PostgreSQL user specified by <code>global.postgresql.auth.username</code> <code>global.postgresql.auth.postgresPassword</code> PostgreSQL default admin password Password for the built-in PostgreSQL superuser (<code>postgres</code>) <code>global.postgresql.auth.existingSecret</code> Postgres Credentials (secret) Existing secret name with authentication credentials <code>postgresql.primary.initdb.password</code> PostgreSQL default admin password Set the same password as in <code>global.postgresql.auth.postgresPassword</code> (if changed) <code>postgresql.primary.persistence.storageClass</code> Storage class The installation to work with a specific storage class rather than the default one"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#keycloakx","title":"Keycloakx","text":"Key Change Description <code>keycloakx.adminUser</code> User name of the internal identity provider administrator This user is the administrator of Keycloak <code>keycloakx.adminPassword</code> Password of the internal identity provider administrator This password is for the administrator of Keycloak <code>keycloakx.existingSecret</code> Keycloakx credentials (secret) Existing secret name with authentication credentials <code>global.keycloakx.host</code> KeyCloak (Run:ai internal identity provider) host path Override the DNS for Keycloak. This can be used to access Keycloak from outside the Run:ai Control Plane cluster via ingress <p>The <code>keycloakx.adminUser</code> can only be set during the initial installation. The admin password, however, can also be changed later through the Keycloak UI, but you must also update the <code>keycloakx.adminPassword</code> value in the Helm chart using helm upgrade. Failing to update the Helm values after changing the password can lead to control plane services encountering errors.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#grafana","title":"Grafana","text":"Key Change Description <code>grafana.db.existingSecret</code> Grafana database connection credentials (secret) Existing secret name with authentication credentials <code>grafana.dbUser</code> Grafana database username Username for accessing the Grafana database <code>grafana.dbPassword</code> Grafana database password Password for the Grafana database user <code>grafana.admin.existingSecret</code> Grafana admin default credentials (secret) Existing secret name with authentication credentials <code>grafana.adminUser</code> Grafana username Override the Run:ai default user name for accessing Grafana <code>grafana.adminPassword</code> Grafana password Override the Run:ai default password for accessing Grafana"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#redis","title":"Redis","text":"Key Change Description <code>redisCache.auth.password</code> Redis (Runai internal cache mechanism) applicative password Override the default password <code>redisCache.auth.existingSecret</code> Redis credentials (secret) Existing secret name with authentication credentials"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#next-steps","title":"Next steps","text":""},{"location":"admin/runai-setup/self-hosted/ocp/backend/#connect-to-runai-user-interface","title":"Connect to Run:ai user interface","text":"<ul> <li>Run: <code>oc get routes -n runai-backend</code> to find the Run:ai Administration User Interface URL.</li> <li>Log in using the default credentials: User: <code>test@run.ai</code>, Password: <code>Abcd!234</code>.</li> <li>Go to the Users area and change the password.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#enable-forgot-password-optional","title":"Enable Forgot Password (optional)","text":"<p>To support the Forgot password functionality, follow the steps below.</p> <ul> <li>Go to <code>runai.&lt;openshift-cluster-domain&gt;/auth</code> and Log in.</li> <li>Under <code>Realm settings</code>, select the <code>Login</code> tab and enable the <code>Forgot password</code> feature.</li> <li>Under the <code>Email</code> tab, define an SMTP server, as explained here</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#install-runai-cluster","title":"Install Run:ai Cluster","text":"<p>Continue with installing a Run:ai Cluster.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/","title":"Self-Hosted installation over OpenShift - Cluster Setup","text":""},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#prerequisites","title":"Prerequisites","text":"<p>Install prerequisites as per System Requirements document.  </p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#create-openshift-projects","title":"Create OpenShift Projects","text":"<p>Run:ai cluster installation uses several namespaces (or projects in OpenShift terminology). Run the following:</p> <pre><code>oc new-project runai\noc new-project runai-reservation\noc new-project runai-scale-adjust\n</code></pre> <p>The last namespace (<code>runai-scale-adjust</code>) is only required if the cluster is a cloud cluster and is configured for auto-scaling.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#cluster-installation","title":"Cluster Installation","text":"ConnectedAirgapped <p>Perform the cluster installation instructions explained in Cluster install. When creating a new cluster, select the OpenShift  target platform.</p> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-cluster</code>.</p> <p>Perform the cluster installation instructions explained in Cluster install. When creating a new cluster, select the OpenShift  target platform.</p> <p>On the second tab of the cluster wizard, when copying the helm command for installation, you will need to use the pre-provided installation file instead of using helm repositories. As such:</p> <ul> <li>Do not add the helm repository and do not run <code>helm repo update</code>.</li> <li>Instead, edit the <code>helm upgrade</code> command. <ul> <li>Replace <code>runai/runai-cluster</code> with <code>runai-cluster-&lt;version&gt;.tgz</code>. </li> <li>Add  <code>--set global.image.registry=&lt;Docker Registry address&gt;</code> where the registry address is as entered in the preparation section</li> <li>Add <code>--set global.customCA.enabled=true</code> and perform the instructions for local certificate authority.</li> </ul> </li> </ul> <p>The command should look like the following: <pre><code>helm upgrade -i runai-cluster runai-cluster-&lt;version&gt;.tgz \\\n    --set controlPlane.url=... \\\n    --set controlPlane.clientSecret=... \\\n    --set cluster.uid=... \\\n    --set cluster.url=... --create-namespace \\\n    --set global.image.registry=registry.mycompany.local \\\n    --set global.customCA.enabled=true\n</code></pre></p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#optional-customize-installation","title":"(Optional) Customize Installation","text":"<p>To customize specific aspects of the cluster installation see customize cluster installation.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#next-steps","title":"Next Steps","text":"<p>Continue to create Run:ai Projects.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/next-steps/","title":"Next Steps","text":"<ul> <li>Create additional Run:ai Users.</li> <li>Set up Project-based Researcher Access Control.</li> <li>Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users.</li> <li>Review advanced setup and maintenace scenarios.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/","title":"Preparing for a Run:ai OpenShift installation","text":"<p>The following section provides IT with the information needed to prepare for a Run:ai installation.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#prerequisites","title":"Prerequisites","text":"<p>See the Prerequisites section above.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#software-artifacts","title":"Software artifacts","text":"ConnectedAirgapped <p>You should receive a file: <code>runai-reg-creds.yaml</code> from Run:ai Customer Support. The file provides access to the Run:ai Container registry.</p> <p>SSH into a node with <code>oc</code> access (<code>oc</code> is the OpenShift command line) to the cluster and <code>Docker</code> installed.</p> <p>Run the following to enable image download from the Run:ai Container Registry on Google cloud:</p> <pre><code>oc apply -f runai-reg-creds.yaml -n runai-backend\n</code></pre> <p>You should receive a single file <code>runai-&lt;version&gt;.tar</code> from Run:ai customer support</p> <p>Run:ai assumes the existence of a Docker registry for images. Most likely installed within the organization. The installation requires the network address and port for the registry (referenced below as <code>&lt;REGISTRY_URL&gt;</code>). </p> <p>SSH into a node with <code>oc</code> access (<code>oc</code> is the OpenShift command line) to the cluster and <code>Docker</code> installed.</p> <p>To extract Run:ai files, replace <code>&lt;VERSION&gt;</code> in the command below and run: </p> <p><pre><code>tar xvf runai-airgapped-package-&lt;VERSION&gt;.tar.gz\n</code></pre> Upload images</p> <p>Upload images to a local Docker Registry. Set the Docker Registry address in the form of <code>NAME:PORT</code> (do not add <code>https</code>):</p> <pre><code>export REGISTRY_URL=&lt;Docker Registry address&gt;\n</code></pre> <p>Run the following script (you must have at least 20GB of free disk space to run): </p> <pre><code>./setup.sh\n</code></pre> <p>(If docker is configured to run as non-root then <code>sudo</code> is not required).</p> <p>The script should create a file named custom-env.yaml which will be used by the control-plane installation.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#private-docker-registry-optional","title":"Private Docker Registry (optional)","text":"<p>To access the organization's docker registry it is required to set the registry's credentials (imagePullSecret)</p> <p>Create the secret named <code>runai-reg-creds</code> in the <code>runai-backend</code> namespace based on your existing credentials. The configuration will be copied over to the <code>runai</code> namespace at cluster install. For more information, see Allowing pods to reference images from other secured registries.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#configure-your-environment","title":"Configure your environment","text":""},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#create-openshift-project","title":"Create OpenShift project","text":"<p>The Run:ai control plane uses a namespace (or project in OpenShift terminology) name <code>runai-backend</code>. You must create it before installing:</p> <pre><code>oc new-project runai-backend\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#local-certificate-authority-air-gapped-only","title":"Local Certificate Authority (air-gapped only)","text":"<p>In Air-gapped environments, you must prepare the public key of your local certificate authority as described here. It will need to be installed in Kubernetes for the installation to succeed.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#mark-runai-system-workers-optional","title":"Mark Run:ai system workers (optional)","text":"<p>You can optionally set the Run:ai control plane to run on specific nodes. Kubernetes will attempt to schedule Run:ai pods to these nodes. If lacking resources, the Run:ai nodes will move to another, non-labeled node.  </p> <p>To set system worker nodes run:</p> <pre><code>kubectl label node &lt;NODE-NAME&gt; node-role.kubernetes.io/runai-system=true\n</code></pre> <p>Warning</p> <p>Do not select the Kubernetes master as a <code>runai-system</code> node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#external-postgres-database-optional","title":"External Postgres database (optional)","text":"<p>If you have opted to use an external PostgreSQL database, you need to perform initial setup to ensure successful installation. Follow these steps:</p> <ol> <li> <p>Create a SQL script file, edit the parameters below, and save it locally:</p> <ul> <li>Replace <code>&lt;DATABASE_NAME&gt;</code> with a dedicate database name for RunAi in your PostgreSQL database.</li> <li>Replace <code>&lt;ROLE_NAME&gt;</code> with a dedicated role name (user) for RunAi database.</li> <li>Replace <code>&lt;ROLE_PASSWORD&gt;</code> with a password for the new PostgreSQL role.</li> <li>Replace <code>&lt;GRAFANA_PASSWORD&gt;</code>  with the password to be set for Grafana integration.</li> </ul> <pre><code>-- Create a new database for runai\nCREATE DATABASE &lt;DATABASE_NAME&gt;; \n\n-- Create the role with login and password\nCREATE ROLE &lt;ROLE_NAME&gt;  WITH LOGIN PASSWORD '&lt;ROLE_PASSWORD&gt;'; \n\n-- Grant all privileges on the database to the role\nGRANT ALL PRIVILEGES ON DATABASE &lt;DATABASE_NAME&gt; TO &lt;ROLE_NAME&gt;; \n\n-- Connect to the newly created database\n\\c &lt;DATABASE_NAME&gt; \n\n-- grafana\nCREATE ROLE grafana WITH LOGIN PASSWORD '&lt;GRAFANA_PASSWORD&gt;'; \nCREATE SCHEMA grafana authorization grafana;\nALTER USER grafana set search_path='grafana';\n-- Exit psql\n\\q\n</code></pre> </li> <li> <p>Run the following command on a machine where PostgreSQL client (<code>pgsql</code>) is installed:</p> <pre><code>psql --host &lt;POSTGRESQL_HOST&gt; \\ # (1)\n--user &lt;POSTGRESQL_USER&gt; \\ # (2)\n--port &lt;POSTGRESQL_PORT&gt; \\ # (3)\n--dbname &lt;POSTGRESQL_DB&gt; \\ # (4)\n-a -f &lt;SQL_FILE&gt; \\ # (5)\n</code></pre> <ol> <li>Replace <code>&lt;POSTGRESQL_HOST&gt;</code> with the PostgreSQL ip address or hostname.</li> <li>Replace <code>&lt;POSTGRESQL_USER&gt;</code> with the PostgreSQL username.</li> <li>Replace <code>&lt;POSTGRESQL_PORT&gt;</code> with the port number where PostgreSQL is running.</li> <li>Replace <code>&lt;POSTGRESQL_DB&gt;</code> with the name of your PostgreSQL database.</li> <li>Replace <code>&lt;POSTGRESQL_DB&gt;</code> with the name of your PostgreSQL database.</li> <li>Replace <code>&lt;SQL_FILE&gt;</code> with the path to the SQL script created in the previous step.</li> </ol> </li> </ol>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#additional-permissions","title":"Additional permissions","text":"<p>As part of the installation, you will be required to install the Control plane and Cluster Helm Charts. The Helm Charts require Kubernetes administrator permissions. You can review the exact permissions provided by using the <code>--dry-run</code> on both helm charts.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#validate-prerequisites","title":"Validate prerequisites","text":"<p>Once you believe that the Run:ai prerequisites and preperations are met, we highly recommend installing and running the Run:ai pre-install diagnostics script. The tool:</p> <ul> <li>Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking.</li> <li>Looks at additional components installed and analyzes their relevancy to a successful Run:ai installation.</li> </ul> <p>To use the script download the latest version of the script and run:</p> <pre><code>chmod +x preinstall-diagnostics-&lt;platform&gt;\n./preinstall-diagnostics-&lt;platform&gt; \n</code></pre> <p>If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file <code>runai-preinstall-diagnostics.txt</code> in the current directory and send it to Run:ai technical support.</p> <p>For more information on the script including additional command-line flags, see here.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#next-steps","title":"Next steps","text":"<p>Continue with installing the Run:ai Control Plane.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/","title":"Self Hosted installation over OpenShift - prerequisites","text":"<p>Before proceeding with this document, please review the installation types documentation to understand the difference between air-gapped and connected installations. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#runai-components","title":"Run:ai components","text":"<p>As part of the installation process you will install:</p> <ul> <li>A control-plane managing cluster</li> <li>One or more clusters</li> </ul> <p>Both the control plane and clusters require Kubernetes. Typically the control plane and first cluster are installed on the same Kubernetes cluster but this is not a must. </p> <p>Important</p> <p>In OpenShift environments, adding a cluster connecting to a remote control plane currently requires the assistance of customer support. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#installer-machine","title":"Installer machine","text":"<p>The machine running the installation script (typically the Kubernetes master) must have:</p> <ul> <li>At least 50GB of free space.</li> <li>Docker installed. </li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#helm","title":"Helm","text":"<p>Run:ai requires Helm 3.14 or later. To install Helm, see Installing Helm. If you are installing an air-gapped version of Run:ai, The Run:ai tar file contains the helm binary. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#cluster-hardware-requirements","title":"Cluster hardware requirements","text":"<p>See Cluster prerequisites hardware requirements.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#runai-software-requirements","title":"Run:ai software requirements","text":""},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#cluster-nodes","title":"Cluster Nodes","text":"<p>Nodes are required to be synchronized by time using NTP (Network Time Protocol) for proper system functionality.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#openshift","title":"OpenShift","text":"<p>Run:ai supports OpenShift. OpenShift Versions supported are detailed in Kubernetes distribution.</p> <ul> <li>OpenShift must be configured with a trusted certificate. Run:ai installation relies on OpenShift to create certificates for subdomains. </li> <li>OpenShift must have a configured identity provider (Idp). </li> <li>If your network is air-gapped, you will need to provide the Run:ai control-plane and cluster with information about the local certificate authority.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#install-prerequisites","title":"Install prerequisites","text":""},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#nvidia-gpu-operator","title":"NVIDIA GPU Operator","text":"<p>See Run:ai Cluster prerequisites installing NVIDIA dependencies in OpenShift.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the NVIDIA prerequisites.</p> <p>Information on how to download the GPU Operator for air-gapped installation can be found in the NVIDIA GPU Operator pre-requisites. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#inference-optional","title":"Inference (optional)","text":"<p>See Run:ai Cluster prerequisites Inference requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the Inference prerequisites. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#external-postgresql-database-optional","title":"External PostgreSQL database (optional)","text":"<p>The Run:ai control plane installation includes a default PostgreSQL database. However, you may opt to use an existing PostgreSQL database if you have specific requirements or preferences. Please ensure that your PostgreSQL database is version 16 or higher.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#next-steps","title":"Next steps","text":"<p>Continue to Preparing for a Run:ai OpenShift Installation .</p>"},{"location":"admin/runai-setup/self-hosted/ocp/project-management/","title":"Self Hosted installation over OpenShift - Create Projects","text":""},{"location":"admin/runai-setup/self-hosted/ocp/project-management/#introduction","title":"Introduction","text":"<p>The Administrator creates Run:ai Projects via the Run:ai User Interface. When enabling Researcher Authentication you also assign users to Projects.</p> <p>Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically:</p> <ol> <li>Creates a namespace by the name of <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> <li>Labels the namespace as managed by Run:ai.</li> <li>Provides access to the namespace for Run:ai services.</li> <li>Associates users with the namespace.</li> </ol> <p>This process may need to be altered if,</p> <ul> <li>Researchers already have existing Kubernetes namespaces</li> <li>The organization's Kubernetes namespace naming convention does not allow the <code>runai-</code> prefix.</li> <li>The organization's policy does not allow the automatic creation of namespaces</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/project-management/#process","title":"Process","text":"<p>Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace:</p> <ul> <li>When setting up a Run:ai cluster, Disable namespace creation by setting the cluster flag <code>createNamespaces</code> to <code>false</code>.</li> <li>Using the Run:ai User Interface, create a new Project <code>&lt;PROJECT-NAME&gt;</code>. A namespace will not be created.</li> <li>Associate and existing namepace <code>&lt;NAMESPACE&gt;</code> with the Run:ai project by running:</li> </ul> <pre><code>oc label ns &lt;NAMESPACE&gt;  runai/queue=&lt;PROJECT_NAME&gt;\n</code></pre> <p>Caution</p> <p>Setting the <code>createNamespaces</code> flag to <code>false</code> moves the responsibility of creating namespaces to match Run:ai Projects to the administrator.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/uninstall/","title":"Uninstall Run:ai","text":"<p>See uninstall section here</p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/","title":"Upgrade Run:ai","text":""},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#preparations","title":"Preparations","text":""},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#helm","title":"Helm","text":"<p>Run:ai requires Helm 3.14 or later. Before you continue, validate your installed helm client version. To install or upgrade Helm, see Installing Helm. If you are installing an air-gapped version of Run:ai, The Run:ai tar file contains the helm binary. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#software-files","title":"Software files","text":"ConnectedAirgapped <p>Run the helm command below:</p> <pre><code>helm repo add runai-backend https://runai.jfrog.io/artifactory/cp-charts-prod\nhelm repo update\n</code></pre> <ul> <li>Ask for a tar file <code>runai-air-gapped-&lt;NEW-VERSION&gt;.tar.gz</code> from Run:ai customer support. The file contains the new version you want to upgrade to. <code>&lt;NEW-VERSION&gt;</code> is the updated version of the Run:ai control plane.</li> <li>Upload the images as described here.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#before-upgrade","title":"Before upgrade","text":"<p>Before proceeding with the upgrade, it's crucial to apply the specific prerequisites associated with your current version of Run:ai and every version in between up to the version you are upgrading to. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-from-version-29","title":"Upgrade from version 2.9","text":"<p>Two significant changes to the control-plane installation have happened with version 2.12: PVC ownership and installation customization. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#pvc-ownership","title":"PVC ownership","text":"<p>Run:ai no longer directly creates the PVCs that store Run:ai data (metrics and database). Instead, going forward, </p> <ul> <li>Run:ai requires a Kubernetes storage class to be installed.</li> <li>The PVCs are created by the Kubernetes StatefulSets. </li> </ul> <p>The storage class, as per Kubernetes standards, controls the reclaim behavior: whether the data is saved or deleted when the Run:ai control plane is deleted.  </p> <p>To remove the ownership in an older installation, run:</p> <pre><code>kubectl patch pvc -n runai-backend pvc-postgresql  -p '{\"metadata\": {\"annotations\":{\"helm.sh/resource-policy\": \"keep\"}}}'\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#installation-customization","title":"Installation customization","text":"<p>The Run:ai control-plane installation has been rewritten and is no longer using a backend values file. Instead, to customize the installation use standard <code>--set</code> flags. If you have previously customized the installation, you must now extract these customizations and add them as <code>--set</code> flag to the helm installation:</p> <ul> <li>Find previous customizations to the control plane if such exist. Run:ai provides a utility for that here <code>https://raw.githubusercontent.com/run-ai/docs/v2.13/install/backend/cp-helm-vals-diff.sh</code>. For information on how to use this utility please contact Run:ai customer support. </li> <li>Search for the customizations you found in the optional configurations table and add them in the new format.  </li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-control-plane","title":"Upgrade Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-from-version-217-or-later","title":"Upgrade from version 2.17, or later","text":"ConnectedAirgapped <pre><code>helm get values runai-backend -n runai-backend &gt; runai_control_plane_values.yaml\nhelm upgrade runai-backend -n runai-backend runai-backend/control-plane --version \"~2.18.0\" -f runai_control_plane_values.yaml --reset-then-reuse-values\n</code></pre> <pre><code>helm get values runai-backend -n runai-backend &gt; runai_control_plane_values.yaml\nhelm upgrade runai-backend control-plane-&lt;NEW-VERSION&gt;.tgz -n runai-backend  -f runai_control_plane_values.yaml --reset-then-reuse-values\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-from-version-29_1","title":"Upgrade from version 2.9","text":"ConnectedAirgapped <pre><code>helm upgrade -i runai-backend -n runai-backend runai-backend/control-plane --version \"~2.18.0\" \\\n--set global.domain=runai.apps.&lt;OPENSHIFT-CLUSTER-DOMAIN&gt; \\ #(1)\n--set global.config.kubernetesDistribution=openshift \\\n--set thanos.query.stores={thanos-grpc-port-forwarder:10901} \\\n--set postgresql.primary.persistence.existingClaim=pvc-postgresql\n</code></pre> <ol> <li>The subdomain configured for the OpenShift cluster.</li> </ol> <p>Note</p> <p>The helm repository name has changed from <code>runai-backend/runai-backend</code> to <code>runai-backend/control-plane</code>.</p> <pre><code>helm upgrade -i runai-backend  ./control-plane-&lt;NEW-VERSION&gt;.tgz -n runai-backend \\\n--set global.domain=runai.apps.&lt;OPENSHIFT-CLUSTER-DOMAIN&gt; \\ #(1)\n--set global.config.kubernetesDistribution=openshift \\\n--set thanos.query.stores={thanos-grpc-port-forwarder:10901} \\\n--set postgresql.primary.persistence.existingClaim=pvc-postgresql\n</code></pre> <ol> <li>The subdomain configured for the OpenShift cluster.</li> </ol>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-cluster","title":"Upgrade Cluster","text":"<p>To upgrade the cluster follow the instructions here.</p>"},{"location":"admin/troubleshooting/diagnostics/","title":"Diagnostic Tools","text":""},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-the-database-container","title":"Add Verbosity to the Database container","text":"<p>Run:ai Self-hosted installation contains an internal database. To diagnose database issues, you can run the database in debug mode.</p> <p>In the runai-backend-values, search for <code>postgresql</code>. Add: </p> <pre><code>postgresql:\n  image:\n    debug: true\n</code></pre> <p>Re-install the Run:ai control-plane and then review the database logs by running: </p> <pre><code>kubectl logs -n runai-backend runai-postgresql-0\n</code></pre>"},{"location":"admin/troubleshooting/diagnostics/#internal-networking-issues","title":"Internal Networking Issues","text":"<p>Run:ai is based on Kubernetes. Kubernetes runs its own internal subnet with a separate DNS service. If you see in the logs that services have trouble connecting, the problem may reside there.  You can find further information on how to debug Kubernetes DNS here. Specifically, it is useful to start a pod with networking utilities and use it for network resolution:</p> <pre><code>kubectl run -i --tty netutils --image=dersimn/netutils -- bash\n</code></pre>"},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-prometheus","title":"Add Verbosity to Prometheus","text":"<p>Add verbosity to Prometheus by editing RunaiConfig:</p> <pre><code>kubectl edit runaiconfig runai -n runai\n</code></pre> <p>Add a <code>debug</code> log level:</p> <pre><code>prometheus-operator:\n  prometheus:\n    prometheusSpec:\n      logLevel: debug\n</code></pre> <p>To view logs, run: <pre><code>kubectl logs prometheus-runai-prometheus-operator-prometheus-0 prometheus \\\n      -n monitoring -f --tail 100\n</code></pre></p>"},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-scheduler","title":"Add Verbosity to Scheduler","text":"<p>To view extended logs run:</p> <pre><code>kubectl edit ruaiconfig runai -n runai\n</code></pre> <p>Then under the <code>scheduler</code> section add:</p> <pre><code>runai-scheduler:\n   args:\n     verbosity: 6\n</code></pre> <p>Warning</p> <p>Verbose scheduler logs consume a significant amount of disk space.</p>"},{"location":"admin/troubleshooting/logs-collection/","title":"Logs Collection","text":"<p>This article provides instructions for IT administrators on collecting Run:ai logs for support, including prerequisites, CLI commands, and log file retrieval. It also covers enabling verbose logging for Prometheus and the Run:ai Scheduler.</p>"},{"location":"admin/troubleshooting/logs-collection/#collect-logs-to-send-to-support","title":"Collect logs to send to support","text":"<p>To collect Run:ai logs, follow these steps:</p>"},{"location":"admin/troubleshooting/logs-collection/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ensure that you have administrator-level access to the Kubernetes cluster where Run:ai is installed.  </li> <li>The Run:ai Administrator Command-Line Interface (CLI) must be installed.</li> </ul>"},{"location":"admin/troubleshooting/logs-collection/#step-by-step-instructions","title":"Step-by-Step Instructions","text":"<ol> <li>Run the Command from your local machine or a Bastion Host (secure server)    Open a terminal on your local machine (or any machine that has network access to the Kubernetes cluster) where the Run:ai Administrator CLI is installed.  </li> <li> <p>Collect the Logs     Execute the following command to collect the logs:  </p> <pre><code>runai-adm collect-logs\n</code></pre> <p>This command gathers all relevant Run:ai logs from the system and generate a compressed file.</p> </li> <li> <p>Locate the Generated File    After running the command, note the location of the generated compressed log file. You can retrieve and send this file to Run:ai Support for further troubleshooting.</p> </li> </ol> <p>Note</p> <p>The tar file packages the logs of Run:ai components only. It does not include logs of researcher containers that may contain private information</p>"},{"location":"admin/troubleshooting/logs-collection/#logs-verbosity","title":"Logs verbosity","text":"<p>Increase log verbosity to capture more detailed information, providing deeper insights into system behavior and make it easier to identify and resolve issues.</p>"},{"location":"admin/troubleshooting/logs-collection/#prerequisites_1","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>Access to the Kubernetes cluster where Run:ai is installed  </li> <li>Including necessary permissions to view and modify configurations.  </li> <li>kubectl installed and configured:  </li> <li>The Kubernetes command-line tool, <code>kubectl</code>, must be installed and configured to interact with the cluster.  </li> <li>Sufficient privileges to edit configurations and view logs.  </li> <li>Monitoring Disk Space  </li> <li>When enabling verbose logging, ensure adequate disk space to handle the increased log output, especially when enabling debug or high verbosity levels.</li> </ul>"},{"location":"admin/troubleshooting/logs-collection/#adding-verbosity","title":"Adding verbosity","text":"Adding verbosity to Prometheus <p>To increase the logging verbosity for Prometheus, follow these steps:</p> <ol> <li>Edit the <code>RunaiConfig</code> to adjust Prometheus log levels. Copy the following command to your terminal:  </li> </ol> <pre><code>kubectl edit runaiconfig runai -n runai\n</code></pre> <ol> <li>In the configuration file that opens, add or modify the following section to set the log level to <code>debug</code>:  </li> </ol> <pre><code>spec:\n    prometheus:\n        spec:\n            logLevel: debug\n</code></pre> <ol> <li>Save the changes. To view the Prometheus logs with the new verbosity level, run:  </li> </ol> <pre><code>kubectl logs -n runai prometheus-runai-0\n</code></pre> <p>This command streams the last 100 lines of logs from Prometheus, providing detailed information useful for debugging.</p> Adding verbosity to the scheduler <p>To enable extended logging for the Run:ai scheduler:</p> <ol> <li>Edit the <code>RunaiConfig</code> to adjust scheduler verbosity:  </li> </ol> <pre><code>kubectl edit runaiconfig runai -n runai\n</code></pre> <p>2  Add or modify the following section under the scheduler settings:  </p> <pre><code>runai-scheduler:\n    args:\n        verbosity: 6\n</code></pre> <p>This increases the verbosity level of the scheduler logs to provide more detailed output.</p> <p>Warning</p> <p>Enabling verbose logging can significantly increase disk space usage. Monitor your storage capacity and adjust the verbosity level as necessary.</p>"},{"location":"admin/troubleshooting/troubleshooting/","title":"Troubleshooting Run:ai","text":""},{"location":"admin/troubleshooting/troubleshooting/#installation","title":"Installation","text":"Upgrade fails with \"Ingress already exists\" <p>Symptom:  The installation fails with error: <code>Error: rendered manifests contain a resource that already exists. Unable to continue with install: IngressClass \"nginx\" in namespace \"\" exists</code></p> <p>Root cause: Run:ai installs <code>NGINX</code>, but there is an existing NGINX on the cluster. </p> <p>Resolution: In the Run:ai cluster YAML file, disable the installation of NGINX by setting:</p> <pre><code>ingress-nginx:\n    enabled: false\n</code></pre> How to get installation logs <p>Symptom: Installation fails and you need to troubleshoot the issue.</p> <p>Resolution: Run the following script to obtain any relevant installation logs in case of an error.</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/run-ai/public/main/installation/get-installation-logs.sh | bash\n</code></pre> Upgrade fails with \"rendered manifests contain a resource that already exists\" error <p>Symptom: The installation fails with error: <code>Error: rendered manifests contain a resource that already exists. Unable to continue with install:...</code></p> <p>Root cause: The Run:ai installation is trying to create a resource that already exists, which may be due to a previous installation that was not properly removed.</p> <p>Resolution: Run the following script to remove all Run:ai resources and reinstall:</p> <pre><code>helm template &lt;release-name&gt; &lt;chart-name&gt; --namespace &lt;namespace&gt; | kubectl delete -f -\n</code></pre> <p>Then reinstall Run:ai.</p> Pods are failing due to certificate issues <p>Symptom: Pods are failing with certificate issues.</p> <p>Root cause: The certificate provided during the Control Plane's installation is not valid.</p> <p>Resolution: Verify that the certificate is valid and trusted. If the certificate is valid, but is signed by a local CA, make sure you have followed the procedure for a local certificate authority.</p>"},{"location":"admin/troubleshooting/troubleshooting/#cluster-health","title":"Cluster Health","text":"<p>See Cluster Health Troubleshooting</p>"},{"location":"admin/troubleshooting/troubleshooting/#dashboard-issues","title":"Dashboard Issues","text":"No Metrics are showing on Dashboard <p>Symptom: No metrics are showing on dashboards at <code>https://&lt;company-name&gt;.run.ai/dashboards/now</code></p> <p>Typical root causes:</p> <ul> <li>Firewall-related issues.</li> <li>Internal clock is not synced.</li> <li>Prometheus pods are not running.</li> </ul> <p>Firewall issues</p> <p>Add verbosity to Prometheus as describe here.Verify that there are no errors. If there are connectivity-related errors you may need to:</p> <ul> <li>Check your firewall for outbound connections. See the required permitted URL list in Network requirements.</li> <li>If you need to set up an internet proxy or certificate, please contact Run:ai customer support. </li> </ul> <p>Machine Clocks are not synced</p> <p>Run: <code>date</code> on cluster nodes and verify that date/time is correct.  If not:</p> <ul> <li>Set the Linux time service (NTP).</li> <li>Restart Run:ai services. Depending on the previous time gap between servers, you may need to reinstall the Run:ai cluster</li> </ul> <p>Prometheus pods are not running</p> <p>Run: <code>kubectl get pods -n monitoring -o wide</code></p> <ul> <li>Verify that all pods are running.</li> <li>The default Prometheus installation is not built for high availability. If a node is down, the Prometheus pod may not recover by itself unless manually deleted. Delete the pod to see it start on a different node and consider adding a second replica to Prometheus.</li> </ul> GPU Related metrics not showing <p>Symptom: GPU-related metrics such as <code>GPU Nodes</code> and <code>Total GPUs</code> are showing zero but other metrics, such as <code>Cluster load</code> are shown.</p> <p>Root cause: An installation issue related to the NVIDIA stack.</p> <p>Resolution: </p> <p>Need to run through the NVIDIA stack and find the issue. The current NVIDIA stack looks as follows:</p> <ol> <li>NVIDIA Drivers (at the OS level, on every node)</li> <li>NVIDIA Docker (extension to Docker, on every node)</li> <li>Kubernetes Node feature discovery (mark node properties)</li> <li>NVIDIA GPU Feature discovery (mark nodes as \u201chaving GPUs\u201d)</li> <li>NVIDIA Device plug-in (Exposes GPUs to Kubernetes)</li> <li>NVIDIA DCGM Exporter (Exposes metrics from GPUs in Kubernetes)</li> </ol> <p>Run:ai requires the installation of the NVIDIA GPU Operator which installs the entire stack above. However, there are two alternative methods for using the operator:</p> <ul> <li>Use the default operator values to install 1 through 6.</li> <li>If  NVIDIA Drivers (#1 above) are already installed on all nodes, use the operator with a flag that disables drivers install. </li> </ul> <p>For more information see [System requirements](../runai-setup/cluster-setup/.</p> <p>NVIDIA GPU Operator</p> <p>Run: <code>kubectl get pods -n gpu-operator | grep nvidia</code> and verify that all pods are running.</p> <p>Node and GPU feature discovery</p> <p>Kubernetes Node feature discovery identifies and annotates nodes. NVIDIA GPU Feature Discovery identifies and annotates nodes with GPU properties. See that: </p> <ul> <li>All such pods are up.</li> <li>The GPU feature discovery pod is available for every node with a GPU.</li> <li>And finally, when describing nodes, they show an active <code>gpu/nvidia</code> resource.</li> </ul> <p>NVIDIA Drivers</p> <ul> <li>If NVIDIA drivers have been installed on the nodes themselves, ssh into each node and run <code>nvidia-smi</code>. Run <code>sudo systemctl status docker</code> and verify that docker is running. Run <code>nvidia-docker</code> and verify that it is installed and working.  Linux software upgrades may require a node restart.</li> <li>If NVIDIA drivers are installed by the Operator, verify that the NVIDIA driver daemonset has created a pod for each node and that all nodes are running. Review the logs of all such pods. A typical problem may be the driver version which is too advanced for the GPU hardware. You can set the driver version via operator flags. </li> </ul> <p>NVIDIA DCGM Exporter</p> <ul> <li>View the logs of the DCGM exporter pod and verify that no errors are prohibiting the sending of metrics. </li> <li>To validate that the dcgm-exporter exposes metrics, find one of the DCGM Exporter pods and run:</li> </ul> <pre><code>kubectl port-forward &lt;dcgm-exporter-pod-name&gt; 9400:9400\n</code></pre> <p>Then browse to http://localhost:9400/metrics and verify that the metrics have reached the DCGM exporter.</p> <ul> <li>The next step after the DCGM Exporter is <code>Prometheus</code>. To validate that metrics from the DCGM Exporter reach Prometheus, run:</li> </ul> <pre><code>kubectl port-forward svc/runai-cluster-kube-prometh-prometheus -n monitoring 9090:9090\n</code></pre> <p>Then browse to localhost:9090. In the UI, type <code>DCGM_FI_DEV_GPU_UTIL</code> as the metric name, and verify that the metric has reached Prometheus. </p> <p>If the DCGM Exporter is running correctly and exposing metrics, but this metric does not appear in Prometheus, there may be a connectivity issue between these components.</p> Allocation-related metrics not showing <p>Symptom: GPU Allocation-related metrics such as <code>Allocated GPUs</code> are showing zero but other metrics, such as <code>Cluster load</code> are shown.</p> <p>Root cause: The origin of such metrics is the scheduler. </p> <p>Resolution:</p> <ul> <li>Run: <code>kubectl get pods -n runai | grep scheduler</code>. Verify that the pod is running.</li> <li>Review the scheduler logs and look for errors. If such errors exist, contact Run:ai customer support. </li> </ul> All metrics are showing \"No Data\" <p>Symptom: All data on all dashboards is showing the text \"No Data\".</p> <p>Root cause: Internal issue with metrics infrastructure.</p> <p>Resolution: Please contact Run:ai customer support.</p>"},{"location":"admin/troubleshooting/troubleshooting/#authentication-issues","title":"Authentication Issues","text":"After a successful login, you are redirected to the same login page <p>For a self-hosted installation, check Linux clock synchronization as described above. Use the Run:ai preinstall diagnostics tool to validate System and network requirements and test this automatically. </p> Single-sign-on issues <p>For single-sign-on issues, see the troubleshooting section in the single-sign-on configuration documents. </p>"},{"location":"admin/troubleshooting/troubleshooting/#user-interface-submit-job-issues","title":"User Interface Submit Job Issues","text":"New Job button is grayed out <p>Symptom: The <code>New Job</code> button on the top right of the Job list is grayed out.</p> <p>Root Cause: This can happen due to multiple configuration issues: </p> <ul> <li>Open Chrome developer tools and refresh the screen.</li> <li>Under <code>Network</code> locate a network call error. Search for the HTTP error code.</li> </ul> <p>Resolution for 401 HTTP Error</p> <ul> <li>The Cluster certificate provided as part of the installation is valid and trusted (not self-signed).</li> <li>Researcher Authentication has not been properly configured. Try running <code>runai login</code> from the Command-line interface. Alternatively, run: <code>kubectl get pods -n kube-system</code>, identify the api-server pod and review its logs. </li> </ul> <p>Resolution for 403 HTTP Error</p> <p>Run: <code>kubectl get pods -n runai</code>, identify the <code>agent</code> pod, see that it's running, and review its logs.</p> New Job button is not showing <p>Symptom: The <code>New Job</code> button on the top right of the Job list does not show.</p> <p>Root Causes: (multiple)</p> <ul> <li>You do not have <code>Researcher</code> or <code>Research Manager</code> permissions.</li> <li>Under <code>Settings | General</code>, verify that <code>Unified UI</code> is on.</li> </ul> Submit form is distorted <p>Symptom: Submit form is showing vertical lines.</p> <p>Root Cause: The control plane does not know the cluster URL.</p> <p>Using the Run:ai user interface, go to the Clusters list. See that there is no cluster URL next to your cluster.</p> <p>Resolution: Cluster must be re-installed. </p> Submit form does not show the list of Projects <p>Symptom: When connected with Single-sign-on, in the Submit form, the list of Projects is empty.</p> <p>Root Cause:  SSO is on and researcher authentication is not properly configured as such.</p> <p>Resolution: Verify API Server settings as described in Researcher Authentication configuration.</p> Job form is not opening on OpenShift <p>Symptom: When clicking on \"New Job\" the Job forms does not load. Network shows 405</p> <p>Root Cause: An installation step has been missed. </p> <p>Resolution: Open the Cluster list and open the cluster installation wizard again. After selecting OpenShift, you will see a <code>patch</code> command at the end of the instruction set. Run it. </p>"},{"location":"admin/troubleshooting/troubleshooting/#networking-issues","title":"Networking Issues","text":"'admission controller' connectivity issue <p>Symptoms:</p> <ul> <li>Pods are failing with 'admission controller' connectivity errors.</li> <li>The command-line <code>runai submit</code> fails with an 'admission controller' connectivity error.</li> <li>Agent or cluster sync pods are crashing in self-hosted installation.</li> </ul> <p>Root cause: Connectivity issues between different nodes in the cluster.</p> <p>Resolution:</p> <ul> <li>Run the preinstall diagnostics tool to validate System and network requirements and test connectivity issues.</li> <li>Run: <code>kubectl get pods -n kube-system -o wide</code>. Verify that all networking pods are running. </li> <li>Run: <code>kubectl get nodes</code>. Check that all nodes are ready and connected.</li> <li>Run: <code>kubectl get pods -o wide -A</code> to see which pods are Pending or in Error and which nodes they belong to. </li> <li>See if pods from different nodes have trouble communicating with each other.</li> <li>Advanced, run: <code>kubectl exec &lt;pod-name&gt; -it /bin/sh</code> from a pod in one node and ping a pod from another. </li> </ul> Projects are not syncing <p>Symptom: Create a Project on the Run:ai user interface, then run: <code>runai list projects</code>. The new Project does not appear.</p> <p>Root cause: The Run:ai agent is not syncing properly. This may be due to firewall issues. </p> <p>Resolution</p> <ul> <li>Run: <code>runai pods -n runai | grep agent</code>. See that the agent is in Running state. Select the agent's full name and run: <code>kubectl logs -n runai runai-agent-&lt;id&gt;</code>.</li> <li>Verify that there are no errors. If there are connectivity-related errors you may need to check your firewall for outbound connections. See the required permitted URL list in Network requirements. </li> <li>If you need to set up an internet proxy or certificate, please contact Run:ai customer support. </li> </ul> Jobs are not syncing <p>Symptom: A Job on the cluster (<code>runai list jobs</code>) does not show in the Run:ai user interface Job list. </p> <p>Root cause: The Run:ai cluster-sync pod is not syncing properly.  </p> <p>Resolution: Search the cluster-sync pod for errors.</p>"},{"location":"admin/troubleshooting/troubleshooting/#job-related-issues","title":"Job-related Issues","text":"Jobs fail with ContainerCannotRun status  <p>Symptom: When running <code>runai list jobs</code>, your Job has a status of <code>ContainerCannotRun</code>.</p> <p>Root Cause: The issue may be caused due to an unattended upgrade of the NVIDIA driver.</p> <p>To verify, run: <code>runai describe job &lt;job-name&gt;</code>, and search for an error <code>driver/library version mismatch</code>.</p> <p>Resolution: Reboot the node on which the Job attempted to run.</p> <p>Going forward, we recommend blacklisting NVIDIA driver from unattended-upgrades. You can do that by editing <code>/etc/apt/apt.conf.d/50unattended-upgrades</code>, and adding <code>nvidia-driver-</code> to the <code>Unattended-Upgrade::Package-Blacklist</code> section. It should look something like that:</p> <pre><code>Unattended-Upgrade::Package-Blacklist {\n    // The following matches all packages starting with linux-\n    //  \"linux-\";\n    \"nvidia-driver-\";\n</code></pre>"},{"location":"admin/troubleshooting/troubleshooting/#inference-issues","title":"Inference Issues","text":"New Deployment button is grayed out <p>Symptoms: </p> <ul> <li>The <code>New workload type</code> -&gt; <code>Inference</code> button is grayed out.</li> <li>Cannot create a deployment via Inference API.</li> </ul> <p>Root Cause: Run:ai Inference prerequisites have not been met.</p> <p>Resolution: Review inference prerequisites and install accordingly.</p> Submitted workload type of inference remains in Pending state <p>Symptom: A submitted inference is not running.</p> <p>Root Cause: The patch statement to add the runai-scheduler has not been performed. </p> Workload of type inference status is \"Failed\" <p>Symptom: Inference status is always <code>Failed</code>.</p> <p>Root Cause: (multiple)</p> <ul> <li>Not enough resources in the cluster.</li> <li>Server model command is misconfigured (i.e sleep infinity).</li> <li>Server port is misconfigured. </li> </ul> Worload of type inference does not scale up from zero <p>Symptom: In the Inference form, when \"Auto-scaling\" is enabled, and \"Minimum Replicas\" is set to zero, the inference cannot scale up from zero.</p> <p>Root Cause: </p> <ul> <li>Clients are not sending requests.</li> <li>Clients are not using the same port/protocol as the server model.</li> <li>Server model command is misconfigured (i.e sleep infinity).</li> </ul>"},{"location":"admin/troubleshooting/troubleshooting/#command-line-interface-issues","title":"Command-line interface Issues","text":"Unable to install CLI due to certificate errors <p>Symptom: The curl command and download button to download the CLI is not working.</p> <p>Root Cause: The cluster is not accessible from the download location</p> <p>Resolution: </p> <p>Use an alternate method for downloading the CLI. Run:</p> <pre><code>kubectl port-forward -n runai svc/researcher-service 4180\n</code></pre> <p>In another shell, run: <pre><code>wget --content-disposition http://localhost:4180/cli/linux\n</code></pre></p> When running the CLI you get an error: open .../.kube/config.lock: permission denied <p>Symptom: When running any CLI command you get a permission denied error.</p> <p>Root Cause: The user running the CLI does not have read permissions to the <code>.kube</code> directory.</p> <p>Resolution: Change permissions for the directory.</p> When running 'runai logs', the logs are delayed <p>Symptom: Printout from the container is not immediately shown in the log. </p> <p>Root Cause: By default, Python buffers stdout, and stderr, which are not flushed in real-time. This may cause logs to appear sometimes minutes after being buffered.</p> <p>Resolution: Set the env var PYTHONUNBUFFERED to any non-empty string or pass -u to Python. e.g. <code>python -u main.py</code>.</p> CLI does not download properly on OpenShift <p>Symptom: When trying to download the CLI on OpenShift, the <code>wget</code> statement downloads a text file named <code>darwin</code> or <code>linux</code> rather than the binary <code>runai</code>.</p> <p>Root Cause: An installation step has been missed. </p> <p>Resolution: Open the Cluster list and open the cluster installation wizard again. After selecting OpenShift, you will see a <code>patch</code> command at the end of the instruction set. Run it. </p>"},{"location":"developer/overview-developer/","title":"Developer Documentation Overview","text":"<p>Developers can access Run:ai through various programmatic interfaces.</p>"},{"location":"developer/overview-developer/#api-architecture","title":"API Architecture","text":"<p>Run:ai is composed of a single, multi-tenant control plane. Each tenant can be connected to one or more GPU clusters. See Run:ai system components for detailed information.</p> <p>The following programming interfaces are available:</p> API Description Purpose Run:ai REST API Get and Modify any Run:ai business object This is the API mostly used by system developers. The API is also used by the Run:ai user interface as well as the new command-line interface Cluster API (Deprecated) Submit Workloads directly to the Cluster A YAML-based API allowing submittion of Workloads directly to the Cluster. With Run:ai 2.18, this API is replaced by the above Run:ai, which is now the recommended method Metrics API (deprecated) Get cluster metrics Get utilization metrics."},{"location":"developer/overview-developer/#runai-rest-api","title":"Run:ai REST API","text":"<p>Allows you to Add, delete, modify and list Run:ai meta-data objects such as Projects, Departments, Users. For Clusters of Run:ai 2.18 and above, allows the submitting of Workloasd. </p> <p>The API is provided as REST and is accessible via the control plane endpoint.  </p> <p>For more information see Run:ai REST API.</p> <p>Important</p> <p>The endpoints and fields specified in the API reference are the ones that are officially supported by Run:ai. Endpoints and fields that are not listed in the API reference are not supported.</p> <p>Run:ai does not recommend using API endpoints and fields marked as <code>deprecated</code> and will not add functionality to them. Once an API endpoint or field is marked as <code>deprecated</code>, Run:ai will stop supporting it after 2 major releases for self-hosted deployments, and after 6 months for SaaS deployments.</p> <p>For details, see the Deprecation notifications.</p>"},{"location":"developer/overview-developer/#cluster-api-deprecated","title":"Cluster API (Deprecated)","text":"<p>The Cluster API allows you to submit and delete Workloads directly to the cluster itself.</p> <p>The API is provided as Kubernetes API.</p> <p>Cluster API is accessible via the GPU cluster itself. As such, multiple clusters may have multiple endpoints.</p> <p>Important</p> <ul> <li>This API is replaced by a Run:ai REST API to submit jobs, which is now the recommended method for cluster versions of 2.18 and above. </li> <li>If you are looking to automate tasks with older versions of Run:ai, it's best to use the Run:ai Command-line interface which provides forward compatibility.  </li> </ul>"},{"location":"developer/overview-developer/#metrics-api","title":"Metrics API","text":"<p>Retrieve metrics from multiple GPU clusters.</p> <p>See the Metrics API document.</p>"},{"location":"developer/overview-developer/#api-authentication","title":"API Authentication","text":"<p>See API Authentication for information on how to gain authenticated access to Run:ai APIs.</p>"},{"location":"developer/rest-auth/","title":"API Authentication","text":"<p>The following document explains how to authenticate with Run:ai APIs.</p> <p>Run:ai APIs are accessed using bearer tokens. A token can be obtained in several ways:</p> <ul> <li>When logging into the Run:ai user interface, you enter an email and password (or authenticated via single sign-on) which are used to obtain a token.</li> <li>When using the Run:ai command-line, you use a Kubernetes profile and are authenticated by pre-running <code>runai login</code> (or oc login with OpenShift). The command attaches a token to the profile and allows you access to Run:ai functionality.</li> <li>When using Run:ai APIs, you need to create an Application through the Run:ai user interface. The Application is created with specific roles and contains a secret. Using the secret you can obtain a token and use it within subsequent API calls.</li> </ul>"},{"location":"developer/rest-auth/#create-a-client-application","title":"Create a Client Application","text":"<ul> <li>Open the Run:ai Run:ai User Interface.</li> <li>Go to <code>Settings &amp; Tools</code>, <code>Application</code> and create a new Application.</li> <li>Copy the <code>&lt;APPLICATION&gt;</code> and <code>&lt;SECRET KEY&gt;</code> to be used below</li> </ul>"},{"location":"developer/rest-auth/#access-rules-for-the-application","title":"Access rules for the Application","text":"<p>For you API requests to be accepted, you will need to set access rules for the application. To assign roles to an application, see Create or Delete rules.</p> <p>Use the Roles table to assign the correct roles to the application.</p>"},{"location":"developer/rest-auth/#request-an-api-token","title":"Request an API Token","text":"<p>Use the above parameters to get a temporary token to access Run:ai as follows.</p>"},{"location":"developer/rest-auth/#example-command-to-get-an-api-token","title":"Example command to get an API token","text":"<p>Replace <code>&lt;COMPANY-URL&gt;</code> below with:</p> <ul> <li> <p>For SaaS installations use <code>&lt;company&gt;.run.ai</code></p> </li> <li> <p>For self-hosted use the Run:ai user interface URL.</p> </li> </ul> cURLPython <pre><code>    curl  -X POST \\\n      'https://&lt;runai_url&gt;/api/v1/token' \\\n      --header 'Accept: */*' \\\n      --header 'Content-Type: application/json' \\\n      --data-raw '{\n      \"grantType\":\"app_token\",\n      \"AppId\":\"&lt;APPLICATION NAME&gt;\",\n      \"AppSecret\" : \"&lt;SECRET KEY&gt;\"\n    }'\n</code></pre> <pre><code>    import requests\n    import json\n    reqUrl = \"https://cp-590d-run-13764-kc-upgrade.runailabs.com/api/v1/token\"\n    headersList = {\n     \"Accept\": \"*/*\",\n     \"Content-Type\": \"application/json\"\n    }\n    payload = json.dumps({\n      \"grantType\":\"app_token\",\n      \"AppId\":\"&lt;APPLICATION NAME&gt;\",\n      \"AppSecret\" : \"&lt;SECRET KEY&gt;\"\n    })\n    response = requests.request(\"POST\", reqUrl, data=payload,  headers=headersList)\n    print(response.text)\n</code></pre>"},{"location":"developer/rest-auth/#response","title":"Response","text":"<p>The API response will look as follows:</p> API Response<pre><code>{\n  \"accessToken\": \"&lt;TOKEN&gt;\", \n}\n</code></pre> <p>To call Run:ai REST APIs, the application must pass the retrieved <code>accessToken</code> as a Bearer token in the Authorization header of your HTTP request.</p>"},{"location":"developer/admin-rest-api/overview/","title":"Run:ai REST API","text":"<p>The purpose of the Run:ai REST API is to provide an easy-to-use programming interface for administrative tasks.</p>"},{"location":"developer/admin-rest-api/overview/#endpoint-url-for-api","title":"Endpoint URL for API","text":"<p>The domain used for Run:ai REST APIs is the same domain used to browse for the Run:ai User Interface. Either <code>&lt;company&gt;.run.ai</code>, or <code>app.run.ai</code> for older tenants or a custom URL used for Self-hosted installations.</p>"},{"location":"developer/admin-rest-api/overview/#authentication","title":"Authentication","text":"<ul> <li>Create a Client Application to make API requests. Use the client application and secret, to obtain a time-bound bearer token (<code>&lt;ACCESS-TOKEN&gt;</code>). For details, see Calling REST APIs.</li> <li>Use the token for subsequent API calls.</li> </ul>"},{"location":"developer/admin-rest-api/overview/#example-usage","title":"Example Usage","text":"<p>For example, if you have an Administrator role, you can get a list of clusters by running:</p> cURLPython <pre><code>curl 'https://&lt;COMPANY-URL&gt;/v1/k8s/clusters' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;ACCESS-TOKEN&gt;'\n</code></pre> <pre><code>import http.client\n\nconn = http.client.HTTPSConnection(\"https://&lt;COMPANY-URL&gt;\")\nheaders = {\n    'content-type': \"application/json\",\n    'authorization': \"Bearer &lt;ACCESS-TOKEN&gt;\"\n    }\nconn.request(\"GET\", \"/v1/k8s/clusters\", headers=headers)\n\nres = conn.getresponse()\ndata = res.read()\n\nprint(data.decode(\"utf-8\"))\n</code></pre> <p>(replace <code>&lt;ACCESS-TOKEN&gt;</code> with the bearer token from above).</p> <p>For an additional example, see the following code. It is an example of how to use the Run:ai REST API to create a User and a Project and set the User to the Project.  </p>"},{"location":"developer/admin-rest-api/overview/#runai-rest-api-documentation","title":"Run:ai REST API Documentation","text":"<p>The Run:ai REST API offers developers a robust interface for interacting with and managing Run:ai metadata objects, including Projects, Departments, Clusters, and Users.</p> <p>Public API documentation is available at api-docs.run.ai. For self-hosted deployments, access the documentation at <code>https://&lt;control-plane-url&gt;/api/docs</code>.</p> <p>View Documentation</p>"},{"location":"developer/admin-rest-api/overview/#runai-api-policy","title":"Run:ai API Policy","text":"<p>At Run:ai, we are dedicated to delivering stable, reliable, and well-documented APIs. Our goal is to ensure that our APIs evolve in a predictable, transparent manner, offering users a seamless experience.</p> <p>Run:ai follows strict API design and operational standards to ensure a consistent and high-quality experience for users.</p>"},{"location":"developer/admin-rest-api/overview/#api-lifecycle-and-deprecation","title":"API Lifecycle and Deprecation","text":"<p>While our goal is to maintain stable and backward-compatible APIs, there may be times when breaking changes or deprecations are necessary.</p> <p>In case of breaking changes, the deprecated version of the API will be supported for two additional versions in self-hosted deployments and for six months in SaaS deployments. During this period, no new features or functionality will be added to the deprecated API.  When an API or API field is deprecated, the following process is followed: Documentation: The deprecated API or field is clearly labeled in the documentation, with a replacement provided where applicable. Release Notes: Information about deprecated APIs, including those scheduled for future removal, is included in the release notes. Customer Notification: Customers are notified of upcoming deprecations as part of the regular release communications.</p>"},{"location":"developer/admin-rest-api/overview/#api-removal","title":"API Removal","text":"<p>After the defined backward compatibility period has ended, deprecated APIs or fields are removed from both the codebase and the documentation.</p>"},{"location":"developer/cluster-api/other-resources/","title":"Support for other Kubernetes Applications","text":""},{"location":"developer/cluster-api/other-resources/#introduction","title":"Introduction","text":"<p>Kubernetes has several built-in resources that encapsulate running Pods. These are called Kubernetes Workloads and should not be confused with Run:ai Workloads.</p> <p>Examples of such resources are a Deployment that manages a stateless application, or a Job that runs tasks to completion.</p> <p>Run:ai natively runs Run:ai Workloads. A Run:ai workload encapsulates all the resources needed to run, creates them, and deletes them together. However, Run:ai, being an open platform allows the scheduling of any Kubernetes Workflow.</p>"},{"location":"developer/cluster-api/other-resources/#how-to","title":"How To","text":"<p>To run Kubernetes Workloads with Run:ai you must add the following to the YAML:</p> <ul> <li>A namespace that is associated with a Run:ai Project.</li> <li>A scheduler name: <code>runai-scheduler</code>.</li> <li>When using Fractions, use a specific syntax for the <code>nvidia/gpu</code> limit.</li> </ul>"},{"location":"developer/cluster-api/other-resources/#example-job","title":"Example: Job","text":"job1.yaml<pre><code>apiVersion: batch/v1\nkind: Job # (1)\nmetadata:\n  name: job1\n  namespace: runai-team-a # (2)\nspec:\n  template:\n    spec:\n      containers:\n      - name: job1-container\n        image: runai.jfrog.io/demo/quickstart\n        resources:\n          limits:\n            nvidia.com/gpu: 1 # (4)\n      restartPolicy: Never\n      schedulerName: runai-scheduler # (3)\n</code></pre> <ol> <li>This is a Kubernetes Job.</li> <li>Namespace: Replace <code>runai-team-a</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).</li> <li>The job to be scheduled with the Run:ai scheduler.</li> <li>To run with half a GPU replace 1 with \"0.5\" (with apostrophes).</li> </ol> <p>To submit the Job run:</p> <pre><code>kubectl apply -f job1.yaml\n</code></pre> <p>You will be able to see the Job in the Run:ai User interface, including all metrics and lists</p>"},{"location":"developer/cluster-api/other-resources/#example-deployment","title":"Example: Deployment","text":"deployment1.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment # (1)\nmetadata:\n  name: inference-1\n  namespace: runai-team-a # (2)\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: inference-1\n  template:\n    metadata:\n      labels:\n        app: inference-1\n    spec:\n      containers:\n        - resources:\n            limits:\n              nvidia.com/gpu: 1 # (4)\n          image: runai/example-marian-server\n          imagePullPolicy: Always\n          name: inference-1\n          ports:\n            - containerPort: 8888\n      schedulerName: runai-scheduler # (3)\n\n---\napiVersion: v1\nkind: Service # (5)\nmetadata:\n  labels:\n    app: inference-1\n  name: inference-1\nspec:\n  type: ClusterIP\n  ports:\n    - port: 8888\n      targetPort: 8888\n  selector:\n    app: inference-1\n</code></pre> <ol> <li>This is a Kubernetes Deployment.</li> <li>Namespace: Replace <code>runai-team-a</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).</li> <li>The job to be scheduled with the Run:ai scheduler.</li> <li>To run with half a GPU replace 1 with \"0.5\" (with apostrophes).</li> <li>This example also contains the creation of a service to connect to the deployment. It is not mandatory.</li> </ol> <p>To submit the Deployment run:</p> <pre><code>kubectl apply -f deployment1.yaml\n</code></pre>"},{"location":"developer/cluster-api/other-resources/#example-submit-a-cron-job-via-yaml","title":"Example: Submit a Cron job via YAML","text":"<p>The cron command-line utility is a job scheduler typically used to set up and maintain software environments at scheduled intervals. Run:ai now supports submitting jobs with cron using a YAML file. </p> <p>To submit a job using cron, run the following command:</p> <pre><code>kubectl apply -f &lt;file_name&gt;.yaml\n</code></pre> <p>The following is an example YAML file:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"* * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n          - (Mandatory) runai/queue: team-a\n        spec:\n          (Mandatory) schedulerName: runai-scheduler\n          containers:\n          - name: hello\n            image: busybox:1.28\n            imagePullPolicy: IfNotPresent\n            command:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n          (Optional) priorityClassName: build / train / inference / interactivePreemptible\n</code></pre>"},{"location":"developer/cluster-api/other-resources/#limitations","title":"Limitations","text":"<p>The Run:ai command line interface provides limited support for Kubernetes Workloads.</p>"},{"location":"developer/cluster-api/other-resources/#see-also","title":"See Also","text":"<p>Run:ai has specific integrations with additional third-party tools such as KubeFlow, MLFlow, and more. These integrations use the same instructions as described above.</p>"},{"location":"developer/cluster-api/reference/","title":"Reference","text":"<p>For a full reference for the YAML API parameters see the YAML Reference document.</p>"},{"location":"developer/cluster-api/submit-rest/","title":"Submitting Workloads via HTTP/REST","text":"<p>You can submit Workloads via HTTP calls, using the Kubernetes REST API.</p>"},{"location":"developer/cluster-api/submit-rest/#submit-workload-example","title":"Submit Workload Example","text":"<p>To submit a workload via HTTP, run the following:</p> <pre><code>curl -X POST \\ # (1) \n'https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/trainingworkloads' \\ \n    --header 'Content-Type: application/yaml' \\\n    --header 'Authorization: Bearer &lt;BEARER&gt;' \\  # (2) \n    --data-raw 'apiVersion: run.ai/v2alpha1\nkind: TrainingWorkload  # (3)\nmetadata:\n  name: job-1    \nspec:\n  gpu:\n    value: \"1\"\n  image:\n    value: runai.jfrog.io/demo/quickstart\n  name:\n    value: job-1  \n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code>, <code>distributedworkloads</code> or <code>inferenceworkloads</code> according to type.</li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> <li>See Submitting a Workload via YAML for an explanation of the YAML-based workload.</li> </ol> <p>Run: <code>runai list jobs</code> to see the new Workload.</p>"},{"location":"developer/cluster-api/submit-rest/#delete-workload-example","title":"Delete Workload Example","text":"<p>To delete a workload run:</p> <pre><code>curl -X DELETE \\ # (1) \n'https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/trainingworkloads/&lt;JOB-NAME&gt;' \\ \n    --header 'Content-Type: application/yaml' \\\n    --header 'Authorization: Bearer &lt;BEARER&gt;'   # (2)\n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code>, <code>distributedworkloads</code> or <code>inferenceworkloads</code> according to type.  Replace <code>&lt;JOB-NAME&gt;</code> with the name of the Job. </li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> </ol>"},{"location":"developer/cluster-api/submit-rest/#suspendstop-workload-example","title":"Suspend/Stop workload example","text":"<p>To suspend or stop a workload run:</p> <pre><code>curl -X PATCH \\ # (1) \n'https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/interactiveworkload/&lt;JOB-NAME&gt;' \\\n    --header 'Content-Type: application/json' \n    --header 'Authorization: Bearer &lt;TOKEN&gt;'# (2) \n    --data '{\"spec\":{\"active\": {\"value\": \"false\"}}}'\n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code>, <code>distributedworkloads</code> or <code>inferenceworkloads</code> according to type.  Replace <code>&lt;JOB-NAME&gt;</code> with the name of the Job. </li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> </ol>"},{"location":"developer/cluster-api/submit-rest/#using-other-programming-languages","title":"Using other Programming Languages","text":"<p>You can use any Kubernetes client library together with the YAML documentation above to submit workloads via other programming languages. For more information see Kubernetes client libraries.</p>"},{"location":"developer/cluster-api/submit-rest/#python-example","title":"Python example","text":"<p>Create the following file and run it via python:</p> create-train.py<pre><code>import json\nimport requests\n\n# (1)\nurl = \"https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/trainingworkloads\"\n\npayload = json.dumps({\n  \"apiVersion\": \"run.ai/v2alpha1\",\n  \"kind\": \"TrainingWorkload\",\n  \"metadata\": {\n    \"name\": \"train1\",\n    \"namespace\": \"runai-team-a\"\n  },\n  \"spec\": {\n    \"image\": {\n      \"value\": \"runai.jfrog.io/demo/quickstart\"\n    },\n    \"name\": {\n      \"value\": \"train1\"\n    },\n    \"gpu\": {\n      \"value\": \"1\"\n    }\n  }\n})\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;TOKEN&gt;' #(2)\n}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload) # (3)\n\nprint(json.dumps(json.loads(response.text), indent=4))\n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code>, <code>distributedworkloads</code>or <code>inferenceworkloads</code> according to type.</li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> <li>if you do not have a valid certificate, you can add the flag <code>verify=False</code>.</li> </ol>"},{"location":"developer/cluster-api/submit-yaml/","title":"Submitting Workloads via YAML","text":"<p>You can use YAML to submit Workloads directly to Run:ai. Below are examples of how to create training, interactive and inference workloads via YAML.</p> <p>For details on YAML parameters, see the YAML Reference.</p>"},{"location":"developer/cluster-api/submit-yaml/#submit-workload-example","title":"Submit Workload Example","text":"<p>Create a file named <code>training1.yaml</code> with the following text:</p> training1.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingWorkload # (1)\nmetadata:\n  name: job-1  # (2) \n  namespace: runai-team-a # (3)\nspec:\n  gpu:\n    value: \"1\"\n  image:\n    value: runai.jfrog.io/demo/quickstart\n  name:\n    value: job-1 # (4)\n</code></pre> <ol> <li>This is a Training workload.</li> <li>Kubernetes object name. Mandatory, but has no functional significance.</li> <li>Namespace: Replace <code>runai-team-a</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).</li> <li>Job name as appears in Run:ai. Can provide name, or create automatically if name prefix is configured. </li> </ol> <p>Change the namespace and run: <code>kubectl apply -f training1.yaml</code></p> <p>Run: <code>runai list jobs</code> to see the new Workload.</p>"},{"location":"developer/cluster-api/submit-yaml/#delete-workload-example","title":"Delete Workload Example","text":"<p>Run: <code>kubectl delete -f training1.yaml</code> to delete the Workload. </p>"},{"location":"developer/cluster-api/submit-yaml/#creating-a-yaml-syntax-from-a-cli-command","title":"Creating a YAML syntax from a CLI command","text":"<p>An easy way to create a YAML for a workload is to generate it from the <code>runai submit</code> command by using the <code>--dry-run</code> flag. For example, run:</p> <pre><code>runai submit build1 -i ubuntu -g 1 --interactive --dry-run \\\n     -- sleep infinity \n</code></pre> <p>The result will be the following Kubernetes object declaration:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: InteractiveWorkload  # (1)\nmetadata:\n  creationTimestamp: null\n  labels:\n    PreviousJob: \"true\"\n  name: job-0-2022-05-02t08-50-57\n  namespace: runai-team-a\nspec:\n  command:\n    value: sleep infinity\n  gpu:\n    value: \"1\"\n  image:\n    value: ubuntu\n  imagePullPolicy:\n    value: Always\n  name:\n    value: job-0\n\n... Additional internal and status properties...\n</code></pre> <ol> <li>This is an Interactive workload.</li> </ol>"},{"location":"developer/cluster-api/submit-yaml/#inference-workload-example","title":"Inference Workload Example","text":"<p>Creating an inference workload is similar to the above two examples.</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: InferenceWorkload\nmetadata:\n  name: inference1\n  namespace: runai-team-a\nspec:\n  name:\n    value: inference1\n  gpu:\n    value: \"0.5\"\n  image:\n    value: \"runai.jfrog.io/demo/example-triton-server\"\n  minScale:\n    value: 1\n  maxScale:\n    value: 2\n  metric:\n    value: concurrency # (1)\n  target:\n    value: 80  # (2)\n  ports:\n      items:\n        port1:\n          value:\n            container: 8000\n            protocol: http\n            serviceType: ServingPort\n</code></pre> <ol> <li>Possible metrics are throughput, concurrency and latency.</li> <li>Inference requires a port to receive requests.</li> </ol>"},{"location":"developer/cluster-api/submit-yaml/#suspendresume-interactivetraining-workload","title":"Suspend/Resume Interactive/Training Workload","text":"<p>To suspend training:</p> <p><pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingWorkload # \nmetadata:\n  name: job-1  #  \n  namespace: runai-team-a # \nspec:\n  gpu:\n    value: \"1\"\n  active:\n    value: false\n  image:\n    value: runai.jfrog.io/demo/quickstart\n  name:\n    value: job-1 # \n</code></pre> In order to suspend the workload, set <code>active</code> to <code>false</code>. To resume the workload, either set <code>active</code> to <code>true</code> or remove it entirely.</p>"},{"location":"developer/cluster-api/submit-yaml/#see-also","title":"See Also","text":"<ul> <li>To understand how to connect to the inference workload, see Inference Quickstart.</li> <li>To learn more about Inference and Run:ai see Inference overview.</li> </ul>"},{"location":"developer/cluster-api/workload-overview-dev/","title":"Cluster API (Deprecated)","text":"<p>The Run:ai Cluster API allows the submission of Workloads via YAML, directly to Kubernetes. </p> <p>Important</p> <p>With Run:ai 2.18 clusters, you can now submit Workloads via the Run:ai REST API. We recommend using this API if your cluster is of that version.  </p>"},{"location":"developer/cluster-api/workload-overview-dev/#workloads","title":"Workloads","text":"<p>Run:ai schedules Workloads. Run:ai workloads contain:</p> <ul> <li>The Kubernetes resource (Job, Deployment, etc) that is used to launch the container inside which the data science code runs.</li> <li>A set of additional resources that is required to run the Workload. Examples: a service entry point that allows access to the Job, a persistent volume claim to access data on the network and more.</li> </ul> <p>Run:ai supports the following Workloads types:</p> Workload Type Kubernetes Name Description Interactive <code>InteractiveWorkload</code> Submit an interactive workload Training <code>TrainingWorkload</code> Submit a training workload Distributed Training <code>DistributedWorkload</code> Submit a distributed training workload using TensorFlow, PyTorch or MPI Inference <code>InferenceWorkload</code> Submit an inference workload"},{"location":"developer/cluster-api/workload-overview-dev/#values","title":"Values","text":"<p>A Workload will typically have a list of values, such as name, image, and resources. A full list of values is available in the runai-submit Command-line reference.  </p> <p>You can also find the exact YAML syntax run:</p> <pre><code>kubectl explain TrainingWorkload.spec\n</code></pre> <p>(and similarly for other Workload types).</p> <p>To get information on a specific value (e.g. <code>node type</code>), you can also run:</p> <pre><code>kubectl explain TrainingWorkload.spec.nodeType\n</code></pre> <p>Result:</p> <pre><code>KIND:     TrainingWorkload\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: nodeType &lt;Object&gt;\n\nDESCRIPTION:\n     Specifies nodes (machines) or a group of nodes on which the workload will\n     run. To use this feature, your Administrator will need to label nodes as\n     explained in the Group Nodes guide at\n     https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag\n     can be used in conjunction with Project-based affinity. In this case, the\n     flag is used to refine the list of allowable node groups set in the\n     Project. For more information consult the Projects guide at\n     https://docs.run.ai/admin/admin-ui-setup/project-setup.\n\nFIELDS:\n   value    &lt;string&gt;\n</code></pre>"},{"location":"developer/cluster-api/workload-overview-dev/#how-to-submit","title":"How to Submit","text":"<p>A Workload can be submitted via various channels:</p> <ul> <li>The Run:ai user interface.</li> <li>The Run:ai command-line interface, via the runai submit command.</li> <li>The Run:ai Cluster API.</li> </ul>"},{"location":"developer/cluster-api/workload-overview-dev/#policies","title":"Policies","text":"<p>An Administrator can set Policies for Workload submission. Policies serve two purposes:</p> <ol> <li>To constrain the values a researcher can specify.</li> <li>To provide default values.</li> </ol> <p>For example, an administrator can,</p> <ul> <li>Set a maximum of 5 GPUs per Workload.</li> <li>Provide a default value of 1 GPU for each container.</li> </ul> <p>Each workload type has a matching kind of workload policy. For example, an <code>InteractiveWorkload</code> has a matching <code>InteractivePolicy</code></p> <p>A Policy of each type can be defined per-project. There is also a global policy that applies to any project that does not have a per-project policy.</p> <p>For further details on policies, see Policies.</p>"},{"location":"developer/metrics/metrics-api/","title":"Metrics via API","text":""},{"location":"developer/metrics/metrics-api/#what-are-metrics","title":"What are Metrics","text":"<p>Metrics are numeric measurements recorded over time that are emitted from the Run:ai cluster. Typical metrics involve utilization, allocation, time measurements and so on. Metrics are used in Run:ai dashboards as well as in the Run:ai administration user interface.</p> <p>The purpose of this document is to detail the structure and purpose of metrics emitted by Run:ai to enable customers to create custom dashboards or integrate metric data into other monitoring systems.</p> <p>Run:ai provides metrics via the Run:ai Control-plane API. In the past, Run:ai provided metrics information via direct access to an internal metrics store. This method is deprecated but is still documented here.</p>"},{"location":"developer/metrics/metrics-api/#metric-scopes","title":"Metric Scopes","text":"<p>Run:ai provides Control-plane API which supports and aggregates metrics at various levels.</p> Level Description Cluster Cluster is a set of Nodes Pools &amp; Nodes. With Cluster metrics, metrics are aggregated at the Cluster level Node Pool Metrics are aggregated at the Node Pool level. Workload Metrics are aggregated at the Workload level. Some Workloads, e.g. with distributed workloads, these metrics aggregate data from all worker pods Pod The basic execution unit"},{"location":"developer/metrics/metrics-api/#supported-metrics","title":"Supported Metrics","text":"Metric Cluster Node Pool Workload Pod API Cluster API Node Pool API Workload API Pod API ALLOCATED_GPU TRUE TRUE AVG_WORKLOAD_WAIT_TIME TRUE TRUE CPU_LIMIT_CORES TRUE CPU_MEMORY_LIMIT_BYTES TRUE CPU_MEMORY_REQUEST_BYTES TRUE CPU_MEMORY_USAGE_BYTES TRUE TRUE CPU_MEMORY_UTILIZATION TRUE TRUE CPU_REQUEST_CORES TRUE CPU_USAGE_CORES TRUE TRUE CPU_UTILIZATION TRUE TRUE GPU_ALLOCATION TRUE GPU_MEMORY_REQUEST_BYTES TRUE GPU_MEMORY_USAGE_BYTES TRUE TRUE GPU_MEMORY_USAGE_BYTES_PER_GPU TRUE GPU_MEMORY_UTILIZATION TRUE TRUE GPU_QUOTA TRUE TRUE GPU_UTILIZATION TRUE TRUE TRUE TRUE GPU_UTILIZATION_PER_GPU TRUE POD_COUNT TRUE RUNNING_POD_COUNT TRUE TOTAL_GPU TRUE TRUE"},{"location":"developer/metrics/metrics-api/#advanced-metrics","title":"Advanced Metrics","text":"<p>NVIDIA provides extended metrics at the Pod level. These are documented here. To enable these metrics please contact Run:ai customer support. </p> Metric Cluster Node Pool Workload Pod GPU_FP16_ENGINE_ACTIVITY_PER_GPU TRUE GPU_FP32_ENGINE_ACTIVITY_PER_GPU TRUE GPU_FP64_ENGINE_ACTIVITY_PER_GPU TRUE GPU_GRAPHICS_ENGINE_ACTIVITY_PER_GPU TRUE GPU_MEMORY_BANDWIDTH_UTILIZATION_PER_GPU TRUE GPU_NVLINK_RECEIVED_BANDWIDTH_PER_GPU TRUE GPU_NVLINK_TRANSMITTED_BANDWIDTH_PER_GPU TRUE GPU_PCIE_RECEIVED_BANDWIDTH_PER_GPU TRUE GPU_PCIE_TRANSMITTED_BANDWIDTH_PER_GPU TRUE GPU_SM_ACTIVITY_PER_GPU TRUE GPU_SM_OCCUPANCY_PER_GPU TRUE GPU_TENSOR_ACTIVITY_PER_GPU TRUE"},{"location":"developer/metrics/metrics/","title":"Metrics API","text":""},{"location":"developer/metrics/metrics/#what-are-metrics","title":"What are Metrics","text":"<p>Metrics are numeric measurements recorded over time that are emitted from the Run:ai cluster. Typical metrics involve utilization, allocation, time measurements and so on. Metrics are used in Run:ai dashboards as well as in the Run:ai administration user interface.</p> <p>The purpose of this document is to detail the structure and purpose of metrics emitted by Run:ai to enable customers to create custom dashboards or integrate metric data into other monitoring systems.</p> <p>Run:ai uses Prometheus for collecting and querying metrics.</p> <p>Warning</p> <p>From cluster version 2.17 and onwards, Run:ai supports metrics via the Run:ai Control-plane API. Direct metrics queries (metrics that are queried directly from Prometheus) are deprecated.</p>"},{"location":"developer/metrics/metrics/#published-runai-metrics","title":"Published Run:ai Metrics","text":"<p>Following is the list of published Run:ai metrics, per cluster version (make sure to pick the right cluster version in the picker at the top of the page):</p> Metric name Labels Measurement Description runai_active_job_cpu_requested_cores {clusterId,  job_name, job_uuid} CPU Cores Workload's requested CPU cores runai_active_job_memory_requested_bytes {clusterId,  job_name, job_uuid} Bytes Workload's requested CPU memory runai_cluster_cpu_utilization {clusterId} 0 to 1 CPU utilization of the entire cluster runai_cluster_memory_used_bytes {clusterId} Bytes Used CPU memory of the entire cluster runai_cluster_memory_utilization {clusterId} 0 to 1 CPU memory utilization of the entire cluster runai_allocated_gpu_count_per_gpu {gpu, clusterId, node} 0/1 Is a GPU hosting a pod runai_last_gpu_utilization_time_per_gpu {gpu, clusterId, node} Unix time Last time GPU was not idle runai_requested_gpu_memory_mb_per_workload {clusterId, job_type, job_uuid, job_name, project, workload_id} MegaBytes Requested GPU memory per workload (0 if not specified by the user) runai_requested_gpus_per_workload {clusterId, workload_type, workload_id, workload_name, project} Double Number of requested GPUs per workload runai_run_time_seconds_per_workload {clusterId, workload_id, workload_name} Seconds Total run time per workload runai_wait_time_seconds_per_workload {clusterId, workload_id, workload_name} Seconds Total wait time per workload runai_node_cpu_requested_cores {clusterId, node} Double Sum of the requested CPU cores of all workloads running in a node runai_node_cpu_utilization {clusterId, node} 0 to 1 CPU utilization per node runai_node_memory_utilization {clusterId, node} 0 to 1 CPU memory utilization per node runai_node_requested_memory_bytes {clusterId, node} Bytes Sum of the requested CPU memory of all workloads running in a node runai_node_used_memory_bytes {clusterId, node} Bytes Used CPU memory per node runai_project_guaranteed_gpus {clusterId, project} Double Guaranteed GPU quota per project runai_project_info {memory_quota, cpu_quota, gpu_guaranteed_quota, clusterId, project, department} N/A Information on CPU, CPU memory, GPU quota per project runai_queue_info {memory_quota, cpu_quota, gpu_guaranteed_quota, clusterId, nodepool, queue_name, department} N/A Information on CPU, CPU memory, GPU quota per project/department per nodepool runai_cpu_limits_per_active_workload {clusterId, job_name , job_uuid} CPU Cores Workloads CPU limit (in number of cores). See link runai_job_cpu_usage {clusterId, workload_id, workload_name, project} Double Workloads CPU usage (in number of cores) runai_memory_limits_per_active_workload {clusterId, job_name, job_uuid} Bytes Workloads CPU memory limit. See link runai_active_job_memory_requested_bytes {clusterId, job_name, job_uuid} Bytes Workloads requested CPU memory. See link runai_job_memory_used_bytes {clusterId, workload_id, workload_name, project} Bytes Workloads used CPU memory runai_mig_mode_gpu_count {clusterId, node} Double Number of GPUs on MIG nodes runai_gpu_utilization_per_gpu {clusterId, gpu, node} % GPU Utilization per GPU runai_gpu_utilization_per_node {clusterId, node} % GPU Utilization per Node runai_gpu_memory_used_mebibytes_per_gpu {clusterId, gpu, node} MiB Used GPU memory per GPU runai_gpu_memory_used_mebibytes_per_node {clusterId, node} MiB Used GPU memory per Node runai_gpu_memory_total_mebibytes_per_gpu {clusterId, gpu, node} MiB Total GPU memory per GPU runai_gpu_memory_total_mebibytes_per_node {clusterId, node} MiB Total GPU memory per Node runai_gpu_count_per_node {clusterId, node, modelName, ready, schedulable} Number Number of GPUs per Node runai_allocated_gpu_count_per_workload {clusterId, workload_id, workload_name, workload_type, user} Double Number of allocated GPUs per Workload runai_allocated_gpu_count_per_project {clusterId, project} Double Number of allocated GPUs per Project runai_gpu_memory_used_mebibytes_per_pod_per_gpu {clusterId, pod_name, pod_uuid, pod_namespace, node, gpu} MiB Used GPU Memory per Pod, per Gpu on which the workload is running runai_gpu_memory_used_mebibytes_per_workload {clusterId, workload_id, workload_name, workload_type, user} MiB Used GPU Memory per Workload runai_gpu_utilization_per_pod_per_gpu {clusterId, pod_name, pod_uuid, pod_namespace, node, gpu} % GPU Utilization per Pod per GPU runai_gpu_utilization_per_workload {clusterId, workload_id, workload_name, workload_type, user} % Average GPU Utilization per Workload runai_gpu_utilization_per_project {clusterId, project} % Average GPU Utilization per Project runai_last_gpu_utilization_time_per_workload {clusterId, workload_id, workload_name, workload_type, user} Seconds (Unix Timestamp) The Last Time (Unix Timestamp) That The Workload Utilized Any Of Its Allocated GPUs runai_gpu_idle_seconds_per_workload {clusterId, workload_id, workload_name, workload_type, user} Seconds Seconds Passed Since The Workload Utilized Any Of Its Allocated GPUs runai_allocated_gpu_count_per_pod {clusterId, pod_name, pod_uuid, pod_namespace, node} Double Number Of Allocated GPUs per Pod runai_allocated_gpu_count_per_node {clusterId, node} Double Number Of Allocated GPUs per Node runai_allocated_millicpus_per_pod {clusterId, pod_name, pod_uuid, pod_namespace, node} Integer Number Of Allocated Millicpus per Pod runai_allocated_memory_per_pod {clusterId, pod_name, pod_uuid, pod_namespace, node} Bytes Allocated Memory per Pod <p>Following is a list of labels appearing in Run:ai metrics:</p> Label Description clusterId Cluster Identifier department Name of Run:ai Department cpu_quota CPU limit per project gpu GPU index gpu_guaranteed_quota Guaranteed GPU quota per project image Name of Docker image namespace_name Namespace deployment_name Deployment name job_name Job name job_type Job type: training, interactive or inference job_uuid Job identifier workload_name Workload name workload_type Workload type: training, interactive or inference workload_uuid Workload identifier pod_name Pod name. A Workload can contain many pods. pod_namespace Pod namespace memory_quota CPU memory limit per project node Node name project Name of Run:ai Project status Workload status: Running, Pending, etc. For more information on Workload statuses see document user User identifier"},{"location":"developer/metrics/metrics/#other-metrics","title":"Other Metrics","text":"<p>Run:ai exports other metrics emitted by NVIDIA and Kubernetes packages, as follows:</p> Metric name Description runai_gpu_utilization_per_gpu GPU utilization kube_node_status_capacity The capacity for different resources of a node kube_node_status_condition The condition of a cluster node kube_pod_container_resource_requests_cpu_cores The number of CPU cores requested by container kube_pod_container_resource_requests_memory_bytes Bytes of memory requested by a container kube_pod_info Information about pod <p>For additional information, see Kubernetes kube-state-metrics and NVIDIA dcgm exporter.</p>"},{"location":"developer/metrics/metrics/#changed-metrics-and-api-mapping","title":"Changed metrics and API mapping","text":"<p>Starting in cluster version 2.17, some of the metrics names have been changed. In addition some Run:ai metrics are available as API endpoints. Using the API endpoints is more efficient and provides an easier way of retrieving metrics in any application. The following table lists the metrics that were changed.</p> Metric name in version 2.16 2.17 Change Description 2.17 API Endpoint runai_active_job_cpu_requested_cores available also via API https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_REQUEST_CORES\" metricType runai_active_job_memory_requested_bytes available also via API https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_MEMORY_REQUEST_BYTES\" metricType runai_cluster_cpu_utilization available also via API https://app.run.ai/api/v2/clusters/{clusterUuid}/metrics ; with \"CPU_UTILIZATION\" metricType runai_cluster_memory_utilization available also via API https://app.run.ai/api/v2/clusters/{clusterUuid}/metrics ; with \"CPU_MEMORY_UTILIZATION\" metricType runai_gpu_utilization_non_fractional_jobs no longer available runai_allocated_gpu_count_per_workload labels changed runai_gpu_utilization_per_pod_per_gpu available also via API https://app.run.ai/api/v1/workloads/{workloadId}/pods/{podId}/metrics ; with \"GPU_UTILIZATION_PER_GPU\" metricType runai_gpu_utilization_per_workload available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"GPU_UTILIZATION\" metricType runai_job_image no longer available runai_job_requested_gpu_memory available also via API and renamed to: \"runai_requested_gpu_memory_mb_per_workload\" with different labels https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"GPU_MEMORY_REQUEST_BYTES\" metricType runai_job_requested_gpus renamed to: \"runai_requested_gpus_per_workload\" with different labels runai_job_total_runtime renamed to: \"runai_run_time_seconds_per_workload\" with different labels runai_job_total_wait_time renamed to: \"runai_wait_time_seconds_per_workload\" with different labels runai_gpu_memory_used_mebibytes_per_workload available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"GPU_MEMORY_USAGE_BYTES\" metricType runai_gpu_memory_used_mebibytes_per_pod_per_gpu available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/pods/{podId}/metrics ; with \"GPU_MEMORY_USAGE_BYTES_PER_GPU\" metricType runai_node_gpu_used_memory_bytes renamed and changed units: \"runai_gpu_memory_used_mebibytes_per_node\" runai_node_total_memory_bytes renamed and changed units: \"runai_gpu_memory_total_mebibytes_per_node\" runai_project_info labels changed runai_active_job_cpu_limits available also via API and renamed to: \"runai_cpu_limits_per_active_workload\" https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_LIMIT_CORES\" metricType runai_job_cpu_usage available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_USAGE_CORES\" metricType runai_active_job_memory_limits available also via API and renamed to: \"runai_memory_limits_per_active_workload\" https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_MEMORY_LIMIT_BYTES\" metricType runai_running_job_memory_requested_bytes was a duplication of \"runai_active_job_memory_requested_bytes\", see above runai_job_memory_used_bytes available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_MEMORY_USAGE_BYTES\" metricType runai_job_swap_memory_used_bytes no longer available runai_gpu_count_per_node added labels runai_last_gpu_utilization_time_per_workload labels changed runai_gpu_idle_time_per_workload renamed to: \"runai_gpu_idle_seconds_per_workload\" with different labels"},{"location":"developer/metrics/metrics/#create-custom-dashboards","title":"Create custom dashboards","text":"<p>To create custom dashboards based on the above metrics, please contact Run:ai customer support.</p>"},{"location":"home/components/","title":"Run:ai System Components","text":""},{"location":"home/components/#components","title":"Components","text":"<p>Run:ai is made up of two components:</p> <ul> <li>The Run:ai cluster provides scheduling services and workload management.</li> <li>The Run:ai control plane provides resource management, Workload submission and cluster monitoring.</li> </ul> <p>Technology-wise, both are installed over a Kubernetes Cluster.</p> <p>Run:ai users:</p> <ul> <li>Researchers submit Machine Learning workloads via the Run:ai Console, the Run:ai Command-Line Interface (CLI), or directly by sending YAML files to Kubernetes.</li> <li>Administrators monitor and set priorities via the Run:ai User Interface</li> </ul> <p></p>"},{"location":"home/components/#runai-cluster","title":"Run:ai Cluster","text":"<ul> <li>Run:ai comes with its own Scheduler. The Run:ai scheduler extends the Kubernetes scheduler. It uses business rules to schedule workloads sent by Researchers.</li> <li>Run:ai schedules Workloads. Workloads include the actual researcher code running as a Kubernetes container, together with all the system resources required to run the code, such as user storage, network endpoints to access the container etc.</li> <li>The cluster uses an outbound-only, secure connection to synchronize with the Run:ai control plane. Information includes meta-data sync and various metrics on Workloads, Nodes etc.</li> <li>The Run:ai cluster is installed as a Kubernetes Operator</li> <li>Run:ai is installed in its own Kubernetes namespace named runai</li> <li>Workloads are run in the context of Run:ai Projects. Each Project is mapped to a Kubernetes namespace with its own settings and access control.</li> </ul>"},{"location":"home/components/#runai-control-plane-on-the-cloud","title":"Run:ai Control Plane on the cloud","text":"<p>The Run:ai control plane is used by multiple customers (tenants) to manage resources (such as Projects &amp; Departments), submit Workloads and monitor multiple clusters.</p> <p>A single Run:ai customer (tenant) defined in the control-plane, can manage multiple Run:ai clusters. So a single customer, can manage mutltiple GPU clusters in multiple locations/subnets from a single interface.</p>"},{"location":"home/components/#self-hosted-control-plane","title":"Self-hosted Control-Plane","text":"<p>The Run:ai control plane can also be locally installed. To understand the various installation options see the installation types document.</p>"},{"location":"home/data-privacy-details/","title":"Data Privacy","text":"<p>This article details the data privacy and compliance considerations for deploying Run:ai. It is intended to help administrators and compliance teams understand the data management practices involved with Run:ai. This ensures the permissions align with organizational policies and regulatory requirements before installation and during integration and onboarding of the various teams.</p> <p>When using the Run:ai SaaS cluster, the Control plane operates through the Run:ai cloud, requiring the transmission of certain data for control and analytics. Below is a detailed breakdown of the specific data sent to the Run:ai cloud in the SaaS offering.</p> <p>Note</p> <p>For organizations where data privacy policies do not align with this data transmission, Run:ai offers a self-hosted version. This version includes the control plane on premise and does not communicate with the cloud.</p>"},{"location":"home/data-privacy-details/#data-sent-to-the-runai-cloud","title":"Data sent to the Run:ai cloud","text":"Asset Details Workload Metrics Includes workload names, CPU, GPU, and memory metrics, as well as parameters provided during the <code>runai submit</code> command. Workload Assets Covers environments, compute resources, and data resources associated with workloads. Resource Credentials Credentials for cluster resources, encrypted with a SHA-512 algorithm specific to each tenant. Node Metrics Node-specific data including names, IPs, and performance metrics (CPU, GPU, memory). Cluster Metrics Cluster-wide metrics such as names, CPU, GPU, and memory usage. Projects &amp; Departments Includes names and quota information for projects and departments. Users User roles within Run:ai, email addresses, and passwords."},{"location":"home/data-privacy-details/#key-consideration","title":"Key consideration","text":"<p>Run:ai ensures that no deep-learning artefacts, such as code, images, container logs, training data, models, or checkpoints, are transmitted to the cloud. These assets remain securely within your organization's firewalls, safeguarding sensitive intellectual property and data.  </p>"},{"location":"home/data-privacy-details/#see-also","title":"See Also","text":"<p>The Run:ai privacy policy. </p>"},{"location":"home/overview/","title":"Run:ai Documentation Library","text":"<p>Welcome to the Run:ai documentation area. For an introduction about what is the Run:ai Platform see Run:ai platform on the run.ai website.</p> <p>The Run:ai documentation is targeting four personas:</p> <ul> <li> <p>Infrastructure Administrator - An IT person, responsible for the installation, setup and IT maintenance of the Run:ai product. Infrastructure Administrator documentation can be found here.</p> </li> <li> <p>Platform Administrator - Responsible for the day-to-day administration of the product. Platform Administrator documentation can be found here.</p> </li> <li> <p>Researcher \u2014 Using Run:ai to spin up notebooks, submit Workloads, prompt models, etc. Researcher documentation can be found here.</p> </li> <li> <p>Developer \u2014 Using various APIs to manipulate Workloads, deploy models, and integrate with other systems. Developer documentation can be found here.</p> </li> </ul>"},{"location":"home/overview/#how-to-get-support","title":"How to Get Support","text":"<p>To get support use the following channels:</p> <ul> <li> <p>On the Run:ai user interface at <code>&lt;company-name&gt;.run.ai</code>, use the 'Contact Support' link on the top right.</p> </li> <li> <p>Or submit a ticket by clicking the button below:</p> </li> </ul> <p>Submit a Ticket</p>"},{"location":"home/overview/#community","title":"Community","text":"<p>Run:ai provides its customers with access to the Run:ai Customer _Community portal to submit tickets, track ticket progress and update support cases.</p> <p>Customer Community Portal</p> <p>Reach out to customer support for credentials.</p>"},{"location":"home/overview/#runai-cloud-status-page","title":"Run:ai Cloud Status Page","text":"<p>Run:ai cloud availability is monitored at status.run.ai.</p>"},{"location":"home/overview/#collect-logs-to-send-to-support","title":"Collect Logs to Send to Support","text":"<p>As an IT Administrator, you can collect Run:ai logs to send to support. For more information see logs collection.</p>"},{"location":"home/overview/#example-code","title":"Example Code","text":"<p>Code for the Docker images referred to on this site is available at https://github.com/run-ai/docs/tree/master/quickstart.</p> <p>The following images are used throughout the documentation:</p> Image Description Source runai.jfrog.io/demo/quickstart Basic training image. Multi-GPU support https://github.com/run-ai/docs/tree/master/quickstart/main runai.jfrog.io/demo/quickstart-distributed Distributed training using MPI and Horovod https://github.com/run-ai/docs/tree/master/quickstart/distributed zembutsu/docker-sample-nginx Build (interactive) with Connected Ports https://github.com/zembutsu/docker-sample-nginx runai.jfrog.io/demo/quickstart-x-forwarding Use X11 forwarding from Docker image https://github.com/run-ai/docs/tree/master/quickstart/x-forwarding runai.jfrog.io/demo/pycharm-demo Image used for tool integration (PyCharm and VSCode) https://github.com/run-ai/docs/tree/master/quickstart/python%2Bssh runai.jfrog.io/demo/example-triton-client and  runai.jfrog.io/demo/example-triton-server Basic Inference https://github.com/run-ai/models/tree/main/models/triton"},{"location":"home/overview/#contributing-to-the-documentation","title":"Contributing to the documentation","text":"<p>This documentation is made better by individuals from our customer and partner community. If you see something worth fixing, please comment at the bottom of the page or create a pull request via GitHub. The public GitHub repository can be found on the top-right of this page. </p>"},{"location":"home/product-support-policy/","title":"Product Support Policy","text":"<p>The product support levels for the Run:ai software are as follows:</p> Critical Bug Fixes Important Bug Fixes Full support V V Extended support V - End of support - - <ul> <li>Full support period: 12 months from the release date of a Major Version.</li> <li>Extended support period: 6 months after the end of the full support period.</li> <li>End of support: 18 months from the release date of a Major Version</li> </ul> <p>For specific versions, see Product Version Lifecycle.</p> <p>Notes:</p> <ol> <li>Run:ai may extend the support periods and/or otherwise amend this Product Support Levels Policy from time to time at its own discretion.</li> <li>Versioning: Run:ai versioning follows Semantic Version (SemVer) numbering scheme, \u201cMa.Mi.Pa\u201d, where:<ul> <li>Ma.Mi is a major version that contains new features, bug fixes and security updates (\u201cMajor Version\u201d).</li> <li>Pa is a patch level version that is focused on bug fixes and security updates. Run:ai version release dates are listed in theRun:ai product documentation.</li> </ul> </li> <li> <p>Critical Bug: a bug that represents a severity 1 support ticket, as listed in the Run:ai support agreement. Important bug:  a bug that represents a severity 1 or severity 2 support ticket, as listed in the Run:ai support agreement.</p> </li> <li> <p>Run:ai is built from 3 components: Run:ai Control Plane, Run:ai Cluster and Run:ai Command Line Inference (CLI). For full details about Run:ai system components see: https://docs.run.ai/latest/home/components.</p> </li> <li> <p>Run:ai Control Plane, Run:ai Cluster &amp; Run:ai CLI are always released together with the same version number.</p> </li> <li> <p>A supported Run:ai environment is built from:</p> <ul> <li> <p>Run:ai Control Plane of a version equal to or greater than the versions of each of the Run:ai Clusters.</p> </li> <li> <p>Run:ai CLI of a version equal to the version on the Run:ai Cluster.</p> </li> </ul> </li> <li> <p>From time to time, Run:ai may provide API deprecation notices under the product     documentation of the applicable Run:ai version. For full details about Run:ai API deprecation notice and support policy see: https://docs.run.ai/latest/developer/overview-developer/#api-support. </p> </li> </ol> <p>Last update: Aug 6 2024</p>"},{"location":"home/version-lifecycle/","title":"Product Version Lifecycle","text":""},{"location":"home/version-lifecycle/#overview","title":"Overview","text":"<p>Run:ai follows a structured product lifecycle to ensure customers receive regular updates, security patches, and long-term stability for their deployments.  This document outlines the support phases for each release version, including the timelines for Full Support and Extended Support.</p> <ul> <li>Full Support - Each version receives critical and important bug fixes.</li> <li>Extended Support - After the full support period ends, the version continues to receive critical bug fixes.</li> <li>End of Support - Once a version reaches this stage, it will no longer receive updates, and customers are encouraged to upgrade to a supported version.</li> </ul>"},{"location":"home/version-lifecycle/#versions","title":"Versions","text":"<p>The table below details the supported versions of Run:ai, along with their respective support timelines.  If your deployment is approaching the end of support, we recommend planning an upgrade to ensure continued security and functionality.</p> Version Release Date End of Full Support End of Extended Support 2.16 25 January 2024 25 January 2025 25 July 2025 2.17 14 April 2024 14 April 2025 14 October 2025 2.18 21 July 2024 21 July 2025 21 January 2026 2.19 8 October 2024 8 October 2025 8 April 2026 2.20 12 January 2025 12 January 2026 13 July 2026"},{"location":"home/whats-new-2-13/","title":"Run:ai version 2.13","text":""},{"location":"home/whats-new-2-13/#version-2137","title":"Version 2.13.7","text":""},{"location":"home/whats-new-2-13/#release-date","title":"Release date","text":"<p>July 2023</p>"},{"location":"home/whats-new-2-13/#release-content","title":"Release content","text":"<ul> <li>Added filters to the historic quota ratio widget on the Quota management dashboard.</li> </ul>"},{"location":"home/whats-new-2-13/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-11080 Fixed an issue in OpenShift environments where log in via SSO with the <code>kubeadmin</code> user, gets blank pages for every page. RUN-11119 Fixed an issue where values that should be the Order of priority column are in the wrong column. RUN-11120 Fixed an issue where the Projects table does not show correct metrics when Run:ai version 2.13 is paired with a Run:ai 2.8 cluster. RUN-11121 Fixed an issue where the wrong over quota memory alert is shown in the Quota management pane in project edit form. RUN-11272 Fixed an issue in OpenShift environments where the selection in the cluster drop down in the main UI does not match the cluster selected on the login page."},{"location":"home/whats-new-2-13/#version-2134","title":"Version 2.13.4","text":""},{"location":"home/whats-new-2-13/#release-date_1","title":"Release date","text":"<p>July 2023</p>"},{"location":"home/whats-new-2-13/#fixed-issues_1","title":"Fixed issues","text":"Internal ID Description RUN-11089 Fixed an issue when creating an environment, commands in the Runtime settings pane and are not persistent and cannot be found in other assets (for example in a new Training)."},{"location":"home/whats-new-2-13/#version-2131","title":"Version 2.13.1","text":""},{"location":"home/whats-new-2-13/#release-date_2","title":"Release date","text":"<p>July 2023</p>"},{"location":"home/whats-new-2-13/#release-content_1","title":"Release content","text":"<ul> <li>Made an improvement so that occurrences of labels that are not in use anymore are deleted.</li> </ul>"},{"location":"home/whats-new-2-13/#fixed-issues_2","title":"Fixed issues","text":"<p>N/A</p>"},{"location":"home/whats-new-2-13/#version-2130","title":"Version 2.13.0","text":""},{"location":"home/whats-new-2-13/#release-content_2","title":"Release content","text":"<p>This version contains features and fixes from previous versions starting with 2.9. Refer to the prior versions for specific features and fixes. </p> <p>Projects</p> <ul> <li>Improved the Projects UI for ease of use. Projects follows UI upgrades and changes that are designed to make setting up of components and assets easier for administrators and researchers. To configure a project, see Projects.</li> </ul> <p>Dashboards</p> <ul> <li> <p>Added a new dashboard for Quota management, which provides an efficient means to monitor and manage resource utilization within the AI cluster. The dashboard filters the display of resource quotas based on Departments, Projects, and Node pools. For more information, see Quota management dashboard.</p> </li> <li> <p>Added to the Overview dashboard, the ability to filter the cluster by one or more node pools. For more information, see Node pools.</p> </li> </ul> <p>Nodes and Node pools</p> <ul> <li> <p>Run:ai scheduler supports 2 scheduling strategies: Bin Packing (default) and Spread. For more information, see Scheduling strategies. You can configure the scheduling strategy in the node pool level to improve the support of clusters with mixed types of resources and workloads. For configuration information, see Creating new node pools.</p> </li> <li> <p>GPU device level DCGM Metrics are collected per GPU and presented by Run:ai in the Nodes table. Each node contains a list of its embedded GPUs with their respective DCGM metrics. See DCGM Metrics for the list of metrics which are provided by NVidia DCGM and collected by Run:ai. Contact your Run:ai customer representative to enable this feature.</p> </li> </ul> <ul> <li>Added per node pool over-quota priority. Over-quota priority sets the relative amount of additional unused resources that an asset can get above its current quota. For more information, see Over-quota priority.</li> </ul> <ul> <li>Added support of associating workspaces to node pool. The association between workspaces and node pools is done using Compute resources section. In order to associate a compute resource to a node pool, in the Compute resource section, press More settings. Press Add new to add more node pools to the configuration. Drag and drop the node pools to set their priority.</li> </ul> <ul> <li>Added Node pool selection as part of the workload submission form. This allows researchers to quickly determine the list of node pools available and their priority. Priority is set by dragging and dropping them in the desired order of priority. In addition, when the node pool priority list is locked by a policy, the list isn't editable by the Researcher even if the workspace is created from a template or copied from another workspace.</li> </ul> <p>Time limit duration</p> <ul> <li> <p>Improved the behavior of any workload time limit (for example, Idle time limit) so that the time limit will affect existing workloads that were created before the time limit was configured. This is an optional feature which provides help in handling situations where researchers leave sessions open even when they do not need to access the resources. For more information, see Limit duration of interactive training jobs.</p> </li> <li> <p>Improved workspaces time limits. Workspaces that reach a time limit will now transition to a state of <code>stopped</code> so that they can be reactivated later.</p> </li> <li> <p>Added time limits for training jobs per project. Administrators (Department Admin, Editor) can limit the duration of Run:ai Training jobs per Project using a specified time limit value. This capability can assist administrators to limit the duration and resources consumed over time by training jobs in specific projects. Each training job that reaches this duration will be terminated.</p> </li> </ul> <p>Workload assets</p> <ul> <li>Extended the collaboration functionality for any workload asset such as Environment, Compute resource, and some Data source types. These assets are now shared with Departments in the organization in addition to being shared with specific projects, or the entire cluster.</li> </ul> <ul> <li>Added a search box for card galleries in any asset based workload creation form to provide an easy way to search for assets and resources. To filter use the asset name or one of the field values of the card.</li> </ul> <p>PVC data sources</p> <ul> <li>Added support for PVC block storage in the New data source form. In the New data source form for a new PVC data source, in the Volume mode field, select from Filesystem or Block. For more information, see Create a PVC data source.</li> </ul> <p>Credentials</p> <ul> <li>Added Docker registry to the Credentials menu. Users can create docker credentials for use in specific projects for image pulling. To configure credentials, see Configuring credentials.</li> </ul> <p>Policies</p> <ul> <li>Improved policy support by adding <code>DEFAULTS</code> in the <code>items</code> section in the policy. The <code>DEFAULTS</code> section sets the default behavior for items declared in this section. For example, this can be use to limit the submission of workloads only to existing PVCs. For more information and an example, see Policies, Complex values.</li> </ul> <ul> <li>Added support for making a PVC data source available to all projects. In the New data source form, when creating a new PVC data source, select All from the Project pane.</li> </ul> <p>Researcher API</p> <ul> <li>Extended researcher's API to allow stopping and starting of workloads using the API. For more information, see Submitting Workloads via HTTP/REST.</li> </ul> <p>Integrations</p> <ul> <li>Added support for Spark and Elastic jobs. For more information, see Running Spark jobs with Run:ai.</li> </ul> <ul> <li> <p>Added support for Ray jobs. Ray is an open-source unified framework for scaling AI and Python applications. For more information, see Integrate Run:ai with Ray.</p> </li> <li> <p>Added integration with Weights &amp; Biases Sweep to allow data scientists to submit hyperparameter optimization workloads directly from the Run:ai UI. To configure sweep, see Sweep configuration.</p> </li> </ul> <ul> <li>Added support for XGBoost. XGBoost, which stands for Extreme Gradient Boosting, is a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library. It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems. For more information, see runai submit-dist xgboost</li> </ul> <p>Compatability</p> <ul> <li>Added support for multiple OpenShift clusters. For configuration information, see Installing additional Clusters.</li> </ul>"},{"location":"home/whats-new-2-13/#installation","title":"Installation","text":"<ul> <li>The manual process of upgrading Kubernetes CRDs is no longer needed when upgrading to the most recent version (2.13) of Run:ai.</li> <li>From Run:ai 2.12 and above, the control-plane installation has been simplified and no longer requires the creation of a backend values file. Instead, install directly using <code>helm</code> as described in Install the Run:ai Control Plane.  </li> <li>From Run:ai 2.12 and above, the air-gapped, control-plane installation now generates a <code>custom-env.yaml</code> values file during the preparation stage. This is used when installing the control-plane.</li> </ul>"},{"location":"home/whats-new-2-13/#known-issues","title":"Known issues","text":"Internal ID Description RUN-11005 Incorrect error messages when trying to run <code>runai</code> CLI commands in an OpenShift environment. RUN-11009 Incorrect error message when a user without permissions to tries to delete another user."},{"location":"home/whats-new-2-13/#fixed-issues_3","title":"Fixed issues","text":"Internal ID Description RUN-9039 Fixed an issue where in the new job screen, after toggling off the preemptible flag, and a job is submitted, the job still shows as preemptible. RUN-9323 Fixed an issue with a non-scaleable error message when scheduling hundreds of nodes is not successful. RUN-9324 Fixed an issue where the scheduler did not take into consideration the amount of storage so there is no explanation that pvc is not ready. RUN-9902 Fixed an issue in OpenShift environments, where there are no metrics in the dashboard because Prometheus doesn\u2019t have permissions to monitor the <code>runai</code> namespace after an installation or upgrade to 2.9. RUN-9920 Fixed an issue where the <code>canEdit</code> key in a policy is not validated properly for itemized fields when configuring an interactive policy. RUN-10052 Fixed an issue when loading a new job from a template gives an error until there are changes made on the form. RUN-10053 Fixed an issue where the Node pool column is unsearchable in the job list. RUN-10422 Fixed an issue where node details show running workloads that were actually finished (successfully/failed/etc.). RUN-10500 Fixed an issue where jobs are shown as running even though they don't exist in the cluster. RUN-10813 Fixed an issue in adding a <code>data source</code> where the path is case sensitive and didn't allow uppercase."},{"location":"home/whats-new-2-15/","title":"What's New 2.15 - December 3, 2023","text":""},{"location":"home/whats-new-2-15/#release-content","title":"Release Content","text":""},{"location":"home/whats-new-2-15/#researcher","title":"Researcher","text":""},{"location":"home/whats-new-2-15/#jobs-workloads-trainings-and-workspaces","title":"Jobs, Workloads, Trainings, and Workspaces","text":"<ul> <li> <p>Added support to run distributed workloads via the training view in the UI. You can configure distributed training on the following:</p> <ul> <li>Trainings form</li> <li>Environments form</li> </ul> <p>You can select <code>single</code> or <code>multi-node (distributed)</code> training. When configuring distributed training, you will need to select a framework from the list. Supported frameworks now include:</p> <ul> <li>PyTorch</li> <li>Tensorflow</li> <li>XGBoost</li> <li>MPI</li> </ul> <p>For Trainings configuration, see Adding trainings. See your Run:ai representative to enable this feature. For Environments configuration, see Creating an Environment.</p> </li> <li> <p>Preview the new Workloads view. Workloads is a new view for jobs that are running in the AI cluster. The Workloads view provides a more advanced UI than the previous Jobs UI. The new table format provides:</p> <ul> <li>Improved views of the data</li> <li>Improved filters and search</li> <li>More information</li> </ul> <p>Use the toggle at the top of the Jobs page to switch to the Workloads view. For more information.</p> </li> <li> <p>Improved support for Kubeflow Notebooks. Run:ai now supports the scheduling of Kubeflow notebooks with fractional GPUs. Kubeflow notebooks are identified automatically and appear with a dedicated icon in the Jobs UI.</p> </li> <li>Improved the Trainings and Workspaces forms. Now the runtime field for Command and Arguments can be edited directly in the new Workspace or Training creation form.</li> <li>Added new functionality to the Run:ai CLI that allows submitting a workload with multiple service types at the same time in a CSV style format. Both the CLI and the UI now offer the same functionality. For more information, see runai submit.</li> <li>Improved functionality in the <code>runai submit</code> command so that the port for the container is specified using the <code>nodeport</code> flag. For more information, see <code>runai submit</code> --service-type <code>nodeport</code>.</li> </ul>"},{"location":"home/whats-new-2-15/#credentials","title":"Credentials","text":"<ul> <li>Improved Credentials creation. A Run:ai scope can now be added to credentials. For more information, see Credentials.</li> </ul>"},{"location":"home/whats-new-2-15/#environments","title":"Environments","text":"<ul> <li>Added support for workload types when creating a new or editing existing environments. Select from <code>single-node</code> or <code>multi-node (distributed)</code> workloads. The environment is available only on feature forms which are relevant to the workload type selected.</li> </ul>"},{"location":"home/whats-new-2-15/#volumes-and-storage","title":"Volumes and Storage","text":"<ul> <li>Added support for Ephemeral volumes in Workspaces. Ephemeral storage is temporary storage that gets wiped out and lost when the workspace is deleted. Adding Ephemeral storage to a workspace ties that storage to the lifecycle of the Workspace to which it was added. Ephemeral storage is added to the Workspace configuration form in the Volume pane. For configuration information, see Create a new workspace.</li> </ul>"},{"location":"home/whats-new-2-15/#templates","title":"Templates","text":"<ul> <li>Added support for Run:ai a Scope in the template form. For configuration information, see Creating templates.</li> </ul>"},{"location":"home/whats-new-2-15/#deployments","title":"Deployments","text":"<ul> <li>Improvements in the New Deployment form include:<ul> <li>Support for Tolerations. Tolerations guide the system to which node each pod can be scheduled to or evicted by matching between rules and taints defined for each Kubernetes node.</li> <li>Support for Multi-Process Service (MPS). MPS is a service which allows the running of parallel processes on the same GPU, which are all run by the same userid. To enable MPS support, use the toggle switch on the Deployments form.</li> </ul> <p>Note</p> <p>If you do not use the same userid, the processes will run in serial and could possibly degrade performance.</p> </li> </ul>"},{"location":"home/whats-new-2-15/#auto-delete-jobs","title":"Auto Delete Jobs","text":"<ul> <li>Added new functionality to the UI and CLI that provides configuration options to automatically delete jobs after a specified amount of time upon completion. Auto-deletion provides more efficient use of resources and makes it easier for researchers to manage their jobs. For more configuration options in the UI, see Auto deletion (Step 9) in Create a new workspace. For more information on the CLI flag, see --auto-deletion-time-after-completion.</li> </ul>"},{"location":"home/whats-new-2-15/#runai-administrator","title":"Run:ai Administrator","text":""},{"location":"home/whats-new-2-15/#authorization","title":"Authorization","text":"<ul> <li>Run:ai has now revised and updated the Role Based Access Control (RBAC) mechanism, expanding the scope of Kubernetes. Using the new RBAC mechanism makes it easier for administrators to manage access policies across multiple clusters and to define specific access rules over specific scopes for specific users and groups. Along with the revised RBAC mechanism, new user interface views are introduced to support the management of users, groups, and access rules. For more information, see Role based access control.</li> </ul>"},{"location":"home/whats-new-2-15/#policies","title":"Policies","text":"<ul> <li>During Workspaces and Training creation, assets that do not comply with policies cannot be selected. These assets are greyed out and have a button on the cards when the item does not comply with a configured policy. The button displays information about which policies are non-compliant.</li> <li>Added configuration options to Policies in order to prevent the submission of workloads that use data sources of type <code>host path</code>. This prevents data from being stored on the node, so that data is not lost when a node is deleted. For configuration information, see Prevent Data Storage on the Node.</li> <li>Improved flexibility when creating policies which provide the ability to allocate a <code>min</code> and a <code>max</code> value for CPU and GPU memory. For configuration information, see GPU and CPU memory limits in Configuring policies.</li> </ul>"},{"location":"home/whats-new-2-15/#nodes-and-node-pools","title":"Nodes and Node Pools","text":"<ul> <li>Node pools are now enabled by default. There is no need to enable the feature in the settings.</li> </ul>"},{"location":"home/whats-new-2-15/#quotas-and-over-quota","title":"Quotas and Over-Quota","text":"<ul> <li>Improved control over how over-quota is managed by adding the ability to block over-subscription of the quota in Projects or Departments. For more information, see Limit Over-Quota.</li> <li>Improved the scheduler fairness for departments using the <code>over quota priority</code> switch (in Settings). When the feature flag is disabled, over-quota weights are equal to the deserved quota and any excess resources are divided in the same proportion as the in-quota resources. For more information, see Over Quota Priority.</li> <li>Added new functionality to always guarantee in-quota workloads at the expense of inter-Department fairness. Large distributed workloads from one department may preempt in-quota smaller workloads from another department. This new setting in the <code>RunaiConfig</code> file preserves in-quota workloads, even if the department quota or over-quota-fairness is not preserved. For more information, see Scheduler Fairness.</li> </ul>"},{"location":"home/whats-new-2-15/#control-and-visibility","title":"Control and Visibility","text":""},{"location":"home/whats-new-2-15/#dashboards","title":"Dashboards","text":"<ul> <li>To ease the management of AI CPU and cluster resources, a new CPU focused dashboard was added for CPU based environments. The dashboards display specific information for CPU based nodes, node-pools, clusters, or tenants. These dashboards also include additional metrics that are specific to CPU based environments. This will help optimize visual information eliminating the views of empty GPU dashlets. For more information see CPU Dashboard.</li> <li>Improved the Consumption report interface by moving the Cost settings to the General settings menu.</li> <li>Added table to the Consumption dashboard that displays the consumption and cost per department. For more information, see Consumption dashboard.</li> </ul>"},{"location":"home/whats-new-2-15/#nodes","title":"Nodes","text":"<ul> <li>Improved the readability of the Nodes table to include more detailed statuses and descriptions. The added information in the table makes it easier to inspect issues that may impact resource availability in the cluster. For more information, see Node and Node Pool Status.</li> </ul>"},{"location":"home/whats-new-2-15/#ui-enhancements","title":"UI Enhancements","text":"<ul> <li>Added the ability to download a CSV file from any page that contains a table. Downloading a CSV provides a snapshot of the page's history over time, and helps with compliance tracking. All the columns that are selected (displayed) in the table are downloaded to the file.</li> </ul>"},{"location":"home/whats-new-2-15/#installation-and-configuration","title":"Installation and Configuration","text":""},{"location":"home/whats-new-2-15/#cluster-installation-and-configuration","title":"Cluster Installation and configuration","text":"<ul> <li>New cluster wizard for adding and installing new clusters to your system.</li> </ul>"},{"location":"home/whats-new-2-15/#openshift-support","title":"OpenShift Support","text":"<ul> <li>Added support for <code>restricted</code> policy for Pod Security Admission (PSA) on OpenShift only. For more information, see [Pod security admission](../admin/runai-setup/cluster-setup/</li> <li>Added the ability, in OpenShift environments, to configure cluster routes created by Run:ai instead of using the OpenShift certificate. For more information, see the table entry Dedicated certificate for the researcher service route.</li> </ul>"},{"location":"home/whats-new-2-16/","title":"Version 2.16","text":""},{"location":"home/whats-new-2-16/#release-content-january-25-2024","title":"Release Content - January 25, 2024","text":""},{"location":"home/whats-new-2-16/#researcher","title":"Researcher","text":"<ul> <li>Added enterprise level security for researcher tools such as Jupyter Notebooks, VSCode, or any other URL associated with the workload. Using this feature, anyone within the organization requesting access to a specific URL will be redirected to the login page to be authenticated and authorized. This results in protected URLs which cannot be reached from outside the organization. Researchers can enhance the URL privacy by using the Private toggle which means that only the researcher who created the workload can is authorized to access it. The Private toggle is available per tool that uses an external URL as a connection type and is located in the workload creation from in the UI in the environment section. This toggle sets a flag of <code>isPrivate</code> in the <code>connections</code> section of a policy for the connection type <code>ExternalUrl</code>. For more information, see Creating a new Workspace.</li> </ul>"},{"location":"home/whats-new-2-16/#jobs-workloads-and-workspaces","title":"Jobs, Workloads, and Workspaces","text":"<ul> <li>Added the capability view and edit policies directly in the project submission form. Pressing on Policy will open a window that displays the effective policy. For more information, see Viewing Project Policies.</li> </ul> <ul> <li> <p>Running machine learning workloads effectively on Kubernetes can be difficult, but Run:ai makes it easy. The new Workloads experience introduces a simpler and more efficient way to manage machine learning workloads, which will appeal to data scientists and engineers alike. The Workloads experience provides a fast, reliable, and easy to use unified interface.</p> <ul> <li>Fast-query of data from the new workloads service.</li> <li>Reliable data retrieval and presentation in the CLI, UI, and API.</li> <li>Easy to use single unified view with all workload types in one place.</li> </ul> <p>For more information, see Workloads Overview.</p> </li> <li> <p>Changed the workload default auto deletion time after completion value from <code>Never</code> to <code>90 days</code>. This ensures that environments will be cleaned from old data. This field is editable by default, allowing researchers the ability to change the value while submitting a workload. Using workload policies, administrators can increase, decrease, set the default value to <code>never</code>, or even lock access to this value so researchers can not edit it when they submit workloads.</p> </li> </ul>"},{"location":"home/whats-new-2-16/#assets","title":"Assets","text":"<ul> <li>When creating an asset such as data sources, credentials, or others, the scope is limited to the cluster selected at the top of the UI.</li> </ul>"},{"location":"home/whats-new-2-16/#runai-administrator","title":"Run:ai Administrator","text":"<ul> <li>Added the capability for administrators to configure messages to users when they log into the platform. Messages are configured using the Message Editor screen. For more information, see Administrator Messages.</li> </ul>"},{"location":"home/whats-new-2-16/#monitoring-and-analytics","title":"Monitoring and Analytics","text":"<ul> <li> <p>Added to the dashboard updated GPU and CPU resource availability.</p> <ul> <li>Added a chart displaying the number of free GPUs per node. Free GPU are GPUs that have not been allocated to a workload.</li> <li>Added a dashlet that displays the total vs. ready resources for GPUs and CPUs. The dashlet indicates how many total nodes are in the platform, and how many are available. </li> </ul> </li> <li> <p>Added additional columns to the consumption report for both Projects and Departments tables. The new columns are:</p> <ul> <li>GPU Idle allocated hours\u2014the portion of time the GPUs spend idle from the total allocation hours.</li> <li>CPU usage hours\u2014the actual usage time of CPU.</li> <li>Memory usage time\u2014the actual usage time of CPU memory.</li> </ul> <p>For more information, see Consumption Dashboard.</p> </li> </ul>"},{"location":"home/whats-new-2-16/#authentication-and-authorization","title":"Authentication and Authorization","text":"<ul> <li>SSO users who have logged into the system will now be visible in the Users table. In addition, added a column to the Users table for the type of user that was created (Local or SSO). For more information, see Adding, Updating, and Deleting Users.</li> </ul>"},{"location":"home/whats-new-2-16/#policies","title":"Policies","text":"<ul> <li> <p>Added new Policy Manager. The new Policy Manager provides administrators the ability to impose restrictions and default values on system resources. The new Policy Manager provides a YAML editor for the configuration of the policies. Administrators can easily add both Workspace or Training policies. The editor makes it easy to see the configuration that has been applied and provides a quick and easy method to edit the policies. The new Policy Editor* brings other important policy features such as the ability to see non-compliant resources in workloads. For more information, see Policies.</p> </li> <li> <p>Added a new policy manager. Enabling the New Policy Manager provides new tools to discover how resources are not compliant. Non-compliant resources and will appear greyed out and cannot be selected. To see how a resource is not compliant, press on the clipboard icon in the upper right hand corner of the resource. Policies can also be applied to specific scopes within the Run:ai platform. For more information, see Viewing Project Policies.</p> </li> </ul>"},{"location":"home/whats-new-2-16/#control-and-visibility","title":"Control and Visibility","text":"<ul> <li>Improved the clarity of the status column in the Clusters view. Now users have more insight about the actual status of Run:ai on the cluster. Users can now see extended details about the state of the Run:ai installation and services on the cluster, and its connectivity state. For more information, see Cluster status.</li> </ul>"},{"location":"home/whats-new-2-16/#deprecation-notifications","title":"Deprecation Notifications","text":"<p>Deprecation notifications allow you to plan for future changes in the Run:ai Platform. Deprecated features will be available for two versions ahead of the notification. For questions, see your Run:ai representative.</p>"},{"location":"home/whats-new-2-16/#project-migration","title":"Project migration","text":"<ul> <li> <p>Run:ai will be deprecating the migration of projects between departments. This affects:</p> <ul> <li>API\u2014the <code>departmentId</code> field will be marked as deprecated in the<code>put</code> endpoint in the <code>projects</code> category.</li> <li>User Interface\u2014there will no longer be an option to:<ul> <li>migrate projects to another department, when deleting departments.</li> <li>change departments, when editing a project.</li> </ul> </li> </ul> </li> </ul>"},{"location":"home/whats-new-2-16/#api-deprecations","title":"API deprecations","text":""},{"location":"home/whats-new-2-16/#removed-apis-and-api-fields-completed-deprecation","title":"Removed APIs and API fields (completed deprecation)","text":"<p>The following list of API endpoints and fields that have completed their deprecation process and therefore will be changed as follows:</p> Endpoint Change /v1/k8s/clusters The endpoint was removed and is replaced by /api/v1/clusters /v1/k8s/clusters/{uuid} The endpoint was removed and is replaced by /api/v1/clusters/{uuid}"},{"location":"home/whats-new-2-17/","title":"Version 2.17","text":""},{"location":"home/whats-new-2-17/#release-content-april-14-2024","title":"Release Content - April 14, 2024","text":"<ul> <li>Deprecation notifications</li> <li>Breaking changes</li> </ul>"},{"location":"home/whats-new-2-17/#researcher","title":"Researcher","text":""},{"location":"home/whats-new-2-17/#scheduler","title":"Scheduler","text":"<ul> <li> <p>Added functionality to configure over provisioning ratios for node pools running any kind of workload. Over provisioning assumes that workloads are either under utilizing or intermittently using GPUs. This indicates that the real utilization is lower than the actual GPU allocation requested. Over provisioning allows the administrator to condense more workloads on a single GPU than what the workload required. For more information, see Optimize performance with Node Level Scheduler.</p> </li> <li> <p>Added the GPU Resource Optimization feature to the UI. Now you can enable and configure GPU Portion (Fraction) limit and GPU Memory Limit from the UI. For more information, see Compute resources UI with Dynamic Fractions. </p> </li> <li> <p>Added the ability to set Run:ai as the default scheduler for any project or namespace. This provides the administrator the ability to ensure that all workloads in a project or namespace are scheduled using the Run:ai scheduler. For more information, see Setting Run:ai as default scheduler.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#jobs-workloads-and-workspaces","title":"Jobs, Workloads, and Workspaces","text":"<ul> <li> <p>Added to the workload details view, the ability to filter by pod. You can now filter metrics and logs per pod or all the pods. Also, the Workloads table now has additional columns including connections and preemtability adding more at a glance information about the workload. In addition, using the Copy &amp; edit button, you can submit a new workload via CLI based on the selected workload. For more information, see Workloads.</p> </li> <li> <p>Added Inference to workload types. Inference workloads can now be created and managed from the unified Workloads table. The Deployments workload type has been deprecated, and replaced with Inference workloads which are submitted using the workload form. For more information, see Inference and for submitting an Inference workload, see Submitting workloads.</p> </li> <li> <p>Added functionality that supports a single workloads submission selection. Now you can submit workloads by pressing + New workloads in the Workloads table. You can submit the following workloads from this table:</p> <ul> <li>Workspace</li> <li>Training</li> <li>Inference</li> </ul> <p>This improvement phases out the previous version's Workspace and Jobs tables. The Jobs table and submission forms have been deprecated and can be reactivated. To reenable the Jobs table and forms, press Tools &amp; settings, then General, then Workloads, and then Toggle the Jobs view and the Jobs submission buttons. For more information, see Submitting workloads.</p> </li> <li> <p>Added the ability to configure a Kubernetes readiness probe. The readiness probe detects resources and workloads that are ready to receive traffic.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#assets","title":"Assets","text":"<ul> <li> <p>Added the capability to use a ConfigMap as a data source. The ability to use a ConfigMap as a data source can be configured in the Data sources UI, the CLI, and as part of a policy. For more information, see Setup a ConfigMap as a data source, Setup a ConfigMap as a volume using the CLI.</p> </li> <li> <p>Added a Status column to the Credentials table, and the Data sources table. The Status column displays the state of the resource and provides troubleshooting information about that asset. For more information, see the Credentials table and the Data sources table.</p> </li> <li> <p>Added functionality for asset creation that validates the asset based on version compatibility of the cluster or the control plane within a specific scope. At time of asset creation, invalid scopes will appear greyed out and will show a pop-up with the reason for the invalidation. This improvement is designed to increase the confidence that an asset is created properly and successfully.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#runai-administrator","title":"Run:ai Administrator","text":""},{"location":"home/whats-new-2-17/#configuration-and-administration","title":"Configuration and Administration","text":"<ul> <li> <p>Introducing a new Tools &amp; Settings menu. The new Tools &amp; Settings menu provides a streamlined UI for administrators to configure the Run:ai environment. The new UI is divided into categories that easily identify the areas where the administrator can change settings. The new categories include:</p> <ul> <li>Analytics\u2014features related to analytics and metrics.</li> <li>Resources\u2014features related to resource configuration and allocation.</li> <li>Workloads\u2014features related to configuration and submission of workloads.</li> <li>Security\u2014features related to configuration of SSO (Single Sign On).</li> <li>Notifications\u2014used for system notifications.</li> <li>Cluster authentication\u2014snippets related to Researcher authentication.</li> </ul> <p>Some features are now labeled either Experimental or Legacy. Experimental features are new features in the environment, that may have certain instabilities and may not perform as expected. Legacy features are features that are in the process of being deprecated, and may be removed in future versions.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#clusters","title":"Clusters","text":"<ul> <li> <p>Added new columns to the Clusters table to show Kubernetes distribution and version. This helps administrators view potential compatibility issues that may arise.</p> </li> <li> <p>Improved the location of the cluster filter. The cluster filter has been relocated to filter bar and the drop down cluster filter in the header of the page has been removed. This improvement creates the following:</p> <ul> <li> <p>Filter assets by cluster in the following tables:</p> <ul> <li>Data sources</li> <li>Environments</li> <li>Computer resources</li> <li>Templates</li> <li>Credentials</li> </ul> </li> <li> <p>Creating a new asset, will automatically display only the scope of the selected cluster.</p> </li> <li>Prevention of account (top most level in the Scope) from being selected when creating assets.</li> <li>Enforcement a cluster specific scope. This increases the confidence that an asset is created properly and successfully.</li> </ul> <p>Note</p> <p>This feature is only applicable if the all the clusters are version 2.17 and above.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#monitoring-and-analytics","title":"Monitoring and Analytics","text":"<ul> <li> <p>Improved GPU Overview dashboard. This improvement provides rich and extensive GPU allocation and performance data and now has interactive tiles that provide direct links to the Nodes, Workloads, and Departments tables. Hover over tiles with graphs to show rich data in the selected time frame filter. Tiles with graphs can be downloaded as CSV files. The new dashboard is enabled by default. Use the Go back to legacy view to return to the previous dashboard style. For more information, see Dashboard analysis.</p> </li> <li> <p>Updated the knative and autoscaler metrics. Run:ai currently supports the following metrics:</p> <ul> <li>Throughput</li> <li>Concurrency</li> </ul> <p>For more information, see Autoscaling metrics.</p> </li> <li> <p>Improved availability of metrics by using Run:ai APIs. Using the API endpoints is now the preferred method to retrieve metrics for use in any application. For more information, see Metrics.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#authentication-and-authorization","title":"Authentication and Authorization","text":"<ul> <li> <p>Added new functionality to SAML 2.0 identity provider configuration in the Security category of the General settings. The added functionality assists with troubleshooting SSO configuration and authentication issues that may arise. Now administrators now have the ability to:</p> <ul> <li>View and edit the identity provider settings for SAML 2.0</li> <li>Upload or download the SAML 2.0 identity provider metadata XML file.</li> </ul> </li> </ul> <p>For more information, see SSO UI configuration.</p>"},{"location":"home/whats-new-2-17/#deprecation-notifications","title":"Deprecation Notifications","text":"<p>Deprecation notifications allow you to plan for future changes in the Run:ai Platform.</p>"},{"location":"home/whats-new-2-17/#feature-deprecations","title":"Feature deprecations","text":"<p>Deprecated features will be available for two versions ahead of the notification. For questions, see your Run:ai representative. The following features have been marked for deprecation:</p> <ul> <li>Jobs\u2014the Jobs feature (submission form and view) has been moved to the category of Legacy. To enable them, go to Tools &amp; Settings, General, open the Workloads pane, and then toggle the Jobs view and Job submission switch to the enabled position.</li> <li>Deployments\u2014the Deployments feature has been removed. It has been replaced by Inference workloads. For more information, see Jobs, Workloads, and Workspaces above.</li> <li>Workspaces view\u2014the Workspaces menu has been removed. You can now submit a Workspace workload using the + New workload form from the Workloads table.</li> </ul>"},{"location":"home/whats-new-2-17/#api-support-and-endpoint-deprecations","title":"API support and endpoint deprecations","text":"<p>The endpoints and parameters specified in the API reference are the ones that are officially supported by Run:ai. For more information about Run:ai's API support policy and deprecation process, see Developer overview.</p>"},{"location":"home/whats-new-2-17/#deprecated-apis-and-api-fields","title":"Deprecated APIs and API fields","text":"<p>The following list of API endpoints and fields that have been marked for deprecation:</p>"},{"location":"home/whats-new-2-17/#jobs-and-pods-api","title":"Jobs and Pods API","text":"Deprecated Replacement /v1/k8s/clusters/{uuid}/jobs /api/v1/workloads /v1/k8s/clusters/{uuid}/jobs/count /api/v1/workloads/count /v1/k8s/clusters/{uuid}/jobs/{jobId}/pods /api/v1/workloads/{workloadId}/pods /v1/k8s/clusters/{uuid}/pods /api/v1/workloads/pods"},{"location":"home/whats-new-2-17/#clusters-api","title":"Clusters API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterUuid}/metrics /api/v1/clusters/{clusterUuid}/metrics"},{"location":"home/whats-new-2-17/#authorization-and-authentication-api","title":"Authorization and Authentication API","text":"Deprecated Replacement /v1/k8s/auth/token/exchange /api/v1/token /v1/k8s/auth/oauth/tokens/refresh /api/v1/token /v1/k8s/auth/oauth/apptoken /api/v1/token /v1/k8s/users/roles /api/v1/authorization/roles /v1/k8s/users /api/v1/users /v1/k8s/users/{userId} /api/v1/users/{userId} /v1/k8s/users/{userId}/roles /api/v1/authorization/access-rules /v1/k8s/apps /api/v1/apps /v1/k8s/apps/{clientId} /api/v1/apps/{appId} /v1/k8s/groups /api/v1/authorization/access-rules /v1/k8s/groups/{groupName} /api/v1/authorization/access-rules /v1/k8s/clusters/{clusterId}/departments/{department-id}/access-control /api/v1/authorization/access-rules /api/v1/authorization/access-rules - <code>subjectIdFilter</code> field Use <code>filterBy</code> / <code>sortBy</code> fields /api/v1/authorization/access-rules - <code>scopeType</code> field Use <code>filterBy</code> / <code>sortBy</code> fields /api/v1/authorization/access-rules - <code>roleId</code> field Use <code>filterBy</code> / <code>sortBy</code> fields"},{"location":"home/whats-new-2-17/#projects-api","title":"Projects API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterId}/projects - <code>permissions</code> field /api/v1/authorization/access-rules /v1/k8s/clusters/{clusterId}/projects - <code>resources</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/projects - <code>deservedGpus</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/projects - <code>maxAllowedGpus</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/projects - <code>gpuOverQuotaWeight</code> field Use <code>nodePoolResources</code> field"},{"location":"home/whats-new-2-17/#departments-api","title":"Departments API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterId}/departments - <code>resources</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/departments - <code>deservedGpus</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/departments - <code>allowOverQuota</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/departments - <code>maxAllowedGpus</code> field Use <code>nodePoolResources</code> field"},{"location":"home/whats-new-2-17/#policy-api","title":"Policy API","text":"Deprecated Replacement /api/v1/policy/workspace /api/v2/policy/workspaces /api/v1/policy/training /api/v2/policy/trainings"},{"location":"home/whats-new-2-17/#logo-api","title":"Logo API","text":"Deprecated Replacement /v1/k8s/tenant/{tenantId}/logo /api/v1/logo"},{"location":"home/whats-new-2-17/#removed-apis-and-api-fields-completed-deprecation","title":"Removed APIs and API fields (completed deprecation)","text":"<p>The following list of API endpoints and fields that have completed their deprecation process and therefore will be changed as follows:</p>"},{"location":"home/whats-new-2-17/#assets-api","title":"Assets API","text":"Endpoint Change /api/v1/asset/compute <code>gpuRequest</code> field was removed and is replaced by the following fields:  * <code>gpuDevicesRequest</code> (New and mandatory)  * <code>gpuRequestType</code> (New and mandatory if  <code>gpuDevicesRequest=1</code> otherwise optional for values 0 or greater than 1)  * <code>gpuPortion</code> was changed to <code>gpuPortionRequest</code> and accepts values between 0 and 1 (for example 0.75)  * <code>gpuPortionLimit</code> (New and optional)  * <code>gpuMemory</code> was changed to <code>gpuMemoryRequest</code>  * <code>gpuMemoryLimit</code> (New and optional)"},{"location":"home/whats-new-2-17/#metrics-deprecations","title":"Metrics deprecations","text":"<p>The following metrics are deprecated and replaced by API endpoints. For details about the replacement APIs, see Changed Metrics:</p> Metric runai_active_job_cpu_requested_cores runai_active_job_memory_requested_bytes runai_cluster_cpu_utilization runai_cluster_memory_utilization runai_gpu_utilization_per_pod_per_gpu runai_gpu_utilization_per_workload runai_job_requested_gpu_memory runai_gpu_memory_used_mebibytes_per_workload runai_gpu_memory_used_mebibytes_per_pod_per_gpu runai_active_job_cpu_limits runai_job_cpu_usage runai_active_job_memory_limits runai_job_memory_used_bytes"},{"location":"home/whats-new-2-17/#breaking-changes","title":"Breaking changes","text":"<p>Breaking changes notifications allow you to plan around potential changes that may interfere your current workflow when interfacing with the Run:ai Platform.</p>"},{"location":"home/whats-new-2-17/#metrics","title":"Metrics","text":"<p>Be aware that some names of metrics have been changed. For more information, see Changed Metrics.</p>"},{"location":"home/whats-new-2-18/","title":"Version 2.18","text":""},{"location":"home/whats-new-2-18/#release-content-june-30-2024","title":"Release Content - June 30, 2024","text":"<ul> <li>Deprecation notifications</li> <li>Breaking changes</li> </ul>"},{"location":"home/whats-new-2-18/#researcher","title":"Researcher","text":""},{"location":"home/whats-new-2-18/#jobs-workloads-and-workspaces","title":"Jobs, Workloads, and Workspaces","text":"<ul> <li> <p>Added to UI backoff limit functionality to Training and Workspace workloads. The backoff limit is the maximum number of retry attempts for failed workloads. After reaching the limit, the workload's status will change to <code>Failed</code>. The UI will display the default number of retries based on 6 attempts for each pod in the workload. (For example, 6 pods = 36 attempts).</p> </li> <li> <p>Updated Auto-deletion time default value from never to 30 days. The Auto-deletion time count starts when any Run:ai workload reaches a a completed, or failed status will be automatically deleted (including logs). This change only affects new or cloned workloads.</p> </li> <li> <p>Added new Data sources of type Secret to workload form. Data sources of type Secret are used to hide 3rd party access credentials when submitting workloads. For more information, see Submitting Workloads.</p> </li> <li> <p>Added new graphs for Inference workloads. The new graphs provide more information for Inference workloads to help analyze performance of the workloads. New graphs include Latency, Throughput, and number of replicas. For more information, see Workloads View. (Requires minimum cluster version v2.18).</p> </li> <li> <p>Added latency metric for autoscaling. This feature allows automatic scale-up/down the number of replicas of a Run:ai inference workload based on the threshold set by the ML Engineer. This ensures that response time is kept under the target SLA. (Requires minimum cluster version v2.18).</p> </li> <li> <p>Improved autoscaling for inference models by taking out ChatBot UI from models images. By moving ChatBot UI to predefined Environments, autoscaling is more accurate by taking into account all types of requests (API, and ChatBot UI). Adding a ChatBot UI environment preset by Run:ai allows AI practitioners to easily connect them to workloads.</p> </li> <li> <p>Added more precision to trigger auto-scaling to zero. Now users can configure a precise consecutive idle threshold custom setting to trigger Run:ai inference workloads to scale-to-zero. (Requires minimum cluster version v2.18).</p> </li> <li> <p>Added Hugging Face catalog integration of community models. Run:ai has added Hugging Face integration directly to the inference workload form, providing the ability to select models (vLLM models) from Hugging Face. This allows organizations to quickly experiment with the latest open source community language models. For more information on how Hugging Face is integrated, see Hugging Face.</p> </li> <li> <p>Improved access permissions to external tools. This improvement now allows more granular control over which personas can access external tools (external URLs) such as Jupyter Notebooks, Chatbot UI, and others. For configuration information, see Submitting workloads. (Requires minimum cluster version v2.18).</p> </li> <li> <p>Added a new API for submitting Run:ai inference workloads. This API allows users to easily submit inference workloads. This new API provides a consistent user experience for workload submission which maintains data integrity across all the user interfaces in the Run:ai platform. (Requires minimum cluster version v2.18).</p> </li> </ul>"},{"location":"home/whats-new-2-18/#command-line-interface-v2","title":"Command Line Interface V2","text":"<ul> <li> <p>Added an improved, researcher-focused Command Line Interface (CLI). The improved CLI brings usability enhancements for the Researcher which include:</p> <ul> <li>Support multiple clusters</li> <li>Self-upgrade</li> <li>Interactive mode</li> <li>Align CLI to be data consistent with UI and API</li> <li>Improved usability and performance</li> </ul> <p>This is an early access feature available for customers to use; however, be aware that there may be functional gaps versus the older, V1 CLI. For more information about installing and using the V2 CLI, see CLI V2. (Requires minimum cluster version v2.18).</p> </li> </ul>"},{"location":"home/whats-new-2-18/#gpu-memory-swap","title":"GPU memory swap","text":"<ul> <li>Added new GPU to CPU memory swap. To ensure efficient usage of an organization\u2019s resources, Run:ai provides multiple features on multiple layers to help administrators and practitioners maximize their existing GPUs resource utilization.  Run:ai\u2019s GPU memory swap feature helps administrators and AI practitioners to further increase the utilization of existing GPU HW by improving GPU sharing between AI initiatives and stakeholders. This is done by expending the GPU physical memory to the CPU memory which is typically an order of magnitude larger than that of the GPU. For more information see, GPU Memory Swap. (Requires minimum cluster version v2.18).</li> </ul>"},{"location":"home/whats-new-2-18/#yaml-workload-reference-table","title":"YAML Workload Reference table","text":"<ul> <li>Added a new YAML reference document that contains the value types and workload YAML references. Each table contains the field name, its description and the supported Run:ai workload types. The YAML field details contains information on the value type and currently available example workload snippets. For more information see, YAML Reference PDF.</li> </ul>"},{"location":"home/whats-new-2-18/#email-notifications-workload-status-and-timeouts","title":"Email Notifications - Workload Status and timeouts","text":"<ul> <li>Added new Email notification system. AI Practitioners can setup the types of workload notifications they want to receive. In order to receive email notifications, you must ensure that the admin has enabled and configured notifications for the tenant. For more information, see Email notifications.</li> </ul>"},{"location":"home/whats-new-2-18/#assets","title":"Assets","text":"<ul> <li>Improved UI asset creation form by adding a Description field. Now asset creators can add a free text description(max 250 characters) to any asset created. The description field is intended to help explain the nature and goal of the asset, this way AI practitioners will be able to make better decisions when choosing their assets in workload creation.</li> </ul>"},{"location":"home/whats-new-2-18/#runai-administrator","title":"Run:ai Administrator","text":""},{"location":"home/whats-new-2-18/#data-sources","title":"Data Sources","text":"<ul> <li> <p>Added Data Volumes new feature. Data Volumes are snapshots of datasets stored in Kubernetes Persistent Volume Claims (PVCs). They act as a central repository for training data, and offer several key benefits. </p> <ul> <li>Managed with dedicated permissions\u2014Data Admins, a new role within Run.ai, have exclusive control over data volume creation, data population, and sharing.</li> <li>Shared between multiple scopes\u2014unlike other Run:ai data sources, data volumes can be shared across projects, departments, or clusters. This promotes data reuse and collaboration within your organization.</li> <li>Coupled to workloads in the submission process\u2014similar to other Run:ai data sources, Data volumes can be easily attached to AI workloads during submission, specifying the data path within the workload environment.</li> </ul> <p>For more information, see Data Volumes. (Requires minimum cluster version v2.18).</p> </li> <li> <p>Added new data source of type Secret. Run:ai now allows you to configure a Credential as a data source. A Data source of type Secret is best used in workloads so that access to 3rd party interfaces and storage used in containers, keep access credentials hidden. For more information, see Secrets as a data source.</p> </li> <li> <p>Updated the logic of data source initializing state which keeps the workload in \u201cinitializing\u201d status until S3 data is fully mapped. For more information see Sidecar containers documentation.</p> </li> <li> <p>Additional storage unit sizes MiB, GiB &amp; TiB (Megabyte, Gigabyte, and Terabyte respectively) added to the UI and API when creating a new data source of type PVC.</p> </li> </ul>"},{"location":"home/whats-new-2-18/#credentials","title":"Credentials","text":"<ul> <li>Added new Generic secret to Credentials. Credentials had been used only for access to data sources (S3, Git, etc.). However, AI practitioners need to use secrets to access sensitive data (interacting with 3rd party APIs, or other services) without having to put their credentials in their source code. Generic secrets leverage multiple key value pairs which helps reduce the number of Kubernetes resources and simplifies resource management by reducing the overhead associated with maintaining multiple Secrets. Generic secrets are best used as a data source of type Secret so that they can be used in containers to keep access credentials hidden. (Requires minimum cluster version v2.18).</li> </ul>"},{"location":"home/whats-new-2-18/#single-sign-on","title":"Single Sign On","text":"<ul> <li> <p>Added support for Single Sign On using OpenShift v4 (OIDC based). When using OpenShift, you must first define OAuthClient which interacts with OpenShift's OAuth server to authenticate users and request access tokens. For more information, see Single Sign-On.</p> </li> <li> <p>Added OIDC scopes to authentication requests. OIDC Scopes are used to specify what access privileges are being requested for access tokens. The scopes associated with the access tokens determine what resource are available when they are used to access OAuth 2.0 protected endpoints. Protected endpoints may perform different actions and return different information based on the scope values and other parameters used when requesting the presented access token. For more information, see UI configuration.</p> </li> </ul>"},{"location":"home/whats-new-2-18/#ownership-protection","title":"Ownership protection","text":"<ul> <li>Added new ownership protection feature. Run:ai Ownership Protection ensures that only authorized users can delete or modify workloads. This feature is designed to safeguard important jobs and configurations from accidental or unauthorized modifications by users who did not originally create the workload. For configuration information, see your Run:ai representative.</li> </ul>"},{"location":"home/whats-new-2-18/#email-notifications","title":"Email notifications","text":"<ul> <li> <p>Added new email notifications feature. Email Notifications sends alerts for critical workload life cycle changes empowering data scientists to take necessary actions and prevent delays.</p> <ul> <li>System administrators will need to configure the email notifications. For more information, see System notifications.</li> </ul> </li> </ul>"},{"location":"home/whats-new-2-18/#policy-for-distributed-and-inference-workloads-in-the-api","title":"Policy for distributed and inference workloads in the API","text":"<ul> <li>Added a new API for creating distributed training workload policies and inference workload policies. These new policies in the API allow to set defaults, enforce rules and impose setup on distributed training and inference workloads. For distributed policies, worker and master may require different rules due to their different specifications. The new capability is currently available via API only. Documentation on submitting policies to follow shortly.</li> </ul>"},{"location":"home/whats-new-2-18/#deprecation-notifications","title":"Deprecation Notifications","text":"<p>Existing notifications feature requires cluster configuration, is being deprecated in favor of an improved Notification System. If you have been using the existing notifications feature in the cluster, you can continue to use it for the next two versions. It is recommend that you change to the new notifications system in the Control Plane for better control and improved message granularity.</p>"},{"location":"home/whats-new-2-18/#feature-deprecations","title":"Feature deprecations","text":"<p>Deprecated features will be available for two versions ahead of the notification. For questions, see your Run:ai representative.</p>"},{"location":"home/whats-new-2-18/#api-support-and-endpoint-deprecations","title":"API support and endpoint deprecations","text":"<p>The endpoints and parameters specified in the API reference are the ones that are officially supported by Run:ai. For more information about Run:ai's API support policy and deprecation process, see note under Developer overview.</p>"},{"location":"home/whats-new-2-18/#deprecated-apis-and-api-fields","title":"Deprecated APIs and API fields","text":""},{"location":"home/whats-new-2-18/#cluster-api-deprecation","title":"Cluster API Deprecation","text":"<p>Run:ai REST API now supports job submission. The older, Cluster API is now deprecated. </p>"},{"location":"home/whats-new-2-18/#departments-api","title":"Departments API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterId}/departments /api/v1/org-unit/departments /v1/k8s/clusters/{clusterId}/departments/{department-id} /api/v1/org-unit/departments/{departmentId} /v1/k8s/clusters/{clusterId}/departments/{department-id} /api/v1/org-unit/departments/{departmentId}+PUT/PATCH /api/v1/org-unit/departments/{departmentId}/resources"},{"location":"home/whats-new-2-18/#projects-api","title":"Projects API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterId}/projects /api/v1/org-unit/projects /v1/k8s/clusters/{clusterId}/projects/{id} /api/v1/org-unit/projects/{projectId} /v1/k8s/clusters/{clusterId}/projects/{id} /api/v1/org-unit/projects/{projectId} +\u00a0/api/v1/org-unit/projects/{projectId}/resources"},{"location":"home/whats-new-2-18/#breaking-changes","title":"Breaking changes","text":"<p>Breaking changes notifications allow you to plan around potential changes that may interfere your current workflow when interfacing with the Run:ai Platform.</p>"},{"location":"home/changelog/hotfixes-2-13/","title":"Changelog Version 2.13","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.13.</p>"},{"location":"home/changelog/hotfixes-2-13/#version-21348-march-14-2024","title":"Version 2.13.48 - March 14, 2024","text":"Internal ID Description RUN-16787 Fixed an issue after an upgrade to 2.13 where distributed PyTorch jobs were not able to run due to PVCs being assigned to only worker pods. RUN-16626 Fixed an issue in SSO environments, where Workspaces created using a template were assigned the template creator's UID/GID and not the Workspace creator's UID/GID. RUN-16357 Fixed an issue where pressing the Project link in Jobs screen redirects the view to the Projects of a different cluster in multi-cluster environments."},{"location":"home/changelog/hotfixes-2-13/#version-21343-february-15-2024","title":"Version 2.13.43 - February 15, 2024","text":"Internal ID Description RUN-14946 Fixed an issue where Dashboards are displaying the hidden Grafana path."},{"location":"home/changelog/hotfixes-2-13/#version-21337","title":"Version 2.13.37","text":"Internal ID Description RUN-13300 Fixed an issue where projects will appear with a status of empty while waiting for the project controller to update its status. This was caused because the cluster-sync works faster than the project controller."},{"location":"home/changelog/hotfixes-2-13/#version-21335-december-19-2023","title":"Version 2.13.35 - December 19, 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content","title":"Release content","text":"<ul> <li>Added the ability to set node affinity for Prometheus.</li> </ul>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-14472 Fixed an issue where template updates were not being applied to the workload. RUN-14434 Fixed an issue where <code>runai_allocated_gpu_count_per_gpu</code> was multiplied by seven. RUN-13956 Fixed an issue where editing templates failed. RUN-13825 Fixed an issue when deleting a job that is allocated a fraction of a GPU, an associated configmap is not deleted. RUN-13343 Fixed an issue in pod status calculation."},{"location":"home/changelog/hotfixes-2-13/#version-21331","title":"Version 2.13.31","text":"Internal ID Description RUN-11367 Fixed an issue where a double click on SSO Users redirects to a blank screen. RUN-10560 Fixed an issue where the <code>RunaiDaemonSetRolloutStuck</code> alert did not work."},{"location":"home/changelog/hotfixes-2-13/#version-21325","title":"Version 2.13.25","text":"Internal ID Description RUN-13171 Fixed an issue when a cluster is not connected the actions in the Workspace and Training pages are still enabled. After the corrections, the actions will be disabled."},{"location":"home/changelog/hotfixes-2-13/#version-21321","title":"Version 2.13.21","text":"Internal ID Description RUN-12563 Fixed an issue where users are unable to login after upgrading the control plane from 2.9.16 to 2.13.16. To correct the issue, secrets need to be upgraded manually in keycloak."},{"location":"home/changelog/hotfixes-2-13/#version-21320-september-28-2023","title":"Version 2.13.20 - September 28, 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content_1","title":"Release content","text":"<ul> <li>Added the prevention of selecting tenant or department scopes for credentials, and the prevention of selecting s3, PVC, and Git data sources if the cluster version does not support these.</li> <li>Quota management is now enabled by default.</li> </ul> Internal ID Description RUN-12923 Fixed an issue in upgrading due to a misconfigured Docker image for airgapped systems in 2.13.19. The helm chart contained an error, and the image is not used even though it is packaged as part of the tar. RUN-12928, RUN-12968 Fixed an issue in upgrading Prometheus due to a misconfigured image for airgapped systems in 2.13.19. The helm chart contained an error, and the image is not used even though it is packaged as part of the tar. RUN-12751 Fixed an issue when upgrading from 2.9 to 2.13 results with a missing engine-config file. RUN-12717 Fixed an issue where the user that is logged in as researcher manager can't see the clusters. RUN-12642 Fixed an issue where assets-sync could not restart due to failing to get token from control plane. RUN-12191 Fixed an issue where there was a timeout while waiting for the <code>runai_allocated_gpu_count_per_project</code> metric to return values. RUN-10474 Fixed an issue where the <code>runai-conatiner-toolkit-exporter</code> DaemonSet fails to start."},{"location":"home/changelog/hotfixes-2-13/#version-21319-september-27-2023","title":"Version 2.13.19 - September 27, 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content_2","title":"Release content","text":"<ul> <li>Added the ability to identify Kubeflow notebooks and display them in the Jobs table.</li> <li>Added the ability to schedule Kubelow workloads.</li> <li>Added functionality that displays Jobs that only belong to the user that is logged in.</li> <li> Added and refined alerts to the state of Run:ai components, schedule latency, and warnings for out of memory on Jobs.</li> <li>Added the ability to work with restricted PSA policy.</li> </ul>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues_1","title":"Fixed issues","text":"Internal ID Description RUN-12650 Fixed an issue that used an incorrect metric in analytics GPU ALLOCATION PER NODE panel. Now the correct allocation is in percentage. RUN-12602 Fixed an issue in <code>runaiconfig</code> where the <code>WorkloadServices</code> spec has memory requests/limits and cpu requests/limits and gets overwritten with the system default. RUN-12585 Fixed an issue where the workload-controller creates a delay in running jobs. RUN-12031 Fixed an issue when upgrading from 2.9 to 2.13 where the Scheduler pod fails to upgrade due to the change of owner. RUN-11091 Fixed an issue where the Departments feature is disabled, you are not able to schedule non-preemable jobs."},{"location":"home/changelog/hotfixes-2-13/#version-21313","title":"Version 2.13.13","text":"Internal ID Description RUN-11321 Fixed an issue where metrics always showed CPU Memory Utilization and CPU Compute Utilization as 0. RUN-11307 Fixed an issue where node affinity might change mid way through a job. Node affinity in now calculated only once at job submission. RUN-11129 Fixed an issue where CRDs are not automatically upgraded when upgrading from 2.9 to 2.13."},{"location":"home/changelog/hotfixes-2-13/#version-21312-august-7-2023","title":"Version 2.13.12 - August 7, 2023","text":"Internal ID Description RUN-11476 Fixed an issue with analytics node pool filter in Allocated GPUs per Project panel."},{"location":"home/changelog/hotfixes-2-13/#version-21311","title":"Version 2.13.11","text":"Internal ID Description RUN-11408 Added to the Run:ai job-controller 2 configurable parameters <code>QPS</code> and <code>Burst</code> which are applied as environment variables in the job-controller Deployment object."},{"location":"home/changelog/hotfixes-2-13/#version-2137-july-2023","title":"Version 2.13.7 - July 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content_3","title":"Release content","text":"<ul> <li>Added filters to the historic quota ratio widget on the Quota management dashboard.</li> </ul>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues_2","title":"Fixed issues","text":"Internal ID Description RUN-11080 Fixed an issue in OpenShift environments where log in via SSO with the <code>kubeadmin</code> user, gets blank pages for every page. RUN-11119 Fixed an issue where values that should be the Order of priority column are in the wrong column. RUN-11120 Fixed an issue where the Projects table does not show correct metrics when Run:ai version 2.13 is paired with a Run:ai 2.8 cluster. RUN-11121 Fixed an issue where the wrong over quota memory alert is shown in the Quota management pane in project edit form. RUN-11272 Fixed an issue in OpenShift environments where the selection in the cluster drop down in the main UI does not match the cluster selected on the login page."},{"location":"home/changelog/hotfixes-2-13/#version-2134","title":"Version 2.13.4","text":""},{"location":"home/changelog/hotfixes-2-13/#release-date","title":"Release date","text":"<p>July 2023</p>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues_3","title":"Fixed issues","text":"Internal ID Description RUN-11089 Fixed an issue when creating an environment, commands in the Runtime settings pane and are not persistent and cannot be found in other assets (for example in a new Training)."},{"location":"home/changelog/hotfixes-2-13/#version-2131-july-2023","title":"Version 2.13.1 - July 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content_4","title":"Release content","text":"<ul> <li>Made an improvement so that occurrences of labels that are not in use anymore are deleted.</li> </ul>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues_4","title":"Fixed issues","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-15/","title":"Changelog Version 2.15","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.15.</p>"},{"location":"home/changelog/hotfixes-2-15/#version-2159-february-5-2024","title":"Version 2.15.9 - February 5, 2024","text":""},{"location":"home/changelog/hotfixes-2-15/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-15296 Fixed an issue where the <code>resources</code> parameter was deprecated in the Projects and Departments API."},{"location":"home/changelog/hotfixes-2-15/#version-2154-january-5-2024","title":"Version 2.15.4 - January 5, 2024","text":""},{"location":"home/changelog/hotfixes-2-15/#fixed-issues_1","title":"Fixed issues","text":"Internal ID Description RUN-15026 Fixed an issue in workloads that were built on a cluster that does not support the NFS field. RUN-14907 Fixed an issue after an upgrade where the Analytics dashboard was missing the time ranges from before the upgrade. RUN-14903 Fixed an issue where internal operations were exposed to the customer audit log. RUN-14062 Fixed an issue in the Overview dashboard where the content for the Running Workload per Type panel did not fit."},{"location":"home/changelog/hotfixes-2-15/#version-2152-february-5-2024","title":"Version 2.15.2 - February 5, 2024","text":""},{"location":"home/changelog/hotfixes-2-15/#fixed-issues_2","title":"Fixed issues","text":"Internal ID Description RUN-14434 Fixed an issue where the Allocated GPUs metric was multiplied by seven."},{"location":"home/changelog/hotfixes-2-15/#version-2151-december-17-2023","title":"Version 2.15.1 - December 17, 2023","text":""},{"location":"home/changelog/hotfixes-2-15/#release-content","title":"Release content","text":"<ul> <li> <p>Added environment variables for customizable QPS and burst support.</p> </li> <li> <p>Added the ability to support running multiple Prometheus replicas.</p> </li> </ul>"},{"location":"home/changelog/hotfixes-2-15/#fixed-issues_3","title":"Fixed issues","text":"Internal ID Description RUN-14292 Fixed an issue where BCM installations were failing due to missing <code>create cluster</code> permissions. RUN-14289 Fixed an issue where metrics were not working due to an incorrect parameter in the cluster-config file. RUN-14198 Fixed an issue in services where multi nodepool jobs were not scheduled due to an unassigned nodepool status. RUN-14191 Fixed an issue where a consolidation failure would cause unnecessary evictions. RUN-14154 Fixed an issue in the New cluster form, whefre the dropdown listed versions that were incompatible with the installed control plane. RUN-13956 Fixed an issue in the Jobs table where templates were not edited successfully. RUN-13891 Fixed an issue where Ray job statuses were shown as empty. RUN-13825 Fixed an issue where GPU sharing configmaps were not deleted. RUN-13628 Fixed an issue where the <code>pre-install</code> pod failed to run <code>pre-install</code> tasks due to the request being denied (Unauthorized). RUN-13550 Fixed an issue where environments were not recovering from a node restart due to a missing GPU runtime class for containerized nodes. RUN-11895 Fixed an issue where the wrong amount of GPU memory usage was shown (is now MB). RUN-11681 Fixed an issue in OpenShift environments where some metrics were not shown on dashboards when the GPU Operator from the RedHat marketplace was installed."},{"location":"home/changelog/hotfixes-2-15/#version-2150","title":"Version 2.15.0","text":""},{"location":"home/changelog/hotfixes-2-15/#fixed-issues_4","title":"Fixed issues","text":"Internal ID Description RUN-13456 Fixed an issue where the Researcher L1 role did not have permissions to create and manage credentials. RUN-13282 Fixed an issue where Workspace logs crashed unexpectedly after restarting. RUN-13121 Fixed an issue in not being able to launch jobs using the API after an upgrade overrode a change in keycloak for applications which have a custom mapping to an email. RUN-13103 Fixed an issue in the Workspaces and Trainings table where the action buttons were not greyed out for users with only the view role. RUN-12993 Fixed an issue where Prometheus was reporting metrics even though the cluster was disconnected. RUN-12978 Fixed an issue after an upgrade, where permissions fail to sync to a project due to a missing application name in the CRD. RUN-12900 Fixed an issue in the Projects table, when sorting by Allocated GPUs, the projects were displayed alphabetically and not numerically. RUN-12846 Fixed an issue after a control-plane upgrade, where GPU, CPU, and Memory Cost fields (in the Consumption Reports) were missing when not using Grafana. RUN-12824 Fixed an issue where airgapped environments tried to pull an image from gcr.io (Internet). RUN-12769 Fixed an issue where SSO users were unable to see projects in Job Form unless the group they belong to was added directly to the project. RUN-12602 Fixed an issue in the documentation where the <code>WorkloadServices</code> configuration in the <code>runaiconfig</code> file was incorrect. RUN-12528 Fixed an issue where the Workspace duration scheduling rule was suspending workspaces regardless of the configured duration. RUN-12298 Fixed an issue where projects were not shown in the Projects table due to the API not sanitizing the project name at time of creation. RUN-12157 Fixed an issue where querying pods completion time returned a negative number. RUN-10560 Fixed an issue where no Prometheus alerts were sent due to a misconfiguration of the parameter <code>RunaiDaemonSetRolloutStuck</code>."},{"location":"home/changelog/hotfixes-2-16/","title":"Changelog Version 2.16","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.16.</p>"},{"location":"home/changelog/hotfixes-2-16/#version-21665","title":"Version 2.16.65","text":"Internal ID Description RUN-21448 Fixed an issue with degraded workload so the condition would reflect the actual state. RUN-20680 Fixed an issue where the workload page did not present the requested GPU."},{"location":"home/changelog/hotfixes-2-16/#version-21657","title":"Version 2.16.57","text":"Internal ID Description RUN-20388 Fixed an issue where cluster-sync caused a memory leak."},{"location":"home/changelog/hotfixes-2-16/#version-21625","title":"Version 2.16.25","text":"Internal ID Description RUN-17241 Fixed an issue where the nodes page showed nodes as not ready due to \"tookit not installed\"."},{"location":"home/changelog/hotfixes-2-16/#version-21621","title":"Version 2.16.21","text":"Internal ID Description RUN-16463 Fixed an issue after a cluster upgrade to v2.16, where some metrics of pre-existing workloads were displayed incorrectly in the Overview Dashboard."},{"location":"home/changelog/hotfixes-2-16/#version-21618","title":"Version 2.16.18","text":"Internal ID Description RUN-16486 Fixed an issue in the Workloads creation form where the GPU fields of the compute resource tiles were showing no data."},{"location":"home/changelog/hotfixes-2-16/#version-21616","title":"Version 2.16.16","text":"Internal ID Description RUN-16340 Fixed an issue in the Workloads table where filters were not saved correctly"},{"location":"home/changelog/hotfixes-2-16/#version-21615","title":"Version 2.16.15","text":""},{"location":"home/changelog/hotfixes-2-16/#release-content","title":"Release content","text":"<ul> <li>Implemented a new Workloads API to support the Workloads feature.</li> </ul>"},{"location":"home/changelog/hotfixes-2-16/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-16070 Fixed an issue where missing metrics caused the Nodepools table to appear empty."},{"location":"home/changelog/hotfixes-2-16/#version-21614","title":"Version 2.16.14","text":""},{"location":"home/changelog/hotfixes-2-16/#release-content_1","title":"Release content","text":"<p>*Improved overall performance by slowing down metrics updates from 10 seconds to 30 seconds.</p>"},{"location":"home/changelog/hotfixes-2-16/#fixed-issues_1","title":"Fixed issues","text":"Internal ID Description RUN-16255 Fixed an issue in the Analytics dashboard where the GPU Allocation per Node and GPU Memory Allocation per Node panels were displaying incorrect data. RUN-16035 Fixed an issue in the Workloads table where completed pods continue to be counted in the requested resources column."},{"location":"home/changelog/hotfixes-2-16/#version-21612","title":"Version 2.16.12","text":""},{"location":"home/changelog/hotfixes-2-16/#fixed-issues_2","title":"Fixed issues","text":"Internal ID Description RUN-16110 Fixed an issue where creating a training workload (single or multi-node) with a new PVC or Volume, resulted in the Workloads table showing the workload in the Unknown/Pending status. RUN-16086 Fixed an issue in airgapped environments where incorrect installation commands were shown when upgrading to V2.15."},{"location":"home/changelog/hotfixes-2-16/#version-21611","title":"Version 2.16.11","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-16/#version-2169","title":"Version 2.16.9","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-16/#version-2168","title":"Version 2.16.8","text":""},{"location":"home/changelog/hotfixes-2-16/#release-content_2","title":"Release content","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-16/#version-2167","title":"Version 2.16.7","text":""},{"location":"home/changelog/hotfixes-2-16/#release-content_3","title":"Release content","text":"<ul> <li>Added an API endpoint that retrieves data from a workloads's pod.</li> </ul>"},{"location":"home/changelog/hotfixes-2-16/#fixed-issues_3","title":"Fixed issues","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-16/#version-2166","title":"Version 2.16.6","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-17/","title":"Changelog Version 2.17","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.17.</p>"},{"location":"home/changelog/hotfixes-2-17/#version-21763","title":"Version 2.17.63","text":"Internal ID Description RUN-21448 Fixed an issue where a degraded workload was stuck and could not be released."},{"location":"home/changelog/hotfixes-2-17/#version-21746","title":"Version 2.17.46","text":"Internal ID Description RUN-20136 Updated postgres version."},{"location":"home/changelog/hotfixes-2-17/#version-21743","title":"Version 2.17.43","text":"Internal ID Description RUN-19949 Fixed an issue where runai submit arguments were not parsed correctly to the command."},{"location":"home/changelog/hotfixes-2-17/#version-21741","title":"Version 2.17.41","text":"Internal ID Description RUN-19870 Added debug logs to cluster-sync"},{"location":"home/changelog/hotfixes-2-17/#version-21726","title":"Version 2.17.26","text":"Internal ID Description RUN-19189 Fixed an issue in cluster-sync that sometimes caused unnecessary sync process to the control-plane."},{"location":"home/changelog/hotfixes-2-17/#version-21725","title":"Version 2.17.25","text":"Internal ID Description RUN-16357 Fixed an issue where the Project button in the Jobs screen redirects to the Projects page but on the wrong cluster."},{"location":"home/changelog/hotfixes-2-17/#version-21710","title":"Version 2.17.10","text":"Internal ID Description RUN-18065 Fixed an issue where the legacy job sumbission configuration was not available in the Settings page"},{"location":"home/changelog/hotfixes-2-17/#version-2170","title":"Version 2.17.0","text":"Internal ID Description RUN-20010 Fixed an issue of reduced permissions that run:ai grants users"},{"location":"home/changelog/hotfixes-2-18/","title":"Changelog Version 2.18","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.18.</p>"},{"location":"home/changelog/hotfixes-2-18/#hotfixes","title":"Hotfixes","text":"Internal ID Hotfix # Description RUN-25382 2.18.96 Fixed an issue where invalid min/max policy values caused an error in the policy pod. RUN-25987 2.18.96 Updated all workload APIs to accurately reflect that both creating and deleting workloads return a 202 status code in the API documentation. RUN-26240 2.18.96 CLI v2: Fixed an issue in the install script, where setting the install path environment variable did not install all the files in the correct path. RUN-26479 2.18.96 CLI v2: Fixed an issue where using the wrong workload type in the workload describe command did not display an error. RUN-26249 2.18.95 Fixed an issue where creating a policy with the fields <code>tty</code> and <code>stdin</code> resulted in a validation error. RUN-26308 2.18.95 CLI v2: Fixed several text mismatches in <code>runai training list --help</code> and deprecated messages. RUN-26304 2.18.94 Fixed an issue where quota numbers was incorrectly displayed in the reclaim message. RUN-26248 2.18.93 CLI v2: Fixed an issue where submitting an interactive workload with attach was not possible after the workload started running. RUN-25323 2.18.92 Reduced memory consumption to improve stability and increase scaling. RUN-25323 2.18.92 Fixed an issue in CLI v2 where \u201cstopping the workload\u201d event was missing when workloads reached the project\u2019s running time limit. RUN-25921 2.18.90 Fixed an issue where the Workloads API did not enforce a minimum cluster version, returning a 400 Bad Request for versions below 2.18. RUN-25659 2.18.89 CLI v2: Fixed an issue where min and max replicas  were able to be submitted using TensorFlow. RUN-25558 2.18.88 Fixed a memory issue when handling external workloads (deployments, ray etc.) which when they were scaled caused ETCD memory to increase. RUN-25466 2.18.88 Fixed an issue where an environment variable with the value SECRET was not valid as only SECRET:xxx was accepted. RUN-24700 2.18.88 CLI v2: Workload describe command no longer requires type or framework flags. RUN-25499 2.18.87 Fixed an issue where policy update request would fail to sync to the cluster\" perhaps mention the cluster version. RUN-25303 2.18.85 Fixed an issue where submitting with the --attach flag was supported only in a workspace workload. RUN-25061 2.18.84 Fixed a security vulnerability in github.com.go-git.go-git.v5 with CVE CVE-2025-21613 with severity HIGH. RUN-24857 2.18.84 Fixed a security vulnerability in golang.org.x.net with CVE CVE-2024-45338 with severity HIGH. RUN-17284 2.18.84 Fixed an issue where workloads were suspended when set with the termination after preemption option. RUN-24521 2.18.83 Fixed a security vulnerability in golang.org.x.crypto with CVE CVE-2024-45337 with severity HIGH. RUN-24733 2.18.83 Fixed an issue where department admins were unable to load the quota management page. RUN-25094 2.18.82 Fixed an issue where OpenShift could not be upgraded due to a broken 3rd binary. RUN-24921 2.18.80 Fixed a security vulnerability in golang.org.x.net and golang.org.x.crypto. RUN-24632 2.18.80 Fixed an issue where an existing monitoring Prometheus setup deployed in an unexpected namespace was reported as missing, causing Run:ai installation to fail on the cluster. The installation mechanism now searches for the monitoring prerequisite in additional relevant namespaces. RUN-24693 2.18.80 Fixed an issue where users were unable to provide metric store authentication details using secret references. RUN-24752 2.18.79 Fixed an issue where a workload would move to a failed state when created with a custom NodePort that was already allocated. RUN-24649 2.18.79 Fixed an issue where submitting a workload with <code>existingPvc=false</code> and not providing a <code>claimName</code> resulted in auto-generating a <code>claimName</code> that included both upper and lower case letters. Since Kubernetes rejects uppercase letters, the workload would fail. The behavior has been updated to generate names using only lowercase letters. RUN-24595 2.18.78 Fixed an issue where the new CLI did not parse master and worker commands/args simultaneously for distributed workloads. RUN-23914 2.18.78 Fixed an issue where unexpected behavior could occur if an application was capturing a graph while memory was being swapped in as part of the GPU memory swap feature. RUN-24020 2.18.77 Fixed a security vulnerability in k8s.io.kubernetes with CVE CVE-2024-0793. RUN-24021 2.18.77 Fixed a security vulnerability in pam with CVE CVE-2024-10963. RUN-23798 2.18.75 Fixed an issue in distributed PyTorch workloads where the worker pods are deleted immediately after completion, not allowing logs to be viewed. RUN-23838 2.18.74 Fixed an issue where the command-line interface could not access resources when configured as single-sign on in a self-hosted environment. RUN-23561 2.18.74 Fixed an issue where the frontend in airgapped environment attempted to download font resources from the internet. RUN-23789 2.18.73 Fixed an issue where in some cases, it was not possible to download the latest version of the command line interface. RUN-23790 2.18.73 Fixed an issue where in some cases it was not possible to download the Windows version of the command line interface. RUN-23855 2.18.73 Fixed an issue where the pods list in the UI showed past pods. RUN-23909 2.18.73 Fixed an issue where users based on group permissions cannot see dashboards. RUN-23857 2.18.72 Dashboard to transition from Grafana v9 to v10. RUN-24010 2.18.72 Fixed an infinite loop issue in the cluster-sync service. RUN-23040 2.18.72 Fixed an edge case where the Run:ai container toolkit hangs when user is spawning hundreds of sub-processes. RUN-23802 2.18.70 Fixed an issue where new scheduling rules were not applied to existing workloads, if those new rules were set on existing projects which had no scheduling rules before. RUN-23211 2.18.70 Fixed an issue where workloads were stuck at \"Pending\" when the command-line interface flag --gpu-memory was set to zero. RUN-23778 2.18.68 Fixed an issue where in single-sign-on configuration, the mapping of UID and other properties would sometimes disappear. RUN-23762 2.18.68 Fixed an issue where the wrong version of a Grafana dashboard was displayed in the UI. RUN-21198 2.18.66 Fixed an issue where creating a training workload via yaml (kubectl apply -f) and specifying spec.namePrefix, created infinite jobs. RUN-23541 2.18.65 Fixed an issue where in some cases workload authorization did not work properly due to wrong oidc configuration. RUN-23283 2.18.64 Fixed a permissions issue with the Analytics dashboard post upgrade for SSO Users. RUN-23420 2.18.63 Replaced Redis with Keydb. RUN-23140 2.18.63 Fixed an issue where distributed workloads were created with the wrong types. RUN-23130 2.18.63 Fixed an issue where inference-workload-controller crashed when WorkloadOwnershipProtection was enabled. RUN-23334 2.18.62 Updated core Dockerfiles to ubi9. RUN-23296 2.18.62 Fixed an issue in the CLI where runai attach did not work with auto-complete. RUN-23215 2.18.62 Fixed an issue where metrics requests from backend to mimir failed for certain tenants. RUN-22138 2.18.62 Fixed an issue where private URL user(s) input was an email and not a string. RUN-23282 2.18.61 CLI documentation fixes. RUN-23055 2.18.60 Fixed unified Distributed and Training CLI commands. RUN-23243 2.18.59 Fixed an issue where the scope tree wasn't calculating permissions correctly. RUN-22463 2.18.59 Fixed an error in CLI bash command. RUN-22314 2.18.59 Fixed distributed framework filtering in API commands. RUN-23142 2.18.58 Fixed an issue where advanced GPU metrics per-gpu don't have gpu label. RUN-23001 2.18.58 Fixed an issue of false overcommit on out-of-memory killed in the \u201cswap\u201d feature. RUN-22851 2.18.58 Fixed an issue where client may get stuck on device lock acquired during \u201cswap\u201d out-migration. RUN-22758 2.18.58 Fixed an issue where inference workload showed wrong status when submission failed. RUN-22544 2.18.58 Updated Grafana version for security vulnerabilities. RUN-23055 2.18.57 Fixed the unified Distributed and Training CLI commands. RUN-23014 2.18.56 Fixed an issue where node-scale-adjuster might not create a scaling pod if it is in cool-down and the pod was not updated after that. RUN-22660 2.18.56 Fixed an issue where workload charts have an unclear state. RUN-22457 2.18.55 Fixed an issue where in rare edge cases the cluster-sync pod was out of memory. RUN-21825 2.18.55 Fixed all CVEs in Run:ai's Goofys-based image used for S3 integration. RUN-22871 2.18.55 Fixed an issue in runai-container-toolkit where in certain cases when a process is preempted, OOMKill metrics were not published correctly. RUN-22250 2.18.55 Fixed an issue where workloads trying to use an ingress URL which is already in use were behaving inconsistently instead of failing immediately. RUN-22880 2.18.55 Fixed an issue where the minAvailable field for training-operator CRDs did not consider all possible replica specs. RUN-22073 2.18.55 Fixed an issue where runai-operator failed to parse cluster URLs ending with '/'. RUN-22453 2.18.55 Fixed an issue where in rare edge cases the workload-overseer pod experienced a crash. RUN-22763 2.18.55 Fixed an issue where in rare edge cases an 'attach' command from CLI-V2 caused a crash in the cluster-api service. RUN-21948 2.18.49 Fixed an issue where in rare edge cases workload child resources could have duplicate names, causing inconsistent behavior. RUN-22623 2.18.49 Fixed an issue in Openshift where workloads were not suspended when reaching their idle GPU time limit. RUN-22600 2.18.49 Fixed an issue in AWS EKS clusters where the V1-CLI returned an empty table when listing all projects as an administrator. RUN-21878 2.18.49 Added a label to disable container toolkit from running on certain nodes <code>run.ai/container-toolkit-enabled</code>. RUN-22452 2.18.47 Fixed an issue where the scheduler has signature errors if TopologySpreadConstraints was partially defined. RUN-22570 2.18.47 Updated git-sync image to version v4.3.0. RUN-22054 2.18.46 Fixed an issue where users could not attach to jobs. RUN-22377 2.18.46 Removed uncached client from accessrule-controller. RUN-21697 2.18.46 Fixed an issue where client may deadlock on suspension during allocation request. RUN-20073 2.18.45 Fixed an issue where it wasn't possible to authenticate with user credentials in the CLI. RUN-21957 2.18.45 Fixed an issue where there was a missing username-loader container in inference workloads. RUN-22276 2.18.39 Fixed an issue where Knative external URL was missing from the Connections modal. RUN-22280 2.18.39 Fixed an issue when setting scale to zero - there was no pod counter in the Workload grid. RUN-19811 2.18.39 Added an option to set k8s tolerations to run:ai daemonsets (container-toolkit, runai-device-plugin, mig-parted, node-exporter, etc..) . RUN-22128 2.18.39 Added GID, UID, Supplemental groups to the V1 CLI. RUN-21800 2.18.37 Fixed an issue with old workloads residing in the cluster. RUN-21907 2.18.34 Fixed an issue where the SSO user credentials contain supplementary groups as string instead of int. RUN-21272 2.18.31 Fixed an issue with multi-cluster credinatils creation, specifically with the same name in different clusters. RUN-20680 2.18.29 Fixed an issue where workloads page do not present requested GPU. RUN-21200 2.18.29 Fixed issues with upgrades and connections from v2.13. RUN-20970 2.18.27 Fixed an issue with PUT APIs. RUN-20927 2.18.26 Fixed an issue where node affinity was not updated correctly in projects edit. RUN-20084 2.18.26 Fixed an issue where default department were deleted instead of a message being displayed. RUN-21062 2.18.26 Fixed issues with the API documentation. RUN-20434 2.18.25 Fixed an issue when creating a Project/Department with memory resources requires 'units'. RUN-20923 2.18.25 Fixed an issue with projects/departments page loading slowly. RUN-19872 2.18.23 Fixed an issue where the Toolkit crashes and fails to create and replace the publishing binaries. RUN-20861 2.18.22 Fixed an issue where a pod is stuck on pending due to a missing resource reservation pod. RUN-20842 2.18.22 Fixed an issue of illegal model name with \".\" in hugging face integration. RUN-20791 2.18.22 Fix an issue where notifications froze after startup. RUN-20865 2.18.22 Fixed an issue where default departments are not deleted when a cluster is deleted. RUN-20698 2.18.21 Fixed an issue where 2 processes requests a device at the same time received the same GPU, causing failures. RUN-20760 2.18.18 Fixed an issue where workload protection UI shows wrong status. RUN-20612 2.18.15 Fixed an issue where it was impossible with the use-table-data to hide node pool columns when there is only one default node pool. RUN-20735 2.18.15 Fixed an issue where nodePool.name is undefined RUN-20721 2.18.12 Added error handling to nodes pages. RUN-20578 2.18.10 Fixed an issue regarding policy enforcement. RUN-20188 2.18.10 Fixed issue with defining SSO in OpenShift identity provider. RUN-20673 2.18.9 Fixed an issue where a researcher uses a distributed elastic job, it is possible that in a specific flow it is scheduled on more than one node-pools. RUN-20360 2.18.7 Fixed an issue where the workload network status was misleading. RUN-22107 2.18.7 Fixed an issue where passwords containing $ were removed from the configuration. RUN-20510 2.18.5 Fixed an issue with external workloads - argocd workflow failed to be updated. RUN-20516 2.18.4 Fixed an issue when after deploying to prod, the cluster-service and authorization-service got multiple OOMKilled every ~1 hour. RUN-20485 2.18.2 Changed policy flags to Beta. RUN-20005 2.18.1 Fixed an issue where a sidecar container failure failed the workload. RUN-20169 2.18.1 Fixed an issue allowing the addition of annotations and labels to workload resources. RUN-20108 2.18.1 Fixed an issue exposing service node ports to workload status. RUN-20160 2.18.1 Fixed an issue with version display when installing a new cluster in an airgapped environment. RUN-19874 2.18.1 Fixed an issue when copying and editing a workload with group access to a tool and the group wasn't removed when selecting users option. RUN-19893 2.18.1 Fixed an issue when using a float number in the scale to zero inactivity value - custom which sometimes caused the submission to fail. RUN-20087 2.18.1 Fixed an issue where inference graphs should be displayed only for minimum cluster versions. RUN-10733 2.18.1 Fixed an issue where we needed to minify and obfuscate our code in production. RUN-19962 2.18.1 Fixed an issue to fix sentry domains regex and map them to relevant projects. RUN-20104 2.18.1 Fixed an issue where frontend Infinite loop on keycloak causes an error. RUN-19906 2.18.1 Fixed an issue where inference workload name validation fails with 2.16 cluster. RUN-19605 2.18.1 Fixed an issue where authorized users should support multiple users (workload-controller) . RUN-19903 2.18.1 Fixed an issue where inference chatbot creation fails with 2.16 cluster. RUN-20409 2.18.1 Fixed an issue where clicking on create new compute during the runai model flow did nothing. RUN-11224 2.18.1 Fixed an issue where ruani-adm collect all logs was not collecting all logs. RUN-20478 2.18.1 Improved workloads error status in overview panel. RUN-19850 2.18.1 Fixed an issue where an application administrator could not submit a job with CLI. RUN-19863 2.18.1 Fixed an issue where department admin received 403 on get tenants and cannot login to UI. RUN-19904 2.18.1 Fixed an issue when filtering by allocatedGPU in get workloads with operator returns incorrect result. RUN-19925 2.18.1 Fixed an issue when upgrade from v2.16 to v2.18 failed on worklaods migrations. RUN-19887 2.18.1 Fixed an issue in the UI when there is a scheduling rule of timeout, the form opened with the rules collapsed and written \"none\". RUN-19941 2.18.1 Fixed an issue where completed and failed jobs were shown in view pods in nodes screen. RUN-19940 2.18.1 Fixed an issue where setting gpu quota failed because the department quota was taken from wrong department. RUN-19890 2.18.1 Fixed an issue where editing a project by removing its node-affinity stuck updating. RUN-20120 2.18.1 Fixed an issue where project update fails when there is no cluster version. RUN-20113 2.18.1 Fixed an issue in the Workloads table where a researcher does not see other workloads once they clear their filters. RUN-19915 2.18.1 Fixed an issue when turning departments toggles on on cluster v2.11+ the gpu limit is -1 and there is ui error. RUN-20178 2.18.1 Fixed an issue where dashboard CPU tabs appeared in new overview. RUN-20247 2.18.1 Fixed an issue where you couldn't create a workload with namespace of a deleted project. RUN-20138 2.18.1 Fixed an issue where the system failed to create node-type on override-backend env. RUN-18994 2.18.1 Fixed an issue where some limitations for department administrator are not working as expected. RUN-19830 2.18.1 Fixed an issue where resources (GPU, CPU, Memory) units were added to k8s events that are published by run:ai scheduler making our messages more readable."},{"location":"home/changelog/hotfixes-2-18/#version-2180-fixes","title":"Version 2.18.0 Fixes","text":"Internal ID Description RUN-20734 Fixed an issue where the enable/disable toggle for the feature was presenting wrong info. RUN-19895 Fixed an issue of empty state for deleted workloads which is incorrect. RUN-19507 Fixed an issue in V1 where get APIs are missing required field in swagger leading to omit empty. RUN-20246 Fixed an issue in Departments v1 org unit where if unrecognizable params are sent, an error is returned. RUN-19947 Fixed an issue where pending multi-nodepool podgroups got stuck after cluster upgrade. RUN-20047 Fixed an issue where Workload status shows as \"deleting\" rather than \"deleted\" in side panel. RUN-20163 Fixed an issue when a DV is shared with a department and a new project is added to this dep - no pvc/pv is created. RUN-20484 Fixed an issue where Create Projects Requests Returned 500 - services is not a valid ResourceType. RUN-20354 Fixed an issue when deleting a department with projects resulted in projects remaining in environment with the status NotReady."},{"location":"platform-admin/overview/","title":"Overview: Platform Administrator","text":"<p>The Platform Administrator is responsible for the day-to-day administration of the product. </p> <p>As part of the Platform Administrator documentation you will find:</p> <ul> <li>Provide the right access level to users.</li> <li>Configure Run:ai meta-data such as Projects, Departments, Node pools etc.  </li> <li>Understand Researcher Workloads and set up Workload Policies and Assets.</li> <li>Analyze system performance and perform suggested actions. </li> </ul>"},{"location":"platform-admin/aiinitiatives/overview/","title":"AI Initiatives","text":"<p>AI initiatives refer to advancing research, development, and implementation of AI technologies. These initiatives represent your business needs and involve collaboration between individuals, teams, and other stakeholders. AI initiatives require compute resources and a methodology to effectively and efficiently use those compute resources and split them among the different AI initiatives stakeholders. The building blocks of AI compute resources are GPUs, CPUs, and CPU memory, which are built into nodes (servers) and can be further grouped into node pools. Nodes and node pools are part of a Kubernetes Cluster.</p> <p>To manage AI initiatives in Run:ai you should:</p> <ul> <li>Map your organization and initiatives to projects and optionally departments  </li> <li>Map compute resources (node pools and quotas) to projects and optionally departments  </li> <li>Assign users (e.g. AI practitioners, ML engineers, Admins) to projects and departments</li> </ul>"},{"location":"platform-admin/aiinitiatives/overview/#mapping-your-organization","title":"Mapping your organization","text":"<p>The way you map your AI initiatives and organization into Run:ai projects and departments should reflect your organization\u2019s structure and Project management practices. There are multiple options, and we provide you here with 3 examples of typical forms in which to map your organization, initiatives, and users into Run:ai, but of course, other ways that suit your requirements are also acceptable.</p>"},{"location":"platform-admin/aiinitiatives/overview/#based-on-individuals","title":"Based on individuals","text":"<p>A typical use case would be students (individual practitioners) within a faculty (business unit) - an individual practitioner may be involved in one or more initiatives. In this example, the resources are accounted for by the student (project) and aggregated per faculty (department). Department = business unit / Project = individual practitioner</p> <p></p>"},{"location":"platform-admin/aiinitiatives/overview/#based-on-business-units","title":"Based on business units","text":"<p>A typical use case would be an AI service (business unit) split into AI capabilities (initiatives) - an individual practitioner may be involved in several initiatives. In this example, the resources are accounted for by Initiative (project) and aggregated per AI service (department).</p> <p>Department = business unit / Project = initiative</p> <p></p>"},{"location":"platform-admin/aiinitiatives/overview/#based-on-the-organizational-structure","title":"Based on the organizational structure","text":"<p>A typical use case would be a business unit split into teams - an individual practitioner is involved in a single team (project) but the team may be involved in several AI initiatives. In this example, the resources are accounted for by team (project) and aggregated per business unit (department).</p> <p>Department = business unit  /  Project = team</p> <p></p>"},{"location":"platform-admin/aiinitiatives/overview/#mapping-your-resources","title":"Mapping your resources","text":"<p>AI initiatives require compute resources such as GPUs and CPUs to run. Compute resources in any organization are limited, either due to the number of servers (nodes) owned by the organization is limited, the budget it can spend to lease resources in the cloud or spending for in-house servers is also limited. Every organization strives to optimize the usage of its resources by maximizing their utilization and providing all users with their needs. Therefore, the organization needs to split resources according to the organization's internal priorities and budget constraints. But even after splitting the resources, the orchestration layer should still provide fairness between the resourced consumers, and allow access to unused resources to minimize scenarios of idle resources.</p> <p>Another aspect of resource management is how to group your resources effectively, especially in large environments, or environments that are made of heterogeneous types of hardware, where some users need to use specific hardware types, or where other users should avoid occupying critical hardware of some users or initiatives.</p> <p>Run:ai assists you with all of these complex issues by allowing you to map your cluster resources to node pools, then map each Project and Department a quota allocation per node pool, and set access rights to unused resources (Over quota) per node pool.</p>"},{"location":"platform-admin/aiinitiatives/overview/#grouping-your-resources","title":"Grouping your resources","text":"<p>There are several reasons why you would group resources (nodes) into node pools:</p> <ul> <li>Control the GPU type to use in heterogeneous hardware environment - in many cases, AI models can be optimized per hardware type they will use, e.g. a training workload that is optimized for H100 does not necessarily run optimally on an A100, and vice versa. Therefore segmenting into node pools, each with a different hardware type gives the AI researcher and ML engineer better control of where to run.  </li> <li>Quota control - splitting to node pools allows the admin to set specific quota per hardware type, e.g. give high priority project guaranteed access to advanced GPU hardware, while keeping lower priority project with a lower quota or even with no quota at all for that high-end GPU, but give it a \u201cbest-effort\u201d access only (i.e. if the high priority guaranteed project is not using those resources).  </li> <li>Multi-region or multi-availability-zone cloud environments - if some or all of your clusters run on the cloud (or even on-premise) but any of your clusters uses different physical locations or different topologies (e.g. racks), you probably want to segment your resources per region/zone/topology to be able to control where to run your workloads, how much quota to assign to specific environments (per project, per department), even if all those locations are all using the same hardware type. This methodology can help in optimizing the performance of your workloads because of the superior performance of local computing such as the locality of distributed workloads, local storage etc.  </li> <li>Explainability and predictability - large environments are complex to understand, this becomes even more complex when an environment is loaded. To maintain users\u2019 satisfaction and their understanding of the resources state, as well as to keep predictability of your workload chances to get scheduled, segmenting your cluster into smaller pools may significantly help.  </li> <li>Scale - Run:ai implementation of node pools has many benefits, one of the main of them is scale. Each node pool has its own scheduler instance, therefore allowing the cluster to handle more nodes and schedule workloads faster when segmented into node pools vs. one large cluster. To allow your workloads to use any resource within a cluster that is split to node pools, a second-level Scheduler is in charge of scheduling workloads between node pools according to your preferences and resource availability.  </li> <li>Prevent mutual exclusion - Some AI workloads consume CPU-only resources, to prevent those workloads from consuming the CPU resources of GPU nodes and thus block GPU workloads from using those nodes, it is recommended to group CPU-only nodes into a dedicated node pool(s) and assign a quota for CPU projects to CPU node-pools only while keeping GPU node-pools with zero quota and optionally \u201cbest-effort\u201d over-quota access for CPU-only projects.</li> </ul>"},{"location":"platform-admin/aiinitiatives/overview/#grouping-examples","title":"Grouping Examples","text":"<p>Set out below are illustrations of different grouping options.                              </p> <p>Example: grouping nodes by topology</p> <p></p> <p>Example: grouping nodes by hardware type</p> <p></p>"},{"location":"platform-admin/aiinitiatives/overview/#assigning-your-resources","title":"Assigning your resources","text":"<p>After the initial grouping of resources, it is time to associate resources to AI initiatives, this is performed by assigning quotas to projects and optionally to departments. Assigning GPU quota to a project, on a node pool basis, means that the workloads submitted by that project are entitled to use those GPUs as guaranteed resources and can use them for all workload types.</p> <p>However, what happens if the project requires more resources than its quota? This depends on the type of workloads that the user wants to submit. If the user requires more resources for non-preemptible workloads, then the quota must be increased, because non-preemptible workloads require guaranteed resources. On the other hand, if the type of workload is, for example, a model Training workload that is preemptible - in this case the project can exploit unused resources of other projects, as long as the other projects don\u2019t need them. Over-quota is set per project on a node-pool basis and per department.</p> <p>Administrators can use quota allocations to prioritize resources between users, teams, and AI initiatives. The administrator can completely prevent the use of certain node pools by a project or department by setting the node pool quota to 0 and disabling over quota for that node pool, or it can keep the quota to 0 and enable over-quota to that node pool and allow access based on resource availability only (e.g. unused GPUs). However, when a project with a non-zero quota needs to use those resources, the Scheduler reclaims those resources back and preempts the preemptible workloads of over-quota projects. As an administrator, you can also have an impact on the amount of over-quota resources a project or department uses.</p> <p>It is essential to make sure that the sum of all projects' quota does NOT surpass that of the Department, and that the sum of all departments does not surpass the number of physical resources, per node pool and for the entire cluster (we call such behavior - \u2018over-subscription\u2019). The reason over-subscription is not recommended is that it may produce unexpected scheduling decisions, especially those that might preempt \u2018non-preemptive\u2019 workloads or fail to schedule workloads within quota, either non-preemptible or preemptible, thus quota cannot be considered anymore as \u2018guaranteed\u2019. Admins can opt-in a system flag that helps to prevent over-subscription scenarios.</p> <p>Example: assigning resources to projects</p> <p></p>"},{"location":"platform-admin/aiinitiatives/overview/#assigning-users-to-projects-and-departments","title":"Assigning users to projects and departments","text":"<p>Run:ai system is using \u2018Role Based Access Control\u2019 (RBAC) to manage users\u2019 access rights to the different objects of the system, its resources, and the set of allowed actions. To allow AI researchers, ML engineers, Project Admins, or any other stakeholder of your AI initiatives to access projects and use AI compute resources with their AI initiatives, the administrator needs to assign users to projects. After a user is assigned to a project with the proper role, e.g. \u2018L1 Researcher\u2019, the user can submit and monitor its workloads under that project. Assigning users to departments is usually done to assign \u2018Department Admin\u2019 to manage a specific department. Other roles, such as \u2018L1 Researcher\u2019, can also be assigned to departments, this allows the researcher access to all projects within that department.</p>"},{"location":"platform-admin/aiinitiatives/overview/#submitting-workloads","title":"Submitting workloads","text":"<p>Now that resources are grouped into node pools, organizational units or business initiatives are mapped into projects and departments, projects\u2019 quota parameters are set per node pool, and users are assigned to projects, you can finally submit workloads from a project and use compute resources to run your AI initiatives.</p> <p>When a workload is submitted, it goes to the chosen Kubernetes cluster, and the Run:ai Scheduler handles it.</p> <p>The Scheduler\u2019s main role is to find the best-suited node or nodes for each submitted workload, so that those nodes match the resources and other characteristics requested by the workload while adhering to the quota and fairness principles of the Run:ai system. A workload can be a single pod running on a single node, or a distributed workload using multiple pods, each running on a node (or part of a node). It is not rare to find large training workloads using 128 nodes and even more, or inference workloads using multiple pods and nodes. There are numerous types of workloads, some are Kubernetes native and some are 3rd party extensions on top of Kubernetes native pods.  The Run:ai Scheduler schedules any Kubernetes native workloads, Run:ai workloads, or any type of 3rd party workload.</p>"},{"location":"platform-admin/aiinitiatives/overview/#scopes-in-the-organization","title":"Scopes in the organization","text":"<p>This is an example of an organization, as represented in the Run:ai platform:</p> <p></p> <p>The organizational tree is structured from top down under a single node headed by the account. The account is comprised of clusters, departments and projects.</p> <p>Note</p> <p>Different roles and permissions can be granted to specific clusters, departments and projects within an organization.</p> <p>The organizational tree is structured from top down under a single node headed by the account. The account is comprised of clusters, departments and projects.</p> <p>After mapping and building your hierarchal structured organization as shown above, you can assign or associate various Run:ai components (e.g. workloads, roles, assets, policies, and more) to different parts of the organization - these organizational parts are the Scopes. The following organizational example consists of 5 optional scopes:</p> <p></p> <p>Note</p> <p>When a scope is selected, the very same unit, including all of its subordinates (both existing and any future subordinates, if added), are selected as well.</p>"},{"location":"platform-admin/aiinitiatives/org/departments/","title":"Departments","text":"<p>This article explains the procedure for managing departments</p> <p>Departments are a grouping of projects. By grouping projects into a department, you can set quota limitations to a set of projects, create policies that are applied to the department, and create assets that can be scoped to the whole department or a partial group of descendent projects</p> <p>For example, in an academic environment, a department can be the Physics Department grouping various projects (AI Initiatives) within the department, or grouping projects where each project represents a single student.</p>"},{"location":"platform-admin/aiinitiatives/org/departments/#departments","title":"Departments","text":"<p>The Departments table can be found under Departments in the Run:ai platform.</p> <p>Note</p> <p>Departments are disabled, by default. If you cannot see Departments in the menu, then it must be enabled by your Administrator, under General Settings \u2192 Resources \u2192 Departments</p> <p>The Departments table lists all departments defined for a specific cluster and allows you to manage them. You can switch between clusters by selecting your cluster using the filter at the top.</p> <p></p> <p>The Departments table consists of the following columns:</p> Column Description Department The name of the department Node pool(s) with quota The node pools associated with this department. By default, all node pools within a cluster are associated with each department. Administrators can change the node pools\u2019 quota parameters for a department. Click the values under this column to view the list of node pools with their parameters (as described below) GPU quota GPU quota associated with the department Total GPUs for projects The sum of all projects\u2019 GPU quotas associated with this department Project(s) List of projects associated with this department Subject(s) The users, SSO groups, or applications with access to the project. Click the values under this column to view the list of subjects with their parameters (as described below). This column is only viewable if your role in Run:ai platform allows you those permissions. Allocated GPUs The total number of GPUs allocated by successfully scheduled workloads in projects associated with this department GPU allocation ratio The ratio of Allocated GPUs to GPU quota. This number reflects how well the department\u2019s GPU quota is utilized by its descendant projects. A number higher than 100% means the department is using over-quota GPUs. A number lower than 100% means not all projects are utilizing their quotas. A quota becomes allocated once a workload is successfully scheduled. Creation time The timestamp for when the department was created Workload(s) The list of workloads under projects associated with this department. Click the values under this column to view the list of workloads with their resource parameters (as described below) Cluster The cluster that the department is associated with"},{"location":"platform-admin/aiinitiatives/org/departments/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/aiinitiatives/org/departments/#node-pools-with-quota-associated-with-the-department","title":"Node pools with quota associated with the department","text":"<p>Click one of the values of Node pool(s) with quota column, to view the list of node pools and their parameters</p> Column Description Node pool The name of the node pool is given by the administrator during node pool creation. All clusters have a default node pool created automatically by the system and named \u2018default\u2019. GPU quota The amount of GPU quota the administrator dedicated to the department for this node pool (floating number, e.g. 2.3 means 230% of a GPU capacity) CPU (Cores) The amount of CPU (cores) quota the administrator has dedicated to the department for this node pool (floating number, e.g. 1.3 Cores = 1300 mili-cores). The \u2018unlimited\u2019 value means the CPU (Cores) quota is not bound and workloads using this node pool can use as many CPU (Cores) resources as they need (if available) CPU memory The amount of CPU memory quota the administrator has dedicated to the department for this node pool (floating number, in MB or GB). The \u2018unlimited\u2019 value means the CPU memory quota is not bounded and workloads using this node pool can use as much CPU memory resource as they need (if available). Allocated GPUs The total amount of GPUs allocated by workloads using this node pool under projects associated with this department. The number of allocated GPUs may temporarily surpass the GPU quota of the department if over-quota is used. Allocated CPU (Cores) The total amount of CPUs (cores) allocated by workloads using this node pool under all projects associated with this department. The number of allocated CPUs (cores) may temporarily surpass the CPUs (Cores) quota of the department if over-quota is used. Allocated CPU memory The actual amount of CPU memory allocated by workloads using this node pool under all projects associated with this department. The number of Allocated CPU memory may temporarily surpass the CPU memory quota if over-quota is used."},{"location":"platform-admin/aiinitiatives/org/departments/#subjects-authorized-for-the-project","title":"Subjects authorized for the project","text":"<p>Click one of the values of the Subject(s) column, to view the list of subjects and their parameters. This column is only viewable if your role in the Run:ai system affords you those permissions.</p> Column Description Subject A user, SSO group, or application assigned with a role in the scope of this department Type The type of subject assigned to the access rule (user, SSO group, or application). Scope The scope of this department within the organizational tree. Click the name of the scope to view the organizational tree diagram, you can only view the parts of the organizational tree for which you have permission to view. Role The role assigned to the subject, in this department\u2019s scope Authorized by The user who granted the access rule Last updated The last time the access rule was updated <p>Note</p> <p>A role given in a certain scope, means the role applies to this scope and any descendant scopes in the organizational tree.</p>"},{"location":"platform-admin/aiinitiatives/org/departments/#adding-a-new-department","title":"Adding a new department","text":"<p>To create a new Department:</p> <ol> <li>Click +NEW DEPARTMENT  </li> <li>Select a scope.     By default, the field contains the scope of the current UI context cluster, viewable at the top left side of your screen. You can change the current UI context cluster by clicking the \u2018Cluster: cluster-name\u2019 field and applying another cluster as the UI context. Alternatively, you can choose another cluster within the \u2018+ New Department\u2019 form by clicking the organizational tree icon on the right side of the scope field, opening the organizational tree and selecting one of the available clusters.  </li> <li>Enter a name for the department. Department names must start with a letter and can only contain lower case latin letters, numbers or a hyphen ('-\u2019).  </li> <li>Under Quota Management, select a quota for the department. The Quota management section may contain different fields depending on pre-created system configuration. Possible system configurations are:  <ul> <li>Existence of Node Pools  </li> <li>CPU Quota - Allow setting a quota for CPU resources.</li> </ul> </li> </ol> <p>When no node pools are configured, you can set the following quota parameters:</p> <ul> <li>GPU Devices   The number of GPUs you want to allocate for this department (decimal number). This quota is consumed by the department\u2019s subordinated project.  </li> <li>CPUs (cores) (when CPU quota is set)    The number of CPU cores you want to allocate for this department (decimal number). This quota is consumed by the department\u2019s subordinated projects  </li> <li>CPUs memory (when CPU quota is set)    The amount of CPU memory you want to allocate for this department (in Megabytes or Gigabytes). This quota is consumed by the department\u2019s subordinated projects</li> </ul> <p>When node pools are enabled, it is possible to set the above quota parameters for each node-pool separately.</p> <ul> <li>In addition, you can decide whether to allow a department to go over-quota. Allowing over-quota at the department level means that one department can receive more resources than its quota when not required by other departments. If the over-quota is disabled, workloads running under subordinated projects are not able to use more resources than the department\u2019s quota, but each project can still go over-quota (if enabled at the project level) up to the department\u2019s quota.</li> </ul> <p>Unlimited CPU(Cores) and CPU memory quotas are an exception - in this case, workloads of subordinated projects can consume available resources up to the physical limitation of the cluster or any of the node pools.</p> <p>Example of Quota management:</p> <p></p> <ol> <li>Click CREATE DEPARTMENT</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/departments/#adding-an-access-rule-to-a-department","title":"Adding an access rule to a department","text":"<p>To create a new access rule for a department:</p> <ol> <li>Select the department you want to add an access rule for  </li> <li>Click ACCESS RULES  </li> <li>Click +ACCESS RULE  </li> <li>Select a subject  </li> <li>Select or enter the subject identifier:  <ul> <li>User Email for a local user created in Run:ai or for SSO user as recognized by the IDP  </li> <li>Group name as recognized by the IDP  </li> <li>Application name as created in Run:ai  </li> </ul> </li> <li>Select a role  </li> <li>Click SAVE RULE  </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/departments/#deleting-an-access-rule-from-a-department","title":"Deleting an access rule from a department","text":"<p>To delete an access rule from a department:</p> <ol> <li>Select the department you want to remove an access rule from  </li> <li>Click ACCESS RULES  </li> <li>Find the access rule you would like to delete  </li> <li>Click on the trash icon  </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/departments/#editing-a-department","title":"Editing a department","text":"<ol> <li>Select the Department you want to edit  </li> <li>Click EDIT  </li> <li>Update the Department and click SAVE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/departments/#viewing-a-departments-policy","title":"Viewing a department\u2019s policy","text":"<p>To view the policy of a department:</p> <ol> <li>Select the department for which you want to view its policies.     This option is only active if the department has defined policies in place.  </li> <li>Click VIEW POLICY and select the workload type for which you want to view the policies:     a. Workspace workload type policy with its set of rules     b. Training workload type policies with its set of rules  </li> <li>In the Policy form, view the workload rules that are enforcing your department for the selected workload type as well as the defaults:  <ul> <li>Parameter - The workload submission parameter that Rule and Default is applied on  </li> <li>Type (applicable for data sources only) - The data source type (Git, S3, nfs, pvc etc.)  </li> <li>Default - The default value of the Parameter  </li> <li>Rule - Set up constraints on workload policy fields  </li> <li>Source - The origin of the applied policy (cluster, department or project)  </li> </ul> </li> </ol> <p>Notes</p> <ul> <li>The policy affecting the department consists of rules and defaults. Some of these rules and defaults may be derived from the policies of a parent cluster (source). You can see the source of each rule in the policy form.  </li> <li>A policy set for a department affects all subordinated projects and their workloads, according to the policy workload type</li> </ul>"},{"location":"platform-admin/aiinitiatives/org/departments/#deleting-a-department","title":"Deleting a department","text":"<ol> <li>Select the department you want to delete  </li> <li>Click DELETE  </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>Deleting a department permanently deletes its subordinated projects, any assets created in the scope of this department, and any of its subordinated projects such as compute resources, environments, data sources, templates, and credentials. However, workloads running within the department\u2019s subordinated projects, or the policies defined for this department or its subordinated projects - remain intact and running.</p>"},{"location":"platform-admin/aiinitiatives/org/departments/#reviewing-a-department","title":"Reviewing a department","text":"<ol> <li>Select the department you want to review  </li> <li>Click REVIEW  </li> <li>Review and click CLOSE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/departments/#using-api","title":"Using API","text":"<p>Go to the Departments API reference to view the available actions</p>"},{"location":"platform-admin/aiinitiatives/org/projects/","title":"Projects","text":"<p>This article explains the procedure to manage Projects.</p> <p>Researchers submit AI workloads. To streamline resource allocation and prioritize work, Run:ai introduces the concept of Projects. Projects are the tool to implement resource allocation policies as well as the segregation between different initiatives. A project may represent a team, an individual, or an initiative that shares resources or has a specific resource quota. Projects may be aggregated in Run:ai departments.</p> <p>For example, you may have several people involved in a specific face-recognition initiative collaborating under one project named \u201cface-recognition-2024\u201d. Alternatively, you can have a project per person in your team, where each member receives their own quota.</p>"},{"location":"platform-admin/aiinitiatives/org/projects/#projects-table","title":"Projects table","text":"<p>The Projects table can be found under Projects in the Run:ai platform.</p> <p>The Projects table provides a list of all projects defined for a specific cluster, and allows you to manage them. You can switch between clusters by selecting your cluster using the filter at the top.</p> <p></p> <p>The Projects table consists of the following columns:</p> Column Description Project The name of the project Department The name of the parent department. Several projects may be grouped under a department. Status The Project creation status. Projects are manifested as Kubernetes namespaces. The project status represents the Namespace creation status. Node pool(s) with quota The node pools associated with the project. By default, a new project is associated with all node pools within its associated cluster. Administrators can change the node pools\u2019 quota parameters for a project. Click the values under this column to view the list of node pools with their parameters (as described below) Subject(s) The users, SSO groups, or applications with access to the project. Click the values under this column to view the list of subjects with their parameters (as described below). This column is only viewable if your role in the Run:ai platform allows you those permissions. Allocated GPUs The total number of GPUs allocated by successfully scheduled workloads under this project GPU allocation ratio The ratio of Allocated GPUs to GPU quota. This number reflects how well the project\u2019s GPU quota is utilized by its descendent workloads. A number higher than 100% indicates the project is using over-quota GPUs. GPU quota The GPU quota allocated to the project. This number represents the sum of all node pools\u2019 GPU quota allocated to this project. Allocated CPUs (Core) The total number of CPU cores allocated by workloads submitted within this project. (This column is only available if the CPU Quota setting is enabled, as described below). Allocated CPU Memory The total number of CPUs allocated by successfully scheduled workloads under this project. (This column is only available if the CPU Quota setting is enabled, as described below). CPU quota (Cores) CPU quota allocated to this project. (This column is only available if the CPU Quota setting is enabled, as described below). This number represents the sum of all node pools\u2019 CPU quota allocated to this project. The \u2018unlimited\u2019 value means the CPU (cores) quota is not bounded and workloads using this project can use as many CPU (cores) resources as they need (if available). CPU memory quota CPU memory quota allocated to this project. (This column is only available if the CPU Quota setting is enabled, as described below). This number represents the sum of all node pools\u2019 CPU memory quota allocated to this project. The \u2018unlimited\u2019 value means the CPU memory quota is not bounded and workloads using this Project can use as much CPU memory resources as they need (if available). CPU allocation ratio The ratio of Allocated CPUs (cores) to CPU quota (cores). This number reflects how much the project\u2019s \u2018CPU quota\u2019 is utilized by its descendent workloads. A number higher than 100% indicates the project is using over-quota CPU cores. CPU memory allocation ratio The ratio of Allocated CPU memory to CPU memory quota. This number reflects how well the project\u2019s \u2018CPU memory quota\u2019 is utilized by its descendent workloads. A number higher than 100% indicates the project is using over-quota CPU memory. Node affinity of training workloads The list of Run:ai node-affinities. Any training workload submitted within this project must specify one of those Run:ai node affinities, otherwise it is not submitted. Node affinity of interactive workloads The list of Run:ai node-affinities. Any interactive (workspace) workload submitted within this project must specify one of those Run:ai node affinities, otherwise it is not submitted. Idle time limit of training workloads The time in days:hours:minutes after which the project stops a training workload not using its allocated GPU resources. Idle time limit of preemptible workloads The time in days:hours:minutes after which the project stops a preemptible interactive (workspace) workload not using its allocated GPU resources. Idle time limit of non preemptible workloads The time in days:hours:minutes after which the project stops a non-preemptible interactive (workspace) workload not using its allocated GPU resources.. Interactive workloads time limit The duration in days:hours:minutes after which the project stops an interactive (workspace) workload Training workloads time limit The duration in days:hours:minutes after which the project stops a training workload Creation time The timestamp for when the project was created Workload(s) The list of workloads associated with the project. Click the values under this column to view the list of workloads with their resource parameters (as described below). Cluster The cluster that the project is associated with"},{"location":"platform-admin/aiinitiatives/org/projects/#node-pools-with-quota-associated-with-the-project","title":"Node pools with quota associated with the project","text":"<p>Click one of the values of Node pool(s) with quota column, to view the list of node pools and their parameters</p> Column Description Node pool The name of the node pool is given by the administrator during node pool creation. All clusters have a default node pool created automatically by the system and named \u2018default\u2019. GPU quota The amount of GPU quota the administrator dedicated to the project for this node pool (floating number, e.g. 2.3 means 230% of GPU capacity). CPU (Cores) The amount of CPUs (cores) quota the administrator has dedicated to the project for this node pool (floating number, e.g. 1.3 Cores = 1300 mili-cores). The \u2018unlimited\u2019 value means the CPU (Cores) quota is not bounded and workloads using this node pool can use as many CPU (Cores) resources as they require, (if available). CPU memory The amount of CPU memory quota the administrator has dedicated to the project for this node pool (floating number, in MB or GB). The \u2018unlimited\u2019 value means the CPU memory quota is not bounded and workloads using this node pool can use as much CPU memory resource as they need (if available). Allocated GPUs The actual amount of GPUs allocated by workloads using this node pool under this project. The number of allocated GPUs may temporarily surpass the GPU quota if over-quota is used. Allocated CPU (Cores) The actual amount of CPUs (cores) allocated by workloads using this node pool under this project. The number of allocated CPUs (cores) may temporarily surpass the CPUs (Cores) quota if over-quota is used. Allocated CPU memory The actual amount of CPU memory allocated by workloads using this node pool under this Project. The number of Allocated CPU memory may temporarily surpass the CPU memory quota if over-quota is used. Order of priority The default order in which the Scheduler uses node-pools to schedule a workload. This is used only if the order of priority of node pools is not set in the workload during submission, either by an admin policy or the user. An empty value means the node pool is not part of the project\u2019s default list, but can still be chosen by an admin policy or the user during workload submission"},{"location":"platform-admin/aiinitiatives/org/projects/#subjects-authorized-for-the-project","title":"Subjects authorized for the project","text":"<p>Click one of the values in the Subject(s) column, to view the list of subjects and their parameters. This column is only viewable, if your role in the Run:ai system affords you those permissions.</p> Column Description Subject A user, SSO group, or application assigned with a role in the scope of this Project Type The type of subject assigned to the access rule (user, SSO group, or application) Scope The scope of this project in the organizational tree. Click the name of the scope to view the organizational tree diagram, you can only view the parts of the organizational tree for which you have permission to view. Role The role assigned to the subject, in this project\u2019s scope Authorized by The user who granted the access rule Last updated The last time the access rule was updated"},{"location":"platform-admin/aiinitiatives/org/projects/#workloads-associated-with-the-project","title":"Workloads associated with the project","text":"<p>Click one of the values of Workload(s) column, to view the list of workloads and their parameters</p> Column Description Workload The name of the workload, given during its submission. Optionally, an icon describing the type of workload is also visible Type The type of the workload, e.g. Workspace, Training, Inference Status The state of the workload and time elapsed since the last status change Created by The subject that created this workload Running/ requested pods The number of running pods out of the number of requested pods for this workload. e.g. a distributed workload requesting 4 pods but may be in a state where only 2 are running and 2 are pending Creation time The date and time the workload was created GPU compute request The amount of GPU compute requested (floating number, represents either a portion of the GPU compute, or the number of whole GPUs requested) GPU memory request The amount of GPU memory requested (floating number, can either be presented as a portion of the GPU memory, an absolute memory size in MB or GB, or a MIG profile) CPU memory request The amount of CPU memory requested (floating number, presented as an absolute memory size in MB or GB) CPU compute request The amount of CPU compute requested (floating number, represents the number of requested Cores)"},{"location":"platform-admin/aiinitiatives/org/projects/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/aiinitiatives/org/projects/#adding-a-new-project","title":"Adding a new project","text":"<p>To create a new Project:</p> <ol> <li>Click +NEW PROJECT  </li> <li>Select a scope, you can only view clusters if you have permission to do so - within the scope of the roles assigned to you  </li> <li>Enter a name for the project    Project names must start with a letter and can only contain lower case Latin letters, numbers or a hyphen ('-\u2019)  </li> <li>Namespace associated with Project    Each project has an associated (Kubernetes) namespace in the cluster.     All workloads under this project use this namespace.    a. By default, Run:ai creates a namespace based on the Project name (in the form of <code>runai-&lt;name&gt;</code>)    b. Alternatively, you can choose an existing namespace created for you by the cluster administrator  </li> <li>In the Quota management section, you can set the quota parameters and prioritize resources  <ul> <li>Order of priority This column is displayed only if more than one node pool exists. The default order in which the Scheduler uses node pools to schedule a workload. This means the Scheduler first tries to allocate resources using the highest priority node pool, then the next in priority, until it reaches the lowest priority node pool list, then the Scheduler starts from the highest again. The Scheduler uses the Project list of prioritized node pools, only if the order of priority of node pools is not set in the workload during submission, either by an admin policy or by the user. Empty value means the node pool is not part of the Project\u2019s default node pool priority list, but a node pool can still be chosen by the admin policy or a user during workload submission  </li> <li>Node pool This column is displayed only if more than one node pool exists. It represents the name of the node pool.  </li> <li>GPU devices The number of GPUs you want to allocate for this project in this node pool (decimal number).  </li> <li>CPUs (Cores) This column is displayed only if CPU quota is enabled via the General settings. Represents the number of CPU cores you want to allocate for this project in this node pool (decimal number).  </li> <li>CPU memory This column is displayed only if CPU quota is enabled via the General settings. The amount of CPU memory you want to allocate for this project in this node pool (in Megabytes or Gigabytes).  </li> <li>Over quota / Over quota priority If over-quota priority is enabled via the General settings then over-quota priority is presented, otherwise over-quota  is presented  <ul> <li>Over quota When enabled, the project can use non-guaranteed overage resources above its quota in this node pool. The amount of the non-guaranteed overage resources for this project is calculated proportionally to the project quota in this node pool. When disabled, the project cannot use more resources than the guaranteed quota in this node pool.  </li> <li>Over quota priority Represents a weight used to calculate the amount of non-guaranteed overage resources a project can get on top of its quota in this node pool. All unused resources are split between projects that require the use of overage resources:  <ul> <li>Medium The default value. The Admin can change the default to any of the following values: High, Low, Lowest, or None.  </li> <li>None When set, the project cannot use more resources than the guaranteed quota in this node pool.  </li> <li>Lowest Over-quota priority \u2018lowest\u2019 has a unique behavior, because its weight is 0, it can only use over-quota (unused overage) resources if no other project needs them, and any project with a higher over-quota priority can snap the average resources at any time.</li> </ul> </li> </ul> </li> </ul> </li> </ol> <p>Note</p> <p>Setting the quota to 0 (either GPU, CPU, or CPU memory) and the over-quota to \u2018disabled\u2019 or over-quota priority to \u2018none\u2019 means the project is blocked from using those resources on this node pool.</p> <p>When no node pools are configured, you can set the same parameters but it is for the whole project, instead of per node pool.</p> <p>After node pools are created, you can set the above parameters for each node-pool separately.</p> <p></p> <ol> <li> <p>Set Scheduling rules as required. You can have a scheduling rule for:  </p> <ul> <li>Idle GPU timeout Preempt a workload that does not use GPUs for more than a specified duration. You can apply a single rule per workload type - Preemptive Workspaces, Non-preemptive Workspaces, and Training.  </li> </ul> <p>Note</p> <p>To make \u2018Idle GPU timeout\u2019 effective, it must be set to a shorter duration than that workload duration of the same workload type.  </p> <ul> <li>Workspace duration Preempt workspaces after a specified duration. This applies to both preemptive and non-preemptive Workspaces.  </li> <li>Training duration Preempt a training workload after a specified duration.  </li> <li>Node type (Affinity) Node type is used to select a group of nodes, usually with specific characteristics such as a hardware feature, storage type, fast networking interconnection, etc. The scheduler uses node type as an indication of which nodes should be used for your workloads, within this project. Node type is a label in the form of run.ai/type and a value (e.g. run.ai/type = dgx200) that the administrator uses to tag a set of nodes. Adding the node type to the project\u2019s scheduling rules enables the user to submit workloads with any node type label/value pairs in this list, according to the workload type - Workspace or Training. The Scheduler then schedules workloads using a node selector, targeting nodes tagged with the Run:ai node type label/value pair. Node pools and a node type can be used in conjunction with each other. For example, specifying a node pool and a smaller group of nodes from that node pool that includes a fast SSD memory or other unique characteristics.  </li> </ul> </li> <li> <p>Click CREATE PROJECT</p> </li> </ol>"},{"location":"platform-admin/aiinitiatives/org/projects/#adding-an-access-rule-to-a-project","title":"Adding an access rule to a project","text":"<p>To create a new access rule for a project:</p> <ol> <li>Select the project you want to add an access rule for  </li> <li>Click ACCESS RULES  </li> <li>Click +ACCESS RULE  </li> <li>Select a subject  </li> <li>Select or enter the subject identifier:  <ol> <li>User Email for a local user created in Run:ai or for SSO user as recognized by the IDP  </li> <li>Group name as recognized by the IDP  </li> <li>Application name as created in Run:ai  </li> </ol> </li> <li>Select a role  </li> <li>Click SAVE RULE  </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/projects/#deleting-an-access-rule-from-a-project","title":"Deleting an access rule from a project","text":"<p>To delete an access rule from a project:</p> <ol> <li>Select the project you want to remove an access rule from  </li> <li>Click ACCESS RULES  </li> <li>Find the access rule you want to delete  </li> <li>Click on the trash icon  </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/projects/#editing-a-project","title":"Editing a project","text":"<p>To edit a project:</p> <ol> <li>Select the project you want to edit  </li> <li>Click EDIT  </li> <li>Update the Project and click SAVE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/projects/#viewing-a-projects-policy","title":"Viewing a project\u2019s policy","text":"<p>To view the policy of a project:</p> <ol> <li>Select the project for which you want to view its policies. This option is only active for projects with defined policies in place.  </li> <li>Click VIEW POLICY and select the workload type for which you want to view the policies:     a. Workspace workload type policy with its set of rules     b. Training workload type policies with its set of rules  </li> <li>In the Policy form, view the workload rules that are enforcing your project for the selected workload type as well as the defaults:  <ul> <li>Parameter - The workload submission parameter that Rules and Defaults are applied to  </li> <li>Type (applicable for data sources only) - The data source type (Git, S3, nfs, pvc etc.)  </li> <li>Default - The default value of the Parameter  </li> <li>Rule - Set up constraints on workload policy fields  </li> <li>Source - The origin of the applied policy (cluster, department or project)  </li> </ul> </li> </ol> <p>Note</p> <p>The policy affecting the project consists of rules and defaults. Some of these rules and defaults may be derived from policies of a parent cluster and/or department (source). You can see the source of each rule in the policy form.</p>"},{"location":"platform-admin/aiinitiatives/org/projects/#deleting-a-project","title":"Deleting a project","text":"<p>To delete a project:</p> <ol> <li>Select the project you want to delete  </li> <li>Click DELETE  </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>Deleting a project does not delete its associated namespace, any of the workloads running using this namespace, or the policies defined for this project. However, any assets created in the scope of this project such as compute resources, environments, data sources, templates and credentials, are permanently deleted from the system.</p>"},{"location":"platform-admin/aiinitiatives/org/projects/#using-api","title":"Using API","text":"<p>Go to the Projects API reference to view the available actions</p>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/","title":"Scheduling Rules","text":"<p>This article explains the procedure of configuring and managing Scheduling rules. Scheduling rules refer to restrictions applied over workloads. These restrictions apply to either the resources (nodes) on which workloads can run or to the duration of the workload run time. Scheduling rules are set for Projects and apply to a specific workload type. Once scheduling rules are set for a project, all matching workloads associated with the project will have the restrictions as defined when the workload was submitted. New scheduling rules added to a project are not applied over already created workloads associated with that project.</p> <p>There are 3 types of scheduling rules:</p> <ul> <li>Workload duration (time limit)     This rule limits the duration of a workload run time. Workload run time is calculated as the total time in which the workload was in status Running. You can apply a single rule per workload type - Preemptive Workspaces, Non-preemptive Workspaces, and Training.  </li> <li>Idle GPU time limit    This rule limits the total GPU time of a workload. Workload idle time is counted from the first time the workload is in status Running and the GPU was idle.   We calculate idleness by employing the <code>runai_gpu_idle_seconds_per_workload</code> metric. This metric determines the total duration of zero GPU utilization within each 30-second interval. If the GPU remains idle throughout the 30-second window, 30 seconds are added to the idleness sum; otherwise, the idleness count is reset.   You can apply a single rule per workload type - Preemptible Workspaces, Non-preemptible Workspaces, and Training.  </li> </ul> <p>Note</p> <p>To make <code>Idle GPU timeout</code> effective, it must be set to a shorter duration than that workload duration of the same workload type. </p> <ul> <li>Node type (Affinity)   Node type is used to select a group of nodes, typically with specific characteristics such as a hardware feature, storage type, fast networking interconnection, etc. The scheduler uses node type as an indication of which nodes should be used for your workloads, within this project.    Node type is a label in the form of <code>run.ai/type</code> and a value (e.g. <code>run.ai/type = dgx200</code>) that the administrator uses to tag a set of nodes. Adding the node type to the project\u2019s scheduling rules enables the user to submit workloads with any node type label/value pairs in this list, according to the workload type - Workspace or Training. The Scheduler then schedules workloads using a node selector, targeting nodes tagged with the Run:ai node type label/value pair. Node pools and a node type can be used in conjunction with each other. For example, specifying a node pool and a smaller group of nodes from that node pool that includes a fast SSD memory or other unique characteristics.</li> </ul>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#adding-a-scheduling-rule-to-a-project","title":"Adding a scheduling rule to a project","text":"<p>To add a scheduling rule:</p> <ol> <li>Select the project you want to add a scheduling rule for  </li> <li>Click EDIT </li> <li>In the Scheduling rules section click +RULE </li> <li>Select the rule type </li> <li>Select the workload type and time limitation period </li> <li>For Node type, choose one or more labels for the desired nodes.  </li> <li>Click SAVE</li> </ol> <p>Note</p> <p>You can review the defined rules in the Projects table in the relevant column.</p>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#editing-the-projects-scheduling-rule","title":"Editing the project\u2019s scheduling rule","text":"<p>To edit a scheduling rule:</p> <ol> <li>Select the project you want to edit its scheduling rule  </li> <li>Click EDIT </li> <li>Find the scheduling rule you would like to edit  </li> <li>Edit the rule  </li> <li>Click SAVE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#deleting-the-projects-scheduling-rule","title":"Deleting the project\u2019s scheduling rule","text":"<p>To delete a scheduling rule:</p> <ol> <li>Select the project you want to delete a scheduling rule from  </li> <li>Click EDIT </li> <li>Find the scheduling rule you would like to delete  </li> <li>Click on the x icon  </li> <li>Click SAVE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#using-api","title":"Using API","text":"<p>Go to the Projects API reference to view the available actions</p>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/","title":"Node Pools","text":"<p>This article explains the procedure for managing Node pools.</p> <p>Node pools assist in managing heterogeneous resources effectively. A node pool is a Run:ai construct representing a set of nodes grouped into a bucket of resources using a predefined node label (e.g. NVidia GPU type) or an administrator-defined node label (any key/value pair).</p> <p>Typically, the grouped nodes share a common feature or property, such as GPU type or other HW capability (such as Infiniband connectivity), or represent a proximity group (i.e. nodes interconnected via a local ultra-fast switch). Researchers and ML Engineers would typically use node pools to run specific workloads on specific resource types.</p> <p>Platform administrators can create, view, edit, and delete node pools. Creating a new node pool creates a new instance of the Run:ai scheduler, workloads submitted to a node pool will be scheduled using the node pool\u2019s designated scheduler instance.</p> <p>Once a new node pool is created, it is automatically assigned to all Projects and Departments with a quota of zero GPU resources, unlimited CPU resources, and over-quota enabled (Medium priority if over-quota priority is enabled). This allows any Project and Department to use any node pool when over-quota is enabled, even if the administrator has not assigned a quota for a specific node pool in a Project or Department.</p> <p>Workloads can be submitted using a prioritized list of node pools, the node pool selector picks one node pool at a time (according to the prioritized list) and the designated node pool scheduler instance handles the submission request and tries to match the requested resources within that node pool. If the scheduler cannot find resources to satisfy the submitted workload, the node pool selector will move the request to the next node pool in the prioritized list, if no node pool satisfies the request, the node pool selector will start from the first node pool again until one of the node pools satisfies the request.</p>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#node-pools-table","title":"Node pools table","text":"<p>The Node pools table can be found under Nodes in the Run:ai platform.</p> <p>The Node pools table lists all the node pools defined in the Run:ai platform and allows you to manage them.</p> <p>Note</p> <p>By default, the Run:ai platform includes a single node pool named \u2018default\u2019. When no other node pool is defined, all existing and new nodes are associated with the \u2018default\u2019 node pool. When deleting a node pool, if no other node pool matches any of the nodes\u2019 labels, the node will be included in the default node pool.</p> <p></p> <p>The Node pools table consists of the following columns:</p> Column Description Node pool The node pool name, set by the administrator during its creation (the node pool name cannot be changed after its creation). Status Node pool status. A \u2018Ready\u2019 status means the scheduler can use this node pool to schedule workloads. \u2018Empty\u2019 status means no nodes are currently included in that node pool. Label key  Label value The node pool controller will use this node-label key-value pair to match nodes into this node pool. Node(s) List of nodes included in this node pool. Click the field to view details (the details are in the Nodes article). GPU devices The total number of GPU devices installed into nodes included in this node pool. For example, a node pool that includes 12 nodes each with 8 GPU devices would show a total number of 96 GPU devices. GPU memory The total amount of GPU memory included in this node pool. The total amount of GPU memory installed in nodes included in this node pool. For example, a node pool that includes 12 nodes, each with 8 GPU devices, and each device with 80 GB of memory would show a total memory amount of 7.68 TB. Projects\u2019 GPU quota The sum of all Projects\u2019 assigned GPU quota in this node pool. Allocated GPUs The total allocation of GPU devices in units of GPUs (decimal number). For example, if 3 GPUs are 50% allocated, the field prints out the value 1.50. This value represents the portion of GPU memory consumed by all running pods using this node pool. \u2018Allocated GPUs\u2019 can be larger than \u2018Projects\u2019 GPU quota\u2019 if over-quota is used by workloads, but not larger than GPU devices. Used GPU memory The actual amount of memory (in GB or MB) used by pods running on nodes that are included in this node pool. GPU compute utilization The average compute utilization of all GPU devices included in this node pool (decimal percentage) GPU memory utilization The average memory utilization of all GPU devices included this node pool (decimal percentage) CPUs (Cores) The number of CPU cores installed on nodes included in this node CPU memory The total amount of CPU memory installed on nodes using this node pool Projects\u2019 CPU quota (Cores) The sum of all Projects\u2019 assigned CPU quota in this node pool. Projects\u2019 CPU memory quota The sum of all Projects\u2019 assigned CPU memory quota in this node pool. Allocated CPUs (Cores) The total allocation of CPU compute in units of Cores (decimal number). This value represents the amount of CPU cores consumed by all running pods using this node pool. \u2018Allocated CPUs\u2019 can be larger than \u2018Projects\u2019 GPU quota\u2019 if over-quota is used by workloads, but not larger than CPUs (Cores). Allocated CPU memory The total allocation of CPU memory in units of TB/GB/MB (decimal number). This value represents the amount of CPU memory consumed by all running pods using this node pool. \u2018Allocated CPUs\u2019 can be larger than \u2018Projects\u2019 CPU memory quota\u2019 if over-quota is used by workloads, but not larger than CPU memory. Used CPU memory The total amount of actually used CPU memory by pods running on nodes included in this node pool. Pods may allocate memory but not use all of it, or go beyond their CPU memory allocation if using Limit &gt; Request for CPU memory (burstable workloads). CPU compute utilization The average utilization of all CPU compute resources on nodes included in this node pool (percentage) CPU memory utilization The average utilization of all CPU memory resources on nodes included in this node pool (percentage) GPU placement strategy Sets the Scheduler strategy for the assignment of pods requesting both GPU and CPU resources to nodes, which can be either Bin-pack or Spread. By default, Bin-Pack is used, but can be changed to Spread by editing the node pool. When set to Bin-pack the scheduler will try to fill nodes as much as possible before using empty or sparse nodes, when set to spread the scheduler will try to keep nodes as sparse as possible by spreading workloads across as many nodes as it succeeds. CPU placement strategy Sets the Scheduler strategy for the assignment of pods requesting only CPU resources to nodes, which can be either Bin-pack or Spread. By default, Bin-Pack is used, but can be changed to Spread by editing the node pool. When set to Bin-pack the scheduler will try to fill nodes as much as possible before using empty or sparse nodes, when set to spread the scheduler will try to keep nodes as sparse as possible by spreading workloads across as many nodes as it succeeds. Last update The date and time when the node pool was last updated Creation time The date and time when the node pool was created Workload(s) List of workloads running on nodes included in this node pool, click the field to view details (described below in this article)"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#workloads-associated-with-the-node-pool","title":"Workloads associated with the node pool","text":"<p>Click one of the values in the Workload(s) column, to view the list of workloads and their parameters.</p> <p>Note</p> <p>This column is only viewable if your role in the Run:ai platform gives you read access to workloads, even if you are allowed to view workloads, you can only view the workloads within your allowed scope. This means, there might be more pods running on this node than appear in the list your are viewing.</p> Column Description Workload The name of the workload. If the workloads\u2019 type is one of the recognized types (for example: Pytorch, MPI, Jupyter, Ray, Spark, Kubeflow, and many more), an appropriate icon is printed. Type The Run:ai platform type of the workload - Workspace, Training, or Inference Status The state of the workload. The Workloads state is described in the \u2018Run:ai Workloads\u2019 article. Created by The User or Application created this workload Running/requested pods The number of running pods out of the number of requested pods within this workload. Creation time The workload\u2019s creation date and time Allocated GPU compute The total amount of GPU compute allocated by this workload. A workload with 3 Pods, each allocating 0.5 GPU, will show a value of 1.5 GPUs for the workload. Allocated GPU memory The total amount of GPU memory allocated by this workload. A workload with 3 Pods, each allocating 20GB, will show a value of 60 GB for the workload. Allocated CPU compute (cores) The total amount of CPU compute allocated by this workload. A workload with 3 Pods, each allocating 0.5 Core, will show a value of 1.5 Cores for the workload. Allocated CPU memory The total amount of CPU memory allocated by this workload. A workload with 3 Pods, each allocating 5 GB of CPU memory, will show a value of 15 GB of CPU memory for the workload."},{"location":"platform-admin/aiinitiatives/resources/node-pools/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV  </li> <li>Show/Hide details - Click to view additional information on the selected row</li> </ul>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#adding-a-new-node-pool","title":"Adding a new node pool","text":"<p>To create a new node pool:</p> <ol> <li>Click +NEW NODE POOL </li> <li>Enter a name for the node pool.     Node pools names must start with a letter and can only contain lowercase Latin letters, numbers or a hyphen ('-\u2019)  </li> <li> <p>Enter the node pool label:    The node pool controller will use this node-label key-value pair to match nodes into this node pool.  </p> <ul> <li> <p>Key is the unique identifier of a node label.  </p> <ul> <li>The key must fit the following regular expression:  <code>^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?/?([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]$</code> </li> <li>The administrator can put an automatically preset label such as the nvidia.com/gpu.product that labels the GPU type or any other key from a node label.  </li> </ul> </li> <li> <p>Value is the value of that label identifier (key). The same key may have different values, in this case, they are    considered as different labels.  </p> <ul> <li>Value must fit the following regular expression: <code>^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$</code> </li> </ul> </li> <li>A node pool is defined by a single key-value pair. You must not use different labels that are set on the same node by    different node pools, this situation may lead to unexpected results.  </li> </ul> </li> <li> <p>Set the GPU placement strategy:  </p> <ul> <li>Bin-pack - Place as many workloads as possible in each GPU and node to use fewer resources and maximize GPU and node vacancy.  </li> <li>Spread Spread workloads across as many GPUs and nodes as possible to minimize the load and maximize the available resources per workload.  </li> <li>GPU workloads are workloads that request both GPU and CPU resources</li> </ul> </li> <li> <p>Set the CPU placement strategy:  </p> <ul> <li>Bin-pack - Place as many workloads as possible in each CPU and node to use fewer resources and maximize CPU and node vacancy.  </li> <li>Spread - Spread workloads across as many CPUs and nodes as possible to minimize the load and maximize the available resources per workload.  </li> <li>CPU workloads are workloads that request purely CPU resources  </li> </ul> </li> <li> <p>Click CREATE NODE POOL</p> </li> </ol>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#labeling-nodes-for-node-pool-grouping","title":"Labeling nodes for node-pool grouping:","text":"<p>The Infrastructure Administrator can use a preset node label such as the <code>nvidia.com/gpu.product</code> that labels the GPU type, or configure any other node label (e.g. <code>faculty=physics</code>).</p> <p>To assign a label to nodes you want to group into a node pool, set a node label on each node:</p> <ol> <li> <p>Get the list of nodes and their current labels using the following command:    <pre><code>kubectl get nodes --show-labels\n</code></pre></p> </li> <li> <p>Annotate a specific node with a new label using the following command:    <pre><code>kubectl label node &lt;node-name&gt; &lt;key&gt;=&lt;value&gt;\n</code></pre></p> </li> </ol>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#editing-a-node-pool","title":"Editing a node pool","text":"<ol> <li>Select the node pool you want to edit  </li> <li>Click EDIT </li> <li>Update the node pool and click SAVE</li> </ol>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#deleting-a-node-pool","title":"Deleting a node pool","text":"<ol> <li>Select the node pool you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>The <code>default</code> node pool cannot be deleted. When deleting a node pool, if no other node pool matches any of the nodes\u2019 labels, the node will be included in the default node pool.</p>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#using-api","title":"Using API","text":"<p>Go to the Node pools API reference to view the available actions</p>"},{"location":"platform-admin/aiinitiatives/resources/nodes/","title":"Nodes","text":"<p>This article explains the procedure for managing Nodes.</p> <p>Nodes are Kubernetes elements automatically discovered by the Run:ai platform. Once a node is discovered by the Run:ai platform, an associated instance is created in the Nodes table, administrators can view the Node\u2019s relevant information, and Run:ai scheduler can use the node for Scheduling.</p>"},{"location":"platform-admin/aiinitiatives/resources/nodes/#nodes-table","title":"Nodes table","text":"<p>The Nodes table can be found under Nodes in the Run:ai platform.</p> <p>The Nodes table displays a list of predefined nodes available to users in the Run:ai platform.</p> <p>Note</p> <ul> <li>It is not possible to create additional nodes, or edit, or delete existing nodes.  </li> <li>Only users with relevant permissions can view the table.</li> </ul> <p></p> <p>The Nodes table consists of the following columns:</p> Column Description Node The Kubernetes name of the node Status The state of the node. Nodes in the Ready state are eligible for scheduling. If the state is Not ready then the main reason appears in parenthesis on the right side of the state field. Hovering the state lists the reasons why a node is Not ready. Node pool The name of the associated node pool. By default, every node in the Run:ai platform is associated with the default node pool, if no other node pool is associated GPU type The GPU model, for example, H100, or V100 GPU devices The number of GPU devices installed on the node. Clicking this field pops up a dialog with details per GPU (described below in this article) Free GPU devices The current number of fully vacant GPU devices GPU memory The total amount of GPU memory installed on this node. For example, if the number is 640GB and the number of GPU devices is 8, then each GPU is installed with 80GB of memory (assuming the node is assembled of homogenous GPU devices) Allocated GPUs The total allocation of GPU devices in units of GPUs (decimal number). For example, if 3 GPUs are 50% allocated, the field prints out the value 1.50. This value represents the portion of GPU memory consumed by all running pods using this node Used GPU memory The actual amount of memory (in GB or MB) used by pods running on this node. GPU compute utilization The average compute utilization of all GPU devices in this node GPU memory utilization The average memory utilization of all GPU devices in this node CPU (Cores) The number of CPU cores installed on this node CPU memory The total amount of CPU memory installed on this node Allocated CPU (Cores) The number of CPU cores allocated by pods running on this node (decimal number, e.g. a pod allocating 350 mili-cores shows an allocation of 0.35 cores). Allocated CPU memory The total amount of CPU memory allocated by pods running on this node (in GB or MB) Used CPU memory The total amount of actually used CPU memory by pods running on this node. Pods may allocate memory but not use all of it, or go beyond their CPU memory allocation if using Limit &gt; Request for CPU memory (burstable workload) CPU compute utilization The utilization of all CPU compute resources on this node (percentage) CPU memory utilization The utilization of all CPU memory resources on this node (percentage) Used swap CPU memory The amount of CPU memory (in GB or MB) used for GPU swap memory (* future) Pod(s) List of pods running on this node, click the field to view details (described below in this article)"},{"location":"platform-admin/aiinitiatives/resources/nodes/#gpu-devices-for-node","title":"GPU devices for node","text":"<p>Click one of the values in the GPU devices column, to view the list of GPU devices and their parameters.</p> Column Description Index The GPU index, read from the GPU hardware. The same index is used when accessing the GPU directly Used memory The amount of memory used by pods and drivers using the GPU (in GB or MB) Compute utilization The portion of time the GPU is being used by applications (percentage) Memory utilization The portion of the GPU memory that is being used by applications (percentage) Idle time The elapsed time since the GPU was used (i.e. the GPU is being idle for \u2018Idle time\u2019)"},{"location":"platform-admin/aiinitiatives/resources/nodes/#pods-associated-with-node","title":"Pods associated with node","text":"<p>Click one of the values in the Pod(s) column, to view the list of pods and their parameters.</p> <p>Note</p> <p>This column is only viewable if your role in the Run:ai platform gives you read access to workloads, even if you are allowed to view workloads, you can only view the workloads within your allowed scope. This means, there might be more pods running on this node than appear in the list your are viewing.</p> Column Description Pod The Kubernetes name of the pod. Usually name of the pod is made of the name of the parent workload if there is one, and an index for unique for that pod instance within the workload Status The state of the pod. In steady state this should be Running and the amount of time the pod is running Project The Run:ai project name the pod belongs to. Clicking this field takes you to the Projects table filtered by this project name Workload The workload name the pod belongs to. Clicking this field takes you to the Workloads table filtered by this workload name Image The full path of the image used by the main container of this pod Creation time The pod\u2019s creation date and time"},{"location":"platform-admin/aiinitiatives/resources/nodes/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV  </li> <li>Show/Hide details - Click to view additional information on the selected row</li> </ul>"},{"location":"platform-admin/aiinitiatives/resources/nodes/#showhide-details","title":"Show/Hide details","text":"<p>Click a row in the Nodes table and then click the Show details button at the upper right side of the action bar. The details screen appears, presenting the following metrics graphs:</p> <ul> <li>GPU utilization   Per GPU graph and an average of all GPUs graph, all on the same chart, along an adjustable period allows you to see the trends of all GPUs compute utilization (percentage of GPU compute) in this node.  </li> <li>GPU memory utilization   Per GPU graph and an average of all GPUs graph, all on the same chart, along an adjustable period allows you to see the trends of all GPUs memory usage (percentage of the GPU memory) in this node.  </li> <li>CPU compute utilization   The average of all CPUs\u2019 cores compute utilization graph, along an adjustable period allows you to see the trends of CPU compute utilization (percentage of CPU compute) in this node.  </li> <li>CPU memory utilization   The utilization of all CPUs memory in a single graph, along an adjustable period allows you to see the trends of CPU memory utilization (percentage of CPU memory) in this node.  </li> <li> <p>CPU memory usage   The usage of all CPUs memory in a single graph, along an adjustable period allows you to see the trends of CPU memory usage (in GB or MB of CPU memory) in this node.</p> </li> <li> <p>For GPUs charts - Click the GPU legend on the right-hand side of the chart, to activate or deactivate any of the GPU lines.  </p> </li> <li>You can click the date picker to change the presented period  </li> <li>You can use your mouse to mark a sub-period in the graph for zooming in, and use the \u2018Reset zoom\u2019 button to go back to the preset period  </li> <li>Changes in the period affect all graphs on this screen.</li> </ul>"},{"location":"platform-admin/aiinitiatives/resources/nodes/#using-api","title":"Using API","text":"<p>Go to the Nodes API reference to view the available actions</p>"},{"location":"platform-admin/authentication/accessrules/","title":"Access Rules","text":"<p>This article explains the procedure to manage Access rules.</p> <p>Access rules provide users, groups, or applications privileges to system entities.</p> <p>An access rule is the assignment of a role to a subject in a scope: <code>&lt;Subject&gt;</code> is a <code>&lt;Role&gt;</code> in a <code>&lt;Scope&gt;</code>.</p> <p>For example, user user@domain.com is a department admin in department A.</p>"},{"location":"platform-admin/authentication/accessrules/#access-rules-table","title":"Access rules table","text":"<p>The Access rules table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The Access rules table provides a list of all the access rules defined in the platform and allows you to manage them.</p> <p>Note</p> <p>Flexible management</p> <p>It is also possible to manage access rules directly for a specific user, application, project, or department.</p> <p></p> <p>The Access rules table consists of the following columns:</p> Column Description Type The type of subject assigned to the access rule (user, SSO group, or application). Subject The user, SSO group, or application assigned with the role Role The role assigned to the subject Scope The scope to which the subject has access. Click the name of the scope to see the scope and its subordinates Authorized by The user who granted the access rule Creation time The timestamp for when the rule was created Last updated The last time the access rule was updated"},{"location":"platform-admin/authentication/accessrules/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/authentication/accessrules/#adding-new-access-rules","title":"Adding new access rules","text":"<p>To add a new access rule:</p> <ol> <li>Click +NEW ACCESS RULE </li> <li>Select a subject User, SSO Group, or Application </li> <li>Select or enter the subject identifier:  <ul> <li>User Email for a local user created in Run:ai or for SSO user as recognized by the IDP  </li> <li>Group name as recognized by the IDP  </li> <li>Application name as created in Run:ai  </li> </ul> </li> <li>Select a role  </li> <li>Select a scope  </li> <li>Click SAVE RULE</li> </ol> <p>Note</p> <p>An access rule consists of a single subject with a single role in a single scope. To assign multiple roles or multiple scopes to the same subject, multiple access rules must be added.</p>"},{"location":"platform-admin/authentication/accessrules/#editing-an-access-rule","title":"Editing an access rule","text":"<p>Access rules cannot be edited. To change an access rule, you must delete the rule, and then create a new rule to replace it.</p>"},{"location":"platform-admin/authentication/accessrules/#deleting-an-access-rule","title":"Deleting an access rule","text":"<ol> <li>Select the access rule you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol>"},{"location":"platform-admin/authentication/accessrules/#using-api","title":"Using API","text":"<p>Go to the Access rules API reference to view the available actions</p>"},{"location":"platform-admin/authentication/applications/","title":"Applications","text":"<p>This article explains the procedure to manage applications and it\u2019s permissions.</p> <p>Applications are used for API integrations with Run:ai. An application contains a secret key. Using the secret key you can obtain a token and use it within subsequent API calls.</p> <p>Applications are managed locally and assigned with Access Rules to manage its permissions.</p> <p>For example, application ci-pipeline-prod assigned with a Researcher role in Cluster: A.</p>"},{"location":"platform-admin/authentication/applications/#applications-table","title":"Applications table","text":"<p>The Applications table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The Applications table provides a list of all the applications defined in the platform, and allows you to manage them.</p> <p></p> <p>The Applications table consists of the following columns:</p> Column Description Application The name of the application Status The status of the application Access rule(s) The access rules assigned to the application Last login The timestamp for the last time the user signed in Created by The user who created the application Creation time The timestamp for when the application was created Last updated The last time the application was updated"},{"location":"platform-admin/authentication/applications/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/authentication/applications/#creating-an-application","title":"Creating an application","text":"<p>To create an application:</p> <ol> <li>Click +NEW APPLICATION </li> <li>Enter the application\u2019s Name </li> <li>Click CREATE </li> <li>Copy the credentials and store it securely:  <ul> <li>Application name </li> <li>Secret key </li> </ul> </li> <li>Click DONE</li> </ol> <p>Note</p> <p>The secret key is visible only at the time of creation, it cannot be recovered but can be regenerated.</p>"},{"location":"platform-admin/authentication/applications/#adding-an-access-rule-to-an-application","title":"Adding an access rule to an application","text":"<p>To create an access rule:</p> <ol> <li>Select the application you want to add an access rule for  </li> <li>Click ACCESS RULES </li> <li>Click +ACCESS RULE </li> <li>Select a role  </li> <li>Select a scope  </li> <li>Click SAVE RULE </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/authentication/applications/#deleting-an-access-rule-from-an-application","title":"Deleting an access rule from an application","text":"<p>To delete an access rule:</p> <ol> <li>Select the application you want to remove an access rule from  </li> <li>Click ACCESS RULES </li> <li>Find the access rule assigned to the user you would like to delete  </li> <li>Click on the trash icon  </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/authentication/applications/#regenerating-key","title":"Regenerating key","text":"<p>To regenerate an application\u2019s key:</p> <ol> <li>Select the application you want to regenerate it\u2019s secret key  </li> <li>Click REGENERATE KEY </li> <li>Click REGENERATE </li> <li>Review the user\u2019s credentials and store it securely:  <ul> <li>Application name  </li> <li>Secret key </li> </ul> </li> <li>Click DONE</li> </ol> <p>Warning</p> <p>Regenerating an application key revokes its previous key.</p>"},{"location":"platform-admin/authentication/applications/#deleting-an-application","title":"Deleting an application","text":"<ol> <li>Select the application you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol>"},{"location":"platform-admin/authentication/applications/#using-api","title":"Using API","text":"<p>Go to the Applications, Access rules API reference to view the available actions</p>"},{"location":"platform-admin/authentication/roles/","title":"Roles","text":"<p>This article explains the available roles in the Run:ai platform.</p> <p>A role is a set of permissions that can be assigned to a subject in a scope.</p> <p>A permission is a set of actions (View, Edit, Create and Delete) over a Run:ai entity (e.g. projects, workloads, users).Roles table</p> <p>The Roles table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The Roles table displays a list of predefined roles available to users in the Run:ai platform. It is not possible to create additional rules or edit or delete existing rules.</p> <p></p> <p>The Roles table consists of the following columns:</p> Column Description Role The name of the role Created by The name of the role creator Creation time The timestamp when the role was created"},{"location":"platform-admin/authentication/roles/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/authentication/roles/#reviewing-a-role","title":"Reviewing a role","text":"<ol> <li>To review a role click the role name on the table  </li> <li>In the role form review the following:  <ul> <li>Role name   The name of the role  </li> <li>Entity  A system-managed object that can be viewed, edited, created or deleted by a user based on their assigned role and scope  </li> <li>Actions  The actions that the role assignee is authorized to perform for each entity  <ul> <li>View If checked, an assigned user with this role can view instances of this type of entity within their defined scope  </li> <li>Edit If checked, an assigned user with this role can change the settings of an instance of this type of entity within their defined scope  </li> <li>Create If checked, an assigned user with this role can create new instances of this type of entity within their defined scope  </li> <li>Delete If checked, an assigned user with this role can delete instances of this type of entity within their defined scope</li> </ul> </li> </ul> </li> </ol>"},{"location":"platform-admin/authentication/roles/#roles-in-runai","title":"Roles in Run:ai","text":"<p>Run:ai supports the following roles and their permissions:  Under each role is a detailed list of the actions that the role assignee is authorized to perform for each entity.</p> Compute resource administrator <p></p> Data source administrator <p></p> Data volume administrator <p></p> Department administrator <p></p> Department viewer <p></p> Editor <p></p> Environment administrator <p></p> L1 researcher <p></p> L2 researcher <p></p> ML engineer <p></p> Research manager <p></p> System administrator <p></p> Template administrator <p></p> Viewer <p></p>"},{"location":"platform-admin/authentication/roles/#permitted-workloads","title":"Permitted workloads","text":"<p>When assigning a role with either one, all or any combination of the View, Edit, Create and Delete permissions for workloads, the subject has permissions to manage not only Run:ai native workloads (Workspace, Training, Inference), but also a list of 3rd party workloads:</p> <ul> <li>k8s: StatefulSet</li> <li>k8s: ReplicaSet</li> <li>k8s: Pod</li> <li>k8s: Deployment</li> <li>batch: Job</li> <li>batch: CronJob</li> <li>machinelearning.seldon.io: SeldonDeployment</li> <li>kubevirt.io: VirtualMachineInstance</li> <li>kubeflow.org: TFJob</li> <li>kubeflow.org: PyTorchJob</li> <li>kubeflow.org: XGBoostJob</li> <li>kubeflow.org: MPIJob</li> <li>kubeflow.org: MPIJob</li> <li>kubeflow.org: Notebook</li> <li>kubeflow.org: ScheduledWorkflow</li> <li>amlarc.azureml.com: AmlJob</li> <li>serving.knative.dev: Service</li> <li>workspace.devfile.io: DevWorkspace</li> <li>ray.io: RayCluster</li> <li>ray.io: RayJob</li> <li>ray.io: RayService</li> <li>ray.io: RayCluster</li> <li>ray.io: RayJob</li> <li>ray.io: RayService</li> <li>tekton.dev: TaskRun</li> <li>tekton.dev: PipelineRun</li> <li>argoproj.io: Workflow</li> </ul>"},{"location":"platform-admin/authentication/roles/#using-api","title":"Using API","text":"<p>Go to the Roles API reference to view the available actions.</p>"},{"location":"platform-admin/authentication/users/","title":"Users","text":"<p>This article explains the procedure to manage users and their permissions.</p> <p>Users can be managed locally, or via the Identity provider, while assigned with Access Rules to manage its permissions.</p> <p>For example, user user@domain.com is a department admin in department A.</p>"},{"location":"platform-admin/authentication/users/#users-table","title":"Users table","text":"<p>The Users table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The users table provides a list of all the users in the platform. You can manage local users and manage user permissions (access rules) for both local and SSO users.</p> <p>Note</p> <p>Single Sign-On users</p> <p>SSO users are managed by the identity provider and appear once they have signed in to Run:ai</p> <p></p> <p>The Users table consists of the following columns:</p> Column Description User The unique identity of the user (email address) Type The type of the user - SSO / local Last login The timestamp for the last time the user signed in Access rule(s) The access rules assigned to the user Created By The user who created the user Creation time The timestamp for when the user was created Last updated The last time the user was updated"},{"location":"platform-admin/authentication/users/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/authentication/users/#creating-a-local-user","title":"Creating a local user","text":"<p>To create a local user:</p> <ol> <li>Click +NEW LOCAL USER </li> <li>Enter the user\u2019s Email address </li> <li>Click CREATE </li> <li>Review and copy the user\u2019s credentials:  <ul> <li>User Email </li> <li>Temporary password to be used on first sign-in  </li> </ul> </li> <li>Click DONE</li> </ol> <p>Note</p> <p>The temporary password is visible only at the time of user\u2019s creation, and must be changed after the first sign-in</p>"},{"location":"platform-admin/authentication/users/#adding-an-access-rule-to-a-user","title":"Adding an access rule to a user","text":"<p>To create an access rule:</p> <ol> <li>Select the user you want to add an access rule for  </li> <li>Click ACCESS RULES </li> <li>Click +ACCESS RULE </li> <li>Select a role  </li> <li>Select a scope  </li> <li>Click SAVE RULE </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/authentication/users/#deleting-users-access-rule","title":"Deleting user\u2019s access rule","text":"<p>To delete an access rule:</p> <ol> <li>Select the user you want to remove an access rule from  </li> <li>Click ACCESS RULES </li> <li>Find the access rule assigned to the user you would like to delete  </li> <li>Click on the trash icon  </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/authentication/users/#resetting-a-user-password","title":"Resetting a user password","text":"<p>To reset a user\u2019s password:</p> <ol> <li>Select the user you want to reset it\u2019s password  </li> <li>Click RESET PASSWORD </li> <li>Click RESET </li> <li>Review and copy the user\u2019s credentials:  <ul> <li>User Email </li> <li>Temporary password to be used on next sign-in  </li> </ul> </li> <li>Click DONE</li> </ol>"},{"location":"platform-admin/authentication/users/#deleting-a-user","title":"Deleting a user","text":"<ol> <li>Select the user you want to delete  </li> <li>Click DELETE </li> <li>In the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>To ensure administrative operations are always available, at least one local user with System Administrator role should exist.</p>"},{"location":"platform-admin/authentication/users/#using-api","title":"Using API","text":"<p>Go to the Users, Access rules API reference to view the available actions</p>"},{"location":"platform-admin/performance/dashboard-analysis/","title":"Introduction","text":"<p>The Run:ai Administration User Interface provides a set of dashboards that help you monitor Clusters, Cluster Nodes, Projects, and Workloads. This document provides the key metrics to monitor, how to assess them as well as suggested actions.</p> <p>Dashboards are used by system administrators to analyze and diagnose issues that relate to:</p> <ul> <li>Physical Resources.</li> <li>Organization resource allocation and utilization.</li> <li>Usage characteristics.</li> </ul> <p>System administrators need to know important information about the physical resources that are currently being used. Important information such as:</p> <ul> <li>Resource health.</li> <li>Available resources and their distribution.</li> <li>Is there a lack of resources.</li> <li>Are resources being utilized correctly.</li> </ul> <p>With this information, system administrators can hone in on:</p> <ul> <li>How resources are allocated across the organization.</li> <li>How the different organizational units utilized quotas and resources within those quotas.</li> <li>The actual performance of the organizational units.</li> </ul> <p>These dashboards give system administrators the ability to drill down to see details of the different types of workloads that each of the organizational units is running. These usage and performance metrics ensure that system administrators can then take actions to correct issues that affect performance.</p> <p>There are 5 dashboards:</p> <ul> <li>GPU/CPU Overview dashboard\u2014Provides information about what is happening right now in the cluster.</li> <li>Quota Management dashboard\u2014Provides information about quota utilization.</li> <li>Analytics dashboard\u2014Provides long term analysis of cluster behavior.</li> <li>Multi-Cluster Overview dashboard\u2014Provides a more holistic, multi-cluster view of what is happening right now. The dashboard is intended for organizations that have more than one connected cluster.</li> <li>Consumption dashboard\u2014Provides information about resource consumption.</li> </ul>"},{"location":"platform-admin/performance/dashboard-analysis/#gpucpu-overview-dashboard-new-and-legacy","title":"GPU/CPU Overview Dashboard (New and legacy)","text":"<p>The Overview dashboard provides information about what is happening right now in the cluster.  Administrators can view high-level information on the state of the cluster. The dashboard has two tabs that change the display to provide a focused view for GPU Dashboards (default view) and CPU Dashboards.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#gpu-dashboard","title":"GPU Dashboard","text":"<p>The GPU dashboard displays specific information for GPU based nodes, node-pools, clusters, or tenants. These dashboards also include additional metrics that specific to GPU based environments. The dashboard contains tiles that show information about specific resource allocation and performance metrics. The tiles are interactive allowing you to link directly to the assets or drill down to specific scopes. Use the time frame selector to choose a time frame for all the tiles in the dashboard.</p> <p>The dashboard has the following tiles:</p> <ul> <li>Ready nodes\u2014displays GPU nodes that are in the ready state.</li> <li>Ready GPU devices\u2014displays the number of GPUs in nodes that are in the ready state.</li> <li>Allocated GPU compute\u2014displays the total number of GPUs allocated from all the nodes.</li> <li>Running workloads\u2014displays the number of running workloads.</li> <li>Pending workloads\u2014displays the number of workloads in the pending status.</li> <li>Allocation ration by node pool\u2014displays the percentage of GPUs allocated per node pool. Hover over the bar for detailed information. Use the scope selected at the bottom of the graph to drill down for more details.</li> <li>Free resources by node pool\u2014the graph displays the amount of free resources per node pool. Press a entry in the graph for more details. Hover over the resource bubbles for specific details for the workers in the node. Use the ellipsis to download the graph as a CSV file.</li> <li>Resource allocation by workload type\u2014displays the resource allocation by workload type. Hover over the bar for detailed information. Use the scope selected at the bottom of the graph to drill down for more details. Use the ellipsis to download the graph as a CSV file.</li> <li>Workload by status\u2014displays the number of workloads for each status in the workloads table. Hover over the bar for detailed information. Use the scope selected at the bottom of the graph to drill down for more details. Use the ellipsis to download the graph as a CSV file.</li> <li>Resources utilization\u2014displays the resource utilization over time. The right pane of the graph shows the average utilization of the selected time frame of the dashboard. Hover over the graph to see details of a specific time in the graph. Use the ellipsis to download the graph as a CSV file.</li> <li>Resource allocation\u2014displays the resource allocation over time. The right pane of the graph shows the average allocation of the selected time frame of the dashboard. Hover over the graph to see details of a specific time in the graph. Use the ellipsis to download the graph as a CSV file.</li> </ul>"},{"location":"platform-admin/performance/dashboard-analysis/#cpu-dashboard","title":"CPU Dashboard","text":"<p>The CPU dashboards display specific information for CPU based nodes, node-pools, clusters, or tenants. These dashboards also include additional metrics that specific to CPU based environments.</p> <p>To enable CPU Dashboards:</p> <ol> <li>Press the <code>Tools &amp; Settings</code> icon, then press <code>General</code>.</li> <li>Open the <code>Analytics</code> pane and toggle the Show CPU dashboard switch to enable the feature.</li> </ol> <p>Toggle the switch to <code>disable</code> to disable CPU Dashboards option.</p> <p>The dashboard contains the following tiles:</p> <ul> <li>Total CPU Nodes\u2014displays the total amount of CPU nodes.</li> <li>Ready CPU nodes\u2014displays the total amount of CPU nodes in the ready state.</li> <li>Total CPUs\u2014displays the total amount of CPUs.</li> <li>Ready CPUs\u2014displays the total amount of CPUs in the ready state.</li> <li>Allocated CPUs\u2014displays the amount of allocated CPUs.</li> <li>Running workloads\u2014displays the amount of workloads in the running state.</li> <li>Pending workloads\u2014displays the amount of workloads in the pending state.</li> <li>Allocated CPUs per project\u2014displays the amount of CPUs allocated per project.</li> <li>Active projects\u2014displays the active projects  with the CPU allocation and amount of running and pending workloads.</li> <li>Utilization per resource type\u2014displays the CPU compute and CPU memory utilization over time.</li> <li>CPU compute utilization\u2014displays the current CPU compute utilization.</li> <li>CPU memory utilization\u2014displays the current CPU memory utilization.</li> <li>Pending workloads\u2014displays the requested resources and wait time for workloads in the pending status.</li> <li>Workloads with error\u2014displays the amount of workloads that are currently not running due to an error.</li> <li>Workload Count per CPU Compute Utilization\u2014</li> <li>5 longest running workloads\u2014displays up to 5 of workloads that have the longest running time.</li> </ul> <p>Analysis and Suggested actions:</p> Review Analysis  &amp; Actions Interactive Workloads are too frequently idle Consider setting time limits for interactive Workloads through the Projects tab.\u00a0  Consider also reducing GPU/CPU quotas for specific Projects to encourage users to run more training Workloads as opposed to interactive Workloads (note that interactive Workloads can not use more than the GPU/CPU quota assigned to their Project). Training Workloads are too frequently idle Identify and notify the right users and work with them to improve the utilization of their training scripts"},{"location":"platform-admin/performance/dashboard-analysis/#workloads-with-an-error","title":"Workloads with an Error","text":"<p>Search for Workloads with an error status. These Workloads may be holding GPUs/CPUs without actually using them.</p> <p>Analysis and Suggested actions:</p> <p>Search for workloads with an Error status on the Workloads view and discuss with the Job owner. Consider deleting these Workloads to free up the resources for other users.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#workloads-with-a-long-duration","title":"Workloads with a Long Duration","text":"<p>View list of 5 longest Workloads.</p> <p>Analysis and Suggested actions:</p> Review Analysis &amp; Actions Training Workloads run for too long Ask users to view their Workloads and analyze whether useful work is being done. If needed, stop their Workloads. Interactive Workloads run for too long Consider setting time limits for interactive Workloads via the Project editor."},{"location":"platform-admin/performance/dashboard-analysis/#job-queue","title":"Job Queue","text":"<p>Identify queueing bottlenecks.</p> <p>Analysis and Suggested actions:</p> Review Analysis &amp; Actions Cluster is fully loaded Go over the table of active Projects and check that fairness between Projects was enforced, by reviewing the number of allocated GPUs/CPUs for each Project, ensuring each Project was allocated with its fair-share portion of the cluster. Cluster is not fully loaded Go to the Workloads view to review the resources requested for that Job (CPU, CPU memory, GPU, GPU memory). Go to the Nodes view to verify that there is no Node with enough free resources that can host that Job. <p>Also, check the command that the user used to submit the job. The Researcher may have requested a specific Node for that Job.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#analytics-dashboard","title":"Analytics Dashboard","text":"<p>The Analytics dashboard provides means for viewing historical data on cluster information such as:</p> <ul> <li>Utilization across the cluster</li> <li>GPU usage by different Projects, including allocation and utilization, broken down into interactive and training Workloads</li> <li>Breakdown of running Workloads into interactive, training, and GPU versus CPU-only Workloads, including information on queueing (number of pending Workloads and requested GPUs),</li> <li>Status of Nodes in terms of availability and allocated and utilized resources.</li> </ul> <p>The dashboard has a dropdown filter for node pools and Departments. From the dropdown, select one or more node pools. The default setting is <code>all</code>.</p> <p>The information presented in Analytics can be used in different ways for identifying problems and fixing them. Below are a few examples.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#node-downtime","title":"Node Downtime","text":"<p>View the overall available resources per Node and identify cases where a Node is down and there was a reduction in the number of available resources.</p> <p>How to: view the following panel.</p> <p></p> <p>Analysis and Suggested actions:</p> <p>Filter according to time range to understand for how long the Node is down.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#gpu-allocation","title":"GPU Allocation","text":"<p>Track GPU allocation across time.</p> <p>How to: view the following panels.</p> <p></p> <p>The panel on the right-hand side shows the cluster-wide GPU allocation and utilization versus time, whereas the panels on the left-hand side show the cluster-wide GPU allocation and utilization averaged across the filtered time range.</p> <p>Analysis and Suggested actions:</p> <p>If the allocation is too low for a long period, work with users to run more workloads and to better utilize the Cluster.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#track-gpu-utilization","title":"Track GPU utilization","text":"<p>Track whether Researchers efficiently use the GPU resources they have allocated for themselves.</p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>If utilization is too low for a long period, you will want to identify the source of the problem:</p> <ul> <li>Go to \u201cAverage GPU Allocation &amp; Utilization\u201d</li> <li>Look for Projects with large GPU allocations for interactive Workloads or Projects that poorly utilize their training Workloads. Users tend to poorly utilize their GPUs in interactive sessions because of the dev &amp; debug nature of their work which typically is an iterative process with long idle GPU time. On many occasions users also don\u2019t shut down their interactive Workloads, holding their GPUs idle and preventing others from using them.</li> </ul> Review Analysis &amp; Actions Low GPU utilization is due to interactive Workloads being used too frequently Consider setting time limits for interactive Workloads through the Projects tab or reducing GPU quotas to encourage users to run more training Workloads as opposed to interactive Workloads (note that interactive Workloads can not use more than the GPU quota assigned to their Project). Low GPU utilization is due to users poorly utilizing their GPUs in training sessions Identify Projects with bad GPU utilization in training Workloads, notify the users and work with them to improve their code and the way they utilize their GPUs."},{"location":"platform-admin/performance/dashboard-analysis/#training-vs-interactive-researcher-maturity","title":"Training vs. Interactive -- Researcher maturity","text":"<p>Track the number of running Workloads and the breakdown into interactive, training, and CPU-only Workloads.</p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>We would want to encourage users to run more training Workloads than interactive Workloads, as it is the key to achieving high GPU utilization across the Cluster:</p> <ul> <li>Training Workloads run to completion and free up their resources automatically when training ends</li> <li>Training Workloads can be preempted, queued, and resumed automatically by the Run:ai system according to predefined policies which increases fairness and Cluster utilization.</li> </ul>"},{"location":"platform-admin/performance/dashboard-analysis/#pending-queue-size","title":"Pending Queue Size","text":"<p>Track how long is the queue for pending Workloads</p> <p>How to: view the following panels:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>Consider buying more GPUs:</p> <ul> <li>When there are too many Workloads are waiting in queue for too long.</li> <li>With a large number of requested GPUs.</li> <li>While the Cluster is fully loaded and well utilized.</li> </ul>"},{"location":"platform-admin/performance/dashboard-analysis/#cpu-memory-utilization","title":"CPU &amp; Memory Utilization","text":"<p>Track CPU and memory Node utilization and identify times where the load on specific Nodes is high.</p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>If the load on specific Nodes is too high, it may cause problems with the proper operation of the Cluster and the way workloads are running.</p> <p>Consider adding more CPUs, or adding additional CPU-only nodes for Workloads that do only CPU processing.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#multi-cluster-overview-dashboard","title":"Multi-Cluster overview dashboard","text":"<p>Provides a holistic, aggregated view across Clusters, including information about Cluster and Node utilization, available resources, and allocated resources. With this dashboard, you can identify Clusters that are down or underutilized and go to the Overview of that Cluster to explore further.</p> <p></p>"},{"location":"platform-admin/performance/dashboard-analysis/#consumption-dashboard","title":"Consumption dashboard","text":"<p>This dashboard enables users and admins to view consumption usage using run:AI services. The dashboard provides views based on configurable filters and timelines. The dashboard also provides costing analysis for GPU, CPU, and memory costs for the system.</p> <p></p> <p>The dashboard has 4 tiles for:</p> <ul> <li>Cumulative GPU allocation per Project or Department</li> <li>Cumulative CPU allocation per Project or Department</li> <li>Cumulative memory allocation per Project or Department</li> <li>Consumption types</li> </ul> <p>Use the drop down menus at the top of the dashboard to apply filters for:</p> <ul> <li>Project or department</li> <li>Per project (single, multiple, or all)</li> <li>Per department (single, multiple or all)</li> <li>Per cluster (single, multiple, all)</li> </ul> <p>To enable the Consumption Dashboard:</p> <ol> <li>Press the <code>Tools &amp; Settings</code> icon, then press <code>General</code>.</li> <li>Open the <code>Analytics</code> pane and toggle the Consumption switch to enable the feature.</li> <li>Enter the cost of:</li> <li>GPU compute / Hour</li> <li>CPU compute / Hour</li> <li>CPU memory / Hour</li> </ol> <p>Use the time picker dropdown to select relative time range options and set custom absolute time ranges. You can change the Timezone and fiscal year settings from the time range controls by clicking the Change time settings button.</p> <p>Note</p> <p>Dashboard data updates once an hour.</p> <p></p> <p>You can change the refresh interval using the refresh interval drop down.</p> <p>The dashboard has a 2 consumption tables that display the total consumption of resources. Hover over an entry in the table to filter it in or out of the table.</p> <p>The Total consumption table includes consumption details based on the filters selected. Fields include:</p> <ul> <li>Project</li> <li>Department</li> <li>GPU hours</li> <li>CPU hours</li> <li>Memory hours</li> <li>GPU Idle allocated hours\u2014the portion of time the GPUs spend idle from the total allocation hours.</li> <li>CPU usage hours\u2014the actual usage time of CPU.</li> <li>Memory usage time\u2014the actual usage time of CPU memory.</li> <li>GPU cost (only when configured)</li> <li>CPU cost (only when configured)</li> <li>CPU memory (only when configured)</li> </ul> <p>The Total department consumption table includes consumption details for each department, or details for departments selected in the filters. Fields include:</p> <ul> <li>Department</li> <li>GPU hours</li> <li>CPU hours</li> <li>Memory hours</li> <li>GPU Idle allocated hours\u2014the portion of time the GPUs spend idle from the total allocation hours.</li> <li>CPU usage hours\u2014the actual usage time of CPU.</li> <li>Memory usage time\u2014the actual usage time of CPU memory.</li> <li>GPU cost (only when configured)</li> <li>CPU cost (only when configured)</li> <li>CPU memory (only when configured)</li> </ul> <p>The dashboard has a graph of the GPU allocation over time.</p> <p>!</p> <p>The dashboard has a graph of the Project over-quota GPU consumption.</p> <p>!</p>"},{"location":"platform-admin/performance/dashboard-analysis/#quota-management-dashboard","title":"Quota management dashboard","text":"<p>The Quota management dashboard provides an efficient means to monitor and manage resource utilization within the AI cluster. The dashboard is divided into sections with essential metrics and data visualizations to identify resource usage patterns, potential bottlenecks, and areas for optimization. The sections of the dashboard include:</p> <ul> <li>Add Filter</li> <li>Quota / Total</li> <li>Allocated / Quota</li> <li>Pending workloads</li> <li>Quota by node pool</li> <li>Allocation by node pool</li> <li>Pending workloads by node pool</li> <li>Departments with lowest allocation by node pool</li> <li>Projects with lowest allocation ratio by node pool</li> <li>Over time allocation / quota</li> </ul>"},{"location":"platform-admin/performance/dashboard-analysis/#add-filter","title":"Add Filter","text":"<p>Use the Add Filter dropdown to select filters for the dashboard. The filters will change the data shown on the dashboard. Available filters are:</p> <ul> <li>Departments</li> <li>Projects</li> <li>Nodes</li> </ul> <p>Select a filter from the dropdown, then select a item from the list, and press apply.</p> <p>Note</p> <p>You can create a filter with multiple categories, but you can use each category and item only once.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#quota-total","title":"Quota / Total","text":"<p>This section shows the number of GPUs that are in the quota based on the filter selection. The quota of GPUs is the number of GPUs that are reserved for use.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#allocated-quota","title":"Allocated / Quota","text":"<p>This section shows the number of GPUs that are allocated based on the filter selection. Allocated GPUs are the number of GPUs that are being used.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#pending-workloads","title":"Pending workloads","text":"<p>This section shows the number workloads that are pending based on the filter selection. Pending workloads are workloads that have not started.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#quota-by-node-pool","title":"Quota by node pool","text":"<p>This section shows the quota of GPUs by node pool based on the filter. The quota is the number of GPUs that are reserved for use. You can drill down into the data in this section by pressing on the graph or the link at the bottom of the section.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#allocation-by-node-pool","title":"Allocation by node pool","text":"<p>This section shows the allocation of GPUs by node pool based on the filter. The allocation is the number of GPUs that are being used. You can drill down into the data in this section by pressing on the graph or the link at the bottom of the section.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#pending-workloads-by-node-pool","title":"Pending workloads by node pool","text":"<p>This section shows the number of pending workloads by node pool. You can drill down into the data in this section by pressing on the graph or the link at the bottom of the section.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#departments-with-lowest-allocation-by-node-pool","title":"Departments with lowest allocation by node pool","text":"<p>This section shows the departments with the lowest allocation of GPUs by percentage relative to the total number of GPUs.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#projects-with-lowest-allocation-ratio-by-node-pool","title":"Projects with lowest allocation ratio by node pool","text":"<p>This section shows the projects with the lowest allocation of GPUS by percentage relative to the total number of GPUs.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#over-time-allocation-quota","title":"Over time allocation / quota","text":"<p>This section shows the allocation of GPUs from the quota over a period of time.</p>"},{"location":"platform-admin/workloads/submitting-workloads/","title":"Submitting Workloads","text":""},{"location":"platform-admin/workloads/submitting-workloads/#how-to-submit-a-workload","title":"How to Submit a Workload","text":"<p>To submit a workload using the UI:</p> <ol> <li>In the left menu press Workloads.</li> <li>Press New Workload, and select Workspace, Training, or Inference.</li> </ol> WorkspaceTrainingInference <ol> <li>In the Projects pane, select a project. Use the search box to find projects that are not listed. If you can't find the project, see your system administrator.</li> <li>In the Templates pane, select a template from the list. Use the search box to find templates that are not listed. If you can't find the specific template you need, create a new one, or see your system administrator.</li> <li>Enter a <code>Workspace</code> name, and press continue.</li> <li> <p>In the Environment pane select or create a new environment. Use the search box to find environments that are not listed.</p> <ol> <li>In the Set the connection for your tool(s) pane, choose a tool for your environment (if available). In the Access pane, edit the field and choose a type of access. Everyone allows all users in the platform to access the selected tool. Group allows you a select a specific group of users (Identity provider group). Press <code>+Group</code> to add more groups. User allows you to grant access individual users (by user email) in the platform. Press <code>+User</code> to add more users. (optional)</li> <li>In the Runtime settings field, Set commands and arguments for the container running in the pod. (optional)</li> <li>In the Environment variable field, you can set one or more environment variables. (optional)</li> </ol> </li> <li> <p>In the Compute resource pane, select resources for your trainings or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</p> </li> <li> <p>Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a Volume persistency.</li> </ol> </li> <li> <p>In the Data sources pane, select a data source. If you need a new data source, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> <p>Note</p> <ul> <li>Data sources that have private credentials, which have the status of issues found, will be greyed out.</li> <li>Data sources can now include Secrets.</li> </ul> </li> <li> <p>In the General pane, add special settings for your training (optional):</p> <ol> <li>Toggle the switch to allow the workspace to exceed the project's quota.</li> <li>Set the backoff limit before workload failure, this can be changed, if necessary. Use integers only. (Default = 6, maximum = 100, minimum = 0). </li> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minuets, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails. (default = 30 days)</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>When complete, press Create workspace.</p> </li> </ol> <ol> <li>In the Projects pane, select the destination project. Use the search box to find projects that are not listed. If you can't find the project, you can create your own, or see your system administrator.</li> <li>In the Multi-node pane, choose <code>Single node</code> for a single node training, or <code>Multi-node (distributed)</code> for distributed training. When you choose <code>Multi-node</code>, select a framework that is listed, then select the <code>multi-node</code> training configuration by selecting either <code>Workers &amp; master</code> or <code>Workers only</code>.</li> <li>In the Templates pane, select a template from the list. Use the search box to find templates that are not listed. If you can't find the specific template you need, see your system administrator.</li> <li>In the Training name pane, enter a name for the Training, then press continue.</li> <li>In the Environment pane select or create a new environment. Use the search box to find environments that are not listed. <ol> <li>In the Set the connection for your tool(s) pane, choose a tool for your environment (if available). In the Access pane, edit the field and choose a type of access. Everyone allows all users in the platform to access the selected tool. Group allows you a select a specific group of users (Identity provider group). Press <code>+Group</code> to add more groups. User allows you to grant access individual users (by user email) in the platform. Press <code>+User</code> to add more users. (optional)</li> <li>In the Runtime settings field, Set commands and arguments for the container running in the pod. (optional)</li> <li>In the Environment variable field, you can set one or more environment variables. (optional)</li> </ol> </li> <li> <p>In the Compute resource pane:</p> <ol> <li>Select the number of workers for your training.</li> <li>Select Compute resources for your training or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> </ol> <p>Note</p> <p>The number of compute resources for the workers is based on the number of workers selected.</p> </li> <li> <p>(Optional) Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a Volume persistency. Choose Persistent or Ephemeral.</li> </ol> </li> <li> <p>(Optional) In the Data sources pane, select a data source. If you need a new data source, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> <p>Note</p> <ul> <li>Data sources that have private credentials, which have the status of issues found, will be greyed out.</li> <li>Data sources can now include Secrets.</li> </ul> </li> <li> <p>(Optional) In the General pane, add special settings for your training (optional):</p> <ol> <li>Set the backoff limit before workload failure, this can be changed, if necessary. Use integers only. (Default = 6, maximum = 100, minimum = 0). </li> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minuets, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails. (default = 30 days)</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>If you if selected  <code>Workers &amp; master</code> Press Continue to <code>Configure the master</code> and go to the next step. If not, then press Create training.</p> </li> <li> <p>If you do not want a different setup for the master, press Create training. If you would like to have a different setup for the master, toggle the switch to enable to enable a different setup.</p> <ol> <li>In the Environment pane select or create a new environment. Use the search box to find environments that are not listed. Press More settings to add an <code>Environment variable</code> or to edit the Command and Arguments field for the environment you selected.<ol> <li>In the Set the connection for your tool(s) pane, choose a tool for your environment (if available). In the Access pane, edit the field and choose a type of access. Everyone allows all users in the platform to access the selected tool. Group allows you a select a specific group of users (Identity provider group). Press <code>+Group</code> to add more groups. User allows you to grant access individual users (by user email) in the platform. Press <code>+User</code> to add more users. (optional)</li> <li>In the Runtime settings field, Set commands and arguments for the container running in the pod. (optional)</li> <li>In the Environment variable field, you can set one or more environment variables. (optional)</li> </ol> </li> <li>In the Compute resource pane, select a Compute resources for your training or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> <li> <p>(Optional) Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a Volume persistency. Choose Persistent or Ephemeral.</li> </ol> </li> <li> <p>(Optional) In the Data sources pane, select a data source. If you need a new data source, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> </li> </ol> <p>!!! Note       * Data sources that have private credentials, which have the status of issues found, will be greyed out.       * Data sources can now include Secrets.</p> <ol> <li> <p>(Optional) In the General pane, add special settings for your training (optional):</p> <ol> <li>Set the backoff limit before workload failure, this can be changed, if necessary. Use integers only. (Default = 6, maximum = 100, minimum = 0). </li> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minuets, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails. (default = 30 days)</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> </ol> </li> <li> <p>When your training configuration is complete. press Create training.</p> </li> </ol> <ol> <li>In the Projects pane, select a project. Use the search box to find projects that are not listed. If you can't find the project, see your system administrator.</li> <li> <p>In the Inference by type pane select Custom or model.</p> <p>When you select Model:</p> <ol> <li>Select a catalog. Choose from Run:ai or Hugging Face.<ol> <li>If you choose Run:ai, select a model from the tiles. Use the search box to find a model that is not listed. If you can't find the model, see your system administrator.</li> <li>If you choose Hugging Face, go to the next step.</li> </ol> </li> <li>In the Inference name field, enter a name for the workload.</li> <li>In the Credentials field, enter the token to access the model catalog.</li> <li>If you selected Hugging Face, enter the name of the model in the Model Name section. This will not appear if you selected Run:ai.</li> <li> <p>In the Compute resource field, select a compute resource from the tiles.</p> <ol> <li>In the Replica autoscaling section, set the minimum and maximum replicas for your inference. </li> <li>In the Set conditions for creating a new replica section, use the drop down to select from <code>Throughput (Requests/sec)</code>, <code>Latency (milliseconds)</code>, or <code>Concurrency (Requests/sec)</code>. Then set the value. (default = 100) This section will only appear if you have 2 or more set as the maximum.</li> <li>In the Set when replicas should be automatically scaled down to zero section, from the drop down select Never, After one, five, 15 or 30 minutes of inactivity.</li> </ol> <p>Note</p> <p>When automatic scaling to zero is enabled, the minimum number of replicas is 0.</p> <ol> <li>In the Nodes field, change the order of priority of the node pools, or add a new node pool to the list.</li> </ol> </li> <li> <p>When complete, press Create inference.</p> </li> </ol> <p>When you select Custom:</p> <ol> <li>In the Inference name field, enter a name for the workload.</li> <li>In the Environment field, select an environment. Use the search box to find an environment that is not listed. If you can't find an environment, press New environment or see your system administrator. <ol> <li>In the Set the connection for your tool(s) pane, choose a tool for your environment (if available). In the Access pane, edit the field and choose a type of access. Everyone allows all users in the platform to access the selected tool. Group allows you a select a specific group of users (Identity provider group). Press <code>+Group</code> to add more groups. User allows you to grant access individual users (by user email) in the platform. Press <code>+User</code> to add more users. (optional)</li> <li>In the Runtime settings field, Set commands and arguments for the container running in the pod. (optional)</li> <li>In the Environment variable field, you can set one or more environment variables. (optional)</li> </ol> </li> <li> <p>In the Compute resource field, select a compute resource from the tiles. Use the search box to find a compute resource that is not listed. If you can't find an environment, press New compute resource or see your system administrator.</p> <ol> <li>In the Replica autoscaling section, set the minimum and maximum replicas for your inference. </li> <li>In the Set conditions for creating a new replica section, use the drop down to select from <code>Throughput (Requests/sec)</code>, <code>Latency (milliseconds)</code>, or <code>Concurrency (Requests/sec)</code>. Then set the value. (default = 100) This section will only appear if you have 2 or more set as the maximum.</li> <li>In the Set when replicas should be automatically scaled down to zero section, from the drop down select Never, After one, five, 15 or 30 minutes of inactivity.</li> </ol> <p>Note</p> <p>When automatic scaling to zero is enabled, the minimum number of replicas is 0.</p> </li> <li> <p>In the Data sources field, add a New data source. (optional)</p> <p>Note</p> <ul> <li>Data sources that are not available will be greyed out.</li> <li>Assets that are cluster syncing will be greyed out.</li> <li>Only PVC, Git, and ConfigMap resources are supported.</li> </ul> </li> <li> <p>In the General field you can:</p> <ol> <li>Add an Auto-deletion time. This sets the timeframe between inference completion/failure and auto-deletion. (optional) (default = 30 days)</li> <li>Add one or more Annotation. (optional)</li> <li>Add one or more Labels. (optional)</li> </ol> </li> <li>When complete, press Create inference.</li> </ol> </li> </ol>"},{"location":"platform-admin/workloads/submitting-workloads/#workload-policies","title":"Workload Policies","text":"<p>As an administrator, you can set Policies on Workloads.  Policies allow administrators to impose restrictions and set default values for Researcher Workloads. For more information see Workload Policies.</p>"},{"location":"platform-admin/workloads/submitting-workloads/#workload-ownership-protection","title":"Workload Ownership Protection","text":"<p>Workload ownership protection in Run:ai ensures that only users who created a workload can delete or modify them. This feature is designed to safeguard important jobs and configurations from accidental or unauthorized modifications by users who did not originally create the workload.</p> <p>By enforcing ownership rules, Run:ai helps maintain the integrity and security of your machine learning operations. This additional layer of security ensures that only users with the appropriate permissions can delete and suspend workloads.</p> <p>This protection maintains workflow stability and prevents disruptions in shared or collaborative environments.</p> <p>This feature is implemented at the cluster management entity level.</p> <p>To enable ownership protection:</p> <ol> <li>Update the runai-public configmap and set <code>workloadOwnershipProtection=true</code>.</li> <li>Perform a cluster-sync to update cluster-service in the CP.</li> <li>Use the workload-service flag to block deletion and suspension of workloads, when appropriate.</li> </ol>"},{"location":"platform-admin/workloads/workload-overview/","title":"Workloads Overview","text":"<p>Runai is an open platform and supports three types of workloads each with a different set of features:</p> <ul> <li>Run:ai native workloads.</li> <li>Third party integrations.</li> <li>Typical Kubernetes workloads.</li> </ul>"},{"location":"platform-admin/workloads/workload-overview/#runai-native-workloads","title":"Run:ai native workloads","text":"<p>Run:ai native workloads are workloads (trainings, workspaces, deployments) that are fully controlled by Run:ai. Run: workloads are the most comprehensive and include Third party integrations and Typical Kubernetes workload types. Specific characteristics of Run: ai native workloads include:</p> <ol> <li>Submitting of workloads via UI/CLI.</li> <li>Workload control (delete/stop/connect).</li> <li>Workload policies (default rules for all policies, specific workload policies, and enforcing of those rules).</li> <li>Scheduling rules.</li> <li>Role based access control.</li> </ol>"},{"location":"platform-admin/workloads/workload-overview/#third-party-integrations","title":"Third party integrations","text":"<p>Third party integrations are tools that Run:ai supports and manages. These are tools that are typically used to build workloads for specific purposes. Third party integrations also include Typical Kubernetes workloads. Specific characteristics of third party tool support include:</p> <ol> <li>Smart gang scheduling (workload aware).</li> <li>Specific workload aware visibility so that different kinds of pods are identified as a single workload (for example, GPU Utilization, workload view, dashboards).</li> </ol> <p>For more information, see Supported integrations.</p>"},{"location":"platform-admin/workloads/workload-overview/#typical-kubernetes-workloads","title":"Typical Kubernetes workloads","text":"<p>Typical Kubernetes workloads are any kind of workload built for Kubernetes. The Run:ai platform allows you to submit standard Kubernetes CRDs. Specific characteristics of Typical Kubernetes workloads that Run:ai can manage include:</p> <ol> <li>Fairness</li> <li>Nodepools</li> <li>Bin packing/spread</li> <li>Fractions</li> <li>Overprovisioning</li> </ol>"},{"location":"platform-admin/workloads/workload-overview/#workloads-view","title":"Workloads View","text":"<p>Run:ai makes it easy to run machine learning workloads effectively on Kubernetes. Run:ai provides both a UI and API interface that introduces a simple and more efficient way to manage machine learning workloads, which will appeal to data scientists and engineers alike.</p> <p>The Workloads table provides:</p> <ul> <li>Changing of the layout of the Workloads table by pressing Columns to add or remove columns from the table.</li> <li>Download the table to a CSV file by pressing More, then pressing Download as CSV.</li> <li>Search for a workload by pressing Search and entering the name of the workload.</li> <li>Advanced workload management.</li> <li>Added workload statuses for better tracking of workload flow.</li> </ul> <p>To create new workloads, press New Workload.</p>"},{"location":"platform-admin/workloads/workload-overview/#api-documentation","title":"API Documentation","text":"<p>Access the platform API documentation for more information on using the API to manage workloads.</p>"},{"location":"platform-admin/workloads/workload-overview/#managing-workloads","title":"Managing Workloads","text":"<p>You can manage a workload by selecting one from the view. Once selected, you can:</p> <ul> <li>Run a workload.</li> <li>Stop a workload.</li> <li>Connect to a workload\u2014provides a connection to the selected workload's designated tool. Press the item in the column to show the connection URL.</li> <li>Delete a workload.</li> <li> <p>Copy and edit a workload\u2014use this function to run another workload based on the selected workload.</p> <ul> <li>If the workload was submitted using the UI, then a copy of the original workload form will open allowing you to make changes to the workload properties.</li> <li>If the workload was submitted using the CLI, then a window shows with the original CLI command. Copy the command and make changes to the submission.</li> </ul> </li> <li> <p>Show details\u2014provides in-depth information about the selected workload including:</p> <ul> <li>Event history\u2014workload status over time. Use the filter to search through the history for specific events.</li> <li> <p>Metrics\u2014use the drop down to filter metrics per pod and over time. Select a category from the list below:</p> <ul> <li>Throughput\u2014total of requests per second across all* replica at any given time</li> <li>Latency\u2014average of time it took to answer any request across all replicas at any given time</li> <li>Number of replicas\u2014Total number of all replicas at any given time</li> <li>GPU compute utilization\u2014hover over for individual GPU details</li> <li>GPU memory usage\u2014hover over for individual GPU details</li> <li>CPU usage\u2014hover over for usage details</li> <li>CPU memory usage\u2014hover over for usage details</li> </ul> </li> <li> <p>Logs\u2014logs of the selected workload. Use the drop down to filter metrics per pod. Use the Download button to download the logs.</p> </li> </ul> </li> </ul>"},{"location":"platform-admin/workloads/workload-overview/#workloads-status","title":"Workloads Status","text":"<p>The Status column shows the current status of the workload. The following table describes the statuses presented:</p> Phase Name Description Entry Condition Exit Condition Creating Workload setup is initiated in the cluster. Resources and pods are now provisioning. A workload is submitted. A multi-pod group is created. Pending Workload is queued and awaiting resource allocation. A pod group exists. All pods are scheduled. Initializing Workload is retrieving images, starting containers, and preparing pods. All pods are scheduled\u2014handling of multi-pod groups TBD. All pods are initialized or a failure to initialize is detected. Running Workload is currently in progress with all pods operational. All pods initialized (all containers in pods are ready). Job completion or failure. Degraded Pods may not align with specifications, network services might be incomplete, or persistent volumes may be detached. Check your logs for specific details. Pending\u2014All pods are running but with issues.  Running\u2014All pods are running with no issues. Running\u2014All resources are OK. Completed\u2014 Job finished with fewer resources.Failed\u2014Job failure or user-defined rules. Deleting Workload and its associated resources are being decommissioned from the cluster. Deleting of the workload. Resources are fully deleted. Stopped Workload is on hold and resources are intact but inactive. Stopping the workload without deleting resources. Transitioning back to the initializing phase or proceeded to deleting the workload. Failed Image retrieval failed or containers experienced a crash. Check your logs for specific details. An error occurs preventing the successful completion of the job. Terminal state. Completed Workload has successfully finished its execution. The job has finished processing without errors. Terminal state."},{"location":"platform-admin/workloads/workload-overview/#successful-flow","title":"Successful flow","text":"<p>A successful flow will follow the following flow chart:</p> <pre><code>flowchart LR\n A(Creating) --&gt; B(Pending)\n B--&gt;C(Initializing)\n C--&gt;D(Running)\n D--&gt;E(Completed)</code></pre> <p>To get the full experience of Run:ai\u2019s environment and platform use the following types of workloads.</p> <ul> <li>Workspaces</li> <li>Trainings (Only available when using the Jobs view)</li> <li>Distributed training</li> <li>Deployments.</li> </ul>"},{"location":"platform-admin/workloads/workload-overview/#workload-related-integrations","title":"Workload-related Integrations","text":"<p>To assist you with other platforms, and other types of workloads use the integrations listed below. These integrations are not regularly tested by Run:ai and are hence provided on an as-is basis. The link below point to the Run:ai customer portal. </p> <ol> <li>Airflow</li> <li>MLflow</li> <li>Kubeflow</li> <li>Seldon Core</li> <li>Spark</li> <li>Ray</li> <li>KubeVirt (VM)</li> </ol>"},{"location":"platform-admin/workloads/assets/compute/","title":"Compute Resources","text":"<p>This article explains what compute resources are and how to create and use them.</p> <p>Compute resources are one type of workload asset. A compute resource is a template that simplifies how workloads are submitted and can be used by AI practitioners when they submit their workloads.</p> <p>A compute resource asset is a preconfigured building block that encapsulates all the specifications of compute requirements for the workload including:</p> <ul> <li>GPU devices and GPU memory  </li> <li>CPU memory and CPU compute</li> </ul>"},{"location":"platform-admin/workloads/assets/compute/#compute-resource-table","title":"Compute resource table","text":"<p>The Compute resource table can be found under Compute resources in the Run:ai UI.</p> <p>The Compute resource table provides a list of all the compute resources defined in the platform and allows you to manage them.</p> <p></p> <p>The Compute resource table consists of the following columns:</p> Column Description Compute resource The name of the compute resource Description A description of the essence of the compute resource GPU devices request per pod The number of requested physical devices per pod of the workload that uses this compute resource GPU memory request per device The amount of GPU memory per requested device that is granted to each pod of the workload that uses this compute resource CPU memory request The minimum amount of CPU memory per pod of the workload that uses this compute resource CPU memory limit The maximum amount of CPU memory per pod of the workload that uses this compute resource CPU compute request The minimum number of CPU cores per pod of the workload that uses this compute resource CPU compute limit The maximum number of CPU cores per pod of the workload that uses this compute resource Scope The scope of this compute resource within the organizational tree. Click the name of the scope to view the organizational tree diagram Workload(s) The list of workloads associated with the compute resource Template(s) The list of workload templates that use this compute resource Created by The name of the user who created the compute resource Creation time The timestamp for when the rule was created Cluster The cluster that the compute resource is associated with"},{"location":"platform-admin/workloads/assets/compute/#workloads-associated-with-the-compute-resource","title":"Workloads associated with the compute resource","text":"<p>Click one of the values in the Workload(s) column to view the list of workloads and their parameters.</p> Column Description Workload The workload that uses the compute resource Type (Workspace/Training/Inference) Status Represents the workload lifecycle. see the full list of workload status"},{"location":"platform-admin/workloads/assets/compute/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table</li> </ul>"},{"location":"platform-admin/workloads/assets/compute/#adding-new-compute-resource","title":"Adding new compute resource","text":"<p>To add a new compute resource:</p> <ol> <li>Go to the Compute resource table  </li> <li>Click +NEW COMPUTE RESOURCE </li> <li>Select under which cluster to create the compute resource  </li> <li>Select a scope </li> <li>Enter a name for the compute resource. The name must be unique.  </li> <li>Optional: Provide a description of the essence of the compute resource  </li> <li> <p>Set the resource types needed within a single node    (The Run:ai scheduler tries to match a single node that complies with the compute resource for each of the workload\u2019s pods)  </p> <ul> <li> <p>GPU </p> <ul> <li>GPU devices per pod The number of devices (physical GPUs) per pod    (for example, if you requested 3 devices per pod and the running workload using this compute resource consists of 3 pods, there are 9 physical GPU devices used in total)  </li> </ul> <p>Note</p> <ul> <li>You can insert a whole number of devices (0; 1; 2; 3; \u2026)  </li> <li>When setting it to zero, the workload using this computer resource neither requests or uses GPU resources while running  </li> <li>Only when setting it to 1, a fraction of a GPU memory can be requested  </li> <li>When setting a number higher than 1, the entire GPU memory of the devices is used by the running workloads  </li> </ul> <ul> <li>GPU memory per device <ul> <li>Select the memory request format  <ul> <li>% (of device) - Fraction of a GPU device\u2019s memory  </li> <li>MB (memory size) - An explicit GPU memory unit  </li> <li>GB (memory size) - An explicit GPU memory unit  </li> <li>Multi-instance GPU (MIG) - MIG profile  </li> </ul> </li> <li>Set the memory Request - The minimum amount of GPU memory that is provisioned per device. This means that any pod of a running workload that uses this compute resource, receives this amount of GPU memory for each device(s) the pod utilizes  </li> <li>Optional: Set the memory Limit - The maximum amount of GPU memory that is provisioned per device. This means that any pod of a running workload that uses this compute resource, receives at most this amount of GPU memory for each device(s) the pod utilizes. To set a Limit, first enable the limit toggle. The limit value must be equal to or higher than the request.  </li> </ul> </li> </ul> <p>Note</p> <ul> <li>GPU memory limit is disabled by default. If you cannot see the Limit toggle in the compute resource form, then it must be enabled by your Administrator, under General Settings \u2192 Resources \u2192 GPU resource optimization  </li> <li>When a Limit is set and is bigger than the Request, the scheduler allows each pod to reach the maximum amount of GPU memory in an opportunistic manner (only upon availability).  </li> <li>If the GPU Memory Limit is bigger that the Request the pod is prone to be killed by the Run:ai toolkit (out of memory signal). The greater the difference between the GPU memory used and the request, the higher the risk of being killed  </li> <li>If GPU resource optimization is turned off, the minimum and maximum are in fact equal  </li> </ul> </li> <li> <p>CPU </p> <ul> <li>CPU compute per pod <ul> <li>Select the units for the CPU compute (Cores / Millicores)  </li> <li>Set the CPU compute Request - the minimum amount of CPU compute that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives this amount of CPU compute for each pod.  </li> <li>Optional: Set the CPU compute Limit - The maximum amount of CPU compute that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives at most this amount of CPU compute. To set a Limit, first enable the limit toggle. The limit value must be equal to or higher than the request. By default, the limit is set to \u201cUnlimited\u201d - which means that the pod may consume all the node's free CPU compute resources.  </li> </ul> </li> <li>CPU memory per pod <ul> <li>Select the units for the CPU memory (MB / GB)  </li> <li>Set the CPU memory Request - The minimum amount of CPU memory that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives this amount of CPU memory for each pod.  </li> <li>Optional: Set the CPU memory Limit - The maximum amount of CPU memory that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives at most this amount of CPU memory. To set a Limit, first enable the limit toggle. The limit value must be equal to or higher than the request. By default, the limit is set to \u201cUnlimited\u201d - Meaning that the pod may consume all the node's free CPU memory resources.  </li> </ul> </li> </ul> <p>Note</p> <p>If the CPU Memory Limit is bigger that the Request the pod is prone to be killed by the operating system (out of memory signal). The greater the difference between the CPU memory used and the request, the higher the risk of being killed.  </p> </li> </ul> </li> <li> <p>Optional: More settings  </p> <ul> <li>Increase shared memory size When enabled, the shared memory size available to the pod is increased from the default 64MB to the node's total available memory or the CPU memory limit, if set above.  </li> <li>Set extended resource(s) Click +EXTENDED RESOURCES to add resource/quantity pairs. For more information on how to set extended resources, see the Extended resources and Quantity guides  </li> </ul> </li> <li> <p>Click CREATE COMPUTE RESOURCE</p> <p>Note</p> <p>It is also possible to add compute resources directly when creating a specific Workspace, training or inference workload.</p> </li> </ol>"},{"location":"platform-admin/workloads/assets/compute/#editing-a-compute-resource","title":"Editing a compute resource","text":"<p>To edit a compute resource:</p> <ol> <li>Select the compute resource from the table  </li> <li>Click RENAME to edit its name and description</li> </ol> <p>Note</p> <p>Additional fields can be edited using the API.</p>"},{"location":"platform-admin/workloads/assets/compute/#copying-editing-a-compute-resource","title":"Copying &amp; editing a compute resource","text":"<p>To copy &amp; edit a compute resource:</p> <ol> <li>Select the compute resource you want to duplicate  </li> <li>Click COPY &amp; EDIT </li> <li>Update the compute resource and click CREATE COMPUTE RESOURCE</li> </ol>"},{"location":"platform-admin/workloads/assets/compute/#deleting-a-compute-resource","title":"Deleting a compute resource","text":"<ol> <li>Select the compute resource you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion  </li> </ol> <p>Note</p> <p>It is not possible to delete a compute resource being used by an existing workload and template.</p>"},{"location":"platform-admin/workloads/assets/compute/#using-api","title":"Using API","text":"<p>Go to the Compute resources API reference to view the available actions</p>"},{"location":"platform-admin/workloads/assets/credentials/","title":"Credentials","text":"<p>Credentials are used to unlock protected resources such as applications, containers, and other assets.</p> <p>The Credentials manager in the Run:ai environment supports the following types of credentials:</p> <p>Docker registry</p> <p>Access key</p> <p>Username and password</p> <p>Generic Secret</p>"},{"location":"platform-admin/workloads/assets/credentials/#secrets","title":"Secrets","text":"<p>Credentials are built using <code>Secrets</code>. A <code>Secret</code> is an object that contains a small amount of sensitive data so that you don't need to include confidential data in your application code. When creating a credential you can either create a new secret or use an existing secret.</p>"},{"location":"platform-admin/workloads/assets/credentials/#existing-secrets","title":"Existing secrets","text":"<p>An existing secret is a secret that you have created before creating the credential. One way to create a secret is to use the Kubernetes Secrets creation tool to create a pre-existing secret for the credential. You must <code>label</code> these secrets so that they are registered in the Run:ai environment.</p> <p>The following command makes the secret available to all projects in the cluster.</p> <pre><code>kubectl label secret -n runai &lt;SECRET_NAME&gt; run.ai/resource=&lt;credential_type&gt; run.ai/cluster-wide=true\n</code></pre> <p>The following command makes the secret available to the entire scope of a department.</p> <pre><code>kubectl label secret -n runai &lt;SECRET_NAME&gt; run.ai/resource=&lt;credential_type&gt; run.ai/department=&lt;department-id&gt;\n</code></pre> <p><code>credential_type</code> is one of the following: <code>password</code> / <code>access-key</code> / <code>docker-registry</code></p> <p>The following command makes the secret available to a specific project in the cluster.</p> <pre><code>kubectl label secret -n &lt;NAMESPACE_OF_PROJECT&gt; &lt;SECRET_NAME&gt; run.ai/credentials=true\n</code></pre>"},{"location":"platform-admin/workloads/assets/credentials/#user-id-and-password","title":"User-id and password","text":"<p>You can create a credential using a user-id and password. Use the user-id and password of the target resource.</p>"},{"location":"platform-admin/workloads/assets/credentials/#configuring-credentials","title":"Configuring Credentials","text":"<p>Important</p> <p>To configure Credentials you need to make sure <code>Workspaces</code> are enabled.</p> <p>To configure Credentials:</p> <ol> <li>Press <code>Credentials</code> in the left menu.</li> <li>Press <code>New Credential</code> and select one from the list.</li> </ol>"},{"location":"platform-admin/workloads/assets/credentials/#docker-registry","title":"<code>Docker registry</code>","text":"<ol> <li>Select a <code>Scope</code> (cluster, department, or project) for the credential.</li> <li>In the <code>Credential name</code> field, enter a name for the credential.</li> <li> <p>In the <code>Secret</code> field, choose from <code>Existing secret</code> or <code>New secret</code>.</p> <ul> <li> <p>If you select <code>Existing secret</code>, select an unused secret from the drop down.</p> <p>Note</p> <p>Existing secrets can't be used more than once.</p> </li> <li> <p>If you choose <code>New secret</code>, enter a username and password.</p> </li> </ul> </li> <li> <p>Enter a URL for the docker registry, then press <code>Create credentials</code> to create the credential.</p> </li> </ol>"},{"location":"platform-admin/workloads/assets/credentials/#access-key","title":"<code>Access key</code>","text":"<ol> <li>Select a <code>Scope</code> (cluster, department, or project) for the credential.</li> <li>In the <code>Credential name</code> field, enter a name for the credential.</li> <li> <p>In the <code>Secret</code> field, choose from <code>Existing secret</code> or <code>New secret</code>.</p> <ul> <li> <p>If you select <code>Existing secret</code>, select an unused secret from the drop down.</p> <p>Note</p> <p>Existing secrets can't be used more than once.  </p> </li> <li> <p>If you choose <code>New secret</code>, enter an access key and access secret.</p> </li> </ul> </li> <li> <p>Press <code>Create credentials</code> to create the credential.</p> </li> </ol>"},{"location":"platform-admin/workloads/assets/credentials/#username-and-password","title":"<code>Username and password</code>","text":"<ol> <li>Select a <code>Scope</code> (cluster, department, or project) for the credential.</li> <li>In the <code>Credential name</code> field, enter a name for the credential.</li> <li> <p>In the <code>Secret</code> field, choose from <code>Existing secret</code> or <code>New secret</code>.</p> <ul> <li> <p>If you select <code>Existing secret</code>, select an unused secret from the drop down.</p> <p>Note</p> <p>Existing secrets can't be used more than once.</p> </li> <li> <p>If you choose <code>New secret</code>, enter a username and password.</p> </li> </ul> </li> <li> <p>Press <code>Create credentials</code> to create the credential.</p> </li> </ol>"},{"location":"platform-admin/workloads/assets/credentials/#generic-secret","title":"Generic Secret","text":"<ol> <li>Select a <code>Scope</code> (cluster, department, or project) for the credential.</li> <li>In the <code>Credential name</code> field, enter a name for the credential, and the in the Description field, enter a description..</li> <li> <p>In the <code>Secret</code> field, choose from <code>Existing secret</code> or <code>New secret</code>.</p> <ul> <li> <p>If you select <code>Existing secret</code>, select an unused secret from the drop down.</p> <p>Note</p> <p>Existing secrets can't be used more than once.</p> </li> <li> <p>If you choose <code>New secret</code>, enter a key, valuer pair.</p> </li> </ul> </li> <li> <p>Press <code>Create credentials</code> to create the credential.</p> </li> </ol>"},{"location":"platform-admin/workloads/assets/credentials/#credentials-table","title":"Credentials Table","text":"<p>The Credentials table contains a column that shows the status of the credential. The following statuses are supported:</p> Status Description No issues found No issues were found when propagating the credential to the configured scope. Issues found Issues were found while propagating the credentials to the configured scope. Issues found The credential could not be created in the cluster. No status Status could not be displayed because the credentials scope is an account. No Status Status could not be displayed because the current version of the cluster is not up to date. <p>You can download the Credentials table to a CSV file. Downloading a CSV can provide a snapshot history of your credentials over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>Use the Cluster filter at the top of the table to see credentials that are assigned to specific clusters.</p> <p>Note</p> <p>The cluster filter will be in the top bar when there are clusters that are installed with version 2.16 or lower.</p> <p>Use the Add filter to add additional filters to the table.</p> <p>To download the Credentials table to a CSV:</p> <ol> <li>Open Credentials.</li> <li>From the Columns icon, select the columns you would like to have displayed in the table.</li> <li>Click on the ellipsis labeled More, and download the CSV.</li> </ol>"},{"location":"platform-admin/workloads/assets/data-volumes/","title":"Data Volumes","text":"<p>Data Volumes offer a powerful solution for storing, managing, and sharing AI training data within your Run.ai environment. This functionality promotes collaboration, simplifies data access control, and streamlines the AI development lifecycle.</p>"},{"location":"platform-admin/workloads/assets/data-volumes/#what-are-data-volumes","title":"What are Data Volumes","text":"<p>Data Volumes are snapshots of datasets stored in Kubernetes Persistent Volume Claims (PVCs). They act as a central repository for training data, and offer several key benefits.</p> <ul> <li>Managed with dedicated permissions\u2014Data admins, a new role within Run.ai, have exclusive control over data volume creation, data population, and sharing.</li> <li>Shared between multiple scopes\u2014Unlike other Run:ai data sources, data volumes can be shared across projects, departments, or clusters. This promotes data reuse and collaboration within your organization.</li> <li>Coupled to workloads in the submission process\u2014 Similar to other Run:ai data sources, Data volumes can be easily attached to AI workloads during submission, specifying the data path within the workload environment.</li> </ul> <p>Note</p> <p>Data volumes are not versioned.</p> <p></p>"},{"location":"platform-admin/workloads/assets/data-volumes/#data-volumes-use-cases","title":"Data volumes use cases","text":"<p>The following are typical use cases for Data Volumes:</p> <ul> <li>Sharing large data sets with multiple researchers in my organization\u2014Sometimes we have data located in a remote location. After moving it inside the cluster, sharing it easily with multiple users is still hard. Data volumes can help you do that seamlessly and with maximum security and control</li> <li>Sharing data created during the AI work cycle\u2014When it is needed to share training results, generated data sets or other artifacts with our team members. Data volume helps you take your data and share it with your colleagues.</li> </ul>"},{"location":"platform-admin/workloads/assets/data-volumes/#data-volumes-authorization","title":"Data volumes authorization","text":"<p>There is now a new role called <code>Data Volumes Administrator</code> which contains the following two sets of permissions and allows you to manage your Data Volumes easily.</p> <p>Note</p> <p>CRUD = Create, Read, Update, and Delete.</p> <p>Data Volumes administrator contains two permission entities:</p> <ul> <li>Data volumes - CRUD</li> <li>Data volumes - sharing list - CRUD</li> </ul> <p>Data volumes (should have the origin project in the scope)</p> <ul> <li>Can create DV in the scope</li> <li>Can read DV in the scope</li> <li>Can update DV in the scope</li> <li>Can delete DV in the scope</li> </ul> <p>Data volumes - sharing list</p> <ul> <li>Can Share DV in the scope</li> <li>Can unshare DV from the scope</li> </ul>"},{"location":"platform-admin/workloads/assets/data-volumes/#data-volume-administrator-permissions","title":"Data volume administrator permissions","text":"Entity Permissions Data volumes\u00a0 CRUD Data volumes - sharing list CRUD Account R Department R Project R Jobs R Workloads R Cluster R Overview dashboard R Consumption dashboard R Analytics dashboard R Policies R workloads R Workspaces R Trainings R Environments R Compute resources R Templates R Data source R Inferences R"},{"location":"platform-admin/workloads/assets/data-volumes/#data-volume-permissions-for-each-role","title":"Data volume permissions for each role","text":"Role DV permissions Data volume administrator DV CRUD, Sharing CRUD System administrator DV CRUD, Sharing CRUD Department admin DV CRUD, Sharing CRUD Department viewer DV R Researcher manager DV CRUD, Sharing CRUD Editor DV CRUD, Sharing CRUD L1 DV CRUD L2 DV R ML engineer DV R Assets admins\u00a0 DV R Application admin DV R Viewer DV R"},{"location":"platform-admin/workloads/assets/data-volumes/#using-data-volumes","title":"Using Data volumes","text":"<p>This section outlines the procedure for creating, sharing, and submitting (Researcher) data volumes.</p>"},{"location":"platform-admin/workloads/assets/data-volumes/#creating-data-volumes","title":"Creating Data Volumes","text":"<p>Note</p> <p>Data volume admins can create data volumes within specific projects. Since data volumes are created from PVCs, there has to be a PVC in the namespace of a run:ai project, and a PV bound to it, for Run:Ai to have access to it and create the Data volume from it. Once the DV is created, the admin manages its sharing configurations.</p> <p>Data Volumes are created using the API endpoint. For more information, see Data Volumes</p>"},{"location":"platform-admin/workloads/assets/data-volumes/#sharing-data-volumes","title":"Sharing Data volumes","text":"<p>Sharing permissions is a sub-entity of the Data volume management permissions. Meaning they can be assigned independently. A user can have permission to create a DV but not to share it. A data volume can be shared with one or multiple scopes. In all the scopes that the DV is shared, it can be used by the users in their workloads.</p> <p>Data Volumes are shared using the API endpoint. For more information, see Data Volumes.</p>"},{"location":"platform-admin/workloads/assets/data-volumes/#using-data-volumes-in-workloads","title":"Using Data Volumes in Workloads","text":"<p>You can attach a data volume to a workload during submission in the same way other data sources are used. You need to specify the desired data path within the data source parameters.</p> <p>Researchers can list available data volumes within their permitted scopes for easy selection.</p> <p>For more information on using a data volume when submitting a workload, see Submitting Workloads.</p> <p>You can also add a data volumes to your workload when submitting a workload via the API. For more information, see Workloads.</p>"},{"location":"platform-admin/workloads/assets/datasources/","title":"Overview","text":""},{"location":"platform-admin/workloads/assets/datasources/#introduction","title":"Introduction","text":"<p>A data source is a location where data sets relevant to the research are stored. Workspaces can be attached to several data sources for reading and writing. The data can be located locally or in the cloud. Run:ai data sources can use a variety of storage technologies such as Git, S3, NFS, PVC, and more.  </p> <p>The data source is an optional building block for the creation of a workspace.</p> <p></p>"},{"location":"platform-admin/workloads/assets/datasources/#create-a-new-data-source","title":"Create a new data source","text":"<p>When you select <code>New Compute Resource</code> you will be presented with various data source options described below.</p>"},{"location":"platform-admin/workloads/assets/datasources/#create-an-nfs-data-source","title":"Create an NFS data source","text":"<p>To create an NFS data source, provide:</p> <ul> <li>A data source name.</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li>An NFS server.</li> <li>The path to the data within the server.</li> <li>The path within the container where the data will be mounted.</li> </ul> <p>The data can be set as read-write or limited to read-only permission regardless of any other user privileges.</p>"},{"location":"platform-admin/workloads/assets/datasources/#create-a-pvc-data-source","title":"Create a PVC data source","text":"<p>To create an PVC data source, provide:</p> <ul> <li>A data source name</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li> <p>Select an existing PVC or create a new one by providing:</p> </li> <li> <p>A claim name</p> </li> <li>A storage class</li> <li>Access mode</li> <li>Required storage size</li> <li> <p>Volume system mode</p> </li> <li> <p>The path within the container where the data will be mounted.</p> </li> </ul> <p>You can see the status of the resources created in the Data sources table.</p>"},{"location":"platform-admin/workloads/assets/datasources/#create-an-s3-data-source","title":"Create an S3 data source","text":"<p>S3 storage saves data in buckets. S3 is typically attributed to AWS cloud service but can also be used as a separate service unrelated to Amazon.</p> <p>To create an S3 data source, provide</p> <ul> <li>A data source name</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li>The relevant S3 service URL server</li> <li>The bucket name of the data.</li> <li>The path within the container where the data will be mounted.</li> </ul> <p>An S3 data source can be public or private. For the latter option, please select the relevant credentials associated with the project to allow access to the data. S3 buckets that use credentials will have a status associated with it. For more information, see Data sources table.</p>"},{"location":"platform-admin/workloads/assets/datasources/#create-a-git-data-source","title":"Create a Git data source","text":"<p>To create a Git data source, provide:</p> <ul> <li>A data source name.</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li>The relevant repository URL.</li> <li>The path within the container where the data will be mounted.</li> </ul> <p>The Git data source can be public or private. To allow access to a private Git data source, you must select the relevant credentials associated with the project. Git data sources that use credentials will have a status associated with it. For more information, see Data sources table.</p>"},{"location":"platform-admin/workloads/assets/datasources/#create-a-host-path-data-source","title":"Create a host path data source","text":"<p>To create a host path data source, provide:</p> <ul> <li>A data source name.</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li>The relevant path on the host.</li> <li>The path within the container where the data will be mounted.</li> </ul> <p>Note</p> <p>The data can be limited to read-only permission regardless of any other user privileges.</p>"},{"location":"platform-admin/workloads/assets/datasources/#create-a-configmap-data-source","title":"Create a ConfigMap data source","text":"<ul> <li> <p>A Run:ai project scope which is assigned to that item and all its subsidiaries.</p> <p>Note</p> <p>You can only choose a project as a scope.</p> </li> </ul> <p>ConfigMaps must be created on the cluster before being used within Run:ai. When created, the ConfigMap must have a label of <code>run.ai/resource: &lt;resource-name&gt;</code>. The resource name specified must be unique to that created resource. </p> <ul> <li>A data source name.</li> <li> <p>A data mount consisting of:</p> </li> <li> <p>A ConfigMap name\u2014select from the drop down.</p> </li> <li>A target location\u2014the path to the container.</li> </ul>"},{"location":"platform-admin/workloads/assets/datasources/#create-a-secret-as-data-source","title":"Create a Secret as data source","text":"<ul> <li>A Run:ai project scope which is assigned to that item and all its subsidiaries.</li> <li> <p>A Credentials. To create a new Credentials, see Configuring Credentials</p> <p>Note</p> <p>You can only choose a project as a scope.</p> </li> <li> <p>A data source name and description.</p> </li> <li> <p>A data mount consisting of:</p> </li> <li> <p>A Credentials\u2014select from the drop down.</p> </li> <li>A target location\u2014the path to the container.</li> </ul>"},{"location":"platform-admin/workloads/assets/datasources/#data-sources-table","title":"Data sources table","text":"<p>The Data sources table contains a column for the status of the data source. The following statuses are supported:</p> Status Description No issues found No issues were found when propagating the data source to the PROJECTS. Issues found Failed to create the data source for some or all of the PROJECTS. Issues found Failed to access the cluster. Deleting The data source is being removed. <p>Note</p> <ul> <li>The Status column in the table shows statuses based on your level of permissions. For example, a user that has create permissions for the scope, will see statuses that are calculated from the entire scope, while users who have only view and use permissions, will only be able to see statuses from a subset of the scope (assets that they have permissions to).</li> <li>The status of \u201c-\u201d indicates that there is no status because this asset is not cluster-syncing.</li> </ul> <p>You can download the Data Sources table to a CSV file. Downloading a CSV can provide a snapshot history of your Data Sources over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>Use the Cluster filter at the top of the table to see data sources that are assigned to specific clusters.</p> <p>Note</p> <p>The cluster filter will be in the top bar when there are clusters that are installed with version 2.16 or lower.</p> <p>Use the Add filter to add additional filters to the table.</p> <p>To download the Data Sources table to a CSV:</p> <ol> <li>Open Data Sources.</li> <li>From the Columns icon, select the columns you would like to have displayed in the table.</li> <li>Click on the ellipsis labeled More, and download the CSV.</li> </ol>"},{"location":"platform-admin/workloads/assets/environments/","title":"Environments","text":"<p>This article explains what environments are and how to create and use them.</p> <p>Environments are one type of workload asset. An environment consists of a configuration that simplifies how workloads are submitted and can be used by AI practitioners when they submit their workloads.</p> <p>An environment asset is a preconfigured building block that encapsulates aspects for the workload such as:</p> <ul> <li>Container image and container configuration  </li> <li>Tools and connections  </li> <li>The type of workload it serves</li> </ul>"},{"location":"platform-admin/workloads/assets/environments/#environments-table","title":"Environments table","text":"<p>The Environments table can be found under Environments in the Run:ai platform.</p> <p>The Environment table provides a list of all the environment defined in the platform and allows you to manage them.</p> <p></p> <p>The Environments table consists of the following columns:</p> Column Description Environment The name of the environment Description A description of the essence of the environment Scope The scope of this environment within the organizational tree. Click the name of the scope to view the organizational tree diagram Image The application or service to be run by the workload Workload Architecture This can be either standard for running workloads on a single node or distributed for running distributed workloads on a multiple nodes Tool(s) The tools and connection types the environment exposes Workload(s) The list of existing workloads that use the environment Workload types The workload types that can use the environment Template(s) The list of workload templates that use this environment Created by The user who created the environment. By default Run:ai UI comes with preinstalled environments created by Run:ai Creation time The timestamp for when the environment was created Cluster The cluster that the environment is associated with"},{"location":"platform-admin/workloads/assets/environments/#tools-associated-with-the-environment","title":"Tools associated with the environment","text":"<p>Click one of the values in the tools column to view the list of tools and their connection type.</p> Column Description Tool name The name of the tool or application AI practitioner can set up within the environment. Connection type The method by which you can access and interact with the running workload. It's essentially the \"doorway\" through which you can reach and use the tools the workload provide. (E.g node port, external URL, etc)"},{"location":"platform-admin/workloads/assets/environments/#workloads-associated-with-the-environment","title":"Workloads associated with the environment","text":"<p>Click one of the values in the Workload(s) column to view the list of workloads and their parameters.</p> Column Description Workload The workload that uses the environment Type The workload type (Workspace/Training/Inference) Status Represents the workload lifecycle. see the full list of workload status"},{"location":"platform-admin/workloads/assets/environments/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/workloads/assets/environments/#environments-created-by-runai","title":"Environments created by Run:ai","text":"<p>When installing Run:ai, you automatically get the environment created by Run:ai to ease up the onboarding process and support different use cases out of the box. These environments are created at the scope of the account.</p> Environment Image Jupiter-lab jupyter/scipy-notebook jupyter-tensorboard gcr.io/run-ai-demo/jupyter-tensorboard tensorboard tensorflow/tensorflow:latest llm-server runai.jfrog.io/core-llm/runai-vllm:v0.5.5-0.5.0 chatbot-ui runai.jfrog.io/core-llm/llm-app gpt2 runai.jfrog.io/core-llm/quickstart-inference:gpt2-cpu"},{"location":"platform-admin/workloads/assets/environments/#adding-a-new-environment","title":"Adding a new environment","text":"<p>Environment creation is limited to specific roles</p> <p>To add a new environment:</p> <ol> <li>Go to the Environments table  </li> <li>Click +NEW ENVIRONMENT </li> <li>Select under which cluster to create the environment  </li> <li>Select a scope </li> <li>Enter a name for the environment. The name must be unique.  </li> <li>Optional: Provide a description of the essence of the environment  </li> <li>Enter the Image URL    If a token or secret is required to pull the image, it is possible to create it via credentials of type docker registry. These credentials are automatically used once the image is pulled (which happens when the workload is submitted)  </li> <li>Set the image pull policy - the condition for when to pull the image from the registry  </li> <li>Set the workload architecture:  <ul> <li>Standard Only standard workloads can use the environment. A standard workload consists of a single process.  </li> <li>Distributed Only distributed workloads can use the environment. A distributed workload consists of multiple processes working together. These processes can run on different nodes.  </li> <li>Select a framework from the list.  </li> </ul> </li> <li>Set the workload type:  <ul> <li>Workspace </li> <li>Training </li> <li>Inference </li> <li>When inference is selected, define the endpoint of the model by providing both the protocol and the container\u2019s serving port  </li> </ul> </li> <li>Optional: Set the connection for your tool(s). The tools must be configured in the image. When submitting a workload using the environment, it is possible to connect to these tools  <ul> <li>Select the tool from the list (the available tools varies from IDE, experiment tracking, and more, including a custom tool for your choice)  </li> <li>Select the connection type  <ul> <li>External URL <ul> <li>Auto generate   A unique URL is automatically created for each workload using the environment  </li> <li>Custom URL   The URL is set manually  </li> </ul> </li> <li>Node port <ul> <li>Auto generate   A unique port is automatically exposed for each workload using the environment  </li> <li>Custom URL   Set the port manually  </li> </ul> </li> <li>Set the container port </li> </ul> </li> </ul> </li> <li>Optional: Set a command and arguments for the container running the pod  <ul> <li>When no command is added, the default command of the image is used (the image entrypoint)  </li> <li>The command can be modified while submitting a workload using the environment  </li> <li>The argument(s) can be modified while submitting a workload using the environment  </li> </ul> </li> <li>Optional: Set the environment variable(s) <ul> <li>The environment variable(s) are added to the default environment variables that are already set within the image  </li> <li>The environment variables can be modified and new variables can be added while submitting a workload using the environment  </li> </ul> </li> <li>Optional: Set the container\u2019s working directory to define where the container\u2019s process starts running. When left empty, the default directory is used.  </li> <li>Optional: Set where the UID, GID and supplementary groups are taken from, this can be:  <ul> <li>From the image </li> <li>From the IdP token (only available in an SSO installations)  </li> <li>Custom (manually set) - decide whether the submitter can modify these value upon submission.  </li> </ul> </li> <li>Optional: Select Linux capabilities - Grant certain privileges to a container without granting all the privileges of the root user. </li> <li>Click CREATE ENVIRONMENT</li> </ol> <p>Note</p> <p>It is also possible to add environments directly when creating a specific workspace, training or inference workload</p>"},{"location":"platform-admin/workloads/assets/environments/#editing-an-environment","title":"Editing an environment","text":"<p>To edit an environment:</p> <ol> <li>Select the environment from the table  </li> <li>Click Rename to edit its name and description</li> </ol> <p>Note</p> <p>Additional fields can be edited using the API</p>"},{"location":"platform-admin/workloads/assets/environments/#copying-editing-an-environment","title":"Copying &amp; Editing an environment","text":"<p>To copy &amp; edit an environment:</p> <ol> <li>Select the project you want to duplicate  </li> <li>Click COPY &amp; EDIT. </li> <li>Update the environment and click SAVE.</li> </ol>"},{"location":"platform-admin/workloads/assets/environments/#deleting-an-environment","title":"Deleting an environment","text":"<p>To delete an environment:</p> <ol> <li>Select the environment you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>It is not possible to delete an environment being used by an existing workload and template.</p>"},{"location":"platform-admin/workloads/assets/environments/#using-api","title":"Using API","text":"<p>Go to the Environment API reference to view the available actions</p>"},{"location":"platform-admin/workloads/assets/existing-PVC/","title":"Persistent Volumes (PVs) &amp; Persistent Volume Claims (PVCs)","text":"<p>Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) are concepts in Kubernetes for managing storage. A PV is a piece of storage in the cluster, provisioned by an administrator or dynamically by Kubernetes using a StorageClass. It is a resource in the cluster, just like a node is a cluster resource.</p> <p>PVCs are requests for storage by a user. They are similar to pods, in that pods consume node resources and PVCs consume PV resources. PVCs allow users to request specific sizes and access modes (for example, read/write once, read-only many) without needing to know the details of the underlying storage infrastructure.</p> <p>Using PVs and PVCs in Kubernetes is essential for AI workloads as they provide a reliable and consistent way to manage storage that persists beyond the lifecycle of individual pods. This ensures that data generated by AI workloads is not lost when pods are rescheduled or updated, providing a seamless and efficient storage solution that can handle the large datasets typically associated with AI projects.</p>"},{"location":"platform-admin/workloads/assets/existing-PVC/#data-source-of-type-persistent-volume-claim-pvc","title":"Data source of type Persistent Volume Claim (PVC)","text":"<p>At Run:ai, a data source of type PVC is an abstraction, mapping directly to a Kubernetes PVC. This type of integration allows you to specify and manage your data storage requirements within the Run:ai platform, while using familiar Kubernetes storage concepts.</p> <p>By leveraging PVCs as data sources, Run:ai enables access to persistent storage for workloads, ensuring that data remains consistent and accessible across different compute resources and workload executions.</p>"},{"location":"platform-admin/workloads/assets/existing-PVC/#creating-a-data-source-of-type-pvc-via-the-ui","title":"Creating a data source of type PVC via the UI","text":"<p>Like any other asset, when creating a data source, the user can select the scope of the data source, based on their permissions set in Run:ai\u2019s Role Based Access Control (RBAC) system.</p> <p>For example: By selecting Department B as the scope of the asset, any user with a role which allows them to view the data source in Department A or any of its subordinate units (current and future) can view this PVC.  </p> <p>There are two different ways of creating data source of type PVC:</p> <ol> <li>Existing PVC - Data source of type PVC using an existing PVC in the cluster.</li> <li>New PVC - Data source of type PVC by creating a new PVC in the cluster. </li> </ol> <p>Note</p> <p>If there are no existing PVCs that Run:ai has visibility or authorization to use, this option is disabled in the Run:ai platform. For details on providing visibility and authorization, see below Existing PVC.</p>"},{"location":"platform-admin/workloads/assets/existing-PVC/#existing-pvc","title":"Existing PVC","text":"<p>To select an existing PVC in the Run:ai platform, the admin is responsible for performing a number of actions prior to creating the data source via the Run:ai UI (or API). These actions provide Run:ai with access to the existing PVC, authorization to share across the selected scope and eventually result in exposing the existing PVC in the UI for the user to select.</p> <p>Click the link for more information on creating a data source of type PVC via API.</p> <p>The actions taken by the admin are based on the scope (cluster, department or project) that the admin wants for data source of type PVC.</p>"},{"location":"platform-admin/workloads/assets/existing-PVC/#for-a-cluster-scope","title":"For a cluster scope","text":"<ol> <li>Locate the PVC in the runai namespace  </li> <li>Provide Run:ai with visibility and authorization to share the PVC to your selected scope by implementing the following label: run.ai/cluster-wide: \"true\"</li> </ol> <p>Note</p> <p>This step is also relevant for creating the data source of type PVC via API.</p> <p>In the Run:ai platform finish creating the data source of type PVC:</p> <ol> <li>Select your cluster as a scope  </li> <li>Select the existing PVC  </li> <li>Complete all mandatory fields  </li> <li>Click Create</li> </ol>"},{"location":"platform-admin/workloads/assets/existing-PVC/#for-a-department-scope","title":"For a department scope","text":"<ol> <li>Locate the PVC in the runai namespace  </li> <li>Provide Run:ai with visibility and authorization to share the PVC to your selected scope by implementing the following label: run.ai/department: \"id\"  </li> <li>In the Run:ai platform finish creating the data source of type PVC:  </li> <li>Select you department as a scope (the same one as in the label)  </li> <li>Select the existing PVC  </li> <li>Complete all mandatory fields  </li> <li>Click Create</li> </ol>"},{"location":"platform-admin/workloads/assets/existing-PVC/#for-a-project-scope","title":"For a project scope","text":"<p>Note</p> <p>For project scope, no labels are required.</p> <ol> <li>In the Run:ai platform finish creating the data source of type PVC:  </li> <li>Select your project as a scope  </li> <li>Select the existing PVC  </li> <li>Complete all mandatory fields  </li> <li>Click Create</li> </ol>"},{"location":"platform-admin/workloads/assets/existing-PVC/#creating-a-new-pvc","title":"Creating a new PVC","text":"<p>When creating a data source of type PVC using a new PVC, Run:ai creates the PVC for you in the cluster.</p> <ol> <li>Select your scope of choice  </li> <li>Select new PVC  </li> <li>Complete all mandatory fields  </li> <li>Click Create</li> </ol> <p>Notes</p> <p>When creating data source of type PVC using a new PVC, the PVC is immediately created in the cluster (even if no workload has requested to use this PVC). PVCs created in the cluster using the 'New PVC' option will not appear in the 'Existing PVC' selection.</p>"},{"location":"platform-admin/workloads/assets/overview/","title":"Overview","text":"<p>Workload assets enable organizations to:</p> <ul> <li>Create and reuse preconfigured setup for code, data, storage and resources to be used by AI practitioners to simplify the process of submitting workloads  </li> <li>Share the preconfigured setup with a wide audience of AI practitioners with similar needs</li> </ul> <p>Note</p> <ul> <li>The creation of assets is possible only via API and the Run:ai UI  </li> <li>The submission of workloads using assets, is possible only via the Run:ai UI</li> </ul>"},{"location":"platform-admin/workloads/assets/overview/#workload-asset-types","title":"Workload asset types","text":"<p>There are four workload asset types used by the workload:</p> <ul> <li>Environments   The container image, tools and connections for the workload  </li> <li>Data sources   The type of data, its origin and the target storage location such as PVCs or cloud storage buckets where datasets are stored  </li> <li>Compute resources   The compute specification, including GPU and CPU compute and memory  </li> <li>Credentials   The secrets to be used to access sensitive data, services, and applications such as docker registry or S3 buckets</li> </ul>"},{"location":"platform-admin/workloads/assets/overview/#asset-scope","title":"Asset scope","text":"<p>When a workload asset is created, a scope is required. The scope defines who in the organization can view and/or use the asset.</p> <p>Note</p> <p>When an asset is created via API, the scope can be the entire account, this is currently an experimental feature.</p>"},{"location":"platform-admin/workloads/assets/overview/#who-can-create-an-asset","title":"Who can create an asset?","text":"<p>Any subject (user, application, or SSO group) with a role that has permissions to Create an asset, can do so within their scope.</p>"},{"location":"platform-admin/workloads/assets/overview/#who-can-use-an-asset","title":"Who can use an asset?","text":"<p>Assets are used when submitting workloads. Any subject (user, application or SSO group) with a role that has permissions to Create workloads, can also use assets.</p>"},{"location":"platform-admin/workloads/assets/overview/#who-can-view-an-asset","title":"Who can view an asset?","text":"<p>Any subject (user, application, or SSO group) with a role that has permission to View an asset, can do so within their scope.  </p>"},{"location":"platform-admin/workloads/assets/secrets/","title":"Secrets in Workloads","text":""},{"location":"platform-admin/workloads/assets/secrets/#kubernetes-secrets","title":"Kubernetes Secrets","text":"<p>Sometimes you want to use sensitive information within your code. For example passwords, OAuth tokens, or ssh keys. The best practice for saving such information in Kubernetes is via Kubernetes Secrets. Kubernetes Secrets let you store and manage sensitive information. Access to secrets is limited via configuration.</p> <p>A Kubernetes secret may hold multiple key - value pairs.</p>"},{"location":"platform-admin/workloads/assets/secrets/#using-secrets-in-runai-workloads","title":"Using Secrets in Run:ai Workloads","text":"<p>Our goal is to provide Run:ai Workloads with secrets as input in a secure way. Using the Run:ai command line, you will be able to pass a reference to a secret that already exists in Kubernetes.</p>"},{"location":"platform-admin/workloads/assets/secrets/#creating-a-secret","title":"Creating a secret","text":"<p>For details on how to create a Kubernetes secret see: https://kubernetes.io/docs/concepts/configuration/secret/. Here is an example:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\n  namespace: runai-&lt;project-name&gt;\ndata:\n  username: am9obgo=\n  password: bXktcGFzc3dvcmQK\n</code></pre> <p>Then run: <pre><code>kubectl apply -f &lt;file-name&gt;\n</code></pre></p> <p>Notes</p> <ul> <li>Secrets are base64 encoded</li> <li>Secrets are stored in the scope of a namespace and will not be accessible from other namespaces. Hence the reference to the Run:ai Project name above. Run:ai provides the ability to propagate secrets throughout all Run:ai Projects. See below.</li> </ul>"},{"location":"platform-admin/workloads/assets/secrets/#attaching-a-secret-to-a-workload-on-submit","title":"Attaching a secret to a Workload on Submit","text":"<p>When you submit a new Workload, you will want to connect the secret to the new Workload. To do that, run:</p> <pre><code>runai submit -e &lt;ENV-VARIABLE&gt;=SECRET:&lt;secret-name&gt;,&lt;secret-key&gt; ....\n</code></pre> <p>For example:</p> <pre><code>runai submit -i ubuntu -e MYUSERNAME=SECRET:my-secret,username\n</code></pre>"},{"location":"platform-admin/workloads/assets/secrets/#secrets-and-projects","title":"Secrets and Projects","text":"<p>As per the note above, secrets are namespace-specific. If your secret relates to all Run:ai Projects, do the following to propagate the secret to all Projects:</p> <ul> <li>Create a secret within the <code>runai</code> namespace.</li> <li>Run the following once to allow Run:ai to propagate the secret to all Run:ai Projects:</li> </ul> <pre><code>runai-adm set secret &lt;secret name&gt; --cluster-wide\n</code></pre> <p>Reminder</p> <p>The Run:ai Administrator CLI can be obtained here.</p> <p>To delete a secret from all Run:ai Projects, run:</p> <pre><code>runai-adm remove secret &lt;secret name&gt; --cluster-wide\n</code></pre>"},{"location":"platform-admin/workloads/assets/secrets/#secrets-and-policies","title":"Secrets and Policies","text":"<p>A Secret can be set at the policy level. For additional information see policies guide.</p>"},{"location":"platform-admin/workloads/assets/templates/","title":"Templates","text":"<p>This article explains the procedure to manage templates.</p> <p>A template is a pre-set configuration that is used to quickly configure and submit workloads using existing assets. A template consists of all the assets a workload needs, allowing researchers to submit a workload in a single click, or make subtle adjustments to differentiate them from each other.</p>"},{"location":"platform-admin/workloads/assets/templates/#workspace-templates-table","title":"Workspace templates table","text":"<p>Access to the Templates table can be found on the left-hand menu in the Run:ai platform.</p> <p>The Templates table provides a list of all the templates defined in the platform, and allows you to manage them.</p> <p>Flexible Management</p> <p>It is also possible to manage templates directly for a specific user, application, project, or department.</p> <p></p> <p>The Templates table consists of the following columns:</p> Column Description Scope The scope to which the subject has access. Click the name of the scope to see the scope and its subordinates Environment The name of the environment related to the workspace template Compute resource The name of the compute resource connected to the workspace template Data source(s) The name of the data source(s) connected to the workspace template Created by The subject that created the template Creation time The timestamp for when the template was created Cluster The cluster name containing the template"},{"location":"platform-admin/workloads/assets/templates/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV  </li> <li>Refresh (optional) - Click REFRESH to update the table with the latest data  </li> <li>Show/Hide details (optional) - Click to view additional information on the selected row</li> </ul>"},{"location":"platform-admin/workloads/assets/templates/#adding-a-new-workspace-template","title":"Adding a new workspace template","text":"<p>To add a new template:</p> <ol> <li>Click +NEW TEMPLATE </li> <li>Set the scope for the template  </li> <li>Enter a name for the template  </li> <li>Select the environment for your workload  </li> <li> <p>Select the node resources needed to run your workload     - or -    Click +NEW COMPUTE RESOURCE</p> </li> <li> <p>Set the volume needed for your workload  </p> </li> <li>Create a new data source  </li> <li>Set auto-deletion, annotations and labels, as required  </li> <li>Click CREATE TEMPLATE</li> </ol>"},{"location":"platform-admin/workloads/assets/templates/#editing-a-template","title":"Editing a template","text":"<p>To edit a template:</p> <ol> <li>Select the template from the table  </li> <li>Click Rename to provide it with a new name  </li> <li>Click Copy &amp; Edit to make any changes to the template</li> </ol>"},{"location":"platform-admin/workloads/assets/templates/#deleting-a-template","title":"Deleting a template","text":"<p>To delete a template:</p> <ol> <li>Select the template you want to delete  </li> <li>Click DELETE </li> <li>Confirm you want to delete the template</li> </ol>"},{"location":"platform-admin/workloads/assets/templates/#using-api","title":"Using API**","text":"<p>Go to the Workload template API reference to view the available actions  </p>"},{"location":"platform-admin/workloads/policies/old-policies/","title":"Policies (YAML-based)","text":"<p>Warning</p> <p>The below describes the old V1 Policies. While these still work, they have been replaced with Control-plane-based v2 policies which are accessible via API and user interface.  For a description of the new policies, see API-based Policies.</p>"},{"location":"platform-admin/workloads/policies/old-policies/#what-are-policies","title":"What are Policies?","text":"<p>Policies allow administrators to impose restrictions and set default values for Researcher Workloads. For example:</p> <ol> <li>Restrict researchers from requesting more than 2 GPUs, or less than 1GB of memory for an interactive workload.</li> <li>Set the default memory of each training job to 1GB, or mount a default volume to be used by any submitted Workload.</li> </ol> <p>Policies are stored as Kubernetes custom resources.</p> <p>Policies are specific to Workload type as such there are several kinds of Policies:</p> Workload Type Kubernetes Workload Name Kubernetes Policy Name Interactive <code>InteractiveWorkload</code> <code>InteractivePolicy</code> Training <code>TrainingWorkload</code> <code>TrainingPolicy</code> Distributed Training <code>DistributedWorkload</code> <code>DistributedPolicy</code> Inference <code>InferenceWorkload</code> <code>InferencePolicy</code> <p>A Policy can be created per Run:ai Project (Kubernetes namespace). Additionally, a Policy resource can be created in the <code>runai</code> namespace. This special Policy will take effect when there is no project-specific Policy for the relevant workload kind.</p> <p>When researchers create a new interactive workload or workspace, they see list of available node pools and their priority. Priority is set by dragging and dropping the node pools in the desired order of priority. When the node pool priority list is locked by an administrator policy, the node pool list isn't editable by the Researcher even if the workspace is created from a template or copied from another workspace.</p> <p>Note</p> <p>Policies on this page cannot be added to platform 2.16 or higher that have the New Policy Manager enabled.</p>"},{"location":"platform-admin/workloads/policies/old-policies/#creating-a-policy","title":"Creating a Policy","text":""},{"location":"platform-admin/workloads/policies/old-policies/#creating-your-first-policy","title":"Creating your First Policy","text":"<p>To create a sample <code>InteractivePolicy</code>, prepare a file (e.g. <code>policy.yaml</code>) containing the following YAML:</p> gpupolicy.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: InteractivePolicy\nmetadata:\n  name: interactive-policy1\n  namespace: runai-team-a # (1)\nspec:\n  gpu:\n    rules:\n      required: true\n      min: \"1\"  # (2)\n      max: \"4\"  \n    value: \"1\"\n</code></pre> <ol> <li>Set the Project namespace here.</li> <li>GPU values are quoted as they can contain non-integer values.</li> </ol> <p>The policy places a default and limit on the available values for GPU allocation. To apply this policy, run:</p> <pre><code>kubectl apply -f gpupolicy.yaml \n</code></pre> <p>Now, try the following command:</p> <pre><code>runai submit --gpu 5 --interactive -p team-a\n</code></pre> <p>The following message will appear:</p> <pre><code>gpu: must be no greater than 4\n</code></pre> <p>A similar message will appear in the New Job form of the Run:ai user interface, when attempting to enter the number of GPUs, which is out of range for a training job.</p>"},{"location":"platform-admin/workloads/policies/old-policies/#gpu-and-cpu-memory-limits","title":"GPU and CPU memory limits","text":"<p>The following policy places a default and limit on the available values for CPU and GPU memory allocation.</p> gpumemorypolicy.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: training-policy\n  namespace: runai\nspec:\n  gpuMemory:\n    rules:\n      min: 100M\n      max: 2G\nmemory:\n    rules:\n      min: 100M\n      max: 2G\n</code></pre>"},{"location":"platform-admin/workloads/policies/old-policies/#read-only-values","title":"Read-only values","text":"<p>When you do not want the user to be able to change a value, you can force the corresponding user interface control to become read-only by using the <code>canEdit</code> key. For example,</p> runasuserpolicy.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: train-policy1\n  namespace: runai-team-a # (1) \n\nspec:\n  runAsUser:\n    rules:\n      required: true  # (2)\n      canEdit: false  # (3)\n    value: true # (4)\n</code></pre> <ol> <li>Set the Project namespace here.</li> <li>The field is required.</li> <li>The field will be shown as read-only in the user interface.</li> <li>The field value is true.  </li> </ol>"},{"location":"platform-admin/workloads/policies/old-policies/#complex-values","title":"Complex Values","text":"<p>The example above illustrated rules for parameters of \"primitive\" types, such as GPU allocation, CPU memory, working directory, etc. These parameters contain a single value.</p> <p>Other workload parameters, such as ports or volumes, are \"complex\", in the sense that they may contain multiple values: a workload may contain multiple ports and multiple volumes.</p> <p>The following is an example of a policy containing the value <code>ports</code>, which is complex: The <code>ports</code> flag typically contains two values: The <code>external</code> port that is mapped to an internal <code>container</code> port. One can have multiple port tuples defined for a single Workload:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: InteractivePolicy\nmetadata:\n  name: interactive-policy\n  namespace: runai\nspec:\n  ports:\n    rules:\n      canAdd: true\n    itemRules:\n      container:\n        min: 30000\n        max: 32767\n      external:\n        max: 32767\n    items:\n      admin-port-a:\n        rules:\n          canRemove: false\n          canEdit: false\n        value:\n          container: 30100\n          external: 8080\n      admin-port-b:\n        value:\n          container: 30101\n          external: 8081\n</code></pre> <p>A policy for a complex field is composed of three parts:</p> <ul> <li>Rules: Rules apply to the <code>ports</code> parameter as a whole. In this example, the administrator specifies <code>canAdd</code> rule with <code>true</code> value, indicating that a researcher submitting an interactive job can add additional ports to the ports listed by the policy (true is the default for <code>canAdd</code>, so it actually could have been omitted from the policy above). When <code>canAdd</code> is set to <code>false</code>, the researcher will not be able to add any additional port except those already specified by the policy.</li> <li>itemRules: itemRules impose restrictions on the data members of each item, in this case - <code>container</code> and <code>external</code>. In the above example, the administrator has limited the value of <code>container</code> to 30000-32767, and the value of <code>external</code> to a maximum of 32767.</li> <li>Items: Specifies a list of default ports. Each port is an item in the ports list and given a label (e.g. <code>admin-port-b</code>). The administrator can also specify whether a researcher can change/delete ports from the submitted workload. In the above example, <code>admin-port-a</code> is hardwired and cannot be changed or deleted, while <code>admin-port-b</code> can be changed or deleted by the researcher when submitting the Workload. It is possible to specify a label using the reserved name of <code>DEFAULTS</code>. This item provides the defaults for all other items.</li> </ul> <p>The following is an example of a complex policy for PVCs which contains <code>DEFAULTS</code>.</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: tp # use your name.\n  namespace: runai-team-a # use your namespace\nspec:\n  pvcs:\n    itemRules:\n      existingPvc:\n        canEdit: false\n      claimName:\n        required: true\n    items:\n      DEFAULTS:\n        value:\n          existingPvc: true\n          path: nil\n</code></pre>"},{"location":"platform-admin/workloads/policies/old-policies/#syntax","title":"Syntax","text":"<p>The complete syntax of the policy YAML can be obtained using the <code>explain</code> command of kubectl. For example:</p> <p><pre><code>kubectl explain trainingpolicy.spec\n</code></pre> Should provide the list of all possible fields in the spec of training policies:</p> <pre><code>KIND:     TrainingPolicy\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: spec &lt;Object&gt;\n\nDESCRIPTION:\nThe specifications of this TrainingPolicy\n\nFIELDS:\nannotations &lt;Object&gt;\nSpecifies annotations to be set in the container running the created\nworkload.\n\narguments   &lt;Object&gt;\nIf set, the arguments are sent along with the command which overrides the\nimage's entry point of the created workload.\n\ncommand &lt;Object&gt;\nIf set, overrides the image's entry point with the supplied command.\n...\n</code></pre> <p>You can further drill down to get the syntax for <code>ports</code> by running:</p> <pre><code>kubectl explain trainingpolicy.spec.ports\n</code></pre> <pre><code>KIND:     TrainingPolicy\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: ports &lt;Object&gt;\n\nDESCRIPTION:\n     Specify the set of ports exposed from the container running the created\n     workload. Used together with --service-type.\n\nFIELDS:\n   itemRules    &lt;Object&gt;\n\n   items    &lt;map[string]Object&gt;\n\n   rules    &lt;Object&gt;\n     these rules apply to a value of type map (=non primitive) as a whole\n     additionally there are rules which apply for specific items of the map\n</code></pre> <p>Drill down into the <code>ports.rules</code> object by running:</p> <pre><code>kubectl explain trainingpolicy.spec.ports.rules\n</code></pre> <pre><code>KIND:     TrainingPolicy\nVERSION:  run.ai/\n\nRESOURCE: rules &lt;Object&gt;\n\nDESCRIPTION:\n     these rules apply to a value of type map (=non primitive) as a whole\n     additionally there are rules which apply for specific items of the map\n\nFIELDS:\n   canAdd   &lt;boolean&gt;\n     is it allowed for a workload to add items to this map\n\n   required &lt;boolean&gt;\n     if the map as a whole is required\n</code></pre> <p>Note that each kind of policy has a slightly different set of parameters. For example, an <code>InteractivePolicy</code> has a <code>jupyter</code> parameter that is not available under <code>TrainingPolicy</code>.</p>"},{"location":"platform-admin/workloads/policies/old-policies/#using-secrets-for-environment-variables","title":"Using Secrets for Environment Variables","text":"<p>It is possible to add values from Kubernetes secrets as the value of environment variables included in the policy. The secret will be extracted from the secret object when the Job is created. For example:</p> <pre><code>  environment:\n    items:\n      MYPASSWORD:\n        value: \"SECRET:my-secret,password\"\n</code></pre> <p>When submitting a workload that is affected by this policy, the created container will have an environment variable called <code>MYPASSWORD</code> whose value is the key <code>password</code> residing in Kubernetes secret <code>my-secret</code> which has been pre-created in the namespace where the workload runs.</p> <p>Note</p> <p>Run:ai provides a secret propagation mechanism from the <code>runai</code> namespace to all project namespaces. For further information see secret propagation.</p>"},{"location":"platform-admin/workloads/policies/old-policies/#prevent-data-storage-on-the-node","title":"Prevent Data Storage on the Node","text":"<p>You can configure policies to prevent the submission of workloads that use data sources that consist of a host path. This setting prevents data from being stored on the node so that in the event when a node is deleted, all data stored on that node is lost.</p> <p>Example for rejecting workloads requesting host path:</p> <pre><code>spec:\n  volumes:\n    itemRules:\n      nfsServer:\n        required: true\n</code></pre>"},{"location":"platform-admin/workloads/policies/old-policies/#terminate-runai-training-jobs-after-preemption-policy","title":"Terminate Run:ai training Jobs after preemption policy","text":"<p>Administrators can set a \u2018termination after preemption\u2019 policy to Run:ai training jobs. After applying this policy, a training job will be terminated once it has been preempted from any reason. For example, a training job that is using over-quota resources (e.g. GPUs) and the owner of those GPUs wants to reclaim them back, the Training job is preempted and typically goes back to the pending queue. However, if the termination policy is applied, the job is terminated instead of reinstated as pending. The Termination after Preemption Policy can be set as a cluster-wide policy (applicable to all namespaces/projects) or per project/namespace.</p> <p>To use this feature the administrator should configure either a cluster wide or namespace policy.</p> <p>For cluster wide (all namespaces/projects) use this YAML based policy:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: training-policy\n  namespace: runai\nspec:\n  terminateAfterPreemption:\n    value: true\n</code></pre> <p>For per namespace (project) use this YAML based policy:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: training-policy\n  namespace: runai-&lt;PROJECT_NAME&gt;\nspec:\n  terminateAfterPreemption:\n    value: false\n</code></pre>"},{"location":"platform-admin/workloads/policies/old-policies/#modifyingdeleting-policies","title":"Modifying/Deleting Policies","text":"<p>Use the standard kubectl get/apply/delete commands to modify and delete policies.</p> <p>For example, to view the global interactive policy:</p> <pre><code>kubectl get interactivepolicies -n runai\n</code></pre> <p>Should return the following:</p> <pre><code>NAME                 AGE\ninteractive-policy   2d3h\n</code></pre> <p>To delete this policy:</p> <pre><code>kubectl delete InteractivePolicy interactive-policy -n runai\n</code></pre> <p>To access project-specific policies, replace the <code>-n runai</code> parameter with the namespace of the relevant project.</p>"},{"location":"platform-admin/workloads/policies/old-policies/#see-also","title":"See Also","text":"<ul> <li>For creating workloads based on policies, see the Run:ai submitting workloads</li> </ul>"},{"location":"platform-admin/workloads/policies/overview/","title":"Overview","text":"<p>At Run:ai, Administrator can access a suite of tools designed to facilitate efficient account management. This article focuses on two key features: workload policies and workload scheduling rules. These features empower admins to establish default values and implement restrictions allowing enhanced control, assuring compatibility with organizational policies and optimizing resources usage and utilization.</p>"},{"location":"platform-admin/workloads/policies/overview/#workload-policies","title":"Workload policies","text":"<p>A workload policy is an end-to-end solution for AI managers and administrators to control and simplify how workloads are submitted. This solution allows them to set best practices, enforce limitations, and standardize processes for the submission of workloads for AI projects within their organization. It acts as a key guideline for data scientists, researchers, ML &amp; MLOps engineers by standardizing submission practices and simplifying the workload submission process.</p>"},{"location":"platform-admin/workloads/policies/overview/#older-and-newer-policy-technologies","title":"Older and Newer Policy technologies","text":"<p>Run:ai provides two policy technologies.</p> <p>YAML-Based policies are the older policies. These policies:</p> <ul> <li>Require access to Kubernetes to view or change.</li> <li>Contact Run:ai support to convert the old policies to the new V2 policies format.</li> </ul> <p>API-based policies which are the newer policies. These are:</p> <ul> <li>Show in the Run:ai user interface.</li> <li>Can be viewed and modified via the user interface and the Control-plane API.</li> <li>Enable new rules addressing differences between project, department and cluster policies.</li> <li>Only available with Run:ai clusters of version 2.18 and up. </li> </ul>"},{"location":"platform-admin/workloads/policies/overview/#why-use-a-workload-policy","title":"Why use a workload policy?","text":"<p>Implementing workload policies is essential when managing complex AI projects within an enterprise for several reasons:</p> <ol> <li>Resource control and management     Defining or limiting the use of costly resources across the enterprise via a centralized management system to ensure efficient allocation and prevent overuse.  </li> <li>Setting best practices     Provide managers with the ability to establish guidelines and standards to follow, reducing errors amongst AI practitioners within the organization.  </li> <li>Security and compliance    Define and enforce permitted and restricted actions to uphold organizational security and meet compliance requirements.  </li> <li>Simplified setup    Conveniently allow setting defaults and streamline the workload submission process for AI practitioners.  </li> <li>Scalability and diversity  <ol> <li>Multi-purpose clusters with various workload types that may have different requirements and characteristics for resource usage.  </li> <li>The organization has multiple hierarchies, each with distinct goals, objectives and degrees of flexibility.  </li> <li>Manage multiple users and projects with distinct requirements and methods, ensuring appropriate utilization of resources.</li> </ol> </li> </ol>"},{"location":"platform-admin/workloads/policies/overview/#understanding-the-mechanism","title":"Understanding the mechanism","text":"<p>The following sections provide details of how the workload policy mechanism works.</p>"},{"location":"platform-admin/workloads/policies/overview/#cross-interface-enforcement","title":"Cross-interface enforcement","text":"<p>The policy enforces the workloads regardless of whether they were submitted via UI, CLI, Rest APIs, or Kubernetes YAMLs.</p>"},{"location":"platform-admin/workloads/policies/overview/#policy-types","title":"Policy types","text":"<p>Run:ai\u2019s policies enforce Run:ai workloads. The policy type is per Run:ai workload type. This allows administrators to set different policies for each workload type.</p> Policy type Workload type Kubernetes name Workspace Workspace Interactive workload Training Standard Training Standard Training workload Distributed* Distributed Distributed workload Inference* Inference Inference workload <p>* The submission of these policy types is supported currently via API only</p>"},{"location":"platform-admin/workloads/policies/overview/#policy-structure-rules-defaults-and-imposed-assets","title":"Policy structure - rules, defaults, and imposed assets","text":"<p>A policy consists of rules for limiting and controlling the values of fields of the workload. In addition to rules, some defaults allow the implementation of default values to different workload fields. These default values are not rules, as they simply suggest values that can be overridden during the workload submission.</p> <p>Furthermore, policies allow the enforcement of workload assets. For example, as an admin, you can impose a data source of type PVC to be used by any workload submitted.</p> <p>For more information see rules, defaults and imposed assets.</p>"},{"location":"platform-admin/workloads/policies/overview/#scope-of-effectiveness","title":"Scope of effectiveness","text":"<p>Numerous teams working on various projects require the use of different tools, requirements, and safeguards. One policy may not suit all teams and their requirements. Hence, administrators can select the scope to cover the effectiveness of the policy. When a scope is selected, all of its subordinate units are also affected. As a result, all workloads submitted within the selected scope are controlled by the policy.</p> <p>For example, if a policy is set for Department A, all workloads submitted by any of the projects within this department are controlled.</p> <p>A scope for a policy can be:  </p> <pre><code>    The entire account  \n        L Specific cluster  \n            L Specific department  \n                L Specific project\n</code></pre> <p>Note</p> <p>The policy submission to the entire account scope is supported via API only</p> <p>The different scoping of policies also allows the breakdown of the responsibility between different administrators. This allows delegation of ownership between different levels within the organization. The policies, containing rules and defaults, propagate* down the organizational tree, forming an \u201ceffective\u201d policy that enforces any workload submitted by users within the project.</p> <p></p> <ul> <li>If a rule for a specific field is already occupied by a policy in the organization, another unit within the same branch cannot submit an additional rule on the same field. As a result, administrators of higher scopes must request lower-scope administrators to free up the specific rule from their policy. However, defaults of the same field can be submitted by different organizational policies, as they are \u201csoft\u201d rules that are not critical to override, and the smallest level of the default is the one that becomes the effective default (project default \u201a\u201dwins\u201d vs department default, department default \u201cwins\u201d vs cluster default etc.).</li> </ul>"},{"location":"platform-admin/workloads/policies/overview/#runai-policies-vs-kyverno-policies","title":"Run:ai Policies vs. Kyverno Policies","text":"<p>Kyverno runs as a dynamic admission controller in a Kubernetes cluster. Kyverno receives validating and mutating admission webhook HTTP callbacks from the Kubernetes API server and applies matching policies to return results that enforce admission policies or reject requests. Kyverno policies can match resources using the resource kind, name, label selectors, and much more. For more information, see How Kyverno Works.</p>"},{"location":"platform-admin/workloads/policies/policy-examples/","title":"Policies Examples","text":"<p>This article provides examples of:</p> <ol> <li>Creating a new rule within a policy </li> <li>Best practices for adding sections to a policy </li> <li>A full example of a policy.</li> </ol>"},{"location":"platform-admin/workloads/policies/policy-examples/#creating-a-new-rule-within-a-policy","title":"Creating a new rule within a policy","text":"<p>This example shows how to add a new limitation to the GPU usage for workloads of type workspace:</p> <ol> <li> <p>Check the workload API fields documentation and select the field(s) that are most relevant for GPU usage.  </p> <pre><code>{\n\"spec\": {\n    \"compute\": {\n    \"gpuDevicesRequest\": 1,\n    \"gpuRequestType\": \"portion\",\n    \"gpuPortionRequest\": 0.5,\n    \"gpuPortionLimit\": 0.5,\n    \"gpuMemoryRequest\": \"10M\",\n    \"gpuMemoryLimit\": \"10M\",\n    \"migProfile\": \"1g.5gb\",\n    \"cpuCoreRequest\": 0.5,\n    \"cpuCoreLimit\": 2,\n    \"cpuMemoryRequest\": \"20M\",\n    \"cpuMemoryLimit\": \"30M\",\n    \"largeShmRequest\": false,\n    \"extendedResources\": [\n        {\n        \"resource\": \"hardware-vendor.example/foo\",\n        \"quantity\": 2,\n        \"exclude\": false\n        }\n    ]\n    },\n}\n}\n</code></pre> </li> <li> <p>Search the field in the Policy YAML fields - reference table. For example, gpuDevicesRequest appears under the Compute fields sub-table and appears as follow:</p> </li> </ol> Fields Description Value type Supported Run:ai workload type gpuDeviceRequest Specifies the number of GPUs to allocate for the created workload. Only if <code>gpuDeviceRequest = 1</code>, the gpuRequestType can be defined. integer Workspace &amp; Training <ol> <li> <p>Use the value type of the gpuDevicesRequest field indicated in the table - \u201cinteger\u201d and navigate to the Value types table to view the possible rules that can be applied to this value type - </p> <p>for integer, the options are: </p> <ul> <li>canEdit  </li> <li>required  </li> <li>min  </li> <li>max  </li> <li>step  </li> </ul> </li> <li> <p>Proceed to the Rule Type table, select the required rule for the limitation of the field - for example \u201cmax\u201d and use the examples syntax to indicate the maximum GPU device requested.</p> </li> </ol> <pre><code>compute:\n    gpuDevicesRequest:\n        max: 2\n</code></pre>"},{"location":"platform-admin/workloads/policies/policy-examples/#policy-yaml-best-practices","title":"Policy YAML best practices","text":"Create a policy that has multiple defaults and rules <p>Best practice description: Presentation of the syntax while adding a set of defaults and rules</p> <pre><code>defaults:\n  createHomeDir: true\n  environmentVariables:\n    instances:\n    - name: MY_ENV\n      value: my_value\n  security:\n    allowPrivilegeEscalation: false\n\nrules:\n  storage:\n    s3:\n      attributes:\n        url:\n          options:\n            - value: https://www.google.com\n            displayed: https://www.google.com\n            - value: https://www.yahoo.com\n            displayed: https://www.yahoo.com\n</code></pre> Allow only single selection out of many <p>Best practice description:  Blocking the option to create all types of data sources except the one that is allowed is the solution.</p> <pre><code>rules:\n  storage:\n    dataVolume:\n      instances:\n        canAdd: false\n    hostPath:\n      instances:\n        canAdd: false\n    pvc:\n      instances:\n        canAdd: false\n    git:\n      attributes:\n        repository:\n          required: true\n        branch:\n          required: true\n        path:\n          required: true\n    nfs:\n      instances:\n        canAdd: false\n    s3:\n      instances:\n        canAdd: false\n</code></pre> Create a robust set of guidelines <p>Best practice description: Set rules for specific compute resource usage, addressing most relevant spec fields </p> <pre><code>rules:\n  compute:\n    cpuCoreRequest:\n      required: true\n      min: 0\n      max: 8\n    cpuCoreLimit:\n      min: 0\n      max: 8\n    cpuMemoryRequest:\n      required: true\n      min: '0'\n      max: 16G\n    cpuMemoryLimit:\n      min: '0'\n      max: 8G\n    migProfile:\n      canEdit: false\n    gpuPortionRequest:\n      min: 0\n      max: 1\n    gpuMemoryRequest:\n      canEdit: false\n    extendedResources:\n      instances:\n        canAdd: false\n</code></pre> Environment creation (specific section) <pre><code>rules:\n  imagePullPolicy:\n    required: true\n    options:\n    - value: Always\n      displayed: Always\n    - value: Never\n      displayed: Never\n  createHomeDir:\n    canEdit: false\n</code></pre> Setting security measures (specific section) <pre><code>rules:\n  security:\n    runAsUid:\n      min: 1\n      max: 32700\n    allowPrivilegeEscalation:\n      canEdit: false\n</code></pre> Policy for distributed training workloads (specific section) <p>Best practice description: Set rules and defaults for a distributed training workload with different settings for master and worker </p> <pre><code>defaults:\n  worker:\n    command: my-command-worker-1\n    environmentVariables:\n      instances:\n        - name: LOG_DIR\n          value: policy-worker-to-be-ignored\n        - name: ADDED_VAR\n          value: policy-worker-added\n   security:\n    runAsUid: 500\n  storage:\n     s3:\n     attributes:\n       bucket: bucket1-worker\n master:\n   command: my-command-master-2\n   environmentVariables:\n     instances:\n       - name: LOG_DIR\n         value: policy-master-to-be-ignored\n       - name: ADDED_VAR\n         value: policy-master-added\n    security:\n      runAsUid: 800\n    storage:\n     s3:\n       attributes:\n         bucket: bucket1-master\n rules:\n   worker:\n     command:\n       options:\n         - value: my-command-worker-1\n           displayed: command1\n         - value: my-command-worker-2\n           displayed: command2\n     storage:\n       nfs:\n         instances:\n           canAdd: false\n       s3:\n         attributes:\n           bucket:\n             options:\n               - value: bucket1-worker\n               - value: bucket2-worker\n   master:\n     command:\n       options:\n         - value: my-command-master-1\n           displayed: command1\n         - value: my-command-master-2\n           displayed: command2\n     storage:\n       nfs:\n         instances:\n           canAdd: false\n       s3:\n         attributes:\n           bucket:\n             options:\n               - value: bucket1-master\n               - value: bucket2-master\n</code></pre> Impose an asset (specific section) <pre><code> defaults: null\n rules: null\n imposedAssets:\n   - f12c965b-44e9-4ff6-8b43-01d8f9e630cc\n</code></pre>"},{"location":"platform-admin/workloads/policies/policy-examples/#example-of-a-full-policy","title":"Example of a full policy","text":"<pre><code>defaults:\n  createHomeDir: true\n  imagePullPolicy: IfNotPresent\n  nodePools:\n    - node-pool-a\n    - node-pool-b\n  environmentVariables:\n    instances:\n      - name: WANDB_API_KEY\n        value: REPLACE_ME!\n      - name: WANDB_BASE_URL\n        value: https://wandb.mydomain.com\n  compute:\n    cpuCoreRequest: 0.1\n    cpuCoreLimit: 20\n    cpuMemoryRequest: 10G\n    cpuMemoryLimit: 40G\n    largeShmRequest: true\n  security:\n    allowPrivilegeEscalation: false\n  storage:\n    git:\n      attributes:\n        repository: https://git-repo.my-domain.com\n        branch: master\n    hostPath:\n      instances:\n        - name: vol-data-1\n          path: /data-1\n          mountPath: /mount/data-1\n        - name: vol-data-2\n          path: /data-2\n          mountPath: /mount/data-2\nrules:\n  createHomeDir:\n    canEdit: false\n  imagePullPolicy:\n    canEdit: false\n  environmentVariables:\n    instances:\n      locked:\n        - WANDB_BASE_URL\n  compute:\n    cpuCoreRequest:\n      max: 32\n    cpuCoreLimit:\n      max: 32\n    cpuMemoryRequest:\n      min: 1G\n      max: 20G\n    cpuMemoryLimit:\n      min: 1G\n      max: 40G\n    largeShmRequest:\n      canEdit: false\n    extendedResources:\n      instances:\n        canAdd: false\n  security:\n    allowPrivilegeEscalation:\n      canEdit: false\n    runAsUid:\n      min: 1\n  storage:\n    hostPath:\n      instances:\n        locked:\n          - vol-data-1\n          - vol-data-2\nimposedAssets:\n  - 4ba37689-f528-4eb6-9377-5e322780cc27\n</code></pre>"},{"location":"platform-admin/workloads/policies/policy-reference/","title":"Policies Reference","text":"<p>A workload policy is an end-to-end solution for AI managers and administrators to control and simplify how workloads are submitted, setting best practices, enforcing limitations, and standardizing processes for AI projects within their organization.</p> <p>This article explains the policy YAML fields and the possible rules and defaults that can be set for each field.</p>"},{"location":"platform-admin/workloads/policies/policy-reference/#policy-yaml-fields-reference-table","title":"Policy YAML fields - reference table","text":"<p>The policy fields are structured in a similar format to the workload API fields. The following tables represent a structured guide designed to help you understand and configure policies in a YAML format. It provides the fields, descriptions, defaults and rules for each workload type.</p> <p>Click the link to view the value type of each field.</p> Fields Description Value type Supported Run:ai workload type args When set, contains the arguments sent along with the command. These override the entry point of the image in the created workload string Workspace Training command A command to serve as the entry point of the container running the workspace string Workspace Training createHomeDir Instructs the system to create a temporary home directory for the user within the container. Data stored in this directory is not saved when the container exists. When the runAsUser flag is set to true, this flag defaults to true as well boolean Workspace Training environmentVariables Set of environmentVariables to populate the container running the workspace string Workspace Training image Specifies the image to use when creating the container running the workload string Workspace Training imagePullPolicy Specifies the pull policy of the image when starting t a container running the created workload. Options are: always, ifNotPresent, or never string Workspace Training workingDir Container\u2019s working directory. If not specified, the container runtime default is used, which might be configured in the container image string Workspace Training nodeType Nodes (machines) or a group of nodes on which the workload runs string Workspace Training nodePools A prioritized list of node pools for the scheduler to run the workspace on. The scheduler always tries to use the first node pool before moving to the next one when the first is not available. array Workspace Training annotations Set of annotations to populate into the container running the workspace itemized Workspace Training labels Set of labels to populate into the container running the workspace itemized Workspace Training terminateAfterPreemtpion Indicates whether the job should be terminated, by the system, after it has been preempted boolean Workspace Training autoDeletionTimeAfterCompletionSeconds Specifies the duration after which a finished workload (Completed or Failed) is automatically deleted. If this field is set to zero, the workload becomes eligible to be deleted immediately after it finishes. integer Workspace Training backoffLimit Specifies the number of retries before marking a workload as failed integer Workspace Training completions Used with Hyperparameter Optimization. Specifies the number of successful pods the job should reach to be completed. The Job is marked as successful once the specified amount of pods has succeeded. integer Workspace Training parallelism Used with Hyperparameters Optimization. Specifies the maximum desired number of pods the workload should run at any given time. itemized Workspace Training exposeUrls Specifies a set of exported URL (e.g. ingress) from the container running the created workload. itemized Workspace Training largeShmRequest Specifies a large /dev/shm device to mount into a container running the created workload. SHM is a shared file system mounted on RAM. boolean Workspace Training PodAffinitySchedulingRule Indicates if we want to use the Pod affinity rule as: the \u201chard\u201d (required) or the \u201csoft\u201d (preferred) option. This field can be specified only if PodAffinity is set to true. string Workspace Training podAffinityTopology Specifies the Pod Affinity Topology to be used for scheduling the job. This field can be specified only if PodAffinity is set to true. string Workspace Training ports Specifies a set of ports exposed from the container running the created workload. More information in Ports fields below. itemized Workspace Training probes Specifies the ReadinessProbe to use to determine if the container is ready to accept traffic. More information in Probes fields below - Workspace Training tolerations Toleration rules which apply to the pods running the workload. Toleration rules guide (but do not require) the system to which node each pod can be scheduled to or evicted from, based on matching between those rules and the set of taints defined for each Kubernetes node. itemized Workspace Training priorityClass Priority class of the workload. The values for workspace are build (default) or interactive-preemptible. For training only, use train. Enum: \"build\", \"train\", \"interactive-preemptible\". string Workspace storage Contains all the fields related to storage configurations. More information in Storage fields below. - Workspace Training security Contains all the fields related to security configurations. More information in Security fields below. - Workspace Training compute Contains all the fields related to compute configurations. More information in Compute fields below. - Workspace Training"},{"location":"platform-admin/workloads/policies/policy-reference/#ports-fields","title":"Ports fields","text":"Fields Description Value type Supported Run:ai workload type container The port that the container running the workload exposes. string Workspace Training serviceType Specifies the default service exposure method for ports. the default shall be sued for ports which do not specify service type. Options are: LoadBalancer, NodePort or ClusterIP. For more information see the External Access to Containers guide. string Workspace Training external The external port which allows a connection to the container port. If not specified, the port is auto-generated by the system. integer Workspace Training toolType The tool type that runs on this port. string Workspace Training toolName A name describing the tool that runs on this port. string Workspace Training"},{"location":"platform-admin/workloads/policies/policy-reference/#probes-fields","title":"Probes fields","text":"Fields Description Value type Supported Run:ai workload type readiness Specifies the Readiness Probe to use to determine if the container is ready to accept traffic. - Workspace Training Readiness field details Spec fields readiness Description Specifies the Readiness Probe to use to determine if the container is ready to accept traffic Supported Run:ai workload types Workspace Training Value type itemized Spec Readiness fields Description Value type initialDelaySeconds Number of seconds after the container has started before liveness or readiness probes are initiated. integer periodSeconds How often (in seconds) to perform the probe. integer timeoutSeconds Number of seconds after which the probe times out integer successThreshold Minimum consecutive successes for the probe to be considered successful after having failed. integer failureThreshod When a probe fails, the number of times to try before giving up. integer <p>Example workload snippet:</p> <pre><code>defaults:\n  probes:\n    readiness:\n        initialDelaySeconds: 2\n</code></pre>"},{"location":"platform-admin/workloads/policies/policy-reference/#security-fields","title":"Security fields","text":"Fields Description Value type Supported Run:ai workload type uidGidSource Indicates the way to determine the user and group ids of the container. The options are: <code>fromTheImage</code> - user and group IDs are determined by the docker image that the container runs. This is the default option. <code>custom</code> - user and group IDs can be specified in the environment asset and/or the workspace creation request. <code>fromIdpToken</code> - user and group IDs are determined according to the identity provider (idp) access token. This option is intended for internal use of the environment UI form. For more information, see Non-root containers string Workspace Training capabilities The capabilities field allows adding a set of unix capabilities to the container running the workload. Capabilities are Linux distinct privileges traditionally associated with superuser which can be independently enabled and disabled Array Workspace Training seccompProfileType Indicates which kind of seccomp profile is applied to the container. The options are: RuntimeDefault - the container runtime default profile should be used Unconfined - no profile should be applied string Workspace Training runAsNonRoot Indicates that the container must run as a non-root user. boolean Workspace Training readOnlyRootFilesystem If true, mounts the container's root filesystem as read-only. boolean Workspace Training runAsUid Specifies the Unix user id with which the container running the created workload should run. integer Workspace Training runasGid Specifies the Unix Group ID with which the container should run. integer Workspace Training supplementalGroups Comma separated list of groups that the user running the container belongs to, in addition to the group indicated by runAsGid. string Workspace Training allowPrivilegeEscalation Allows the container running the workload and all launched processes to gain additional privileges after the workload starts boolean Workspace Training hostIpc Whether to enable hostIpc. Defaults to false. boolean Workspace Training hostNetwork Whether to enable host network. boolean Workspace Training"},{"location":"platform-admin/workloads/policies/policy-reference/#compute-fields","title":"Compute fields","text":"Fields Description Value type Supported Run:ai workload type cpuCoreRequest CPU units to allocate for the created workload (0.5, 1, .etc). The workload receives at least this amount of CPU. Note that the workload is not scheduled unless the system can guarantee this amount of CPUs to the workload. number Workspace Training cpuCoreLimit Limitations on the number of CPUs consumed by the workload (0.5, 1, .etc). The system guarantees that this workload is not able to consume more than this amount of CPUs. number Workspace Training cpuMemoryRequest The amount of CPU memory to allocate for this workload (1G, 20M, .etc). The workload receives at least this amount of memory. Note that the workload is not scheduled unless the system can guarantee this amount of memory to the workload quantity Workspace Training cpuMemoryLimit Limitations on the CPU memory to allocate for this workload (1G, 20M, .etc). The system guarantees that this workload is not be able to consume more than this amount of memory. The workload receives an error when trying to allocate more memory than this limit. quantity Workspace Training largeShmRequest A large /dev/shm device to mount into a container running the created workload (shm is a shared file system mounted on RAM). boolean Workspace Training gpuRequestType Sets the unit type for GPU resources requests to either portion, memory or mig profile. Only if <code>gpuDeviceRequest = 1</code>, the request type can be stated as <code>portion</code>, <code>memory</code> or <code>migProfile</code>. string Workspace Training migProfile Specifies the memory profile to be used for workload running on NVIDIA Multi-Instance GPU (MIG) technology. string Workspace Training gpuPortionRequest Specifies the fraction of GPU to be allocated to the workload, between 0 and 1. For backward compatibility, it also supports the number of gpuDevices larger than 1, currently provided using the gpuDevices field. number Workspace Training gpuDeviceRequest Specifies the number of GPUs to allocate for the created workload. Only if <code>gpuDeviceRequest = 1</code>, the gpuRequestType can be defined. integer Workspace Training gpuPortionLimit When a fraction of a GPU is requested, the GPU limit specifies the portion limit to allocate to the workload. The range of the value is from 0 to 1. number Workspace Training gpuMemoryRequest Specifies GPU memory to allocate for the created workload. The workload receives this amount of memory. Note that the workload is not scheduled unless the system can guarantee this amount of GPU memory to the workload. quantity Workspace Training gpuMemoryLimit Specifies a limit on the GPU memory to allocate for this workload. Should be no less than the gpuMemory. quantity Workspace Training extendedResources Specifies values for extended resources. Extended resources are third-party devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that you want to allocate to your Job. itemized Workspace Training"},{"location":"platform-admin/workloads/policies/policy-reference/#storage-fields","title":"Storage fields","text":"Fields Description Value type Supported Run:ai workload type dataVolume Set of data volumes to use in the workload. Each data volume is mapped to a file-system mount point within the container running the workload. itemized Workspace Training hostPath Maps a folder to a file-system mount point within the container running the workload. itemized Workspace Training git Details of the git repository and items mapped to it. itemized Workspace Training pvc Specifies persistent volume claims to mount into a container running the created workload. itemized Workspace Training nfs Specifies NFS volume to mount into the container running the workload. itemized Workspace Training s3 Specifies S3 buckets to mount into the container running the workload. itemized Workspace Training configMapVolumes Specifies ConfigMaps to mount as volumes into a container running the created workload. itemized Workspace Training secretVolume Set of secret volumes to use in the workload. A secret volume maps a secret resource in the cluster to a file-system mount point within the container running the workload. itemized Workspace Training Storage field details Spec fields hostPath Description Maps a folder to a file system mount oint within the container running the workload Supported Run:ai workload types Workspace Training Value type itemized hostPath fields Description Value type name Unique name to identify the instance. primarily used for policy locked rules. string path Local path within the controller to which the host volume is mapped. string readOnly Force the volume to be mounted with read-only permissions. Defaults to false. boolean mountPath The path that the host volume is mounted to when in use. string mountPropagation Enum: \"None\" \"HostToContainer\" Share this volume mount with other containers. If set to HostToContainer, this volume mount receives all subsequent mounts that are mounted to this volume or any of its subdirectories. In case of multiple hostPath entries, this field should have the same value for all of them string <p>Example workload snippet:</p> <pre><code>defaults:\n  storage:\n    hostPath:\n      instances:\n        - path: h3-path-1\n          mountPath: h3-mount-1\n        - path: h3-path-2\n          mountPath: h3-mount-2\n      attributes:\n        - readOnly: true\n</code></pre> Spec fields git Description Details of the git repository and items mapped to it. Supported Run:ai workload types Workspace Training Value type itemized Git fields Description Value type repository URL to a remote git repository. The content of this repository is mapped to the container running the workload string revision Specific revision to synchronize the repository from string path Local path within the workspace to which the S3 bucket is mapped. string secretName Optional name of Kubernetes secret that holds your git username and password. string username If secretName is provided, this field should contain the key, within the provided Kubernetes secret, which holds the value of your git username. Otherwise, this field should specify your git username in plain text (example: myuser). string <p>Example workload snippet:</p> <pre><code>defaults:\n  storage:\n    git:\n      attributes:\n        Repository: https://runai.public.github.com\n      instances\n        - branch: \"master\"\n          path: /container/my-repository\n          passwordSecret: my-password-secret\n</code></pre> Spec fields pvc Description Specifies persistent volume claims to mount into a container running the created workload Supported Run:ai workload types Workspace Training Value type itemized Spec PVC fields Description Value type claimName (manadatory) A given name for the PVC. Allowed referencing it across workspaces. string ephemeral Use true to set PVC to ephemeral. If set to true, the PVC is deleted when the workspace is stopped. boolean path Local path within the workspace to which the PVC bucket is mapped. string readonly Permits read only from the PVC, prevents additions or modifications to its content. boolean ReadwriteOnce Requesting claim that can be mounted in read/write mode to exactly 1 host. If none of the modes are specified, the default is readWriteOnce. boolean size Requested size for the PVC. Mandatory when existing PVC is false. string storageClass Storage class name to associate with the PVC. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Further details at Kubernetes storage classes. string readOnlyMany Requesting claim that can be mounted in read-only mode to many hosts. boolean readWriteMany Requesting claim that can be mounted in read/write mode to many hosts. boolean <p>Example workload snippet:</p> <pre><code>defaults:\n  storage:\n    pvc:\n      instances:\n        - claimName: pvc-staging-researcher1-home\n          existingPvc: true\n          path: /myhome\n          readOnly: false\n          claimInfo:\n            accessModes:\n              readWriteMany: true\n</code></pre> Spec fields nfs Description Specifies NFS volume to mount into the container running the workload Supported Run:ai workload types Workspace Training Value type itemized nfs fields Description Value type mountpath The path that the NFS volume is mounted to when in use. string path Path that is exported by the NFS server. string readOnly Whether to force the NFS export to be mounted with read-only permissions. boolean nfsServer The hostname or IP address of the NFS server. string <p>Example workload snippet:</p> <pre><code>defaults:\nstorage:\n  nfs:\n    instances:\n      - path: nfs-path\n        readOnly: true\n        server: nfs-server\n        mountPath: nfs-mount\nrules:\n  storage:\n    nfs:\n      instances:\n        canAdd: false\n</code></pre> Spec fields s3 Description Specifies S3 buckets to mount into the container running the workload Supported Run:ai workload types Workspace Training Value type itemized s3 fields Description Value type Bucket The name of the bucket string path Local path within the workspace to which the S3 bucket is mapped string url The URL of the S3 service provider. The default is the URL of the Amazon AWS Se service string <p>Example workload snippet:</p> <pre><code>defaults:\n  storage:\n    s3:\n      instances:\n        - bucket: bucket-opt-1\n          path: /s3/path\n          accessKeySecret: s3-access-key\n          secretKeyOfAccessKeyId: s3-secret-id\n          secretKeyOfSecretKey: s3-secret-key\n      attributes:\n        url: https://amazonaws.s3.com\n</code></pre>"},{"location":"platform-admin/workloads/policies/policy-reference/#value-types","title":"Value types","text":"<p>Each field has a specific value type. The following value types are supported.</p> Value type Description Supported rule type Defaults Boolean A binary value that can be either True or False canEdit required true/false String A sequence of characters used to represent text. It can include letters, numbers, symbols, and spaces canEdit required options abc Itemized An ordered collection of items (objects), which can be of different types (all items in the list are of the same type). For further information see the chapter below the table. canAdd locked See below Integer An Integer is a whole number without a fractional component. canEdit required min max step 100 Number Capable of having non-integer values canEdit required min 10.3 Quantity Holds a string composed of a number and a unit representing a quantity canEdit required min max 5M Array Set of values that are treated as one, as opposed to Itemized in which each item can be referenced separately. canEdit required node-a node-b node-c"},{"location":"platform-admin/workloads/policies/policy-reference/#itemized","title":"Itemized","text":"<p>Workload fields of type itemized have multiple instances, however in comparison to objects, each can be referenced by a key field. The key field is defined for each field.</p> <p>Consider the following workload spec:</p> <pre><code>spec:\n  image: ubuntu\n  compute:\n    extendedResources:\n      - resource: added/cpu\n        quantity: 10\n      - resource: added/memory\n        quantity: 20M\n</code></pre> <p>In this example, extendedResources have two instances, each has two attributes: resource (the key attribute) and quantity.</p> <p>In policy, the defaults and rules for itemized fields have two sub sections:</p> <ul> <li>Instances: default items to be added to the policy or rules which apply to an instance as a whole.  </li> <li>Attributes: defaults for attributes within an item or rules which apply to attributes within each item.</li> </ul> <p>Consider the following example:</p> <pre><code>defaults:\n  compute:\n    extendedResources:\n      instances: \n        - resource: default/cpu\n          quantity: 5\n        - resource: default/memory\n          quantity: 4M\n      attributes:\n        quantity: 3\nrules:\n  compute:\n    extendedResources:\n      instances:\n        locked: \n          - default/cpu\n      attributes:\n        quantity: \n          required: true\n</code></pre> <p>Assume the following workload submission is requested:</p> <pre><code>spec:\n  image: ubuntu\n  compute:\n    extendedResources:\n      - resource: default/memory\n        exclude: true\n      - resource: added/cpu\n      - resource: added/memory\n        quantity: 5M\n</code></pre> <p>The effective policy for the above mentioned workload has the following extendedResources instances:</p> Resource Source of the instance Quantity Source of the attribute quantity default/cpu Policy defaults 5 The default of this instance in the policy defaults section added/cpu Submission request 3 The default of the quantity attribute from the attributes section added/memory Submission request 5M Submission request <p>Note</p> <p>The default/memory is not populated to the workload, this is because it has been excluded from the workload using \u201cexclude: true\u201d.</p> <p>A workload submission request cannot exclude the default/cpu resource, as this key is included in the locked rules under the instances section. {#a-workload-submission-request-cannot-exclude-the-default/cpu-resource,-as-this-key-is-included-in-the-locked-rules-under-the-instances-section.}</p>"},{"location":"platform-admin/workloads/policies/policy-reference/#rule-types","title":"Rule types","text":"Rule types Description Supported value types Rule type example canAdd Whether the submission request can add items to an itemized field other than those listed in the policy defaults for this field. itemized <code>storage:   hostPath:      instances:        canAdd: false</code> locked Set of items that the workload is unable to modify or exclude. In this example, a workload policy default is given to HOME and USER, that the submission request cannot modify or exclude from the workload. itemized <code>storage:   hostPath:     Instances:       locked:         - HOME         - USER</code> canEdit Whether the submission request can modify the policy default for this field. In this example, it is assumed that the policy has default for imagePullPolicy. As canEdit is set to false, submission requests are not able to alter this default. string boolean integer number quantity array <code>imagePullPolicy:     canEdit: false</code> required When set to true, the workload must have a value for this field. The value can be obtained from policy defaults. If no value specified in the policy defaults, a value must be specified for this field in the submission request. string boolean integer number quantity array <code>image:     required: true</code> min The minimal value for the field. integer number quantity <code>compute:   gpuDevicesRequest:     min: 3</code> max The maximal value for the field. integer number quantity <code>compute:   gpuMemoryRequest:      max: 2G</code> step The allowed gap between values for this field. In this example the allowed values are: 1, 3, 5, 7 integer number <code>compute:   cpuCoreRequest:     min: 1     max: 7     Step: 2</code> options Set of allowed values for this field. string <code>image:   options:     - value: image-1     - value: image-2</code>"},{"location":"platform-admin/workloads/policies/policy-reference/#policy-spec-sections","title":"Policy Spec Sections","text":"<p>For each field of a specific policy, you can specify both rules and defaults. A policy spec consists of the following sections:</p> <ul> <li>Rules  </li> <li>Defaults  </li> <li>Imposed Assets</li> </ul>"},{"location":"platform-admin/workloads/policies/policy-reference/#rules","title":"Rules","text":"<p>Rules set up constraints on workload policy fields. For example, consider the following policy:</p> <pre><code>rules:\n  compute:\n    gpuDevicesRequest: \n      max: 8\n  security:\n    runAsUid: \n      min: 500\n</code></pre> <p>Such a policy restricts the maximum value for gpuDeviceRequests to 8, and the minimal value for runAsUid, provided in the security section to 500.</p>"},{"location":"platform-admin/workloads/policies/policy-reference/#defaults","title":"Defaults","text":"<p>The defaults section is used for providing defaults for various workload fields. For example, consider the following policy:</p> <pre><code>defaults:\n  imagePullPolicy: Always\n  security:\n    runAsNonRoot: true\n    runAsUid: 500\n</code></pre> <p>Assume a submission request with the following values:</p> <ul> <li>Image: ubuntu  </li> <li>runAsUid: 501</li> </ul> <p>The effective workload that runs has the following set of values:</p> Field Value Source Image Ubuntu Submission request ImagePullPolicy Always Policy defaults security.runAsNonRoot true Policy defaults security.runAsUid 501 Submission request <p>Note</p> <p>It is possible to specify a rule for each field, which states if a submission request is allowed to change the policy default for that given field, for example:</p> <pre><code>defaults:\n  imagePullPolicy: Always\n  security:\n    runAsNonRoot: true\n    runAsUid: 500\nrules:\n  security:\n    runAsUid:\n      canEdit: false\n</code></pre> <p>If this policy is applied, the submission request above fails, as it attempts to change the value of secuirty.runAsUid from 500 (the policy default) to 501 (the value provided in the submission request), which is forbidden due to canEdit rule set to false for this field.</p>"},{"location":"platform-admin/workloads/policies/policy-reference/#imposed-assets","title":"Imposed Assets","text":"<p>Default instances of a storage field can be provided using a datasource containing the details of this storage instance. To add such instances in the policy, specify those asset IDs in the imposedAssets section of the policy.</p> <pre><code>defaults: null\nrules: null\nimposedAssets:\n  - f12c965b-44e9-4ff6-8b43-01d8f9e630cc\n</code></pre> <p>Assets with references to credentials assets (for example: private S3, containing reference to an AccessKey asset) cannot be used as imposedAssets.</p>"},{"location":"platform-admin/workloads/policies/workspaces-policy/","title":"Policies","text":"<p>This article explains the procedure to manage workload policies.</p>"},{"location":"platform-admin/workloads/policies/workspaces-policy/#workload-policies-table","title":"Workload policies table","text":"<p>The Workload policies table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>Note</p> <p>Workload policies are disabled by default. If you cannot see Workload policies in the menu, then it must be enabled by your administrator, under General Settings \u2192 Workloads \u2192 Policies</p> <p>The Workload policies table provides a list of all the policies defined in the platform, and allows you to manage them.</p> <p></p> <p>The Workload policies table consists of the following columns:</p> Column Description Policy The policy name which is a combination of the policy scope and the policy type Type The policy type is per Run:ai workload type. This allows administrators to set different policies for each workload type. Status Representation of the policy lifecycle (one of the following - \u201cCreating\u2026\u201d, \u201cUpdating\u2026\u201d, \u201cDeleting\u2026\u201d, Ready or Failed) Scope The scope the policy affects. Click the name of the scope to view the organizational tree diagram. You can only view the parts of the organizational tree for which you have permission to view. Created by The user who created the policy Creation time The timestamp for when the policy was created Last updated The last time the policy was updated"},{"location":"platform-admin/workloads/policies/workspaces-policy/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Refresh - Click REFRESH to update the table with the latest data</li> </ul>"},{"location":"platform-admin/workloads/policies/workspaces-policy/#adding-a-policy","title":"Adding a policy","text":"<p>To create a new policy:</p> <ol> <li>Click +NEW POLICY </li> <li>Select a scope </li> <li>Select the workload type </li> <li>Click +POLICY YAML </li> <li>In the YAML editor type or paste a YAML policy with defaults and rules.     You can utilize the following references and examples:  </li> <li>Policy YAML reference </li> <li>Policy YAML examples </li> <li>Click SAVE POLICY</li> </ol>"},{"location":"platform-admin/workloads/policies/workspaces-policy/#editing-a-policy","title":"Editing a policy","text":"<ol> <li>Select the policy you want to edit  </li> <li>Click EDIT </li> <li>Update the policy and click APPLY </li> <li>Click SAVE POLICY</li> </ol>"},{"location":"platform-admin/workloads/policies/workspaces-policy/#troubleshooting","title":"Troubleshooting","text":"<p>Listed below are issues that might occur when creating or editing a policy via the YAML Editor:</p> Issue Message Mitigation Cluster connectivity issues There's no communication from cluster \u201ccluster_name\u201c. Actions may be affected, and the data may be stale. Verify that you are on a network that has been allowed access to the cluster. Reach out to your cluster administrator for instructions on verifying the issue. Policy can\u2019t be applied due to a rule that is occupied by a different policy Field \u201cfield_name\u201d already has rules in cluster: \u201ccluster_id\u201d Remove the rule from the new policy or adjust the old policy for the specific rule. Policy is not visible in the UI - Check that the policy hasn\u2019t been deleted. Policy syntax is no valid Add a valid policy YAML;json: unknown field \"field_name\" For correct syntax check the Policy YAML reference or the Policy YAML examples. Policy can\u2019t be saved for some reason The policy couldn't be saved due to a network or other unknown issue. Download your draft and try pasting and saving it again later. Possible cluster connectivity issues. Try updating the policy once again at a different time. Policies were submitted before version 2.18, you upgraded to version 2.18 or above and wish to submit new policies If you have policies and want to create a new one, first contact Run:ai support to prevent potential conflicts Contact Run:ai support. R&amp;D can migrate your old policies to the new version."},{"location":"platform-admin/workloads/policies/workspaces-policy/#viewing-a-policy","title":"Viewing a policy","text":"<p>To view a policy:</p> <ol> <li>Select the policy for which you want to view its policies.  </li> <li>Click VIEW POLICY </li> <li>In the Policy form per workload section, view the workload rules and defaults:  <ul> <li>Parameter   The workload submission parameter that Rules and Defaults are applied to  </li> <li>Type (applicable for data sources only)   The data source type (Git, S3, nfs, pvc etc.)  </li> <li>Default   The default value of the Parameter  </li> <li>Rule   Set up constraint on workload policy field  </li> <li>Source   The origin of the applied policy (cluster, department or project)  </li> </ul> </li> </ol> <p>Note</p> <p>Some of the rules and defaults may be derived from policies of a parent cluster and/or department. You can see the source of each rule in the policy form. For more information, check the Scope of effectiveness documentation</p>"},{"location":"platform-admin/workloads/policies/workspaces-policy/#deleting-a-policy","title":"Deleting a policy","text":"<ol> <li>Select the policy you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol>"},{"location":"platform-admin/workloads/policies/workspaces-policy/#using-api","title":"Using API","text":"<p>Go to the Policies API reference to view the available actions.</p>"}]}