{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Researcher/overview-researcher/","title":"Overview: Researcher Documentation","text":"<p>Researchers, or AI practitioners, use Run:ai to submit Workloads. </p> <p>As part of the Researcher documentation you will find:</p> <ul> <li>Quickstart Guides which provide step-by-step guides to Run:ai technology.</li> <li>Command line interface reference documentation.</li> <li>Best Practices for Deep Learning with Run:ai.</li> <li>Information about the Run:ai Scheduler.</li> <li>Using Run:ai with various developer tools. </li> </ul>"},{"location":"Researcher/use-cases/","title":"Use Cases","text":"<p>This is a collection of various client-requested use cases. Each use case is accompanied by a short live-demo video, along with all the files used.</p> <p>Note</p> <p>For the most up-to-date information, check out the official Run:ai use-cases GitHub page.  </p> <ul> <li>MLflow with Run:ai: experiment management is important for Data Scientists. This is a demo of how to set up and use MLflow with Run:ai.  </li> <li>Introduction to Docker: Run:ai runs using Docker images. This is a brief introduction to Docker, image creation, and how to use them in the context of Run:ai. Please also check out the Persistent Environments use case if you wish to keep the creation of Docker images to a minimum.  </li> <li>Tensorboard with Jupyter (ResNet demo): Many Data Scientists like to use Tensorboard to keep an eye on the their current training experiments. They also like to have it side-by-side with Jupyter. In this demo, we will show how to integrate Tensorboard and Jupyter Lab within the context of Run:ai.  </li> <li>Persistent Environments (with Conda/Mamba &amp; Jupyter): Some Data Scientists find creating Docker images for every single one of their environments a bit of a hindrance. They would often prefer the ability to create and alter environments on the fly and to have those environments remain, even after an image has finished running in a job. This demo shows users how they can create and persist Conda/Mamba environments using an NFS.  </li> <li>Weights &amp; Biases with Run:ai: W&amp;B (Weights &amp; Biases) is one of the best tools for experiment tracking and management. W&amp;B is an official Run:ai partner. In this tutorial, we will demo how to use W&amp;B alongside Run:ai</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/","title":"Quickstart: Launch an Inference Workload","text":""},{"location":"Researcher/Walkthroughs/quickstart-inference/#introduction","title":"Introduction","text":"<p>Machine learning (ML) inference refers to the process of using a trained machine learning model to make predictions or generate outputs based on new, unseen data. After a model has been trained on a dataset, inference involves applying this model to new examples to produce results such as classifications, predictions, or other types of insights.</p> <p>The quickstart below shows an inference server running the model and an inference client.</p> <p>There are various ways to submit a Workload:</p> <ul> <li>Run:ai command-line interface (CLI)</li> <li>Run:ai user interface</li> <li>Run:ai API</li> </ul> <p>At this time, Inference services cannot be created via the CLI. The CLI can be used for creating a client to query the inference service.</p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Infrastructure Administrator will need to install some optional inference prerequisites as described here.</p> <p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>ML Engineer access to Project in Run:ai named \"team-a\"</li> <li>The project should be assigned a quota of at least 1 GPU. </li> <li>The URL of the Run:ai Console. E.g. https://acme.run.ai.</li> </ul> <p>As described, the inference client can be created via CLI. To perform this, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/quickstart-inference/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#create-an-inference-server-environment","title":"Create an Inference Server Environment","text":"<p>To complete this Quickstart via the UI, you will need to create a new Inference Server Environment asset. </p> <p>This is a one-time step for all Inference workloads using the same image.</p> <p>Under <code>Environments</code> Select NEW ENVIRONMENT. Then select:</p> <ul> <li>A default (cluster) scope.</li> <li>Use the environment name <code>inference-server</code>.</li> <li>The image <code>runai.jfrog.io/demo/example-triton-server</code>.</li> <li>Under <code>type of workload</code> select <code>inference</code>.</li> <li>Under <code>endpoint</code> set the container port as <code>8000</code> which is the port that the triton server is using. </li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#run-an-inference-workload","title":"Run an Inference Workload","text":"CLI V1CLI V2User InterfaceAPI <p>Not available right now.</p> <p>Not available right now.</p> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Inference</li> <li>You should already have <code>Cluster</code> and <code>Project</code> selected. Enter <code>inference-server-1</code> as the name and press CONTINUE.</li> <li>Under <code>Environment</code>,  select <code>inference-server</code>.</li> <li>Under <code>Compute Resource</code>, select <code>half-gpu</code>. </li> <li>Under `Replica autoscaling, select a minimum of 1 and a maximum of 2. </li> <li>Under <code>conditions for a new replica</code> select <code>Concurrency</code> and set the value as 3.</li> <li>Set the <code>scale to zero</code> option to <code>5 minutes</code></li> <li>Select CREATE INFERENCE.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/inferences' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"inference-server-1\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"image\": \"runai.jfrog.io/demo/example-triton-server\",\n        \"servingPort\": {\n            \"protocol\": \"http\",\n            \"container\": 8000\n        },\n        \"autoscaling\": {\n            \"minReplicas\": 1,\n            \"maxReplicas\": 2,\n            \"metric\": \"concurrency\",\n            \"metricThreshold\": 3,\n            \"scaleToZeroRetentionSeconds\": 300\n        },\n        \"compute\": {\n            \"cpuCoreRequest\": 0.1,\n            \"gpuRequestType\": \"portion\",\n            \"cpuMemoryRequest\": \"100M\",\n            \"gpuDevicesRequest\": 1,\n            \"gpuPortionRequest\": 0.5\n        }\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Inference Submit API see API Documentation </li> </ul> <p>This would start a triton inference server with a maximum of 2 instances, each instance consumes half a GPU. </p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#query-the-inference-server","title":"Query the Inference Server","text":"<p>You can use the Run:ai Triton demo client to send requests to the server</p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#find-the-inference-server-endpoint","title":"Find the Inference Server Endpoint","text":"<ul> <li>Under <code>Workloads</code>, select <code>Columns</code> on the top right. Add the column <code>Connections</code>.</li> <li>See the connections of the <code>inference-server-1</code> workload: </li> </ul> <ul> <li>Copy the inference endpoint URL.</li> </ul> CLI V1CLI V2User Interface <p>Open a terminal and run:</p> <pre><code>runai config project team-a   \nrunai submit inference-client-1  -i runai.jfrog.io/demo/example-triton-client \\\n-- perf_analyzer -m inception_graphdef  -p 3600000 -u  &lt;INFERENCE-ENDPOINT&gt;    \n</code></pre> <p>Open a terminal and run:</p> <pre><code>runai project set team-a\nrunai training submit inference-client-1  -i runai.jfrog.io/demo/example-triton-client \\\n-- perf_analyzer -m inception_graphdef  -p 3600000 -u  &lt;INFERENCE-ENDPOINT&gt;    \n</code></pre> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Training</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. Enter <code>inference-client-1</code> as the name and press CONTINUE.</li> <li>Select NEW ENVIRONMENT. Enter <code>inference-client</code> as the name and <code>runai.jfrog.io/demo/example-triton-client</code> as the image.  Select CREATE ENVIRONMENT.</li> <li>When the previous screen comes up, select <code>cpu-only</code> under the Compute resource.</li> <li>Under <code>runtime settings</code> enter the command as <code>perf_analyzer</code> and arguments <code>-m inception_graphdef  -p 3600000 -u  &lt;INFERENCE-ENDPOINT&gt;</code> (replace inference endpoint with the above URL).</li> <li>Select CREATE TRAINING.</li> </ul> <p>In the user interface, under <code>inference-server-1</code>, go to the <code>Metrics</code> tab and watch as the various GPU and inference metrics graphs rise. </p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <p>Not available right now</p> <p>Not available right now</p> <p>Select the two workloads and press DELETE.</p>"},{"location":"Researcher/Walkthroughs/quickstart-jupyter/","title":"Running Jupyter Notebook using workspaces","text":"<p>This guide provides a step-by-step walkthrough for running a Jupyter Notebook using workspaces.</p> <p>A workspace contains the setup and configuration needed for building your model, including the container, images, data sets, and resource requests, as well as the required tools for the research, all in one place. See Running workspaces. for more information.</p>"},{"location":"Researcher/Walkthroughs/quickstart-jupyter/#prerequisites","title":"Prerequisites","text":"<p>Before you start, make sure:</p> <ul> <li>You have created a project or have one created for you.</li> <li>The project has an assigned quota of at least 1 GPU.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-jupyter/#step-1-logging-in","title":"Step 1: Logging in","text":"User InterfaceCLI V1CLI V2API <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>Log in using the following command. You will be prompted to enter your username and password:</p> <pre><code>runai login\n</code></pre> <p>Run the below --help command to obtain the login options and log in according to your setup:</p> <pre><code>runai login --help  \n</code></pre> <p>To use the API, you will need to obtain a token. Please follow the API authentication article.</p>"},{"location":"Researcher/Walkthroughs/quickstart-jupyter/#step-2-submitting-a-workspace","title":"Step 2: Submitting a workspace","text":"User InterfaceCLI V1CLI V2API <ol> <li>Go to the Workload manager \u2192 Workloads</li> <li>Select +NEW WORKLOAD and then Workspace. Within the New workspace form:  </li> <li>Select under which cluster to create the workload</li> <li>Select the project in which your workspace will run</li> <li>Select a preconfigured template or select the Start from scratch to launch a new workspace quickly</li> <li>Enter a name for the workspace (If the name already exists in the project, you will be    requested to submit a different name)</li> <li>Click CONTINUE. In the next step:</li> <li> <p>Select the \u2018jupyter-lab\u2019 environment for your workspace (Image URL: jupyter/scipy-notebook)</p> <ul> <li>If the \u2018jupyter-lab\u2019 is not displayed in the gallery, follow the step-by-step guide: </li> </ul> Create a jupyter-lab environment <ol> <li>Click +NEW ENVIRONMENT</li> <li>Select under which cluster to create the environment</li> <li>Select a scope. </li> <li>Enter a name for the environment. The name must be unique.</li> <li>Enter the jupyter-lab Image URL - jupyter/scipy-notebook</li> <li> <p>Tools - Set the connection for your tool </p> <ul> <li>Click +TOOL</li> <li>Select Jupyter tool from the list</li> </ul> </li> <li> <p>Set the runtime settings for the environment </p> <ul> <li>Click +COMMAND </li> <li>Enter command - start-notebook.sh</li> <li>Enter arguments - <code>--NotebookApp.base_url=/${RUNAI_PROJECT}/${RUNAI_JOB_NAME} --NotebookApp.token=''</code></li> </ul> </li> </ol> <p>Note</p> <p>If host-based routing is enabled on the cluster, enter the argument <code>--NotebookApp.token=''</code> only.</p> <ol> <li>Click CREATE ENVIRONMENT</li> </ol> <ul> <li>The newly created jupyter-lab will be selected automatically</li> </ul> </li> <li> <p>Select the \u2018one-gpu\u2019 compute resource for your workspace (GPU devices: 1) </p> <ul> <li>If the \u2018one-gpu\u2019 is not displayed in the gallery, follow the step-by-step guide: </li> </ul> Create a one-gpu compute resource <ol> <li>Click +NEW COMPUTE RESOURCE</li> <li>Select under which cluster to create the compute resource</li> <li>Select a scope</li> <li>Enter a name for the compute resource. The name must be unique.</li> <li>Set GPU devices per pod - 1</li> <li> <p>Set GPU memory per device </p> <ul> <li>Select % (of device) - Fraction of a GPU device\u2019s memory</li> <li>Set the memory Request - 100 (The workload will allocate 100% of the GPU memory)</li> </ul> </li> <li> <p>Optional: set the CPU compute per pod - 0.1 cores (default)</p> </li> <li>Optional: set the CPU memory per pod - 100 MB (default)</li> <li>Click CREATE COMPUTE RESOURCE</li> </ol> <ul> <li>The newly created one-gpu compute resource will be selected automatically</li> </ul> </li> <li> <p>Click CREATE WORKSPACE</p> <p>After the workspace is created, it is added to the workloads table.</p> </li> </ol> <p>Copy the following command to your terminal. Make sure to update the below with the name of your project:</p> <pre><code>runai config project \"project-name\"  \nrunai submit jup1 --jupyter -g 1\n</code></pre> <p>This would start a workspace with a pre-configured Jupyter image with one GPU allocated.</p> <p>Copy the following command to your terminal. Make sure to update the below with the name of your project:</p> <pre><code>runai project set \"project-name\"\nrunai workspace submit jup1  --image jupyter/scipy-notebook --gpu-devices-request 1 \\\n    --external-url container=8888  --command start-notebook.sh  \\\n    -- --NotebookApp.base_url=/\\${RUNAI_PROJECT}/\\${RUNAI_JOB_NAME} --NotebookApp.token=''\n</code></pre> <p>This would start a workspace with a pre-configured Jupyter image with one GPU allocated.</p> <p>Copy the following command to your terminal. Make sure to update the below parameters according to the comments. For more details, see Workspaces API reference:</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/workspaces' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"jup1\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"command\" : \"start-notebook.sh\",\n        \"args\" : \"--NotebookApp.base_url=/${RUNAI_PROJECT}/${RUNAI_JOB_NAME} --NotebookApp.token=''\",\n        \"image\": \"jupyter/scipy-notebook\",\n        \"compute\": {\n            \"gpuDevicesRequest\": 1\n        },\n        \"exposedUrls\" : [\n            { \n                \"container\" : 8888,\n                \"toolType\": \"jupyter-notebook\", \\ # (5)\n                \"toolName\": \"Jupyter\" \\ # (6)\n            }\n        ]\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface.</li> <li><code>&lt;TOKEN&gt;</code> is the API access token obtained in Step 1. </li> <li><code>&lt;PROJECT-ID&gt;</code> is #The ID of the Project the workspace is running on. You can get the Project ID via the Get Projects API Get Projects API.</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> <li><code>toolType</code> will show the Jupyter icon when connecting to the Jupyter tool via the user interface. </li> <li><code>toolName</code> text will show when connecting to the Jupyter tool via the user interface.</li> </ol> <p>Note</p> <p>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</p>"},{"location":"Researcher/Walkthroughs/quickstart-jupyter/#step-3-connecting-to-the-jupyter-notebook","title":"Step 3: Connecting to the Jupyter Notebook","text":"User InterfaceCLI V1CLI V2API <ol> <li>Select the newly created workspace with the Jupyter application that you want to connect to</li> <li>Click CONNECT</li> <li>Select the Jupyter tool </li> <li>The selected tool is opened in a new tab on your browser</li> </ol> <p>To connect to the Jupyter Notebook, browse directly to <code>https://&lt;COMPANY-URL&gt;/&lt;PROJECT-NAME&gt;/jup1</code>.</p> <p>To connect to the Jupyter Notebook, browse directly to <code>https://&lt;COMPANY-URL&gt;/&lt;PROJECT-NAME&gt;/jup1</code>.</p> <p>To connect to the Jupyter Notebook, browse directly to <code>https://&lt;COMPANY-URL&gt;/&lt;PROJECT-NAME&gt;/jup1</code>.</p>"},{"location":"Researcher/Walkthroughs/quickstart-jupyter/#next-steps","title":"Next Steps","text":"<p>Manage and monitor your newly created workspace using the workloads table.</p>"},{"location":"Researcher/Walkthroughs/quickstart-overview/","title":"Run:ai Quickstart Guides","text":"<p>Below is a set of Quickstart documents. The purpose of these documents is to get you acquainted with an aspect of Run:ai in the simplest possible form.</p> <p>Note</p> <p>The Quickstart documents are based solely on the command-line interface. The same functionality can be achieved by using the Workloads User interface which allows for Workload submission and log viewing. </p> <p>Follow the Quickstart documents below to learn more:</p> <ul> <li>Training Quickstart documents:<ul> <li>Unattended training sessions</li> <li>Distributed Training</li> </ul> </li> <li>Build Quickstart documents: <ul> <li>Basic Interactive build sessions</li> <li>Interfactive build session with connected ports</li> <li>Jupyter Notebook</li> <li>Visual Studio Web</li> </ul> </li> <li>Inference</li> <li>GPU Allocation documents:<ul> <li>Using GPU Fractions </li> </ul> </li> <li>Scheduling documents:<ul> <li>Over-Quota, Basic Fairness &amp; Bin Packing</li> <li>Fairness</li> </ul> </li> </ul> <p>Most quickstarts rely on an image called <code>runai.jfrog.io/demo/quickstart</code>. The image is based on  TensorFlow Release 20-08. This TensorFlow image has minimal requirements for CUDA and NVIDIA Compute Capability. </p> <p>If your GPUs do not meet these requirements, use <code>runai.jfrog.io/demo/quickstart:legacy</code> instead. </p>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/","title":"Quickstart: Launch Workspace with a Visual Studio Code for Web","text":""},{"location":"Researcher/Walkthroughs/quickstart-vscode/#introduction","title":"Introduction","text":"<p>The purpose of this article is to provide a quick ramp-up to running a Workspace running Visual Studio Code (Web edition). Workspaces are containers that live forever until deleted by the user. </p> <p>There are various ways to submit a Workspace:</p> <ul> <li>Run:ai command-line interface (CLI)</li> <li>Run:ai user interface</li> <li>Run:ai API</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Infrastructure Administrator will need to configure a wildcard certificate to Run:ai as described here.</p> <p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Researcher access to Project in Run:ai named \"team-a\"</li> <li>The project should be assigned a quota of at least 1 GPU. </li> <li>A URL of the Run:ai Console. E.g. https://acme.run.ai.</li> </ul> <p>To complete this Quickstart via the CLI, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/quickstart-vscode/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/#create-a-visual-studio-environment","title":"Create a Visual Studio Environment","text":"<p>To complete this Quickstart via the UI, you will need to create a new Visual Studio Environment asset. </p> <p>This is a one-time step for all VSCode Workloads.</p> <p>Under <code>Environments</code> Select NEW ENVIRONMENT. Then select:</p> <ul> <li>A scope (where you want your environment to live).</li> <li>Use the environment name <code>vscode</code>.</li> <li>The image <code>quay.io/opendatahub-contrib/workbench-images:vscode-datascience-c9s-py311_2023c_latest</code>.</li> <li>Under <code>Tools</code>, add Visual Studio Code and change the port to <code>8787</code>.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/#run-workload","title":"Run Workload","text":"CLI V1CLI V2User InterfaceAPI <p>Open a terminal and run:</p> <pre><code>runai config project team-a   \nrunai submit vs1 --jupyter -g 1\n</code></pre> <p>Note</p> <p>For more information on the workload submit command, see cli documentation.</p> <p>Open a terminal and run:</p> <pre><code>runai project set team-a\nrunai workspace submit vs1  --image quay.io/opendatahub-contrib/workbench-images:vscode-datascience-c9s-py311_2023c_latest \\\n    --gpu-devices-request 1  --external-url container=8787  \n</code></pre> <p>Note</p> <p>For more information on the workspace submit command, see cli documentation.</p> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Workspace</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. Enter <code>vs1</code> as the name and press CONTINUE.</li> <li>Under <code>Environment</code>,  select select the previously created <code>vscode</code> environment.</li> <li>Under <code>Compute Resource</code>, select <code>one-gpu</code>. </li> <li>Select CREATE WORKSPACE.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/workspaces' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"vs1\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"image\": \"quay.io/opendatahub-contrib/workbench-images:vscode-datascience-c9s-py311_2023c_latest\",\n        \"compute\": {\n            \"gpuDevicesRequest\": 1\n        },\n        \"exposedUrls\" : [\n            { \n                \"container\" : 8787,\n                \"toolType\": \"visual-studio-code\", \\ # (5)\n                \"toolName\": \"Visual Studio\" \\ # (6)\n            }\n        ]\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> <li><code>toolType</code> will show the Visual Studio icon when connecting to the Visual Studio tool via the user interface. </li> <li><code>toolName</code> text will show when connecting to the Visual Studio tool via the user interface.</li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Training Submit API see API Documentation </li> </ul> <p>This would start a Workspace with a pre-configured Visual Studio Code image with an allocation of a single GPU. </p>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/#accessing-visual-studio-web","title":"Accessing Visual Studio Web","text":"<p>Via the Run:ai user interface, go to <code>Workloads</code>, select the <code>vs1</code> Workspace and press <code>Connect</code>.</p>"},{"location":"Researcher/Walkthroughs/quickstart-vscode/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <pre><code>runai delete job vs1\n</code></pre> <pre><code>runai workspace delete vs1\n</code></pre> <p>Select the Workspace and press DELETE.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/","title":"Quickstart: Launch Interactive Build Workloads with Connected Ports","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#introduction","title":"Introduction","text":"<p>This Quickstart is an extension of the Quickstart document: Start and Use Interactive Build Workloads </p> <p>When starting a container with the Run:ai Command-Line Interface (CLI), it is sometimes needed to expose internal ports to the user. Examples are: accessing a Jupyter notebook, using the container from a development environment such as PyCharm. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#exposing-a-container-port","title":"Exposing a Container Port","text":"<p>There are three ways to expose ports in Kubernetes: Port Forwarding, NodePort, and LoadBalancer. The first two will always work. The other requires a special setup by your administrator. The four methods are explained here. </p> <p>The document below provides an example based on Port Forwarding.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#port-forwarding-step-by-step-walkthrough","title":"Port Forwarding, Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named <code>team-a</code>.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#run-workload","title":"Run Workload","text":"<ul> <li>At the command-line run:</li> </ul> <pre><code>runai config project team-a\nrunai submit nginx-test -i zembutsu/docker-sample-nginx --interactive\nrunai port-forward nginx-test --port 8080:80\n</code></pre> <ul> <li>The Job is based on a sample NGINX webserver docker image <code>zembutsu/docker-sample-nginx</code>. Once accessed via a browser, the page shows the container name. </li> <li>Note the interactive flag which means the Job will not have a start or end. It is the Researcher's responsibility to close the Job.  </li> <li>In this example, we have chosen the simplest scheme to expose ports which is port forwarding. We temporarily expose port 8080 to localhost as long as the <code>runai port-forward</code> command is not stopped</li> <li>It is possible to forward traffic from multiple IP addresses by using the \"--address\" parameter. Check the CLI reference for further details. </li> </ul> <p>The result will be:</p> <pre><code>The job 'nginx-test-0' has been submitted successfully\nYou can run `runai describe job nginx-test-0 -p team-a` to check the job status\n\nForwarding from 127.0.0.1:8080 -&gt; 80\nForwarding from [::1]:8080 -&gt; 80\n</code></pre>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#access-the-webserver","title":"Access the Webserver","text":"<p>Open the following in the browser at http://localhost:8080.</p> <p>You should see a web page with the name of the container.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#stop-workload","title":"Stop Workload","text":"<p>Press Ctrl-C in the shell to stop port forwarding. Then delete the Job by running <code>runai delete job nginx-test</code></p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#see-also","title":"See Also","text":"<ul> <li>Develop on Run:ai using Visual Studio Code</li> <li>Develop on Run:ai using PyCharm</li> <li>Use a Jupyter notbook with Run:ai.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/","title":"Quickstart: Launch Interactive Build Workloads","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build/#introduction","title":"Introduction","text":"<p>The purpose of this article is to provide a quick ramp-up to running an interactive Workspace to allow building data science programs. data scientists typically use various tools such as Jupyter Notebook,  PyCharm, or Visual Studio code. However, in this quickstart, we will start by launching a bare-bones Workspace without such tools. </p> <p>With this Quickstart you will learn how to:</p> <ul> <li>Start a workspace.</li> <li>Open an ssh session to the workspace.</li> <li>Stop the workspace.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Researcher access to Project in Run:ai named \"team-a\"</li> <li>The project should be assigned a quota of at least 1 GPU. </li> <li>A URL of the Run:ai Console. E.g. https://acme.run.ai.</li> </ul> <p>To complete this Quickstart via the CLI, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#step-by-step-quickstart","title":"Step by Step Quickstart","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#create-a-workspace","title":"Create a Workspace","text":"CLI V1CLI V2User InterfaceAPI <p>Open a terminal and run:</p> <pre><code>runai config project team-a   \nrunai submit build1 -i ubuntu -g 1 --interactive -- sleep infinity\n</code></pre> <p>Note</p> <p>For more information on the workload submit command, see cli documentation.</p> <p>Open a terminal and run:</p> <pre><code>runai project set team-a\nrunai workspace submit build1 -i ubuntu -g 1 --command -- sleep infinity\n</code></pre> <p>Note</p> <p>For more information on the workspace submit command, see cli documentation.</p> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Workspace</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. Enter <code>build1</code> as the name and press CONTINUE.</li> <li>Select NEW ENVIRONMENT. Enter <code>ubuntu</code> as the name and <code>ubuntu</code> as the image. Then select CREATE ENVIRONMENT.</li> <li>When the previous screen comes up, select <code>one-gpu</code> under the Compute resource. </li> <li>Select CREATE WORKSPACE.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/workspaces' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"build1\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"command\" : \"sleep\",\n        \"args\" : \"infinity\"\n        \"image\": \"ubuntu\",\n        \"compute\": {\n        \"gpuDevicesRequest\": 1\n        }\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Workspace Submit API see API Documentation </li> </ul> <ul> <li>This would start a workload of type Workspace for <code>team-a</code> with an allocation of a single GPU. </li> <li>We named the Workload <code>build1</code>. </li> <li>Note that, unlike a Training workload, a Workspace workload will not end automatically. It is the Researcher's responsibility to stop the Workload. </li> <li>The command provided is <code>sleep infinity</code>. You must provide a command or the container will start and then exit immediately. Alternatively, when using the command line, replace these flags with <code>--attach</code> to attach immediately to a session.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#list-workloads","title":"List Workloads","text":"<p>Follow up on the Workload's progress by running:</p> CLI V1CLI V2User Interface <p><pre><code>runai list jobs\n</code></pre> The result: </p> <pre><code>runai workspace list\n</code></pre> <p>The result:</p> <pre><code>Workload     Type        Status      Project     Preemptible      Running/Requested Pods     GPU Allocation\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nvs1          Workspace   Running     team-a      No               1/1                        1.00\n</code></pre> <ul> <li>Open the Run:ai user interface.</li> <li>Under \"Workloads\" you can view the new Workspace:</li> </ul> <p></p> <p>Select the Workloads and press <code>Show Details</code> to see the Workload details</p> <p> </p> <p>Typical statuses you may see:</p> <ul> <li>ContainerCreating - The docker container is being downloaded from the cloud repository</li> <li>Pending - the job is waiting to be scheduled</li> <li>Running - the job is running</li> </ul> <p>A full list of Job statuses can be found here</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#describe-workload","title":"Describe Workload","text":"<p>To get additional status on your Workload run:</p> CLI V1CLI V2User Interface <pre><code>runai describe job build1\n</code></pre> <pre><code>runai workspace describe build1\n</code></pre> <p>Workload parameters can be viewed by adding more columns to the Workload list and by reviewing the <code>Event History</code> tab for the specific Workload. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#get-a-shell-to-the-container","title":"Get a Shell to the container","text":"CLI V1CLI V2 <p>Run: <pre><code>runai bash build1\n</code></pre></p> <pre><code>runai workspace bash build1\n</code></pre> <p>This should provide a direct shell into the computer</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <pre><code>runai delete job build1\n</code></pre> <pre><code>runai workspace delete build1\n</code></pre> <p>Select the Workspace and press DELETE.</p> <p>This would stop the workspace. You can verify this by running the list command again.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/","title":"Quickstart: Launch Distributed Training Workloads","text":""},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#introduction","title":"Introduction","text":"<p>Distributed Training is the ability to split the training of a model among multiple processors. Each processor is called a worker. Worker nodes work in parallel to speed up model training. There is also a master which coordinates the workers. </p> <p>Distributed Training should not be confused with multi-GPU training. Multi-GPU training is the allocation of more than a single GPU to your workload which runs on a single container.</p> <p>Getting distributed training to work is more complex than a single-container training as it requires syncing of data and timing between the different workers. However, it is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Several Deep Learning frameworks support distributed training. This example will focus on PyTorch.</p> <p>Run:ai provides the ability to run, manage, and view distributed training workloads. The following is a Quickstart document for such a scenario.</p> <p>There are various ways to submit a distributed training Workload:</p> <ul> <li>Run:ai command-line interface (CLI)</li> <li>Run:ai user interface</li> <li>Run:ai API</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Infrastructure Administrator will need to install the optional Kubeflow Training Operator as described here</p> <p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Researcher access to Project in Run:ai named \"team-a\"</li> <li>The project should be assigned a quota of at least 1 GPU. </li> <li>A URL of the Run:ai Console. E.g. https://acme.run.ai.</li> </ul> <p>To complete this Quickstart via the CLI, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#run-a-distributed-training-workload","title":"Run a Distributed Training Workload","text":"CLI V1CLI V2User InterfaceAPI <p>Open a terminal and run:</p> <pre><code>runai config project team-a\nrunai submit-dist pytorch dist-train1 --workers=2 -g 0.1 \\\n    -i kubeflow/pytorch-dist-mnist:latest\n</code></pre> <p>Note</p> <p>For more information on the workload submit command, see cli documentation.</p> <p>Open a terminal and run:</p> <pre><code>runai project set team-a\nrunai distributed submit dist-train1  --framework PyTorch \\\n    -i kubeflow/pytorch-dist-mnist:latest --workers 2 \n    --gpu-request-type portion --gpu-portion-request 0.1 --gpu-devices-request 1 --cpu-memory-request 100M\n</code></pre> <p>Note</p> <p>For more information on the training submit command, see cli documentation.</p> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Training</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. </li> <li>Under <code>Workload architecture</code> select <code>Distributed</code> and choose <code>PyTorch</code>. Set the distributed training configuration to <code>Workers &amp; master</code>.</li> <li>Enter <code>train1</code> as the name and press CONTINUE.</li> <li>Select NEW ENVIRONMENT. Enter <code>pytorch-dt</code> as the name and <code>kubeflow/pytorch-dist-mnist:latest</code> as the image. Then select CREATE ENVIRONMENT.</li> <li>When the previous screen comes up, under <code>Compute resource</code> enter 2 workers and select <code>small-fraction</code> as the Compute resource. </li> <li>Select CONTINUE and then CREATE TRAINING.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/distributed' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"dist-train1\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"compute\": {\n            \"cpuCoreRequest\": 0.1,\n            \"gpuRequestType\": \"portion\",\n            \"cpuMemoryRequest\": \"100M\",\n            \"gpuDevicesRequest\": 1,\n            \"gpuPortionRequest\": 0.1\n        },\n        \"image\": \"kubeflow/pytorch-dist-mnist:latest\",  \n        \"numWorkers\": 2,  \\ # (5)\n        \"distributedFramework\": \"PyTorch\" \\ # (6)\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> <li>Use 2 workers.</li> <li>Use PyTorch training operator </li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Distributed Training Submit API see API Documentation </li> </ul> <p>This would start a distributed training Workload for <code>team-a</code>. The Workload will have one master and two workers. We named the Workload <code>dist-train1</code></p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#list-workloads","title":"List Workloads","text":"<p>Follow up on the Workload's progress by running:</p> CLI V1CLI V2User Interface <p><pre><code>runai list jobs\n</code></pre> The result: </p> <pre><code>runai distributed list\n</code></pre> <p>The result:</p> <pre><code>Workload     Type         Status      Project     Preemptible      Running/Requested Pods     GPU Allocation\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndist-train1  Distributed  Running      team-a      Yes              0/2                        0.00\n</code></pre> <ul> <li>Open the Run:ai user interface.</li> <li>Under \"Workloads\" you can view the new Training Workload:</li> </ul> <p></p> <ul> <li>Select the <code>0/2</code> under Running/Requested Pods and watch the worker pod status:</li> </ul> <p></p> <p>Select the <code>dist-train1</code> workload and press <code>Show Details</code> to see the Workload details</p> <p> </p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#describe-workload","title":"Describe Workload","text":"<p>The Run:ai scheduler ensures that all pods can run together. You can see the list of workers as well as the main \"launcher\" pod by running:</p> CLI V1CLI V2User Interface <pre><code>runai describe job train1\n</code></pre> <pre><code>runai training describe train1\n</code></pre> <p>Workload parameters can be viewed by adding more columns to the Workload list and by reviewing the <code>Event History</code> tab for the specific Workload. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#view-logs","title":"View Logs","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <p>Get the name of the worker pods from the above <code>describe</code> command, then run: </p> <pre><code>runai logs dist-train1 --pod dist-train1-worker-0\n</code></pre> <p>(where <code>dist-train1-worker-0</code> is the name of the first worker)</p> <p>You should see a log of a running container</p> <p>Get the name of the worker pods from the above <code>describe</code> command, then run: </p> <pre><code>runai distributed logs dist-train1 --pod dist-train1-worker-0\n</code></pre> <p>(where <code>dist-train1-worker-0</code> is the name of the first worker)</p> <p>You should see a log of a running container:</p> <p>Select the Workload, and press Show Details. Under <code>Logs</code> you can select each of the workers and see the logs emitted from the container</p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <pre><code>runai delete job dist-train1\n</code></pre> <pre><code>runai training delete dist-train1\n</code></pre> <p>Select the Workload and press DELETE.</p> <p>This would stop the training workload. You can verify this by listing training workloads again.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/","title":"Quickstart: Launch Workloads with GPU Fractions","text":""},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#introduction","title":"Introduction","text":"<p>Run:ai provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU, enabling companies to run more workloads such as computer vision, voice recognition and natural language processing on the same hardware, lowering costs.</p> <p>Run:ai\u2019s fractional GPU system effectively creates logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. This enables several workloads to run in containers side-by-side on the same GPU without interfering with each other. The solution is transparent, simple, and portable; it requires no changes to the containers themselves.</p> <p>A typical use-case could see a couple of Workloads running on the same GPU, meaning you could multiply the work with the same hardware.</p> <p>The purpose of this article is to provide a quick ramp-up to running a training Workload with fractions of a GPU.  </p> <p>There are various ways to submit a  Workload:</p> <ul> <li>Run:ai command-line interface (CLI)</li> <li>Run:ai user interface</li> <li>Run:ai API </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Researcher access to Run:ai </li> <li>To a Project named \"team-a\"</li> <li>With at least 1 GPU assigned to the project. </li> <li>A link to the Run:ai Console. E.g. https://acme.run.ai.</li> <li>To complete this Quickstart via the CLI, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:<ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul> </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#run-workload","title":"Run Workload","text":"<p>Open a terminal and run:</p> CLI V1CLI V2User InterfaceAPI <pre><code>runai config project team-a   \nrunai submit frac05 -i runai.jfrog.io/demo/quickstart -g 0.5\nrunai submit frac05-2 -i runai.jfrog.io/demo/quickstart -g 0.5 \n</code></pre> <pre><code>runai project set team-a\nrunai training submit frac05 -i runai.jfrog.io/demo/quickstart --gpu-portion-request 0.5\nrunai training submit frac05-2 -i runai.jfrog.io/demo/quickstart --gpu-portion-request 0.5\n</code></pre> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Training</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. Enter <code>frac05</code> as the name and press CONTINUE.</li> <li>Select NEW ENVIRONMENT. Enter <code>quickstart</code> as the name and <code>runai.jfrog.io/demo/quickstart</code> as the image. Then select CREATE ENVIRONMENT.</li> <li>When the previous screen comes up, select <code>half-gpu</code> under the Compute resource. </li> <li>Select CREATE TRAINING.</li> <li>Follow the process again to submit a second workload called <code>frac05-2</code>.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/trainings' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"frac05\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"image\": \"runai.jfrog.io/demo/quickstart\",\n        \"compute\": {\n        \"gpuRequestType\": \"portion\",\n        \"gpuPortionRequest\" : 0.5\n        }\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Training Submit API see API Documentation </li> </ul> <ul> <li>The Workloads are based on a sample docker image <code>runai.jfrog.io/demo/quickstart</code> the image contains a startup script that runs a deep learning TensorFlow-based workload.</li> <li>We named the Workloads frac05 and frac05-2 respectively. </li> <li>The Workloads are assigned to team-a with an allocation of half a GPU. </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#list-workloads","title":"List Workloads","text":"<p>Follow up on the Workload's progress by running:</p> CLI V1CLI V2User Interface <p><pre><code>runai list jobs\n</code></pre> The result:</p> <pre><code>Showing jobs for project team-a\nNAME      STATUS   AGE  NODE                  IMAGE                          TYPE   PROJECT  USER   GPUs Allocated (Requested)  PODs Running (Pending)  SERVICE URL(S)\nfrac05    Running  9s   runai-cluster-worker  runai.jfrog.io/demo/quickstart  Train  team-a   yaron  0.50 (0.50)                 1 (0)\nfrac05-2  Running  8s   runai-cluster-worker  runai.jfrog.io/demo/quickstart  Train  team-a   yaron  0.50 (0.50)                 1 (0)\n</code></pre> <pre><code>runai training list\n</code></pre> <p>The result:</p> <pre><code>Workload               Type        Status      Project     Preemptible      Running/Requested Pods     GPU Allocation\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfrac05      Training    Running  team-a      Yes              0/1                        0.00\nfrac05-2    Training    Running  team-a      Yes              0/1                        0.00    \n</code></pre> <ul> <li>Open the Run:ai user interface.</li> <li>Under <code>Workloads</code> you can view the two new Training Workloads</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#view-partial-gpu-memory","title":"View Partial GPU memory","text":"<p>To verify that the Workload sees only parts of the GPU memory run:</p> CLI V1CLI V2 <pre><code>runai exec frac05 nvidia-smi\n</code></pre> <pre><code>runai training exec frac05 nvidia-smi\n</code></pre> <p>The result:</p> <p></p> <p>Notes:</p> <ul> <li>The total memory is circled in red. It should be 50% of the GPUs memory size. In the picture above we see 8GB which is half of the 16GB of Tesla V100 GPUs.</li> <li>The script running on the container is limited by 8GB. In this case, TensorFlow, which tends to allocate almost all of the GPU memory has allocated 7.7GB RAM (and not close to 16 GB). Overallocation beyond 8GB will lead to an out-of-memory exception </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#use-exact-gpu-memory","title":"Use Exact GPU Memory","text":"<p>Instead of requesting a fraction of the GPU, you can ask for specific GPU memory requirements. For example:</p> CLI V1CLI V2User Interface <pre><code>runai submit  -i runai.jfrog.io/demo/quickstart --gpu-memory 5G\n</code></pre> <pre><code>runai training submit -i runai.jfrog.io/demo/quickstart --gpu-memory-request 5G\n</code></pre> <p>As part of the Workload submission, Create a new <code>Compute Resource</code>, with 1 GPU Device and 5GB of <code>GPU memory per device</code>. See picture below: </p> <p>Which will provide 5GB of GPU memory. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/","title":"Quickstart: Over-Quota and Bin Packing","text":""},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#goals","title":"Goals","text":"<p>The goal of this Quickstart is to explain the concepts of over-quota and bin-packing (consolidation) and how they help in maximizing cluster utilization: </p> <ul> <li>Show the simplicity of resource provisioning, and how resources are abstracted from users.</li> <li>Show how the system eliminates compute bottlenecks by allowing teams/users to go over their resource quota if there are free GPUs in the cluster.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#setup-and-configuration","title":"Setup and configuration:","text":"<p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Your cluster should have 4 GPUs on 2 machines with 2 GPUs each.</li> <li>Researcher access to two Projects  named \"team-a\" and \"team-b\"</li> <li>Each project should be assigned an exact quota of 2 GPUs. </li> <li>A URL of the Run:ai Console. E.g. https://acme.run.ai.</li> <li> <p>Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul> </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#login","title":"Login","text":"<p>Run <code>runai login</code> and enter your credentials.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-i-over-quota","title":"Part I: Over-quota","text":"<p>Open a terminal and run the following command:</p> CLI V1CLI V2 <pre><code>runai submit a2 -i runai.jfrog.io/demo/quickstart -g 2 -p team-a\nrunai submit a1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai submit b1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\n</code></pre> <pre><code>runai training submit a2 -i runai.jfrog.io/demo/quickstart -g 2 -p team-a\nrunai training submit a1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai training submit b1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <ul> <li>team-a has 3 GPUs allocated. Which is over its quota by 1 GPU. </li> <li>The system allows this over-quota as long as there are available resources</li> <li>The system is at full capacity with all GPUs utilized. </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-2-basic-fairness-via-preemption","title":"Part 2: Basic Fairness via Preemption","text":"<p>Run the following command:</p> CLI V1CLI V2 <pre><code>runai submit b2 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\n</code></pre> <pre><code>runai training submit b2 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <ul> <li>team-a can no longer remain in over-quota. Thus, one Job, must be preempted: moved out to allow team-b to grow.</li> <li>Run:ai scheduler chooses to preempt Job a1.</li> <li>It is important that unattended Jobs will save checkpoints. This will ensure that whenever Job a1 resume, it will do so from where it left off.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-3-bin-packing","title":"Part 3: Bin Packing","text":"<p>Run the following command:</p> CLI V1CLI V2 <p><code>runai delete job a2 -p team-a</code></p> <pre><code>runai training delete a2\n</code></pre> <p>a1 is now going to start running again.</p> <p>Run:</p> CLI V1CLI V2 <pre><code>runai list jobs -A\n</code></pre> <pre><code>runai training list -A\n</code></pre> <p>You have two Jobs that are running on the first node and one Job that is running alone the second node. </p> <p>Choose one of the two Jobs from the full node and delete it:</p> CLI V1CLI V2 <pre><code>runai delete job &lt;job-name&gt; -p &lt;project&gt;\n</code></pre> <pre><code>runai training delete &lt;job-name&gt; -p &lt;project&gt;\n</code></pre> <p>The status now is: </p> <p>Now, run a 2 GPU Job:</p> CLI V1CLI V2 <pre><code>runai submit a2 -i runai.jfrog.io/demo/quickstart -g 2 -p team-a\n</code></pre> <pre><code>runai training submit a2 -i runai.jfrog.io/demo/quickstart -g 2 -p team-a\n</code></pre> <p>_ The status now is: </p> <p>Discussion</p> <p>Note that Job a1 has been preempted and then restarted on the second node, to clear space for the new a2 Job. This is bin-packing or consolidation</p>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/","title":"Quickstart: Queue Fairness","text":""},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#goal","title":"Goal","text":"<p>The goal of this Quickstart is to explain fairness. The over-quota Quickstart shows basic fairness where allocated GPUs per Project are adhered to such that if a Project is in over-quota, its Job will be preempted once another Project requires its resources.</p> <p>This Quickstart is about queue fairness. It shows that Jobs will be scheduled fairly regardless of the time they have been submitted. As such, if a person in Project A has submitted 50 Jobs and soon after that, a person in Project B has submitted 25 Jobs, the Jobs in the queue will be processed fairly.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#setup-and-configuration","title":"Setup and configuration:","text":"<p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Your cluster should have 4 GPUs on 2 machines with 2 GPUs each.</li> <li>Researcher access to two Projects  named \"team-a\" and \"team-b\"</li> <li>Each project should be assigned an exact quota of 1 GPU. </li> <li>A URL of the Run:ai Console. E.g. https://acme.run.ai.</li> <li> <p>Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul> </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#login","title":"Login","text":"<p>Run <code>runai login</code> and enter your credentials.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-i-immediate-displacement-of-over-quota","title":"Part I: Immediate Displacement of Over-Quota","text":"<p>Run the following commands:</p> CLI V1CLI V2 <pre><code>runai submit a1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai submit a2 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai submit a3 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai submit a4 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\n</code></pre> <pre><code>runai training submit a1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai training submit a2 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai training submit a3 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\nrunai training submit a4 -i runai.jfrog.io/demo/quickstart -g 1 -p team-a\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <p>team-a, even though it has a single GPU as quota, is now using all 4 GPUs.</p> <p>Run the following commands:</p> CLI V1CLI V2 <pre><code>runai submit b1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\nrunai submit b2 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\nrunai submit b3 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\nrunai submit b4 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\n</code></pre> <pre><code>runai training submit b1 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\nrunai training submit b2 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\nrunai training submit b3 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\nrunai training submit b4 -i runai.jfrog.io/demo/quickstart -g 1 -p team-b\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <ul> <li>Two team-b Jobs have immediately displaced team-a. </li> <li>team-a and team-b each have a quota of 1 GPU, thus the remaining over-quota (2 GPUs) is distributed equally between the Projects.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-2-queue-fairness","title":"Part 2: Queue Fairness","text":"<p>Now lets start deleting Jobs. Alternatively, you can wait for Jobs to complete.</p> CLI V1CLI V2 <pre><code>runai delete job b2 -p team-b\n</code></pre> <pre><code>runai training delete b2 -p team-b\n</code></pre> <p>Discussion</p> <p>As the quotas are equal (1 for each Project, the remaining pending Jobs will get scheduled one by one alternating between Projects, regardless of the time in which they were submitted. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/","title":"Quickstart: Launch Unattended Training Workloads","text":""},{"location":"Researcher/Walkthroughs/walkthrough-train/#introduction","title":"Introduction","text":"<p>The purpose of this article is to provide a quick ramp-up to running an unattended training Workload. Training Workloads are containers that execute a program on start and close down automatically when the task is done. </p> <p>With this Quickstart you will learn how to:</p> <ul> <li>Start a deep learning training workload.</li> <li>View training workload status and resource consumption using the Run:ai user interface and the Run:ai CLI.</li> <li>View training workload logs.</li> <li>Stop the training workload.</li> </ul> <p>There are various ways to submit a training Workload:</p> <ul> <li>Run:ai command-line interface (CLI)</li> <li>Run:ai user interface</li> <li>Run:ai API</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart, the Platform Administrator will need to provide you with:</p> <ul> <li>Researcher access to Project in Run:ai named \"team-a\"</li> <li>The project should be assigned a quota of at least 1 GPU. </li> <li>A URL of the Run:ai Console. E.g. https://acme.run.ai.</li> </ul> <p>To complete this Quickstart via the CLI, you will need to have the Run:ai CLI installed on your machine. There are two available CLI variants:</p> <ul> <li>The older V1 CLI. See installation here</li> <li>A newer V2 CLI, supported with clusters of version 2.18 and up. See installation here</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-train/#login","title":"Login","text":"CLI V1CLI V2User InterfaceAPI <p>Run <code>runai login</code> and enter your credentials.</p> <p>Run <code>runai login</code> and enter your credentials.</p> <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <p>To use the API, you will need to obtain a token. Please follow the api authentication article.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#run-workload","title":"Run Workload","text":"CLI V1CLI V2User InterfaceAPI <p>Open a terminal and run:</p> <pre><code>runai config project team-a   \nrunai submit train1 -i runai.jfrog.io/demo/quickstart -g 1\n</code></pre> <p>Note</p> <p>For more information on the workload submit command, see cli documentation.</p> <p>Open a terminal and run:</p> <pre><code>runai project set team-a\nrunai training submit train1 -i runai.jfrog.io/demo/quickstart -g 1\n</code></pre> <p>Note</p> <p>For more information on the training submit command, see cli documentation.</p> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Training</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. Enter <code>train1</code> as the name and press CONTINUE.</li> <li>Select NEW ENVIRONMENT. Enter <code>quickstart</code> as the name and <code>runai.jfrog.io/demo/quickstart</code> as the image. Then select CREATE ENVIRONMENT.</li> <li>When the previous screen comes up, select <code>one-gpu</code> under the Compute resource. </li> <li>Select CREATE TRAINING.</li> </ul> <p>Note</p> <p>For more information on submitting Workloads and creating Assets via the user interface, see Workload documentation.</p> <pre><code>curl -L 'https://&lt;COMPANY-URL&gt;/api/v1/workloads/trainings' \\ # (1)\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer &lt;TOKEN&gt;' \\ # (2)\n-d '{ \n    \"name\": \"train1\", \n    \"projectId\": \"&lt;PROJECT-ID&gt;\", '\\ # (3)\n    \"clusterId\": \"&lt;CLUSTER-UUID&gt;\", \\ # (4)\n    \"spec\": {\n        \"image\": \"runai.jfrog.io/demo/quickstart\",\n        \"compute\": {\n        \"gpuDevicesRequest\": 1\n        }\n    }\n}'\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is the link to the Run:ai user interface. For example <code>acme.run.ai</code></li> <li><code>&lt;TOKEN&gt;</code> is an API access token. see above on how to obtain a valid token.</li> <li><code>&lt;PROJECT-ID&gt;</code> is the the ID of the <code>team-a</code> Project. You can get the Project ID via the Get Projects API</li> <li><code>&lt;CLUSTER-UUID&gt;</code> is the unique identifier of the Cluster. You can get the Cluster UUID by adding the \"Cluster ID\" column to the Clusters view. </li> </ol> <p>Note</p> <ul> <li>The above API snippet will only work with Run:ai clusters of 2.18 and above. For older clusters, use, the now deprecated Cluster API.</li> <li>For more information on the Training Submit API see API Documentation </li> </ul> <p>This would start an unattended training Workload for <code>team-a</code> with an allocation of a single GPU. The Workload is based on a sample docker image <code>runai.jfrog.io/demo/quickstart</code>. We named the Workload <code>train1</code></p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#list-workloads","title":"List Workloads","text":"<p>Follow up on the Workload's progress by running:</p> CLI V1CLI V2User Interface <p><pre><code>runai list jobs\n</code></pre> The result: </p> <pre><code>runai training list\n</code></pre> <p>The result:</p> <pre><code>Workload               Type        Status      Project     Preemptible      Running/Requested Pods     GPU Allocation\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntrain1                 Training    Running     team-a      Yes              1/1                        0.00\n</code></pre> <ul> <li>Open the Run:ai user interface.</li> <li>Under \"Workloads\" you can view the new Training Workload:</li> </ul> <p></p> <p>Select the Workloads and press <code>Show Details</code> to see the Workload details</p> <p> </p> <p>Under Metrics you can see utilization graphs:</p> <p></p> <p>Typical statuses you may see:</p> <ul> <li>ContainerCreating - The docker container is being downloaded from the cloud repository</li> <li>Pending - the Workload is waiting to be scheduled</li> <li>Running - the Workload is running</li> <li>Succeeded - the Workload has ended</li> </ul> <p>A full list of Workload statuses can be found here</p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#describe-workload","title":"Describe Workload","text":"<p>To get additional status on your Workload run:</p> CLI V1CLI V2User Interface <pre><code>runai describe job train1\n</code></pre> <pre><code>runai training describe train1\n</code></pre> <p>Workload parameters can be viewed by adding more columns to the Workload list and by reviewing the <code>Event History</code> tab for the specific Workload. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#view-logs","title":"View Logs","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <pre><code>runai logs train1\n</code></pre> <p>You should see a log of a running container:</p> <p></p> <pre><code>runai training logs train1\n</code></pre> <p>You should see a log of a running container:</p> <p></p> <p>Select the Workload, and press Show Details. Under <code>Logs</code> you can see the logs emitted from the container</p> <p></p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> CLI V1CLI V2User Interface <pre><code>runai delete job train1\n</code></pre> <pre><code>runai training delete train1\n</code></pre> <p>Select the Workload and press DELETE.</p> <p>This would stop the training workload. You can verify this by listing training workloads again.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Quickstart document: Launch Interactive Workloads.</li> <li>Use your container to run an unattended training workload.</li> </ul>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/","title":"Best Practice: From Bare Metal to Docker Images","text":""},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#introduction","title":"Introduction","text":"<p>Some Researchers do data science on bare metal. The term bare-metal relates to connecting to a server and working directly on its operating system and disks.</p> <p>This is the fastest way to start working, but it introduces problems when the data science organization scales:</p> <ul> <li>More Researchers mean that the machine resources need to be efficiently shared</li> <li>Researchers need to collaborate and share data, code, and results</li> </ul> <p>To overcome that, people working on bare-metal typically write scripts to gather data, code as well as code dependencies. This soon becomes an overwhelming task.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#why-use-docker-images","title":"Why Use Docker Images?","text":"<p>Docker images and containerization in general provide a level of abstraction which, by large, frees developers and Researchers from the mundane tasks of setting up an environment. The image is an operating system by itself and thus the 'environment' is by large, a part of the image.</p> <p>When a docker image is instantiated, it creates a container. A container is the running manifestation of a docker image.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#moving-a-data-science-environment-to-docker","title":"Moving a Data Science Environment to Docker","text":"<p>A data science environment typically includes:</p> <li>Training data</li> <li>Machine Learning (ML) code and inputs</li> <li>Libraries: Code dependencies that must be installed before the ML code can be run</li>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#training-data","title":"Training data","text":"<p>Training data is usually significantly large (from several Gigabytes to Petabytes) and is read-only in nature. Thus, training data is typically left outside of the docker image. Instead, the data is mounted onto the image when it is instantiated. Mounting a volume allows the code within the container to access the data as though it was within a directory on the local file system.</p> <p>The best practice is to store the training data on a shared file system. This allows the data to be accessed uniformly on whichever machine the Researcher is currently using, allowing the Researcher to easily migrate between machines. </p> <p>Organizations without a shared file system typically write scripts to copy data from machine to machine.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#machine-learning-code-and-inputs","title":"Machine Learning Code and Inputs","text":"<p>As a rule, code needs to be saved and versioned in a code repository.</p> <p>There are two alternative practices:</p> <ul> <li>The code resides in the image and is being periodically pulled from the repository. This practice requires building a new container image each time a change is introduced to the code.</li> <li>When a shared file system exists, the code can reside outside the image on a shared disk and mounted via a volume onto the container. </li> </ul> <p>Both practices are valid.</p> <p>Inputs to machine learning models and artifacts of training sessions, like model checkpoints, are also better stored in and loaded from a shared file system.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#code-dependencies","title":"Code Dependencies","text":"<p>Any code has code dependencies. These libraries must be installed for the code to run. As the code is changing, so do the dependencies.</p> <p>ML Code is typically python and python dependencies are typically declared together in a single <code>requirements.txt</code> file which is saved together with the code.</p> <p>The best practice is to have your docker startup script (see below) run this file using <code>pip install -r requirements.txt</code>. This allows the flexibility of adding and removing code dependencies dynamically.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#ml-lifecycle-build-and-train","title":"ML Lifecycle: Build and Train","text":"<p>Deep learning workloads can be divided into two generic types:</p> <li>Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter Notebook, remote PyCharm, or similar and accesses GPU resources directly. Build workloads are typically meant for debugging and development sessions. </li> <li>Unattended \"training\" sessions. Training is characterized by a machine learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the data scientist can examine the results. A Training session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU and at the end of the run automatically frees the resources. </li> <p>Getting your docker ready is also a matter of which type of workload you are currently running.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#build-workloads","title":"Build Workloads","text":"<p>With \"build\" you are actually coding and debugging small experiments. You are interactive. In that mode, you can typically take a well known standard image (e.g. https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow) and use it directly.</p> <p>Start a docker container by running:</p> <pre><code>docker run -it .... \"the well known image\" -v /where/my/code/resides bash </code></pre> <p>You get a shell prompt to a container with a mounted volume of where your code is. You can then install your prerequisites and run your code via ssh.</p> <p>You can also access the container remotely from tools such as PyCharm, Jupyter Notebook, and more. In this case, the docker image needs to be customized to install the \"server software\" (e.g. a Jupyter Notebook service).</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#training-workloads","title":"Training Workloads","text":"<p>For training workloads, you can use a well-known image (e.g. the TensorFlow image from the link above) but more often than not, you want to create your own docker image. The best practice is to use the well-known image (e.g. TensorFlow from above) as a base image and add your own customizations on top of it. To achieve that, you create a Dockerfile. A Dockerfile is a declarative way to build a docker image and is built in layers. e.g.:</p> <ol><li>Base image is nvidia-tensorflow</li> <li>Install popular software</li> <li>(Optional) Run a script</li> </ol> <p>The script can be part of the image or can be provided as part of the command line to run the docker. It will typically include additional dependencies to install as well as a reference to the ML code to be run. </p> <p>The best practice for running training workloads is to test the container image in a \"build\" session and then send it for execution as a training Job. For further information on how to set up and parameterize a training workload via docker or Run:ai see Converting your Workload to use Unattended Training Execution.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/","title":"Best Practice: Convert your Workload to Run Unattended","text":""},{"location":"Researcher/best-practices/convert-to-unattended/#motivation","title":"Motivation","text":"<p>Run:ai allows non-interactive training workloads to extend beyond guaranteed quotas and into over-quota as long as computing resources are available. To achieve this kind of flexibility, the system needs to be able to safely stop a workload and restart it again later. This requires Researchers to switch workloads from running interactively, to running unattended, thus allowing Run:ai to pause/resume the run.</p> <p>Unattended workloads are a good fit for long-duration runs, or sets of smaller hyperparameter optimization runs.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#best-practices","title":"Best Practices","text":""},{"location":"Researcher/best-practices/convert-to-unattended/#docker-image","title":"Docker Image","text":"<p>A docker container is based on a docker image. Some Researchers use generic images such as ones provided by Nvidia, for example: NVIDIA NGC TensorFlow. Others, use generic images as the base image to a more customized image using Dockerfiles.</p> <p>Realizing that Researchers are not always proficient with building docker files, as a best practice, you will want to:</p> <ul> <li>Use the same docker image both for interactive and unattended jobs. In this way, you can keep the difference between both methods of invocation to a minimum. This can be a stock image from Nvidia or a custom image.</li> <li>Leave some degree of flexibility, which allows the Researcher to add/remove python dependencies without re-creating images.</li> </ul>"},{"location":"Researcher/best-practices/convert-to-unattended/#code-location","title":"Code Location","text":"<p>You will want to minimize the cycle of code change-and-run. There are a couple of best practices which you can choose from:</p> <ol> <li>Code resides on the network file storage. This way you can change the code and immediately run the Job. The Job picks up the new files from the network.</li> <li>Use the <code>runai submit</code> flag <code>--git-sync</code>. The flag allows the Researcher to provide details of a Git repository. The repository will be automatically cloned into a specified directory when the container starts.</li> <li>The code can be embedded within the image. In this case, you will want to create an automatic CI/CD process, which packages the code into a modified image.</li> </ol> <p>The document below assumes option #1.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#create-a-startup-script","title":"Create a Startup Script","text":"<p>Gather the commands you ran inside the interactive Job into a single script. The script will be provided with the command-line at the start of the unattended execution (see the section running the job below). This script should be kept next to your code, on a shared network drive (e.g. /nfs/john).</p> <p>An example of a common startup script start.sh:</p> <pre><code>pip install -r requirements.txt\n...\npython training.py\n</code></pre> <p>The first line of this script is there to make sure that all required python libraries are installed before the training script executes, it also allows the Researcher to add/remove libraries without needing changes to the image itself.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#support-variance-between-different-runs","title":"Support Variance Between Different Runs","text":"<p>Your training script must be flexible enough to support variance in execution without changing the code. For example, you will want to change the number of epochs to run, apply a different set of hyperparameters, etc. There are two ways to handle this in your script. You can use one or both methods:</p> <ol> <li> <p>Your script can read arguments passed to the script:</p> <p><pre><code>python training.py --number-of-epochs=30</code></pre></p> </li> </ol> <p>In which case, change your start.sh script to:</p> <pre><code>pip install -r requirements.txt\n...\npython training.py $@</code></pre> <ol> <li>Your script can read from environment variables during script execution. In case you use environment variables, the variables will be passed to the training script automatically. No special action is required in this case.</li> </ol>"},{"location":"Researcher/best-practices/convert-to-unattended/#checkpoints","title":"Checkpoints","text":"<p>Run:ai can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:ai will give you back the resources and restore your workload. Thus, it is a good practice to save your weights at various checkpoints and start a workload from the latest checkpoint (typically between epochs).</p> <p>TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for PyTorch).</p> <p>It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node</p> <p>For more information on best practices for saving checkpoints, see Saving Deep Learning Checkpoints.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#running-the-job","title":"Running the Job","text":"<p>Using <code>runai submit</code>, drop the flag <code>--interactive</code>. For submitting a Job using the script created above, please use <code>-- [COMMAND]</code> flag to specify a command, use the <code>--</code> syntax to pass arguments, and pass environment variables using the flag <code>--environment</code>.</p> <p>Example with Environment variables:</p> <pre><code>runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3  \n    -v /nfs/john:/mydir -g 1  --working-dir /mydir/  \n    -e 'EPOCHS=30'  -e 'LEARNING_RATE=0.02'  \n    -- ./startup.sh  \n</code></pre> <p>Example with Command-line arguments:</p> <pre><code>runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3  \n    -v /nfs/john:/mydir -g 1  --working-dir /mydir/  \n    -- ./startup.sh batch-size=64 number-of-epochs=3\n</code></pre> <p>Please refer to Command-Line Interface, runai submit for a list of all arguments accepted by the Run:ai CLI.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#use-cli-policies","title":"Use CLI Policies","text":"<p>Different run configurations may vary significantly and can be tedious to be written each time on the command-line. To make life easier, our CLI offers a way to set administrator policies for these configurations and use pre-configured configuration when submitting a Workload. Please refer to Configure Policies.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#attached-files","title":"Attached Files","text":"<p>The 3 relevant files mentioned in this document can be downloaded from Github</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#see-also","title":"See Also","text":"<p>See the unattended training Quickstart: Launch Unattended Training Workloads</p>"},{"location":"Researcher/best-practices/env-variables/","title":"Environment Variables inside a Run:ai Workload","text":""},{"location":"Researcher/best-practices/env-variables/#identifying-a-job","title":"Identifying a Job","text":"<p>There may be use cases where your container may need to uniquely identify the Job it is currently running in. A typical use case is for saving Job artifacts under a unique name.  Run:ai provides pre-defined environment variables you can use. These variables are guaranteed to be unique even if the Job is preempted or evicted and then runs again. </p> <p>Run:ai provides the following environment variables:</p> <ul> <li><code>JOB_NAME</code> - the name of the Job.</li> <li><code>JOB_UUID</code> - a unique identifier for the Job. </li> </ul> <p>Note that the Job can be deleted and then recreated with the same name. A Job UUID will be different even if the Job names are the same.</p>"},{"location":"Researcher/best-practices/env-variables/#gpu-allocation","title":"GPU Allocation","text":"<p>Run:ai provides an environment variable, visible inside the container, to help identify the number of GPUs allocated for the container. Use <code>RUNAI_NUM_OF_GPUS</code></p>"},{"location":"Researcher/best-practices/env-variables/#node-name","title":"Node Name","text":"<p>There may be use cases where your container may need to identify the node it is currently running on. Run:ai provides an environment variable, visible inside the container, to help identify the name of the node on which the pod was scheduled. Use <code>NODE_NAME</code></p>"},{"location":"Researcher/best-practices/env-variables/#usage-example-in-python","title":"Usage Example in Python","text":"<pre><code>import os\n\njobName = os.environ['JOB_NAME']\njobUUID = os.environ['JOB_UUID']\n</code></pre>"},{"location":"Researcher/best-practices/researcher-notifications/","title":"Researcher Email Notifications","text":""},{"location":"Researcher/best-practices/researcher-notifications/#importance-of-email-notifications-for-data-scientists","title":"Importance of Email Notifications for Data Scientists","text":"<p>Managing numerous data science workloads requires monitoring various stages, including submission, scheduling, initialization, execution, and completion. Additionally, handling suspensions and failures is crucial for ensuring timely workload completion. Email Notifications address this need by sending alerts for critical workload life cycle changes. This empowers data scientists to take necessary actions and prevent delays.</p> <p>Once the system administrator configures the email notifications, users will receive notifications about their jobs that transition from one status to another. In addition, the user will get warning notifications before workload termination due to project-defined timeouts. Details included in the email are:</p> <ul> <li>Workload type</li> <li>Project and cluster information</li> <li>Event timestamp</li> </ul> <p>To configure the types of email notifications you can receive:</p> <ol> <li>The user must log in to their account.</li> <li>Press the user icon, then select settings.</li> <li>In the Email notifications, and in the Send me an email about my workloads when section, select the relevant workload statuses.</li> <li>When complete, press Save.</li> </ol>"},{"location":"Researcher/best-practices/save-dl-checkpoints/","title":"Best Practice: Save Deep-Learning Checkpoints","text":""},{"location":"Researcher/best-practices/save-dl-checkpoints/#introduction","title":"Introduction","text":"<p>Run:ai can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:ai will give you back the resources and restore your workload. Thus, it is a good practice to save the state of your run at various checkpoints and start a workload from the latest checkpoint (typically between epochs).</p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#how-to-save-checkpoints","title":"How to Save Checkpoints","text":"<p>TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for PyTorch).</p> <p>This document uses Keras as an example. The code itself can be found here</p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#where-to-save-checkpoints","title":"Where to Save Checkpoints","text":"<p>It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node. Example:</p> <pre><code>runai submit train-with-checkpoints -i tensorflow/tensorflow:1.14.0-gpu-py3 \\\n  -v /mnt/nfs_share/john:/mydir -g 1  --working-dir /mydir --command -- ./startup.sh\n</code></pre> <p>The command saves the checkpoints in an NFS checkpoints folder <code>/mnt/nfs_share/john</code></p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#when-to-save-checkpoints","title":"When to Save Checkpoints","text":""},{"location":"Researcher/best-practices/save-dl-checkpoints/#save-periodically","title":"Save Periodically","text":"<p>It is a best practice to save checkpoints at intervals. For example, every epoch as the Keras code below shows:</p> <pre><code>checkpoints_file = \"weights.best.hdf5\"\ncheckpoint = ModelCheckpoint(checkpoints_file, monitor='val_acc', verbose=1, \n    save_best_only=True, mode='max')\n</code></pre>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#save-on-exit-signal","title":"Save on Exit Signal","text":"<p>If periodic checkpoints are not enough, you can use a signal-hook provided by Run:ai (via Kubernetes). The hook is python code that is called before your Job is suspended and allows you to save your checkpoints as well as other state data you may wish to store.</p> <pre><code>import signal\nimport time\n\ndef graceful_exit_handler(signum, frame):\n    # save your checkpoints to shared storage\n\n    # exit with status \"1\" is important for the Job to return later.  \n    exit(1)\n\nsignal.signal(signal.SIGTERM, graceful_exit_handler)\n</code></pre> <p>By default, you will have 30 seconds to save your checkpoints.</p> <p>Important</p> <p>For the signal to be captured, it must be propagated from the startup script to the python child process. See code here</p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#resuming-using-saved-checkpoints","title":"Resuming using Saved Checkpoints","text":"<p>A Run:ai unattended workload that is resumed, will run the same startup script as on the first run. It is the responsibility of the script developer to add code that:</p> <ul> <li>Checks if saved checkpoints exist (see above)</li> <li>If saved checkpoints exist, load them and start the run using these checkpoints</li> </ul> <pre><code>import os\n\ncheckpoints_file = \"weights.best.hdf5\"\nif os.path.isfile(checkpoints_file):\n    print(\"loading checkpoint file: \" + checkpoints_file)\n    model.load_weights(checkpoints_file)\n</code></pre>"},{"location":"Researcher/best-practices/secrets-as-env-var-in-cli/","title":"Propogating secrets as environment variables to workloads via the CLI","text":"<p>The following is a specific knowledge articles for Run:ai command-line interface users who wish to propagate a Kubernetes secret an an environment variable.</p>"},{"location":"Researcher/best-practices/secrets-as-env-var-in-cli/#kubernetes-secrets","title":"Kubernetes Secrets","text":"<p>Sometimes you want to use sensitive information within your code. For example passwords, OAuth tokens, or ssh keys. The best practice for saving such information in Kubernetes is via Kubernetes Secrets. Kubernetes Secrets let you store and manage sensitive information. Access to secrets is limited via configuration.</p> <p>A Kubernetes secret may hold multiple key - value pairs.</p>"},{"location":"Researcher/best-practices/secrets-as-env-var-in-cli/#using-secrets-in-runai-workloads","title":"Using Secrets in Run:ai Workloads","text":"<p>Our goal is to provide Run:ai Workloads with secrets as input in a secure way. Using the Run:ai command line, you will be able to pass a reference to a secret that already exists in Kubernetes.</p>"},{"location":"Researcher/best-practices/secrets-as-env-var-in-cli/#creating-a-secret","title":"Creating a secret","text":"<p>For details on how to create a Kubernetes secret see: https://kubernetes.io/docs/concepts/configuration/secret/. Here is an example:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\n  namespace: runai-&lt;project-name&gt;\ndata:\n  username: am9obgo=\n  password: bXktcGFzc3dvcmQK\n</code></pre> <p>Then run: <pre><code>kubectl apply -f &lt;file-name&gt;\n</code></pre></p> <p>Notes</p> <ul> <li>Secrets are base64 encoded</li> <li>Secrets are stored in the scope of a namespace and will not be accessible from other namespaces. Hence the reference to the Run:ai Project name above. Run:ai provides the ability to propagate secrets throughout all Run:ai Projects. See below.</li> </ul>"},{"location":"Researcher/best-practices/secrets-as-env-var-in-cli/#attaching-a-secret-to-a-workload-on-submit-via-cli","title":"Attaching a secret to a Workload on Submit via CLI","text":"<p>When you submit a new Workload, you will want to connect the secret to the new Workload. To do that, run:</p> <pre><code>runai submit -e &lt;ENV-VARIABLE&gt;=SECRET:&lt;secret-name&gt;,&lt;secret-key&gt; ....\n</code></pre> <p>For example:</p> <pre><code>runai submit -i ubuntu -e MYUSERNAME=SECRET:my-secret,username\n</code></pre>"},{"location":"Researcher/cli-reference/Introduction/","title":"Introduction","text":"<p>The Run:ai Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc.</p> <p>To install and configure the Run:ai CLI see Researcher Setup - Start Here</p>"},{"location":"Researcher/cli-reference/runai-attach/","title":"runai attach","text":""},{"location":"Researcher/cli-reference/runai-attach/#description","title":"Description","text":"<p>Attach to a running Job.</p> <p>The command attaches to the standard input, output, and error streams of a running Job. If the Job has multiple pods the job will attach to the first pod unless otherwise set.</p>"},{"location":"Researcher/cli-reference/runai-attach/#synopsis","title":"Synopsis","text":"<pre><code>runai attach &lt;job-name&gt;\n    [--no-stdin ]\n    [--no-tty]   \n    [--pod string]\n    .\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-attach/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-attach/#-no-stdin","title":"--no-stdin","text":"<p>Do not attach STDIN.</p>"},{"location":"Researcher/cli-reference/runai-attach/#-no-tty","title":"--no-tty","text":"<p>Do not allocate a pseudo-TTY</p>"},{"location":"Researcher/cli-reference/runai-attach/#-pod-string","title":"--pod string","text":"<p>Attach to a specific pod within the Job. To find the list of pods run <code>runai describe job &lt;job-name&gt;</code> and then use the pod name with the <code>--pod</code> flag.</p>"},{"location":"Researcher/cli-reference/runai-attach/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-attach/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-attach/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-attach/#output","title":"Output","text":"<p>None</p>"},{"location":"Researcher/cli-reference/runai-bash/","title":"runai bash","text":""},{"location":"Researcher/cli-reference/runai-bash/#description","title":"Description","text":"<p>Get a bash session inside a running Job</p> <p>This command is a shortcut to runai exec (<code>runai exec -it job-name bash</code>). See runai exec for full documentation of the exec command.</p>"},{"location":"Researcher/cli-reference/runai-bash/#synopsis","title":"Synopsis","text":"<pre><code>runai bash &lt;job-name&gt; \n    [--pod string]\n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-bash/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-bash/#-pod-string","title":"--pod string","text":"<p>Specify a pod of a running Job. To get a list of the pods of a specific Job, run <code>runai describe job &lt;job-name&gt;</code> command</p>"},{"location":"Researcher/cli-reference/runai-bash/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-bash/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>"},{"location":"Researcher/cli-reference/runai-bash/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-bash/#-help-h","title":"--help | -h","text":"<p>Show help text</p>"},{"location":"Researcher/cli-reference/runai-bash/#output","title":"Output","text":"<p>The command will access the container that should be currently running in the current cluster and attempt to create a command-line shell based on bash.</p> <p>The command will return an error if the container does not exist or has not been in a running state yet.</p>"},{"location":"Researcher/cli-reference/runai-bash/#see-also","title":"See also","text":"<p>Build Workloads. See Quickstart document: Launch Interactive Build Workloads.</p>"},{"location":"Researcher/cli-reference/runai-config/","title":"runai config","text":""},{"location":"Researcher/cli-reference/runai-config/#description","title":"Description","text":"<p>Set a default Project or Cluster</p>"},{"location":"Researcher/cli-reference/runai-config/#synopsis","title":"Synopsis","text":"<pre><code>runai  config project &lt;project-name&gt;\n    [--loglevel value] \n    [--help | -h]\n\nrunai  config cluster &lt;cluster-name&gt;\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-config/#options","title":"Options","text":"<p>&lt;project-name&gt;  - The name of the Project you want to set as default. Mandatory.</p> <p>&lt;cluster-name&gt; - The name of the cluster you want to set as the current cluster. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-config/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-config/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-config/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-config/#output","title":"Output","text":"<p>None</p>"},{"location":"Researcher/cli-reference/runai-delete/","title":"runai delete","text":""},{"location":"Researcher/cli-reference/runai-delete/#description","title":"Description","text":"<p>Delete a Workload and its associated Pods.</p> <p>Note that once you delete a Workload, its entire data will be gone:</p> <ul> <li>You will no longer be able to enter it via bash.</li> <li>You will no longer be able to access logs.</li> <li>Any data saved on the container and not stored in a shared location will be lost.</li> </ul>"},{"location":"Researcher/cli-reference/runai-delete/#synopsis","title":"Synopsis","text":"<pre><code>runai delete job &lt;job-name&gt; \n    [--all | -A]\n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-delete/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Workload to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-delete/#-all-a","title":"--all | -A","text":"<p>Delete all Workloads.</p>"},{"location":"Researcher/cli-reference/runai-delete/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-delete/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-delete/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-delete/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-delete/#output","title":"Output","text":"<ul> <li> <p>The Workload will be deleted and not available via the command runai list jobs.</p> </li> <li> <p>The Workloads will show as <code>deleted</code> from the Run:ai user interface Job list.</p> </li> </ul>"},{"location":"Researcher/cli-reference/runai-delete/#see-also","title":"See Also","text":"<ul> <li> <p>Build Workloads. See Quickstart document: Launch Interactive Build Workloads.</p> </li> <li> <p>Training Workloads. See Quickstart document:  Launch Unattended Training Workloads.</p> </li> </ul>"},{"location":"Researcher/cli-reference/runai-describe/","title":"runai describe","text":""},{"location":"Researcher/cli-reference/runai-describe/#description","title":"Description","text":"<p>Display details of a Workload or Node.</p>"},{"location":"Researcher/cli-reference/runai-describe/#synopsis","title":"Synopsis","text":"<pre><code>runai describe job &lt;job-name&gt; \n    [--output value | -o value]  \n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n    [--output string | -o string]  \n\n\nrunai describe node [node-name] \n\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-describe/#options","title":"Options","text":"<ul> <li>&lt;job-name&gt; - The name of the Workload to run the command with. Mandatory.</li> <li>&lt;node-name&gt; - The name of the Node to run the command with. If a Node name is not specified, a description of all Nodes is shown.</li> </ul> <p>-o | --output</p> <p>Output format. One of: json|yaml|wide. Default is 'wide'</p>"},{"location":"Researcher/cli-reference/runai-describe/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-describe/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-describe/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project, use: <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-describe/#-help-h","title":"--help | -h","text":"<p>Show help text</p>"},{"location":"Researcher/cli-reference/runai-describe/#output","title":"Output","text":"<ul> <li>The <code>runai describe job</code> command will show Workload properties and status as well as lifecycle events and the list of related resources and pods.</li> <li>The <code>runai describe node</code> command will show Node properties. </li> </ul>"},{"location":"Researcher/cli-reference/runai-exec/","title":"runai exec","text":""},{"location":"Researcher/cli-reference/runai-exec/#description","title":"Description","text":"<p>Execute a command inside a running Job</p> <p>Note: to execute a bash command, you can also use the shorthand runai bash</p>"},{"location":"Researcher/cli-reference/runai-exec/#synopsis","title":"Synopsis","text":"<pre><code>runai exec &lt;job-name&gt; &lt;command&gt; \n    [--stdin | -i] \n    [--tty | -t]\n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-exec/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p> <p>&lt;command&gt; the command itself (e.g. bash).</p>"},{"location":"Researcher/cli-reference/runai-exec/#-stdin-i","title":"--stdin | -i","text":"<p>Keep STDIN open even if not attached.</p>"},{"location":"Researcher/cli-reference/runai-exec/#-tty-t","title":"--tty | -t","text":"<p>Allocate a pseudo-TTY.</p>"},{"location":"Researcher/cli-reference/runai-exec/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-exec/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-exec/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-exec/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-exec/#output","title":"Output","text":"<p>The command will run in the context of the container.</p>"},{"location":"Researcher/cli-reference/runai-exec/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-list/","title":"runai list","text":""},{"location":"Researcher/cli-reference/runai-list/#description","title":"Description","text":"<p>Show lists of Workloads, Projects, Clusters or Nodes.</p>"},{"location":"Researcher/cli-reference/runai-list/#synopsis","title":"Synopsis","text":"<pre><code>runai list jobs \n    [--all-projects | -A]  \n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n\nrunai list projects \n    [--loglevel value] \n    [--help | -h]\n\nrunai list clusters  \n    [--loglevel value] \n    [--help | -h]\n\nrunai list nodes [node-name]\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-list/#options","title":"Options","text":"<p><code>node-name</code> - Name of a specific node to list (optional).</p>"},{"location":"Researcher/cli-reference/runai-list/#-all-projects-a","title":"--all-projects | -A","text":"<p>Show Workloads from all Projects.</p>"},{"location":"Researcher/cli-reference/runai-list/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-list/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-list/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-list/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-list/#output","title":"Output","text":"<ul> <li>A list of Workloads, Nodes, Projects, or Clusters. </li> <li>To filter 'runai list nodes' for a specific Node, add the Node name.</li> </ul>"},{"location":"Researcher/cli-reference/runai-list/#see-also","title":"See Also","text":"<p>To show details for a specific Workload or Node see runai describe.</p>"},{"location":"Researcher/cli-reference/runai-login/","title":"runai login","text":""},{"location":"Researcher/cli-reference/runai-login/#description","title":"Description","text":"<p>Login to Run:ai</p> <p>When Researcher Authentication is enabled, you will need to login to Run:ai using your username and password before accessing resources </p>"},{"location":"Researcher/cli-reference/runai-login/#synopsis","title":"Synopsis","text":"<pre><code>runai login \n    [--loglevel value]\n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-login/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-login/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-login/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-login/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-login/#output","title":"Output","text":"<p>You will be prompted for a user name and password</p>"},{"location":"Researcher/cli-reference/runai-login/#see-also","title":"See Also","text":"<ul> <li>runai logout.</li> </ul>"},{"location":"Researcher/cli-reference/runai-logout/","title":"runai logout","text":""},{"location":"Researcher/cli-reference/runai-logout/#description","title":"Description","text":"<p>Log out from Run:ai</p>"},{"location":"Researcher/cli-reference/runai-logout/#synopsis","title":"Synopsis","text":"<pre><code>runai logout \n    [--loglevel value]\n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-logout/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-logout/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-logout/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-logout/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-logout/#output","title":"Output","text":"<p>You will be logged out from Run:ai</p>"},{"location":"Researcher/cli-reference/runai-logout/#see-also","title":"See Also","text":"<ul> <li>runai login.</li> </ul>"},{"location":"Researcher/cli-reference/runai-logs/","title":"runai logs","text":""},{"location":"Researcher/cli-reference/runai-logs/#description","title":"Description","text":"<p>Show the logs of a Job.</p>"},{"location":"Researcher/cli-reference/runai-logs/#synopsis","title":"Synopsis","text":"<pre><code>runai logs &lt;job-name&gt; \n    [--follow | -f] \n    [--pod string | -p string] \n    [--since duration] \n    [--since-time date-time] \n    [--tail int | -t int] \n    [--timestamps]  \n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-logs/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-follow-f","title":"--follow | -f","text":"<p>Stream the logs.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-pod-p","title":"--pod | -p","text":"<p>Specify a specific pod name. When a Job fails, it may start a couple of times in an attempt to succeed. The flag allows you to see the logs of a specific instance (called 'pod'). Get the name of the pod by running <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-instance-string-i-string","title":"--instance (string) | -i (string)","text":"<p>Show logs for a specific instance in cases where a Job contains multiple pods.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-since-duration","title":"--since (duration)","text":"<p>Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. The flags since and since-time cannot be used together.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-since-time-date-time","title":"--since-time (date-time)","text":"<p>Return logs after specified date. Date format should be RFC3339, example: <code>2020-01-26T15:00:00Z</code>.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-tail-int-t-int","title":"--tail (int) | -t (int)","text":"<p># of lines of recent log file to display.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-timestamps","title":"--timestamps","text":"<p>Include timestamps on each line in the log output.</p>"},{"location":"Researcher/cli-reference/runai-logs/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-logs/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-logs/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-logs/#output","title":"Output","text":"<p>The command will show the logs of the first process in the container. For training Jobs, this would be the command run at startup. For interactive Jobs, the command may not show anything.</p>"},{"location":"Researcher/cli-reference/runai-logs/#see-also","title":"See Also","text":"<ul> <li>Training Workloads. See Quickstart document:  Launch Unattended Training Workloads.</li> </ul>"},{"location":"Researcher/cli-reference/runai-port-forwarding/","title":"runai port-forward","text":""},{"location":"Researcher/cli-reference/runai-port-forwarding/#description","title":"Description","text":"<p>Forward one or more local ports to the selected job or a pod within the job. The forwarding session ends when the selected job terminates or the terminal is interrupted.</p>"},{"location":"Researcher/cli-reference/runai-port-forwarding/#examples","title":"Examples","text":"<ol> <li> <p>Port forward connections from localhost:8080 (localhost is the default) to  on port 8090. <p><code>runai port-forward &lt;job-name&gt; --port 8080:8090</code></p> <li> <p>Port forward connections from 192.168.1.23:8080 to  on port 8080. <p><code>runai port-forward &lt;job-name&gt; --port 8080 --address 192.168.1.23</code></p> <li> <p>Port forward multiple connections from localhost:8080 to  on port 8090 and localhost:6443 to  on port 443. <p><code>runai port-forward &lt;job-name&gt; --port 8080:8090  --port 6443:443</code></p> <li> <p>Port forward into a specific pod in a multi-pod job.</p> <p><code>runai port-forward &lt;job-name&gt; --port 8080:8090 --pod &lt;pod-name&gt;</code></p> </li>"},{"location":"Researcher/cli-reference/runai-port-forwarding/#global-flags","title":"Global flags","text":"<p><code>--loglevel &lt;string&gt;</code>\u2014Set the logging level. Choose:  (default \"info\"). <p><code>-p | --project &lt;string&gt;</code>\u2014Specify the project name. To change the default project use <code>runai config project &lt;project name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-port-forwarding/#flags","title":"Flags","text":"<p><code>--address &lt;string&gt; | [local-interface-ip\\host] |localhost | 0.0.0.0 [privileged]</code>\u2014The listening address of your local machine. (default \"localhost\").</p> <p><code>-h | --help</code>\u2014Help for the command.</p> <p><code>--port</code>\u2014forward ports based on one of the following arguments:</p> <ul> <li> <p><code>&lt;stringArray&gt;</code>\u2014a list of port forwarding combinations.</p> </li> <li> <p><code>[local-port]:[remote-port]</code>\u2014different local and remote ports.</p> </li> <li> <p><code>[local-port=remote-port]</code>\u2014the same port is used for both local and remote.</p> </li> </ul> <p><code>--pod</code>\u2014Specify a pod of a running job. To get a list of the pods of a specific job, run the command <code>runai describe &lt;job-name&gt;</code>.</p> <p><code>--pod-running-timeout</code>\u2014The length of time (like 5s, 2m, or 3h, higher than zero) to wait until the pod is running. Default is 10 minutes.</p> <p>Filter based flags</p> <p><code>--mpi</code>\u2014search only for mpi jobs.</p> <p><code>--interactive</code>\u2014search only for interactive jobs.</p> <p><code>--pytorch</code>\u2014search only for pytorch jobs.</p> <p><code>--tf</code>\u2014search only for tensorflow jobs.</p> <p><code>--train</code>\u2014search only for training jobs.</p>"},{"location":"Researcher/cli-reference/runai-resume/","title":"runai resume","text":""},{"location":"Researcher/cli-reference/runai-resume/#description","title":"Description","text":"<p>Resume a suspended Job</p> <p>Resuming a previously suspended Job will return it to the queue for scheduling. The Job may or may not start immediately, depending on available resources. </p> <p>Suspend and resume do not work with mpi Jobs. </p>"},{"location":"Researcher/cli-reference/runai-resume/#synopsis","title":"Synopsis","text":"<pre><code>runai resume &lt;job-name&gt;\n    [--all | -A]\n\n    [--loglevel value]\n    [--project string | -p string]\n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-resume/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-resume/#-all-a","title":"--all | -A","text":"<p>Resume all suspended Jobs in the current Project.</p>"},{"location":"Researcher/cli-reference/runai-resume/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-resume/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-resume/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-resume/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-resume/#output","title":"Output","text":"<ul> <li>The Job will be resumed. When running runai list jobs the Job status will no longer by Suspended.</li> </ul>"},{"location":"Researcher/cli-reference/runai-resume/#see-also","title":"See Also","text":"<ul> <li>Suspending Jobs: Suspend.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/","title":"runai submit-dist tf","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#description","title":"Description","text":"<p>Submit a distributed TensorFlow training run:ai job to run.</p> <p>Note</p> <p>To use distributed training you need to have installed the TensorFlow operator as specified in Distributed training.</p> <p>Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#examples","title":"Examples","text":"<pre><code>runai submit-dist tf --name distributed-job --workers=2 -g 1 \\\n    -i &lt;image_name\n&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#distributed","title":"Distributed","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-clean-pod-policy-string","title":"--clean-pod-policy &lt; string &gt;","text":"<p>The\u00a0CleanPodPolicy\u00a0controls deletion of pods when a job terminates. The policy can be one of the following values:</p> <ul> <li>Running\u2014only pods still running when a job completes (for example, parameter servers) will be deleted immediately. Completed pods will not be deleted so that the logs will be preserved. (Default)</li> <li>All\u2014all (including completed) pods will be deleted immediately when the job finishes.</li> <li>None\u2014no pods will be deleted when the job completes.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-workers-int","title":"--workers &lt; int&gt;","text":"<p>Number of replicas for Inference jobs</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-args-string-string","title":"--master-args string <code>&lt;string&gt;</code>  <p>Arguments to pass to the master pod container command. If used together with <code>--command</code>, overrides the image's entrypoint of the master pod container with the given command.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-environment-stringarray","title":"--master-environment <code>&lt;stringArray&gt;</code>  <p>Set environment variables in the master pod container. To prevent from a worker environment variable from being set in the master, use the format: <code>name=-</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-extended-resource-stringarray","title":"--master-extended-resource <code>&lt;stringArray&gt;</code>  <p>Request access to an extended resource in the master pod. Use the format: <code>resource_name=quantity</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-gpu-float","title":"--master-gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-no-pvcs","title":"--master-no-pvcs  <p>Do not mount any persistent volumes in the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-no-master","title":"--no-master  <p>Do not create a separate pod for the master.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-mig-profile-string-deprecated","title":"--mig-profile <code>&lt;string&gt;</code> (Deprecated)  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-ttl-after-finish-duration","title":"--ttl-after-finish &lt; duration &gt;  <p>The duration, after which a finished job is automatically deleted (e.g. 5s, 2m, 3h).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#output","title":"Output","text":"<p>The command will attempt to submit an mpi Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#see-also","title":"See Also","text":"<ul> <li>See Quickstart document Running Distributed Training.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/","title":"runai submit-dist mpi","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#description","title":"Description","text":"<p>Submit a Distributed Training (MPI) Run:ai Job to run.</p> <p>Note</p> <p>To use distributed training you need to have installed the Kubeflow MPI Operator as specified in Distributed training.</p> <p>Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#examples","title":"Examples","text":"<p>You can start an unattended mpi training Job of name dist1, based on Project team-a using a quickstart-distributed image:</p> <pre><code>runai submit-dist mpi --name dist1 --workers=2 -g 1 \\\n    -i runai.jfrog.io/demo/quickstart-distributed:v0.3.0 -e RUNAI_SLEEP_SECS=60\n</code></pre> <p>(see: distributed training Quickstart).</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#distributed","title":"Distributed","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-clean-pod-policy-string","title":"--clean-pod-policy &lt; string &gt;","text":"<p>The\u00a0CleanPodPolicy\u00a0controls deletion of pods when a job terminates. The policy can be one of the following values:</p> <ul> <li>Running\u2014only pods still running when a job completes (for example, parameter servers) will be deleted immediately. Completed pods will not be deleted so that the logs will be preserved. (Default)</li> <li>All\u2014all (including completed) pods will be deleted immediately when the job finishes.</li> <li>None\u2014no pods will be deleted when the job completes.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-workers-int","title":"--workers &lt; int &gt;","text":"<p>Number of replicas for Inference jobs.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-slots-per-worker-int","title":"--slots-per-worker &lt; int &gt;","text":"<p>Number of slots to allocate for each worker.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-args-string-string","title":"--master-args string <code>&lt;string&gt;</code>  <p>Arguments to pass to the master pod container command. If used together with <code>--command</code>, overrides the image's entrypoint of the master pod container with the given command.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-environment-stringarray","title":"--master-environment <code>&lt;stringArray&gt;</code>  <p>Set environment variables in the master pod container. To prevent from a worker environment variable from being set in the master, use the format: <code>name=-</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-extended-resource-stringarray","title":"--master-extended-resource <code>&lt;stringArray&gt;</code>  <p>Request access to an extended resource in the master pod. Use the format: <code>resource_name=quantity</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-gpu-float","title":"--master-gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-no-pvcs","title":"--master-no-pvcs  <p>Do not mount any persistent volumes in the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-mig-profile-string-deprecated","title":"--mig-profile <code>&lt;string&gt;</code> (Deprecated)  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-ttl-after-finish-duration","title":"--ttl-after-finish &lt; duration &gt;  <p>The duration, after which a finished job is automatically deleted (e.g. 5s, 2m, 3h).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#output","title":"Output","text":"<p>The command will attempt to submit an mpi Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#see-also","title":"See Also","text":"<ul> <li>See Quickstart document Running Distributed Training.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/","title":"runai submit-dist pytorch","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#description","title":"Description","text":"<p>Submit a distributed PyTorch training run:ai job to run.</p> <p>Note</p> <p>To use distributed training you need to have installed the Pytorch operator as specified in Distributed training.</p> <p>Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#examples","title":"Examples","text":"<pre><code>runai submit-dist pytorch --name distributed-job --workers=2 -g 1 \\\n    -i &lt;image_name&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#distributed","title":"Distributed","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-clean-pod-policy-string","title":"--clean-pod-policy &lt; string &gt;","text":"<p>The\u00a0CleanPodPolicy\u00a0controls deletion of pods when a job terminates. The policy can be one of the following values:</p> <ul> <li>Running\u2014only pods still running when a job completes (for example, parameter servers) will be deleted immediately. Completed pods will not be deleted so that the logs will be preserved. (Default)</li> <li>All\u2014all (including completed) pods will be deleted immediately when the job finishes.</li> <li>None\u2014no pods will be deleted when the job completes.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-max-replicas-int","title":"--max-replicas &lt; int &gt;","text":"<p>Maximum number of replicas for elastic PyTorch job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-min-replicas-int","title":"--min-replicas &lt; int &gt;","text":"<p>Minimum number of replicas for elastic PyTorch job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-workers-int","title":"--workers &lt; int&gt;","text":"<p>Number of replicas for Inference jobs</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-args-string-string","title":"--master-args string <code>&lt;string&gt;</code>  <p>Arguments to pass to the master pod container command. If used together with <code>--command</code>, overrides the image's entrypoint of the master pod container with the given command.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-environment-stringarray","title":"--master-environment <code>&lt;stringArray&gt;</code>  <p>Set environment variables in the master pod container. To prevent from a worker environment variable from being set in the master, use the format: <code>name=-</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-extended-resource-stringarray","title":"--master-extended-resource <code>&lt;stringArray&gt;</code>  <p>Request access to an extended resource in the master pod. Use the format: <code>resource_name=quantity</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-gpu-float","title":"--master-gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-no-pvcs","title":"--master-no-pvcs  <p>Do not mount any persistent volumes in the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-no-master","title":"--no-master  <p>Do not create a separate pod for the master.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-mig-profile-string-deprecated","title":"--mig-profile <code>&lt;string&gt;</code> (Deprecated)  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-ttl-after-finish-duration","title":"--ttl-after-finish &lt; duration &gt;  <p>The duration, after which a finished job is automatically deleted (e.g. 5s, 2m, 3h).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#output","title":"Output","text":"<p>The command will attempt to submit a distributed pytorch workload. You can follow up on the workload by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/","title":"runai submit-dist xgboost","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#description","title":"Description","text":"<p>Submit a distributed XGBoost training run:ai job to run.</p> <p>Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#examples","title":"Examples","text":"<pre><code>runai submit-dist xgboost --name distributed-job --workers=2 -g 1 \\\n    -i &lt;image_name\n&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#distributed","title":"Distributed","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-clean-pod-policy-string","title":"--clean-pod-policy &lt; string &gt;","text":"<p>The\u00a0CleanPodPolicy\u00a0controls deletion of pods when a job terminates. The policy can be one of the following values:</p> <ul> <li>Running\u2014only pods still running when a job completes (for example, parameter servers) will be deleted immediately. Completed pods will not be deleted so that the logs will be preserved. (Default)</li> <li>All\u2014all (including completed) pods will be deleted immediately when the job finishes.</li> <li>None\u2014no pods will be deleted when the job completes.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-workers-int","title":"--workers &lt; int&gt;","text":"<p>Number of replicas for Inference jobs</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-args-string-string","title":"--master-args string <code>&lt;string&gt;</code>  <p>Arguments to pass to the master pod container command. If used together with <code>--command</code>, overrides the image's entrypoint of the master pod container with the given command.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-environment-stringarray","title":"--master-environment <code>&lt;stringArray&gt;</code>  <p>Set environment variables in the master pod container. To prevent from a worker environment variable from being set in the master, use the format: <code>name=-</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-extended-resource-stringarray","title":"--master-extended-resource <code>&lt;stringArray&gt;</code>  <p>Request access to an extended resource in the master pod. Use the format: <code>resource_name=quantity</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-gpu-float","title":"--master-gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-no-pvcs","title":"--master-no-pvcs  <p>Do not mount any persistent volumes in the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-mig-profile-string-deprecated","title":"--mig-profile <code>&lt;string&gt;</code> (Deprecated)  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-ttl-after-finish-duration","title":"--ttl-after-finish &lt; duration &gt;  <p>The duration, after which a finished job is automatically deleted (e.g. 5s, 2m, 3h).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#output","title":"Output","text":"<p>The command will attempt to submit an mpi Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#see-also","title":"See Also","text":"<ul> <li>See Quickstart document Running Distributed Training.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit/","title":"Description","text":"<p>Submit a Run:ai Job for execution.</p> <p>Syntax notes:</p> <ul> <li>Flags of type stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit/#examples","title":"Examples","text":"<p>All examples assume a Run:ai Project has been setup using <code>runai config project &lt;project-name&gt;</code>.</p> <p>Start an interactive Job:</p> <pre><code>runai submit -i ubuntu --interactive --attach -g 1\n</code></pre> <p>Or</p> <pre><code>runai submit --name build1 -i ubuntu -g 1 --interactive -- sleep infinity \n</code></pre> <p>(see: build Quickstart).</p> <p>Externalize ports:</p> <pre><code>runai submit --name build-remote -i rastasheep/ubuntu-sshd:14.04 --interactive \\\n   --service-type=nodeport --port 30022:22\n   -- /usr/sbin/sshd -D\n</code></pre> <p>(see: build with ports Quickstart).</p> <p>Start a Training Job</p> <pre><code>runai submit --name train1 -i runai.jfrog.io/demo/quickstart -g 1 \n</code></pre> <p>(see: training Quickstart).</p> <p>Use GPU Fractions</p> <pre><code>runai submit --name frac05 -i runai.jfrog.io/demo/quickstart -g 0.5\n</code></pre> <p>(see: GPU fractions Quickstart).</p> <p>Submit a Job without a name (automatically generates a name)</p> <pre><code>runai submit -i runai.jfrog.io/demo/quickstart -g 1 \n</code></pre> <p>Submit a job using the system autogenerated name to an external URL:</p> <pre><code>runai submit -i ubuntu --interactive --attach -g 1 service-type=external-url --port 3745 --custom-url=&lt;destination_url&gt;\n</code></pre> <p>Submit a job without a name to a system generated a URL :</p> <pre><code>runai submit -i ubuntu --interactive --attach -g 1 service-type=external-url --port 3745\n</code></pre> <p>Submit a Job without a name with a pre-defined prefix and an incremental index suffix</p> <pre><code>runai submit --job-name-prefix -i runai.jfrog.io/demo/quickstart -g 1 \n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit/#job-type","title":"Job Type","text":""},{"location":"Researcher/cli-reference/runai-submit/#-interactive","title":"--interactive","text":"<p>Mark this Job as interactive.</p>"},{"location":"Researcher/cli-reference/runai-submit/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit/#-completions-int","title":"--completions &lt; int &gt;","text":"<p>Number of successful pods required for this job to be completed. Used with HPO.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-parallelism-int","title":"--parallelism &lt; int &gt;","text":"<p>Number of pods to run in parallel at any given time.  Used with HPO.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-preemptible","title":"--preemptible","text":"<p>Interactive preemptible jobs can be scheduled above guaranteed quota but may be reclaimed at any time.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-auto-deletion-time-after-completion","title":"--auto-deletion-time-after-completion","text":"<p>The timeframe after which a completed or failed job is automatically deleted. Configured in seconds, minutes, or hours (for example 5s, 2m, or 3h). If set to 0, the job will be deleted immediately after completing or failing.</p>"},{"location":"Researcher/cli-reference/runai-submit/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-e-stringarray-environment-stringarray","title":"-e <code>&lt;stringArray&gt;</code> | --environment <code>&lt;stringArray&gt;</code>","text":"<p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>"},{"location":"Researcher/cli-reference/runai-submit/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>","text":"<p>Image to use when creating the container for this Job</p>"},{"location":"Researcher/cli-reference/runai-submit/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>","text":"<p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>","text":"<p>Set labels variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>","text":"<p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>","text":"<p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-stdin","title":"--stdin","text":"<p>Keep stdin open for the container(s) in the pod, even if nothing is attached.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-t-tty","title":"-t | --tty","text":"<p>Allocate a pseudo-TTY.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>","text":"<p>Starts the container with the specified directory as the current directory.</p>"},{"location":"Researcher/cli-reference/runai-submit/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>","text":"<p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>","text":"<p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-extended-resource-stringarray","title":"--extended-resource <code>&lt;stringArray&gt;</code>","text":"<p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>","text":"<p>GPU units to allocate for the Job (0.5, 1).</p>"},{"location":"Researcher/cli-reference/runai-submit/#-gpu-memory","title":"--gpu-memory","text":"<p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-memory-string","title":"--memory <code>&lt;string&gt;</code>","text":"<p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-memory-limit-string","title":"--memory-limit <code>&lt;string&gt;</code>","text":"<p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-mig-profile-string-deprecated","title":"--mig-profile <code>&lt;string&gt;</code> (Deprecated)","text":"<p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>"},{"location":"Researcher/cli-reference/runai-submit/#job-lifecycle_1","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>","text":"<p>The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the <code>--interactive</code> flag is not specified).</p>"},{"location":"Researcher/cli-reference/runai-submit/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>","text":"<p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-large-shm","title":"--large-shm","text":"<p>Mount a large /dev/shm device.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-mount-propagation","title":"--mount-propagation","text":"<p>Enable HostToContainer mount propagation for all container volumes</p>"},{"location":"Researcher/cli-reference/runai-submit/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>","text":"<p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>"},{"location":"Researcher/cli-reference/runai-submit/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>","text":"<p>Mount a persistent volume claim into a container.</p> <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--new-pvc</code>.</p> <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class.</p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p> <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only</p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only</p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write</p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write</p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>"},{"location":"Researcher/cli-reference/runai-submit/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>","text":"<p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running</li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-new-pvc-stringarray","title":"--new-pvc  <code>&lt;stringArray&gt;</code>","text":"<p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p> <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>","text":"<p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'","text":"<p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to jobs. Options are:</p> <ul> <li><code>nodeport</code> - add one or more ports using <code>--port</code>.</li> <li><code>external-url</code> - add one port and an optional custom URL using <code>--custom-url</code>.</li> </ul> <p>For example:</p> <p><code>runai submit test-jup -p team-a -i runai.jfrog.io/demo/jupyter-tensorboard --service-type external-url --port 8888</code></p> <p><code>runai submit test-np -p team-a -i ubuntu --service-type nodeport --port 30000:7070</code></p> <p>This flag supports more than one <code>service-type</code>. Multiple service types are supported in CSV style using multiple instances of the same option and commas to separate the values for them.</p> <p>For example:</p> <p><code>runai submit test-np -p team-a -i ubuntu --service-type nodeport,port=30000:7070 --service-type external-url,port=30001</code></p> <p><code>runai submit test-np -p team-a -i ubuntu --service-type nodeport,port=30000:7070,port=9090 --service-type external-url,port=8080,custom-url=https://my.domain.com/url</code> </p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container. You can use a port number (for example 9090) or use the numbers of <code>hostport:containerport</code> (for example, 30000:7070).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-custom-url-string","title":"--custom-url <code>&lt;string&gt;</code>  <p>An optional argument that specifies a custom URL when using the <code>external-url</code> service type. If not provided, the system will generate a URL automatically.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node. This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#output","title":"Output","text":"<p>The command will attempt to submit a Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p> <p>Note that the submit call may use a policy to provide defaults to any of the above flags.</p>"},{"location":"Researcher/cli-reference/runai-submit/#see-also","title":"See Also","text":"<ul> <li>See any of the Quickstart documents here:.</li> <li>See policy configuration for a description on how policies work.</li> </ul>"},{"location":"Researcher/cli-reference/runai-suspend/","title":"runai suspend","text":""},{"location":"Researcher/cli-reference/runai-suspend/#description","title":"Description","text":"<p>Suspend a Job</p> <p>Suspending a Running Job will stop the Job and will not allow it to be scheduled until it is resumed using <code>runai resume</code>. This means that,</p> <ul> <li>You will no longer be able to enter it via <code>runai bash</code>.</li> <li>The Job logs will be deleted.</li> <li>Any data saved on the container and not stored in a shared location will be lost.</li> </ul> <p>Technically, the command deletes the Kubernetes pods associated with the Job and marks the Job as suspended until it is manually released. </p> <p>Suspend and resume do not work with MPI and Inference </p>"},{"location":"Researcher/cli-reference/runai-suspend/#synopsis","title":"Synopsis","text":"<pre><code>runai suspend &lt;job-name&gt;\n    [--all | -A]\n\n    [--loglevel value]\n    [--project string | -p string]\n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-suspend/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#-all-a","title":"--all | -A","text":"<p>Suspend all Jobs in the current Project.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-suspend/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-suspend/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#output","title":"Output","text":"<ul> <li>The Job will be suspended. When running runai list jobs the Job will be marked as Suspended.</li> </ul>"},{"location":"Researcher/cli-reference/runai-suspend/#see-also","title":"See Also","text":"<ul> <li>Resuming Jobs: Resume.</li> </ul>"},{"location":"Researcher/cli-reference/runai-top-node/","title":"runai top node","text":""},{"location":"Researcher/cli-reference/runai-top-node/#description","title":"Description","text":"<p>Show list of Nodes (machines), their capacity and utilization.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#synopsis","title":"Synopsis","text":"<pre><code>runai top node \n    [--help | -h]\n    [--details | -d]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-top-node/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-top-node/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-top-node/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-top-node/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#-details-d","title":"--details | -d","text":"<p>Show additional details.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#output","title":"Output","text":"<p>Shows a list of Nodes their capacity and utilization.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-update/","title":"runai update","text":""},{"location":"Researcher/cli-reference/runai-update/#description","title":"Description","text":"<p>Find and install the latest version of the runai command-line utility. The command must be run with sudo permissions.</p> <pre><code>sudo runai update\n</code></pre>"},{"location":"Researcher/cli-reference/runai-update/#synopsis","title":"Synopsis","text":"<pre><code>runai update \n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-update/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-update/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-update/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-update/#output","title":"Output","text":"<p>Update of the Run:ai command-line interface.</p>"},{"location":"Researcher/cli-reference/runai-update/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-version/","title":"runai version","text":""},{"location":"Researcher/cli-reference/runai-version/#description","title":"Description","text":"<p>Show the version of this utility.</p>"},{"location":"Researcher/cli-reference/runai-version/#synopsis","title":"Synopsis","text":"<pre><code>runai version \n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-version/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-version/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-version/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-version/#output","title":"Output","text":"<p>The version of the Run:ai command-line interface.</p>"},{"location":"Researcher/cli-reference/runai-version/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-whoami/","title":"runai whoami","text":""},{"location":"Researcher/cli-reference/runai-whoami/#description","title":"Description","text":"<p>Show the user name currently logged in</p>"},{"location":"Researcher/cli-reference/runai-whoami/#synopsis","title":"Synopsis","text":"<pre><code>runai whoami \n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-whoami/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-whoami/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-whoami/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-whoami/#output","title":"Output","text":"<p>The name of the User currently logged in with the Run:ai command-line interface.</p>"},{"location":"Researcher/cli-reference/runai-whoami/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/new-cli/cli-examples/","title":"CLI Examples","text":"<p>This article provides examples of popular use cases illustrating how to use the Command Line Interface (CLI)</p>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#login","title":"Login","text":""},{"location":"Researcher/cli-reference/new-cli/cli-examples/#login-via-runai-sign-in-page-web","title":"Login via run:ai sign in page (web)","text":"<p>You can login from the UI, if you are using SSO or credentials <pre><code>runai login\n</code></pre></p>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#login-via-terminal-credentials","title":"Login via terminal (credentials)","text":"<pre><code>runai login user -u john@acme.com -p \"password\"\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#submitting-a-workload","title":"Submitting a workload","text":""},{"location":"Researcher/cli-reference/new-cli/cli-examples/#naming-a-workload","title":"Naming a workload","text":"<p>Use the commands below to provide a name for a workload.</p>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#setting-a-the-workload-name-my_workload_name","title":"Setting a the workload name ( my_workload_name)","text":"<pre><code>runai workspace submit my-workload-name -p test -i ubuntu \n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#setting-a-random-name-with-prefix-prefixworkload-type","title":"Setting a random name with prefix (prefix=workload type)","text":"<pre><code>    runai workspace submit -p test -i ubuntu \n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#setting-a-random-name-with-specific-prefix-prefix-determined-by-flag","title":"Setting a random name with specific prefix (prefix determined by flag)","text":"<pre><code>runai workspace submit --prefix-name my-prefix-workload-name -p test -i ubuntu \n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#labels-and-annotations","title":"Labels and annotations","text":""},{"location":"Researcher/cli-reference/new-cli/cli-examples/#labels","title":"Labels","text":"<pre><code>runai workspace submit -p test -i ubuntu --label name=value --label name2=value2\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#annotations","title":"Annotations","text":"<pre><code>runai workspace submit -p test -i ubuntu --annotation name=value --annotation name2=value2\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#containers-environment-variables","title":"Container's environment variables","text":"<pre><code>runai workspace submit -p test -i ubuntu -e name=value -e name2=value2\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#requests-and-limits","title":"Requests and limits","text":"<pre><code>runai workspace submit  -p alon -i runai.jfrog.io/demo/quickstart-demo   --cpu-core-request 0.3 --cpu-core-limit 1 --cpu-memory-request 50M --cpu-memory-limit 1G  --gpu-devices-request 1 --gpu-memory-request 1G\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#submit-and-attach-to-process","title":"Submit and attach to process","text":"<pre><code>runai workspace submit  -p alon -i python  --attach -- python3\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/cli-examples/#submit-a-jupiter-notebook","title":"Submit a jupiter notebook","text":"<pre><code>runai workspace submit --image jupyter/scipy-notebook -p alon --gpu-devices-request 1 --external-url container=8888 --name-prefix jupyter --command -- start-notebook.sh --NotebookApp.base_url='/${RUNAI_PROJECT}/${RUNAI_JOB_NAME}' --NotebookApp.token=''\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/overview/","title":"Run:ai V2 Command-line Interface","text":"<p>The Run:ai Command-line Interface (CLI) tool for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, and access other features in the Run:ai platform.</p>"},{"location":"Researcher/cli-reference/new-cli/overview/#the-new-v2-command-line-interface","title":"The new V2 Command-line interface","text":"<p>This command-line interface is a complete revamp of the command-line interface. Few highlights:</p> <ul> <li>The CLI internally uses the Control-plane API. This provides a single point of view on Workloads removing dissimilarities between the user interface, programming interface and the command-line interface. </li> <li>As such, it also removes the need to configure the Kubernetes API server for authentication. </li> <li>The CLI is only available for Run:ai cluster version 2.18 and up.</li> <li>The new V2 CLI is backward compatible with the older V1 CLI.</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/overview/#installing-the-improved-command-line-interface","title":"Installing the Improved Command Line Interface","text":"<p>See installation instructions here.</p>"},{"location":"Researcher/cli-reference/new-cli/overview/#reference","title":"Reference","text":"<p>List of all commands can be found here</p>"},{"location":"Researcher/cli-reference/new-cli/runai/","title":"CLI Reference","text":""},{"location":"Researcher/cli-reference/new-cli/runai/#runai","title":"runai","text":"<p>Run:ai Command-line Interface</p>"},{"location":"Researcher/cli-reference/new-cli/runai/#synopsis","title":"Synopsis","text":"<p>runai - The Run:ai Researcher Command Line Interface</p> <p>Description:   A tool for managing Run:ai workloads and monitoring available resources.   It provides researchers with comprehensive control over their AI development environment.</p> <pre><code>runai [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai/#options","title":"Options","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -h, --help                 help for runai\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai/#see-also","title":"SEE ALSO","text":"<ul> <li>runai attach    - [Deprecated] attach</li> <li>runai cluster  - cluster management</li> <li>runai config    - configuration management</li> <li>runai kubeconfig    - kubeconfig management</li> <li>runai describe    - [Deprecated] Display detailed information about resources</li> <li>runai exec    - [Deprecated] exec</li> <li>runai list    - [Deprecated] display resource list. By default displays the job list</li> <li>runai login  - login to the control plane</li> <li>runai logout    - logout from control plane</li> <li>runai logs    - [Deprecated] logs</li> <li>runai mpi  - alias for mpi management</li> <li>runai node    - node management</li> <li>runai nodepool    - node pool management</li> <li>runai port-forward    - [Deprecated] port forward</li> <li>runai project  - project management</li> <li>runai pytorch  - alias for pytorch management</li> <li>runai report    - [Experimental] report management</li> <li>runai submit    - [Deprecated] Submit a new workload</li> <li>runai tensorflow    - alias for tensorflow management</li> <li>runai training    - training management</li> <li>runai upgrade  - upgrades the CLI to the latest version</li> <li>runai version  - show the current version of the CLI</li> <li>runai whoami    - show the current logged in user</li> <li>runai workload    - workload management</li> <li>runai workspace  - workspace management</li> <li>runai xgboost  - alias for xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_attach/","title":"Runai attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_attach/#runai-attach","title":"runai attach","text":"<p>[Deprecated] attach</p> <pre><code>runai attach WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --type string                    The type of the workload (training, workspace)\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster/","title":"Runai cluster","text":""},{"location":"Researcher/cli-reference/new-cli/runai_cluster/#runai-cluster","title":"runai cluster","text":"<p>cluster management</p> <pre><code>runai cluster [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster/#options","title":"Options","text":"<pre><code>  -h, --help                 help for cluster\n      --interactive enable   set interactive mode (enabled|disabled)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai cluster list    - cluster list command</li> <li>runai cluster set  - set cluster context</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_list/","title":"Runai cluster list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_cluster_list/#runai-cluster-list","title":"runai cluster list","text":"<p>cluster list command</p> <pre><code>runai cluster list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_list/#options","title":"Options","text":"<pre><code>  -h, --help         help for list\n      --json         Output structure JSON\n      --no-headers   Output structure table without headers\n      --table        Output structure table\n      --yaml         Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai cluster  - cluster management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_set/","title":"Runai cluster set","text":""},{"location":"Researcher/cli-reference/new-cli/runai_cluster_set/#runai-cluster-set","title":"runai cluster set","text":"<p>set cluster context</p> <pre><code>runai cluster set [CLUSTER_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_set/#options","title":"Options","text":"<pre><code>  -h, --help        help for set\n      --id string   set by cluster ID\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_set/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_cluster_set/#see-also","title":"SEE ALSO","text":"<ul> <li>runai cluster  - cluster management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_config/","title":"Runai config","text":""},{"location":"Researcher/cli-reference/new-cli/runai_config/#runai-config","title":"runai config","text":"<p>configuration management</p> <pre><code>runai config [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config/#options","title":"Options","text":"<pre><code>  -h, --help                 help for config\n      --interactive enable   set interactive mode (enabled|disabled)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai config generate  - generate config file</li> <li>runai config project    - [Deprecated] Configure a default project</li> <li>runai config set    - Set configuration values</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_config_generate/","title":"Runai config generate","text":""},{"location":"Researcher/cli-reference/new-cli/runai_config_generate/#runai-config-generate","title":"runai config generate","text":"<p>generate config file</p> <pre><code>runai config generate [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_generate/#options","title":"Options","text":"<pre><code>      --file string   Output structure to file\n  -h, --help          help for generate\n      --json          Output structure JSON\n      --yaml          Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_generate/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_generate/#see-also","title":"SEE ALSO","text":"<ul> <li>runai config    - configuration management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_config_project/","title":"Runai config project","text":""},{"location":"Researcher/cli-reference/new-cli/runai_config_project/#runai-config-project","title":"runai config project","text":"<p>[Deprecated] Configure a default project</p> <pre><code>runai config project PROJECT_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_project/#options","title":"Options","text":"<pre><code>  -h, --help   help for project\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_project/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_project/#see-also","title":"SEE ALSO","text":"<ul> <li>runai config    - configuration management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_config_set/","title":"Runai config set","text":""},{"location":"Researcher/cli-reference/new-cli/runai_config_set/#runai-config-set","title":"runai config set","text":"<p>Set configuration values</p> <pre><code>runai config set [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_set/#examples","title":"Examples","text":"<pre><code>runai config set --status-timeout-duration 5s\nrunai config set --status-timeout-duration 300ms\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_set/#options","title":"Options","text":"<pre><code>      --auth-url string                  set the authorization URL; most likely the same as the control plane URL\n      --cp-url string                    set the control plane URL\n  -h, --help                             help for set\n      --interactive enable               set interactive mode (enabled|disabled)\n      --output string                    set the default output type\n      --status-timeout-duration string   set cluster status call timeout duration value, the default is 3 second (\"3s\")\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_set/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_config_set/#see-also","title":"SEE ALSO","text":"<ul> <li>runai config    - configuration management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_describe/","title":"Runai describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_describe/#runai-describe","title":"runai describe","text":"<p>[Deprecated] Display detailed information about resources</p>"},{"location":"Researcher/cli-reference/new-cli/runai_describe/#options","title":"Options","text":"<pre><code>  -h, --help   help for describe\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai describe job    - [Deprecated] Display details of a job</li> <li>runai describe node  - [Deprecated] Display detailed information about nodes in the cluster</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_job/","title":"Runai describe job","text":""},{"location":"Researcher/cli-reference/new-cli/runai_describe_job/#runai-describe-job","title":"runai describe job","text":"<p>[Deprecated] Display details of a job</p> <pre><code>runai describe job JOB_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_job/#options","title":"Options","text":"<pre><code>  -h, --help             help for job\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --type string      The type of the workload (training, workspace)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_job/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_job/#see-also","title":"SEE ALSO","text":"<ul> <li>runai describe    - [Deprecated] Display detailed information about resources</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_node/","title":"Runai describe node","text":""},{"location":"Researcher/cli-reference/new-cli/runai_describe_node/#runai-describe-node","title":"runai describe node","text":"<p>[Deprecated] Display detailed information about nodes in the cluster</p> <pre><code>runai describe node [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_node/#options","title":"Options","text":"<pre><code>  -h, --help   help for node\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_node/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_describe_node/#see-also","title":"SEE ALSO","text":"<ul> <li>runai describe    - [Deprecated] Display detailed information about resources</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_exec/","title":"Runai exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_exec/#runai-exec","title":"runai exec","text":"<p>[Deprecated] exec</p> <pre><code>runai exec WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --type string                    The type of the workload (training, workspace)\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig/","title":"Runai kubeconfig","text":""},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig/#runai-kubeconfig","title":"runai kubeconfig","text":"<p>kubeconfig management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig/#options","title":"Options","text":"<pre><code>  -h, --help   help for kubeconfig\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai kubeconfig set    - kubeconfig set login token</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig_set/","title":"Runai kubeconfig set","text":""},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig_set/#runai-kubeconfig-set","title":"runai kubeconfig set","text":"<p>kubeconfig set login token</p> <pre><code>runai kubeconfig set [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig_set/#options","title":"Options","text":"<pre><code>  -h, --help   help for set\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig_set/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_kubeconfig_set/#see-also","title":"SEE ALSO","text":"<ul> <li>runai kubeconfig    - kubeconfig management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_list/","title":"Runai list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_list/#runai-list","title":"runai list","text":"<p>[Deprecated] display resource list. By default displays the job list</p> <pre><code>runai list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list/#options","title":"Options","text":"<pre><code>  -A, --all-projects     list workloads from all projects\n  -h, --help             help for list\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai list clusters  - [Deprecated] list all available clusters</li> <li>runai list jobs  - [Deprecated] list all jobs</li> <li>runai list nodes    - [Deprecated] list all nodes</li> <li>runai list projects  - [Deprecated] list all available projects</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_list_clusters/","title":"Runai list clusters","text":""},{"location":"Researcher/cli-reference/new-cli/runai_list_clusters/#runai-list-clusters","title":"runai list clusters","text":"<p>[Deprecated] list all available clusters</p> <pre><code>runai list clusters [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_clusters/#options","title":"Options","text":"<pre><code>  -h, --help         help for clusters\n      --json         Output structure JSON\n      --no-headers   Output structure table without headers\n      --table        Output structure table\n      --yaml         Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_clusters/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_clusters/#see-also","title":"SEE ALSO","text":"<ul> <li>runai list    - [Deprecated] display resource list. By default displays the job list</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_list_jobs/","title":"Runai list jobs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_list_jobs/#runai-list-jobs","title":"runai list jobs","text":"<p>[Deprecated] list all jobs</p> <pre><code>runai list jobs [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_jobs/#options","title":"Options","text":"<pre><code>  -A, --all-projects     list workloads from all projects\n  -h, --help             help for jobs\n      --json             Output structure JSON\n      --no-headers       Output structure table without headers\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_jobs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_jobs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai list    - [Deprecated] display resource list. By default displays the job list</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_list_nodes/","title":"Runai list nodes","text":""},{"location":"Researcher/cli-reference/new-cli/runai_list_nodes/#runai-list-nodes","title":"runai list nodes","text":"<p>[Deprecated] list all nodes</p> <pre><code>runai list nodes [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_nodes/#options","title":"Options","text":"<pre><code>  -h, --help         help for nodes\n      --json         Output structure JSON\n      --no-headers   Output structure table without headers\n      --table        Output structure table\n      --yaml         Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_nodes/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_nodes/#see-also","title":"SEE ALSO","text":"<ul> <li>runai list    - [Deprecated] display resource list. By default displays the job list</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_list_projects/","title":"Runai list projects","text":""},{"location":"Researcher/cli-reference/new-cli/runai_list_projects/#runai-list-projects","title":"runai list projects","text":"<p>[Deprecated] list all available projects</p> <pre><code>runai list projects [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_projects/#options","title":"Options","text":"<pre><code>  -h, --help         help for projects\n      --json         Output structure JSON\n      --no-headers   Output structure table without headers\n      --table        Output structure table\n      --yaml         Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_projects/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_list_projects/#see-also","title":"SEE ALSO","text":"<ul> <li>runai list    - [Deprecated] display resource list. By default displays the job list</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_login/","title":"Runai login","text":""},{"location":"Researcher/cli-reference/new-cli/runai_login/#runai-login","title":"runai login","text":"<p>login to the control plane</p> <pre><code>runai login [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login/#examples","title":"Examples","text":"<pre><code>  # Login using browser\n  runai login\n\n  # Login using SSO without browser\n  runai login sso\n\n  # Login using username and password without browser\n  runai login user -u &lt;username&gt; \n\n  # Login using browser with specific port and host\n  runai login --listen-port=43121 --listen-host=localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login/#options","title":"Options","text":"<pre><code>  -h, --help                 help for login\n      --listen-host string   the host to listen on for the authentication callback (for browser mode only) (default \"localhost\")\n      --listen-port int      the port to listen on for the authentication callback (for browser mode only) (default 43121)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai login application  - login as an application</li> <li>runai login sso  - login using sso without browser</li> <li>runai login user    - login for local user without browser</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_login_application/","title":"Runai login application","text":""},{"location":"Researcher/cli-reference/new-cli/runai_login_application/#runai-login-application","title":"runai login application","text":"<p>login as an application</p> <pre><code>runai login application [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_application/#examples","title":"Examples","text":"<pre><code>  # Login interactive using application credentials\n  runai login app\n\n  # Login using application credentials\n  login app --name=&lt;app_name&gt; --secret=&lt;app_secret&gt; --interactive=disabled\n\n  # Login and Save application credentials\n  login app --name=&lt;app_name&gt; --secret=&lt;app_secret&gt; --interactive=disabled --save\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_application/#options","title":"Options","text":"<pre><code>  -h, --help                 help for application\n      --interactive enable   set interactive mode (enabled|disabled)\n      --name string          application name\n      --save                 save application credentials in config file\n      --secret string        application secret\n      --secret-file string   use application secret from file\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_application/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_application/#see-also","title":"SEE ALSO","text":"<ul> <li>runai login  - login to the control plane</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_login_sso/","title":"Runai login sso","text":""},{"location":"Researcher/cli-reference/new-cli/runai_login_sso/#runai-login-sso","title":"runai login sso","text":"<p>login using sso without browser</p> <pre><code>runai login sso [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_sso/#options","title":"Options","text":"<pre><code>  -h, --help   help for sso\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_sso/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_sso/#see-also","title":"SEE ALSO","text":"<ul> <li>runai login  - login to the control plane</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_login_user/","title":"Runai login user","text":""},{"location":"Researcher/cli-reference/new-cli/runai_login_user/#runai-login-user","title":"runai login user","text":"<p>login for local user without browser</p>"},{"location":"Researcher/cli-reference/new-cli/runai_login_user/#synopsis","title":"Synopsis","text":"<p>Login to the control plane using a local user without browser</p> <pre><code>runai login user [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_user/#examples","title":"Examples","text":"<pre><code># Login with a username. the password will be prompted via stdin afterward (recommended)\nrunai login user -u &lt;username&gt;\n\n# Login with a username and plain password (not recommended for security reasons)\nrunai login user --user=user --password=pass\n\n# Login with a username and password (not recommended for security reasons)\nrunai login user -u=user -p=pass\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_user/#options","title":"Options","text":"<pre><code>  -h, --help              help for user\n  -p, --password string   plaintext password of the given username. not recommended for security reasons\n  -u, --user string       the username to login with\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_user/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_login_user/#see-also","title":"SEE ALSO","text":"<ul> <li>runai login  - login to the control plane</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_logout/","title":"Runai logout","text":""},{"location":"Researcher/cli-reference/new-cli/runai_logout/#runai-logout","title":"runai logout","text":"<p>logout from control plane</p> <pre><code>runai logout [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_logout/#options","title":"Options","text":"<pre><code>  -h, --help   help for logout\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_logout/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_logout/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_logs/","title":"Runai logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_logs/#runai-logs","title":"runai logs","text":"<p>[Deprecated] logs</p> <pre><code>runai logs WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --type string             The type of the workload (training, workspace)\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi/","title":"Runai mpi","text":""},{"location":"Researcher/cli-reference/new-cli/runai_mpi/#runai-mpi","title":"runai mpi","text":"<p>alias for mpi management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi/#options","title":"Options","text":"<pre><code>  -h, --help   help for mpi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai mpi attach    - attach to a running container in a mpi training job</li> <li>runai mpi bash    - open a bash shell in a training mpi job</li> <li>runai mpi delete    - delete mpi training workload</li> <li>runai mpi describe    - describe mpi training</li> <li>runai mpi exec    - execute a command in a training mpi job</li> <li>runai mpi list    - list mpi training</li> <li>runai mpi logs    - view logs of a mpi training job</li> <li>runai mpi port-forward    - forward one or more local ports to a mpi training job</li> <li>runai mpi submit    - submit mpi training</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_attach/","title":"Runai mpi attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_mpi_attach/#runai-mpi-attach","title":"runai mpi attach","text":"<p>attach to a running container in a mpi training job</p> <pre><code>runai mpi attach [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_attach/#examples","title":"Examples","text":"<pre><code># Attaching to the main worker of a mpi training\nrunai training mpi attach mpi-01 --tty --stdin\n\n# Attaching to a specific pod of a mpi training\nrunai training mpi attach mpi-01 --pod mpi-01-worker-1 --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai mpi  - alias for mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_bash/","title":"Runai mpi bash","text":""},{"location":"Researcher/cli-reference/new-cli/runai_mpi_bash/#runai-mpi-bash","title":"runai mpi bash","text":"<p>open a bash shell in a training mpi job</p> <pre><code>runai mpi bash [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_bash/#examples","title":"Examples","text":"<pre><code># Open a bash shell in the training mpi's main worker\nrunai training mpi bash mpi-01\n\n# Open a bash shell in a specific training mpi worker\nrunai training mpi bash mpi-01 --pod mpi-01-worker-1\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_bash/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for bash\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_bash/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_bash/#see-also","title":"SEE ALSO","text":"<ul> <li>runai mpi  - alias for mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_delete/","title":"Runai mpi delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_mpi_delete/#runai-mpi-delete","title":"runai mpi delete","text":"<p>delete mpi training workload</p> <pre><code>runai mpi delete [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai mpi  - alias for mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_describe/","title":"Runai mpi describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_mpi_describe/#runai-mpi-describe","title":"runai mpi describe","text":"<p>describe mpi training</p> <pre><code>runai mpi describe [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_describe/#examples","title":"Examples","text":"<pre><code># Describe a mpi training workload with a default project\nrunai training mpi describe &lt;mpi-name&gt;\n\n# Describe a mpi training workload in a specific project\nrunai training mpi describe &lt;mpi-name&gt; -p &lt;project_name&gt;\n\n# Describe a mpi training workload by UUID\nrunai training mpi describe --uuid=&lt;mpi_uuid&gt;\n\n# Describe a mpi training workload with specific output format\nrunai training mpi describe &lt;mpi-name&gt; -o json\n\n# Describe a mpi training workload with specific sections\nrunai training mpi describe &lt;mpi-name&gt; --general --compute --pods --events --networks\n\n# Describe a mpi training workload with container details and custom limits\nrunai training mpi describe &lt;mpi-name&gt; --containers --pod-limit 20 --event-limit 100\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai mpi  - alias for mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_exec/","title":"Runai mpi exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_mpi_exec/#runai-mpi-exec","title":"runai mpi exec","text":"<p>execute a command in a training mpi job</p> <pre><code>runai mpi exec [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_exec/#examples","title":"Examples","text":"<pre><code># Execute bash in the training mpi's main worker\nrunai training mpi exec mpi-01 --tty --stdin -- /bin/bash \n\n# Execute ls command in the training mpi's main worker\nrunai training mpi exec mpi-01 -- ls\n\n# Execute a command in a specific training mpi worker\nrunai training mpi exec mpi-01 --pod mpi-01-worker-1 -- nvidia-smi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai mpi  - alias for mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_list/","title":"Runai mpi list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_mpi_list/#runai-mpi-list","title":"runai mpi list","text":"<p>list mpi training</p> <pre><code>runai mpi list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_list/#examples","title":"Examples","text":"<pre><code># List all mpi training workloads\nrunai training mpi list -A\n\n# List mpi training workloads with default project\nrunai training mpi list\n\n# List mpi training workloads in a specific project\nrunai training mpi list -p &lt;project_name&gt;\n\n# List all training mpi workloads with a specific output format\nrunai training mpi list -o wide\n\n# List mpi training workloads with pagination\nrunai training mpi list --limit 20 --offset 40\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_list/#options","title":"Options","text":"<pre><code>  -A, --all              list workloads from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list (default 50)\n      --no-headers       Output structure table without headers\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai mpi  - alias for mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_logs/","title":"Runai mpi logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_mpi_logs/#runai-mpi-logs","title":"runai mpi logs","text":"<p>view logs of a mpi training job</p> <pre><code>runai mpi logs [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_logs/#examples","title":"Examples","text":"<pre><code># Get logs for a mpi training\nrunai training mpi logs mpi-01\n\n# Get logs for a specific pod in a mpi training\nrunai training mpi logs mpi-01 --pod=mpi-01-worker-0\n\n# Get logs for a specific container in a mpi training\nrunai training mpi logs mpi-01 --container=mpi-worker\n\n# Get the last 100 lines of logs\nrunai training mpi logs mpi-01 --tail=100\n\n# Get logs with timestamps\nrunai training mpi logs mpi-01 --timestamps\n\n# Follow the logs\nrunai training mpi logs mpi-01 --follow\n\n# Get logs for the previous instance of the training mpi\nrunai training mpi logs mpi-01 --previous\n\n# Limit the logs to 1024 bytes\nrunai training mpi logs mpi-01 --limit-bytes=1024\n\n# Get logs since the last 5 minutes\nrunai training mpi logs mpi-01 --since=300s\n\n# Get logs since a specific timestamp\nrunai training mpi logs mpi-01 --since-time=2023-05-30T10:00:00Z\n\n# Wait up to 30 seconds for mpi training to be ready for logs\nrunai training mpi logs mpi-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai mpi  - alias for mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_port-forward/","title":"Runai mpi port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_mpi_port-forward/#runai-mpi-port-forward","title":"runai mpi port-forward","text":"<p>forward one or more local ports to a mpi training job</p> <pre><code>runai mpi port-forward [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to training mpi on port 8090:\nrunai training mpi port-forward mpi-01 --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to mpi training on port 8080:\nrunai training mpi port-forward mpi-01 --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to mpi training on port 8090 and from localhost:6443 to training mpi on port 443:\nrunai training mpi port-forward mpi-01 --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai mpi  - alias for mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_submit/","title":"Runai mpi submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_mpi_submit/#runai-mpi-submit","title":"runai mpi submit","text":"<p>submit mpi training</p> <pre><code>runai mpi submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_submit/#examples","title":"Examples","text":"<pre><code># Submit a mpi training job\nrunai training mpi submit &lt;name&gt; -p &lt;project_name&gt; -i runai.jfrog.io/demo/quickstart-demo\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --attach                                         If true, wait for the pod to start running, and then attach to the pod as if 'runai attach' was called. Attach makes tty and stdin true by default. Defaults to false\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --master-args                                    Arguments to pass to the master pod container command. If used together with --master-command, overrides the image's entrypoint of the master pod container with the given command\n      --master-environment stringArray                 Set master environment variables in the container\n      --master-extended-resource stringArray           Request access to an extended resource. Use the format: resource_name=quantity\n      --master-no-pvcs                                 Do not mount any persistent volumes in the master pod\n      --mig-profile string                             [Deprecated] MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                NFS storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --pod-running-timeout duration                   Pod check for running state timeout.\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-gid int                                 The group ID the container will run with\n      --run-as-uid int                                 The user ID the container will run with\n      --run-as-user                                    takes the uid, gid, and supplementary groups fields from the token, if all the fields do not exist, uses the local running terminal user credentials. if any of the fields exist take only the existing fields\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --slots-per-worker int32                         Number of slots to allocate for each worker\n      --stdin                                          Keep stdin open on the container(s) in the pod, even if nothing is attached\n      --supplemental-groups ints                       Comma seperated list of groups (IDs) that the user running the container belongs to\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n  -t, --tty                                            Allocate a TTY for the container\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --wait-for-submit duration                       Waiting duration for the workload to be created in the cluster. Defaults to 1 minute (1m)\n      --workers int32                                  the number of workers that will be allocated for running the workload\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_mpi_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai mpi  - alias for mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_node/","title":"Runai node","text":""},{"location":"Researcher/cli-reference/new-cli/runai_node/#runai-node","title":"runai node","text":"<p>node management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_node/#options","title":"Options","text":"<pre><code>  -h, --help   help for node\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_node/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_node/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai node list  - List node</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_node_list/","title":"Runai node list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_node_list/#runai-node-list","title":"runai node list","text":"<p>List node</p> <pre><code>runai node list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_node_list/#options","title":"Options","text":"<pre><code>  -h, --help         help for list\n      --json         Output structure JSON\n      --no-headers   Output structure table without headers\n      --table        Output structure table\n      --yaml         Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_node_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_node_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai node    - node management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool/","title":"Runai nodepool","text":""},{"location":"Researcher/cli-reference/new-cli/runai_nodepool/#runai-nodepool","title":"runai nodepool","text":"<p>node pool management</p> <pre><code>runai nodepool [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool/#options","title":"Options","text":"<pre><code>  -h, --help   help for nodepool\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai nodepool list  - List node pool</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool_list/","title":"Runai nodepool list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_nodepool_list/#runai-nodepool-list","title":"runai nodepool list","text":"<p>List node pool</p> <pre><code>runai nodepool list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool_list/#options","title":"Options","text":"<pre><code>  -h, --help         help for list\n      --json         Output structure JSON\n      --no-headers   Output structure table without headers\n      --table        Output structure table\n      --yaml         Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_nodepool_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai nodepool    - node pool management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_port-forward/","title":"Runai port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_port-forward/#runai-port-forward","title":"runai port-forward","text":"<p>[Deprecated] port forward</p> <pre><code>runai port-forward WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --type string                    The type of the workload (training, workspace)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_project/","title":"Runai project","text":""},{"location":"Researcher/cli-reference/new-cli/runai_project/#runai-project","title":"runai project","text":"<p>project management</p> <pre><code>runai project [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project/#options","title":"Options","text":"<pre><code>  -h, --help                 help for project\n      --interactive enable   set interactive mode (enabled|disabled)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai project list    - list available project</li> <li>runai project set  - set default project name</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_project_list/","title":"Runai project list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_project_list/#runai-project-list","title":"runai project list","text":"<p>list available project</p> <pre><code>runai project list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project_list/#options","title":"Options","text":"<pre><code>  -h, --help         help for list\n      --json         Output structure JSON\n      --no-headers   Output structure table without headers\n      --table        Output structure table\n      --yaml         Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai project  - project management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_project_set/","title":"Runai project set","text":""},{"location":"Researcher/cli-reference/new-cli/runai_project_set/#runai-project-set","title":"runai project set","text":"<p>set default project name</p> <pre><code>runai project set PROJECT_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project_set/#options","title":"Options","text":"<pre><code>  -h, --help   help for set\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project_set/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_project_set/#see-also","title":"SEE ALSO","text":"<ul> <li>runai project  - project management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch/","title":"Runai pytorch","text":""},{"location":"Researcher/cli-reference/new-cli/runai_pytorch/#runai-pytorch","title":"runai pytorch","text":"<p>alias for pytorch management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch/#options","title":"Options","text":"<pre><code>  -h, --help   help for pytorch\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai pytorch attach    - attach to a running container in a pytorch training job</li> <li>runai pytorch bash    - open a bash shell in a training pytorch job</li> <li>runai pytorch delete    - delete pytorch training workload</li> <li>runai pytorch describe    - describe pytorch training</li> <li>runai pytorch exec    - execute a command in a training pytorch job</li> <li>runai pytorch list    - list pytorch training</li> <li>runai pytorch logs    - view logs of a pytorch training job</li> <li>runai pytorch port-forward    - forward one or more local ports to a pytorch training job</li> <li>runai pytorch submit    - submit pytorch training</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_attach/","title":"Runai pytorch attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_attach/#runai-pytorch-attach","title":"runai pytorch attach","text":"<p>attach to a running container in a pytorch training job</p> <pre><code>runai pytorch attach [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_attach/#examples","title":"Examples","text":"<pre><code># Attaching to the main worker of a pytorch training\nrunai training pytorch attach pytorch-01 --tty --stdin\n\n# Attaching to a specific pod of a pytorch training\nrunai training pytorch attach pytorch-01 --pod pytorch-01-worker-1 --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai pytorch  - alias for pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_bash/","title":"Runai pytorch bash","text":""},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_bash/#runai-pytorch-bash","title":"runai pytorch bash","text":"<p>open a bash shell in a training pytorch job</p> <pre><code>runai pytorch bash [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_bash/#examples","title":"Examples","text":"<pre><code># Open a bash shell in the training pytorch's main worker\nrunai training pytorch bash pytorch-01\n\n# Open a bash shell in a specific training pytorch worker\nrunai training pytorch bash pytorch-01 --pod pytorch-01-worker-1\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_bash/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for bash\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_bash/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_bash/#see-also","title":"SEE ALSO","text":"<ul> <li>runai pytorch  - alias for pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_delete/","title":"Runai pytorch delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_delete/#runai-pytorch-delete","title":"runai pytorch delete","text":"<p>delete pytorch training workload</p> <pre><code>runai pytorch delete [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai pytorch  - alias for pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_describe/","title":"Runai pytorch describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_describe/#runai-pytorch-describe","title":"runai pytorch describe","text":"<p>describe pytorch training</p> <pre><code>runai pytorch describe [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_describe/#examples","title":"Examples","text":"<pre><code># Describe a pytorch training workload with a default project\nrunai training pytorch describe &lt;pytorch-name&gt;\n\n# Describe a pytorch training workload in a specific project\nrunai training pytorch describe &lt;pytorch-name&gt; -p &lt;project_name&gt;\n\n# Describe a pytorch training workload by UUID\nrunai training pytorch describe --uuid=&lt;pytorch_uuid&gt;\n\n# Describe a pytorch training workload with specific output format\nrunai training pytorch describe &lt;pytorch-name&gt; -o json\n\n# Describe a pytorch training workload with specific sections\nrunai training pytorch describe &lt;pytorch-name&gt; --general --compute --pods --events --networks\n\n# Describe a pytorch training workload with container details and custom limits\nrunai training pytorch describe &lt;pytorch-name&gt; --containers --pod-limit 20 --event-limit 100\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai pytorch  - alias for pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_exec/","title":"Runai pytorch exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_exec/#runai-pytorch-exec","title":"runai pytorch exec","text":"<p>execute a command in a training pytorch job</p> <pre><code>runai pytorch exec [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_exec/#examples","title":"Examples","text":"<pre><code># Execute bash in the training pytorch's main worker\nrunai training pytorch exec pytorch-01 --tty --stdin -- /bin/bash \n\n# Execute ls command in the training pytorch's main worker\nrunai training pytorch exec pytorch-01 -- ls\n\n# Execute a command in a specific training pytorch worker\nrunai training pytorch exec pytorch-01 --pod pytorch-01-worker-1 -- nvidia-smi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai pytorch  - alias for pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_list/","title":"Runai pytorch list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_list/#runai-pytorch-list","title":"runai pytorch list","text":"<p>list pytorch training</p> <pre><code>runai pytorch list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_list/#examples","title":"Examples","text":"<pre><code># List all pytorch training workloads\nrunai training pytorch list -A\n\n# List pytorch training workloads with default project\nrunai training pytorch list\n\n# List pytorch training workloads in a specific project\nrunai training pytorch list -p &lt;project_name&gt;\n\n# List all training pytorch workloads with a specific output format\nrunai training pytorch list -o wide\n\n# List pytorch training workloads with pagination\nrunai training pytorch list --limit 20 --offset 40\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_list/#options","title":"Options","text":"<pre><code>  -A, --all              list workloads from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list (default 50)\n      --no-headers       Output structure table without headers\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai pytorch  - alias for pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_logs/","title":"Runai pytorch logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_logs/#runai-pytorch-logs","title":"runai pytorch logs","text":"<p>view logs of a pytorch training job</p> <pre><code>runai pytorch logs [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_logs/#examples","title":"Examples","text":"<pre><code># Get logs for a pytorch training\nrunai training pytorch logs pytorch-01\n\n# Get logs for a specific pod in a pytorch training\nrunai training pytorch logs pytorch-01 --pod=pytorch-01-worker-0\n\n# Get logs for a specific container in a pytorch training\nrunai training pytorch logs pytorch-01 --container=pytorch-worker\n\n# Get the last 100 lines of logs\nrunai training pytorch logs pytorch-01 --tail=100\n\n# Get logs with timestamps\nrunai training pytorch logs pytorch-01 --timestamps\n\n# Follow the logs\nrunai training pytorch logs pytorch-01 --follow\n\n# Get logs for the previous instance of the training pytorch\nrunai training pytorch logs pytorch-01 --previous\n\n# Limit the logs to 1024 bytes\nrunai training pytorch logs pytorch-01 --limit-bytes=1024\n\n# Get logs since the last 5 minutes\nrunai training pytorch logs pytorch-01 --since=300s\n\n# Get logs since a specific timestamp\nrunai training pytorch logs pytorch-01 --since-time=2023-05-30T10:00:00Z\n\n# Wait up to 30 seconds for pytorch training to be ready for logs\nrunai training pytorch logs pytorch-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai pytorch  - alias for pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_port-forward/","title":"Runai pytorch port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_port-forward/#runai-pytorch-port-forward","title":"runai pytorch port-forward","text":"<p>forward one or more local ports to a pytorch training job</p> <pre><code>runai pytorch port-forward [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to training pytorch on port 8090:\nrunai training pytorch port-forward pytorch-01 --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to pytorch training on port 8080:\nrunai training pytorch port-forward pytorch-01 --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to pytorch training on port 8090 and from localhost:6443 to training pytorch on port 443:\nrunai training pytorch port-forward pytorch-01 --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai pytorch  - alias for pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_submit/","title":"Runai pytorch submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_submit/#runai-pytorch-submit","title":"runai pytorch submit","text":"<p>submit pytorch training</p> <pre><code>runai pytorch submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_submit/#examples","title":"Examples","text":"<pre><code># Submit a pytorch training job\nrunai training pytorch submit &lt;name&gt; -p &lt;project_name&gt; -i runai.jfrog.io/demo/quickstart-demo\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --attach                                         If true, wait for the pod to start running, and then attach to the pod as if 'runai attach' was called. Attach makes tty and stdin true by default. Defaults to false\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --master-args                                    Arguments to pass to the master pod container command. If used together with --master-command, overrides the image's entrypoint of the master pod container with the given command\n      --master-environment stringArray                 Set master environment variables in the container\n      --master-extended-resource stringArray           Request access to an extended resource. Use the format: resource_name=quantity\n      --master-gpu-devices-request int32               GPU units to allocate for the job (e.g. 1, 2)\n      --master-gpu-portion-limit float                 GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-gpu-portion-request float               GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-no-pvcs                                 Do not mount any persistent volumes in the master pod\n      --max-replicas int32                             Maximum number of replicas for an elastic PyTorch job\n      --mig-profile string                             [Deprecated] MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --min-replicas int32                             Minimum number of replicas for an elastic PyTorch job\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                NFS storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --no-master                                      Do not create a separate pod for the master\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --pod-running-timeout duration                   Pod check for running state timeout.\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-gid int                                 The group ID the container will run with\n      --run-as-uid int                                 The user ID the container will run with\n      --run-as-user                                    takes the uid, gid, and supplementary groups fields from the token, if all the fields do not exist, uses the local running terminal user credentials. if any of the fields exist take only the existing fields\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --stdin                                          Keep stdin open on the container(s) in the pod, even if nothing is attached\n      --supplemental-groups ints                       Comma seperated list of groups (IDs) that the user running the container belongs to\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n  -t, --tty                                            Allocate a TTY for the container\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --wait-for-submit duration                       Waiting duration for the workload to be created in the cluster. Defaults to 1 minute (1m)\n      --workers int32                                  the number of workers that will be allocated for running the workload\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_pytorch_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai pytorch  - alias for pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_report/","title":"Runai report","text":""},{"location":"Researcher/cli-reference/new-cli/runai_report/#runai-report","title":"runai report","text":"<p>[Experimental] report management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_report/#options","title":"Options","text":"<pre><code>  -h, --help   help for report\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai report metrics    - [Experimental] metrics management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics/","title":"Runai report metrics","text":""},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics/#runai-report-metrics","title":"runai report metrics","text":"<p>[Experimental] metrics management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics/#options","title":"Options","text":"<pre><code>  -h, --help   help for metrics\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics/#see-also","title":"SEE ALSO","text":"<ul> <li>runai report    - [Experimental] report management</li> <li>runai report metrics clear    - metrics logs deletion</li> <li>runai report metrics config  - metrics configuration</li> <li>runai report metrics output  - metrics logs output</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_clear/","title":"Runai report metrics clear","text":""},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_clear/#runai-report-metrics-clear","title":"runai report metrics clear","text":"<p>metrics logs deletion</p> <pre><code>runai report metrics clear [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_clear/#options","title":"Options","text":"<pre><code>  -h, --help   help for clear\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_clear/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_clear/#see-also","title":"SEE ALSO","text":"<ul> <li>runai report metrics    - [Experimental] metrics management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_config/","title":"Runai report metrics config","text":""},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_config/#runai-report-metrics-config","title":"runai report metrics config","text":"<p>metrics configuration</p> <pre><code>runai report metrics config [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_config/#options","title":"Options","text":"<pre><code>      --age int          metrics max file age (default 14)\n      --files int        metrics max file number (default 30)\n  -h, --help             help for config\n      --metrics enable   metrics enable flag (enabled|disabled)\n      --size int         metrics max file size (default 10)\n      --type reporter    report generated type (none|logger|local)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_config/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_config/#see-also","title":"SEE ALSO","text":"<ul> <li>runai report metrics    - [Experimental] metrics management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_output/","title":"Runai report metrics output","text":""},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_output/#runai-report-metrics-output","title":"runai report metrics output","text":"<p>metrics logs output</p> <pre><code>runai report metrics output [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_output/#options","title":"Options","text":"<pre><code>  -h, --help       help for output\n      --tail int   number of tail metrics (default 100)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_output/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_report_metrics_output/#see-also","title":"SEE ALSO","text":"<ul> <li>runai report metrics    - [Experimental] metrics management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_submit/","title":"Runai submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_submit/#runai-submit","title":"runai submit","text":"<p>[Deprecated] Submit a new workload</p> <pre><code>runai submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_submit/#options","title":"Options","text":"<pre><code>      --add-capability stringArray                     The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --attach                                         If true, wait for the pod to start running, and then attach to the pod as if 'runai attach' was called. Attach makes tty and stdin true by default. Defaults to false\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --completions int32                              Number of successful pods required for this job to be completed. Used with HPO\n      --configmap-volume stringArray                   Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu float                                      CPU core request (e.g. 0.5, 1)\n      --cpu-limit float                                CPU core limit (e.g. 0.5, 1)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu float                                      GPU units to allocate for the job (e.g. 0.5, 1)\n      --gpu-memory string                              GPU memory to allocate for the job (e.g. 1G, 500M)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --interactive                                    Mark this job as interactive\n      --job-name-prefix string                         Set defined prefix for the workload name and add index as a suffix\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --memory string                                  CPU memory to allocate for the job (e.g. 1G, 500M)\n      --memory-limit string                            CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --mig-profile string                             [Deprecated] MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --parallelism int32                              Number of pods to run in parallel at any given time. Used with HPO\n      --pod-running-timeout duration                   Pod check for running state timeout.\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preemptible                                    Workspace preemptible workloads can be scheduled above guaranteed quota but may be reclaimed at any time\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-gid int                                 The group ID the container will run with\n      --run-as-uid int                                 The user ID the container will run with\n      --run-as-user                                    takes the uid, gid, and supplementary groups fields from the token, if all the fields do not exist, uses the local running terminal user credentials. if any of the fields exist take only the existing fields\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --stdin                                          Keep stdin open on the container(s) in the pod, even if nothing is attached\n      --supplemental-groups ints                       Comma seperated list of groups (IDs) that the user running the container belongs to\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n  -t, --tty                                            Allocate a TTY for the container\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n  -v, --volume stringArray                             Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n      --wait-for-submit duration                       Waiting duration for the workload to be created in the cluster. Defaults to 1 minute (1m)\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow/","title":"Runai tensorflow","text":""},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow/#runai-tensorflow","title":"runai tensorflow","text":"<p>alias for tensorflow management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow/#options","title":"Options","text":"<pre><code>  -h, --help   help for tensorflow\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai tensorflow attach  - attach to a running container in a tf training job</li> <li>runai tensorflow bash  - open a bash shell in a training tf job</li> <li>runai tensorflow delete  - delete tf training workload</li> <li>runai tensorflow describe  - describe tf training</li> <li>runai tensorflow exec  - execute a command in a training tf job</li> <li>runai tensorflow list  - list tf training</li> <li>runai tensorflow logs  - view logs of a tf training job</li> <li>runai tensorflow port-forward  - forward one or more local ports to a tf training job</li> <li>runai tensorflow submit  - submit tf training</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_attach/","title":"Runai tensorflow attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_attach/#runai-tensorflow-attach","title":"runai tensorflow attach","text":"<p>attach to a running container in a tf training job</p> <pre><code>runai tensorflow attach [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_attach/#examples","title":"Examples","text":"<pre><code># Attaching to the main worker of a tf training\nrunai training tf attach tf-01 --tty --stdin\n\n# Attaching to a specific pod of a tf training\nrunai training tf attach tf-01 --pod tf-01-worker-1 --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai tensorflow    - alias for tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_bash/","title":"Runai tensorflow bash","text":""},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_bash/#runai-tensorflow-bash","title":"runai tensorflow bash","text":"<p>open a bash shell in a training tf job</p> <pre><code>runai tensorflow bash [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_bash/#examples","title":"Examples","text":"<pre><code># Open a bash shell in the training tf's main worker\nrunai training tf bash tf-01\n\n# Open a bash shell in a specific training tf worker\nrunai training tf bash tf-01 --pod tf-01-worker-1\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_bash/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for bash\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_bash/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_bash/#see-also","title":"SEE ALSO","text":"<ul> <li>runai tensorflow    - alias for tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_delete/","title":"Runai tensorflow delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_delete/#runai-tensorflow-delete","title":"runai tensorflow delete","text":"<p>delete tf training workload</p> <pre><code>runai tensorflow delete [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai tensorflow    - alias for tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_describe/","title":"Runai tensorflow describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_describe/#runai-tensorflow-describe","title":"runai tensorflow describe","text":"<p>describe tf training</p> <pre><code>runai tensorflow describe [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_describe/#examples","title":"Examples","text":"<pre><code># Describe a tf training workload with a default project\nrunai training tf describe &lt;tf-name&gt;\n\n# Describe a tf training workload in a specific project\nrunai training tf describe &lt;tf-name&gt; -p &lt;project_name&gt;\n\n# Describe a tf training workload by UUID\nrunai training tf describe --uuid=&lt;tf_uuid&gt;\n\n# Describe a tf training workload with specific output format\nrunai training tf describe &lt;tf-name&gt; -o json\n\n# Describe a tf training workload with specific sections\nrunai training tf describe &lt;tf-name&gt; --general --compute --pods --events --networks\n\n# Describe a tf training workload with container details and custom limits\nrunai training tf describe &lt;tf-name&gt; --containers --pod-limit 20 --event-limit 100\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai tensorflow    - alias for tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_exec/","title":"Runai tensorflow exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_exec/#runai-tensorflow-exec","title":"runai tensorflow exec","text":"<p>execute a command in a training tf job</p> <pre><code>runai tensorflow exec [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_exec/#examples","title":"Examples","text":"<pre><code># Execute bash in the training tf's main worker\nrunai training tf exec tf-01 --tty --stdin -- /bin/bash \n\n# Execute ls command in the training tf's main worker\nrunai training tf exec tf-01 -- ls\n\n# Execute a command in a specific training tf worker\nrunai training tf exec tf-01 --pod tf-01-worker-1 -- nvidia-smi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai tensorflow    - alias for tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_list/","title":"Runai tensorflow list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_list/#runai-tensorflow-list","title":"runai tensorflow list","text":"<p>list tf training</p> <pre><code>runai tensorflow list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_list/#examples","title":"Examples","text":"<pre><code># List all tf training workloads\nrunai training tf list -A\n\n# List tf training workloads with default project\nrunai training tf list\n\n# List tf training workloads in a specific project\nrunai training tf list -p &lt;project_name&gt;\n\n# List all training tf workloads with a specific output format\nrunai training tf list -o wide\n\n# List tf training workloads with pagination\nrunai training tf list --limit 20 --offset 40\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_list/#options","title":"Options","text":"<pre><code>  -A, --all              list workloads from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list (default 50)\n      --no-headers       Output structure table without headers\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai tensorflow    - alias for tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_logs/","title":"Runai tensorflow logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_logs/#runai-tensorflow-logs","title":"runai tensorflow logs","text":"<p>view logs of a tf training job</p> <pre><code>runai tensorflow logs [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_logs/#examples","title":"Examples","text":"<pre><code># Get logs for a tf training\nrunai training tf logs tf-01\n\n# Get logs for a specific pod in a tf training\nrunai training tf logs tf-01 --pod=tf-01-worker-0\n\n# Get logs for a specific container in a tf training\nrunai training tf logs tf-01 --container=tf-worker\n\n# Get the last 100 lines of logs\nrunai training tf logs tf-01 --tail=100\n\n# Get logs with timestamps\nrunai training tf logs tf-01 --timestamps\n\n# Follow the logs\nrunai training tf logs tf-01 --follow\n\n# Get logs for the previous instance of the training tf\nrunai training tf logs tf-01 --previous\n\n# Limit the logs to 1024 bytes\nrunai training tf logs tf-01 --limit-bytes=1024\n\n# Get logs since the last 5 minutes\nrunai training tf logs tf-01 --since=300s\n\n# Get logs since a specific timestamp\nrunai training tf logs tf-01 --since-time=2023-05-30T10:00:00Z\n\n# Wait up to 30 seconds for tf training to be ready for logs\nrunai training tf logs tf-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai tensorflow    - alias for tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_port-forward/","title":"Runai tensorflow port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_port-forward/#runai-tensorflow-port-forward","title":"runai tensorflow port-forward","text":"<p>forward one or more local ports to a tf training job</p> <pre><code>runai tensorflow port-forward [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to training tf on port 8090:\nrunai training tf port-forward tf-01 --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to tf training on port 8080:\nrunai training tf port-forward tf-01 --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to tf training on port 8090 and from localhost:6443 to training tf on port 443:\nrunai training tf port-forward tf-01 --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai tensorflow    - alias for tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_submit/","title":"Runai tensorflow submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_submit/#runai-tensorflow-submit","title":"runai tensorflow submit","text":"<p>submit tf training</p> <pre><code>runai tensorflow submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_submit/#examples","title":"Examples","text":"<pre><code># Submit a tf training job\nrunai training tf submit &lt;name&gt; -p &lt;project_name&gt; -i runai.jfrog.io/demo/quickstart-demo\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --attach                                         If true, wait for the pod to start running, and then attach to the pod as if 'runai attach' was called. Attach makes tty and stdin true by default. Defaults to false\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --master-args                                    Arguments to pass to the master pod container command. If used together with --master-command, overrides the image's entrypoint of the master pod container with the given command\n      --master-environment stringArray                 Set master environment variables in the container\n      --master-extended-resource stringArray           Request access to an extended resource. Use the format: resource_name=quantity\n      --master-gpu-devices-request int32               GPU units to allocate for the job (e.g. 1, 2)\n      --master-gpu-portion-limit float                 GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-gpu-portion-request float               GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-no-pvcs                                 Do not mount any persistent volumes in the master pod\n      --max-replicas int32                             Maximum number of replicas for an elastic PyTorch job\n      --mig-profile string                             [Deprecated] MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --min-replicas int32                             Minimum number of replicas for an elastic PyTorch job\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                NFS storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --no-master                                      Do not create a separate pod for the master\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --pod-running-timeout duration                   Pod check for running state timeout.\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-gid int                                 The group ID the container will run with\n      --run-as-uid int                                 The user ID the container will run with\n      --run-as-user                                    takes the uid, gid, and supplementary groups fields from the token, if all the fields do not exist, uses the local running terminal user credentials. if any of the fields exist take only the existing fields\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --stdin                                          Keep stdin open on the container(s) in the pod, even if nothing is attached\n      --supplemental-groups ints                       Comma seperated list of groups (IDs) that the user running the container belongs to\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n  -t, --tty                                            Allocate a TTY for the container\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --wait-for-submit duration                       Waiting duration for the workload to be created in the cluster. Defaults to 1 minute (1m)\n      --workers int32                                  the number of workers that will be allocated for running the workload\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_tensorflow_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai tensorflow    - alias for tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training/","title":"Runai training","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training/#runai-training","title":"runai training","text":"<p>training management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_training/#options","title":"Options","text":"<pre><code>  -h, --help   help for training\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai training attach  - attach to a running container in a standard training job</li> <li>runai training bash  - open a bash shell in a training standard job</li> <li>runai training delete  - delete standard training workload</li> <li>runai training describe  - describe standard training</li> <li>runai training exec  - execute a command in a training standard job</li> <li>runai training list  - list all training frameworks</li> <li>runai training logs  - view logs of a standard training job</li> <li>runai training mpi    - mpi management</li> <li>runai training port-forward  - forward one or more local ports to a standard training job</li> <li>runai training pytorch    - pytorch management</li> <li>runai training resume  - resume standard training</li> <li>runai training standard  - standard training management</li> <li>runai training submit  - submit standard training</li> <li>runai training suspend    - suspend standard training</li> <li>runai training tensorflow  - tensorflow management</li> <li>runai training xgboost    - xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_attach/","title":"Runai training attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_attach/#runai-training-attach","title":"runai training attach","text":"<p>attach to a running container in a standard training job</p> <pre><code>runai training attach [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_attach/#examples","title":"Examples","text":"<pre><code># Attaching to the main worker of a standard training\nrunai training standard attach standard-01 --tty --stdin\n\n# Attaching to a specific pod of a standard training\nrunai training standard attach standard-01 --pod standard-01-worker-1 --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_bash/","title":"Runai training bash","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_bash/#runai-training-bash","title":"runai training bash","text":"<p>open a bash shell in a training standard job</p> <pre><code>runai training bash [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_bash/#examples","title":"Examples","text":"<pre><code># Open a bash shell in the training standard's main worker\nrunai training standard bash standard-01\n\n# Open a bash shell in a specific training standard worker\nrunai training standard bash standard-01 --pod standard-01-worker-1\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_bash/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for bash\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_bash/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_bash/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_delete/","title":"Runai training delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_delete/#runai-training-delete","title":"runai training delete","text":"<p>delete standard training workload</p> <pre><code>runai training delete [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_describe/","title":"Runai training describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_describe/#runai-training-describe","title":"runai training describe","text":"<p>describe standard training</p> <pre><code>runai training describe [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_describe/#examples","title":"Examples","text":"<pre><code># Describe a standard training workload with a default project\nrunai training standard describe &lt;standard-name&gt;\n\n# Describe a standard training workload in a specific project\nrunai training standard describe &lt;standard-name&gt; -p &lt;project_name&gt;\n\n# Describe a standard training workload by UUID\nrunai training standard describe --uuid=&lt;standard_uuid&gt;\n\n# Describe a standard training workload with specific output format\nrunai training standard describe &lt;standard-name&gt; -o json\n\n# Describe a standard training workload with specific sections\nrunai training standard describe &lt;standard-name&gt; --general --compute --pods --events --networks\n\n# Describe a standard training workload with container details and custom limits\nrunai training standard describe &lt;standard-name&gt; --containers --pod-limit 20 --event-limit 100\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_exec/","title":"Runai training exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_exec/#runai-training-exec","title":"runai training exec","text":"<p>execute a command in a training standard job</p> <pre><code>runai training exec [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_exec/#examples","title":"Examples","text":"<pre><code># Execute bash in the training standard's main worker\nrunai training standard exec standard-01 --tty --stdin -- /bin/bash \n\n# Execute ls command in the training standard's main worker\nrunai training standard exec standard-01 -- ls\n\n# Execute a command in a specific training standard worker\nrunai training standard exec standard-01 --pod standard-01-worker-1 -- nvidia-smi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_list/","title":"Runai training list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_list/#runai-training-list","title":"runai training list","text":"<p>list all training frameworks</p> <pre><code>runai training list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_list/#examples","title":"Examples","text":"<pre><code>runai training list -A\nrunai training list --state=&lt;training_state&gt; --limit=20\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_list/#options","title":"Options","text":"<pre><code>  -A, --all                list workloads from all projects\n      --framework string   filter by workload framework\n  -h, --help               help for list\n      --json               Output structure JSON\n      --limit int32        number of workload in list (default 50)\n      --no-headers         Output structure table without headers\n      --offset int32       offset number of limit, default 0 (first offset)\n  -p, --project string     Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string      filter by workload state\n      --table              Output structure table\n      --yaml               Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_logs/","title":"Runai training logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_logs/#runai-training-logs","title":"runai training logs","text":"<p>view logs of a standard training job</p> <pre><code>runai training logs [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_logs/#examples","title":"Examples","text":"<pre><code># Get logs for a standard training\nrunai training standard logs standard-01\n\n# Get logs for a specific pod in a standard training\nrunai training standard logs standard-01 --pod=standard-01-worker-0\n\n# Get logs for a specific container in a standard training\nrunai training standard logs standard-01 --container=standard-worker\n\n# Get the last 100 lines of logs\nrunai training standard logs standard-01 --tail=100\n\n# Get logs with timestamps\nrunai training standard logs standard-01 --timestamps\n\n# Follow the logs\nrunai training standard logs standard-01 --follow\n\n# Get logs for the previous instance of the training standard\nrunai training standard logs standard-01 --previous\n\n# Limit the logs to 1024 bytes\nrunai training standard logs standard-01 --limit-bytes=1024\n\n# Get logs since the last 5 minutes\nrunai training standard logs standard-01 --since=300s\n\n# Get logs since a specific timestamp\nrunai training standard logs standard-01 --since-time=2023-05-30T10:00:00Z\n\n# Wait up to 30 seconds for standard training to be ready for logs\nrunai training standard logs standard-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi/","title":"Runai training mpi","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi/#runai-training-mpi","title":"runai training mpi","text":"<p>mpi management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi/#options","title":"Options","text":"<pre><code>  -h, --help   help for mpi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> <li>runai training mpi attach  - attach to a running container in a mpi training job</li> <li>runai training mpi bash  - open a bash shell in a training mpi job</li> <li>runai training mpi delete  - delete mpi training workload</li> <li>runai training mpi describe  - describe mpi training</li> <li>runai training mpi exec  - execute a command in a training mpi job</li> <li>runai training mpi list  - list mpi training</li> <li>runai training mpi logs  - view logs of a mpi training job</li> <li>runai training mpi port-forward  - forward one or more local ports to a mpi training job</li> <li>runai training mpi submit  - submit mpi training</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_attach/","title":"Runai training mpi attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_attach/#runai-training-mpi-attach","title":"runai training mpi attach","text":"<p>attach to a running container in a mpi training job</p> <pre><code>runai training mpi attach [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_attach/#examples","title":"Examples","text":"<pre><code># Attaching to the main worker of a mpi training\nrunai training mpi attach mpi-01 --tty --stdin\n\n# Attaching to a specific pod of a mpi training\nrunai training mpi attach mpi-01 --pod mpi-01-worker-1 --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_bash/","title":"Runai training mpi bash","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_bash/#runai-training-mpi-bash","title":"runai training mpi bash","text":"<p>open a bash shell in a training mpi job</p> <pre><code>runai training mpi bash [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_bash/#examples","title":"Examples","text":"<pre><code># Open a bash shell in the training mpi's main worker\nrunai training mpi bash mpi-01\n\n# Open a bash shell in a specific training mpi worker\nrunai training mpi bash mpi-01 --pod mpi-01-worker-1\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_bash/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for bash\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_bash/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_bash/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_delete/","title":"Runai training mpi delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_delete/#runai-training-mpi-delete","title":"runai training mpi delete","text":"<p>delete mpi training workload</p> <pre><code>runai training mpi delete [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_describe/","title":"Runai training mpi describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_describe/#runai-training-mpi-describe","title":"runai training mpi describe","text":"<p>describe mpi training</p> <pre><code>runai training mpi describe [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_describe/#examples","title":"Examples","text":"<pre><code># Describe a mpi training workload with a default project\nrunai training mpi describe &lt;mpi-name&gt;\n\n# Describe a mpi training workload in a specific project\nrunai training mpi describe &lt;mpi-name&gt; -p &lt;project_name&gt;\n\n# Describe a mpi training workload by UUID\nrunai training mpi describe --uuid=&lt;mpi_uuid&gt;\n\n# Describe a mpi training workload with specific output format\nrunai training mpi describe &lt;mpi-name&gt; -o json\n\n# Describe a mpi training workload with specific sections\nrunai training mpi describe &lt;mpi-name&gt; --general --compute --pods --events --networks\n\n# Describe a mpi training workload with container details and custom limits\nrunai training mpi describe &lt;mpi-name&gt; --containers --pod-limit 20 --event-limit 100\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_exec/","title":"Runai training mpi exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_exec/#runai-training-mpi-exec","title":"runai training mpi exec","text":"<p>execute a command in a training mpi job</p> <pre><code>runai training mpi exec [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_exec/#examples","title":"Examples","text":"<pre><code># Execute bash in the training mpi's main worker\nrunai training mpi exec mpi-01 --tty --stdin -- /bin/bash \n\n# Execute ls command in the training mpi's main worker\nrunai training mpi exec mpi-01 -- ls\n\n# Execute a command in a specific training mpi worker\nrunai training mpi exec mpi-01 --pod mpi-01-worker-1 -- nvidia-smi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_list/","title":"Runai training mpi list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_list/#runai-training-mpi-list","title":"runai training mpi list","text":"<p>list mpi training</p> <pre><code>runai training mpi list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_list/#examples","title":"Examples","text":"<pre><code># List all mpi training workloads\nrunai training mpi list -A\n\n# List mpi training workloads with default project\nrunai training mpi list\n\n# List mpi training workloads in a specific project\nrunai training mpi list -p &lt;project_name&gt;\n\n# List all training mpi workloads with a specific output format\nrunai training mpi list -o wide\n\n# List mpi training workloads with pagination\nrunai training mpi list --limit 20 --offset 40\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_list/#options","title":"Options","text":"<pre><code>  -A, --all              list workloads from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list (default 50)\n      --no-headers       Output structure table without headers\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_logs/","title":"Runai training mpi logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_logs/#runai-training-mpi-logs","title":"runai training mpi logs","text":"<p>view logs of a mpi training job</p> <pre><code>runai training mpi logs [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_logs/#examples","title":"Examples","text":"<pre><code># Get logs for a mpi training\nrunai training mpi logs mpi-01\n\n# Get logs for a specific pod in a mpi training\nrunai training mpi logs mpi-01 --pod=mpi-01-worker-0\n\n# Get logs for a specific container in a mpi training\nrunai training mpi logs mpi-01 --container=mpi-worker\n\n# Get the last 100 lines of logs\nrunai training mpi logs mpi-01 --tail=100\n\n# Get logs with timestamps\nrunai training mpi logs mpi-01 --timestamps\n\n# Follow the logs\nrunai training mpi logs mpi-01 --follow\n\n# Get logs for the previous instance of the training mpi\nrunai training mpi logs mpi-01 --previous\n\n# Limit the logs to 1024 bytes\nrunai training mpi logs mpi-01 --limit-bytes=1024\n\n# Get logs since the last 5 minutes\nrunai training mpi logs mpi-01 --since=300s\n\n# Get logs since a specific timestamp\nrunai training mpi logs mpi-01 --since-time=2023-05-30T10:00:00Z\n\n# Wait up to 30 seconds for mpi training to be ready for logs\nrunai training mpi logs mpi-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_port-forward/","title":"Runai training mpi port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_port-forward/#runai-training-mpi-port-forward","title":"runai training mpi port-forward","text":"<p>forward one or more local ports to a mpi training job</p> <pre><code>runai training mpi port-forward [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to training mpi on port 8090:\nrunai training mpi port-forward mpi-01 --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to mpi training on port 8080:\nrunai training mpi port-forward mpi-01 --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to mpi training on port 8090 and from localhost:6443 to training mpi on port 443:\nrunai training mpi port-forward mpi-01 --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_submit/","title":"Runai training mpi submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_submit/#runai-training-mpi-submit","title":"runai training mpi submit","text":"<p>submit mpi training</p> <pre><code>runai training mpi submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_submit/#examples","title":"Examples","text":"<pre><code># Submit a mpi training job\nrunai training mpi submit &lt;name&gt; -p &lt;project_name&gt; -i runai.jfrog.io/demo/quickstart-demo\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --attach                                         If true, wait for the pod to start running, and then attach to the pod as if 'runai attach' was called. Attach makes tty and stdin true by default. Defaults to false\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --master-args                                    Arguments to pass to the master pod container command. If used together with --master-command, overrides the image's entrypoint of the master pod container with the given command\n      --master-environment stringArray                 Set master environment variables in the container\n      --master-extended-resource stringArray           Request access to an extended resource. Use the format: resource_name=quantity\n      --master-no-pvcs                                 Do not mount any persistent volumes in the master pod\n      --mig-profile string                             [Deprecated] MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                NFS storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --pod-running-timeout duration                   Pod check for running state timeout.\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-gid int                                 The group ID the container will run with\n      --run-as-uid int                                 The user ID the container will run with\n      --run-as-user                                    takes the uid, gid, and supplementary groups fields from the token, if all the fields do not exist, uses the local running terminal user credentials. if any of the fields exist take only the existing fields\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --slots-per-worker int32                         Number of slots to allocate for each worker\n      --stdin                                          Keep stdin open on the container(s) in the pod, even if nothing is attached\n      --supplemental-groups ints                       Comma seperated list of groups (IDs) that the user running the container belongs to\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n  -t, --tty                                            Allocate a TTY for the container\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --wait-for-submit duration                       Waiting duration for the workload to be created in the cluster. Defaults to 1 minute (1m)\n      --workers int32                                  the number of workers that will be allocated for running the workload\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_mpi_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training mpi    - mpi management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_port-forward/","title":"Runai training port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_port-forward/#runai-training-port-forward","title":"runai training port-forward","text":"<p>forward one or more local ports to a standard training job</p> <pre><code>runai training port-forward [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to training standard on port 8090:\nrunai training standard port-forward standard-01 --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to standard training on port 8080:\nrunai training standard port-forward standard-01 --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to standard training on port 8090 and from localhost:6443 to training standard on port 443:\nrunai training standard port-forward standard-01 --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch/","title":"Runai training pytorch","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch/#runai-training-pytorch","title":"runai training pytorch","text":"<p>pytorch management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch/#options","title":"Options","text":"<pre><code>  -h, --help   help for pytorch\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> <li>runai training pytorch attach  - attach to a running container in a pytorch training job</li> <li>runai training pytorch bash  - open a bash shell in a training pytorch job</li> <li>runai training pytorch delete  - delete pytorch training workload</li> <li>runai training pytorch describe  - describe pytorch training</li> <li>runai training pytorch exec  - execute a command in a training pytorch job</li> <li>runai training pytorch list  - list pytorch training</li> <li>runai training pytorch logs  - view logs of a pytorch training job</li> <li>runai training pytorch port-forward  - forward one or more local ports to a pytorch training job</li> <li>runai training pytorch submit  - submit pytorch training</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_attach/","title":"Runai training pytorch attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_attach/#runai-training-pytorch-attach","title":"runai training pytorch attach","text":"<p>attach to a running container in a pytorch training job</p> <pre><code>runai training pytorch attach [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_attach/#examples","title":"Examples","text":"<pre><code># Attaching to the main worker of a pytorch training\nrunai training pytorch attach pytorch-01 --tty --stdin\n\n# Attaching to a specific pod of a pytorch training\nrunai training pytorch attach pytorch-01 --pod pytorch-01-worker-1 --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training pytorch    - pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_bash/","title":"Runai training pytorch bash","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_bash/#runai-training-pytorch-bash","title":"runai training pytorch bash","text":"<p>open a bash shell in a training pytorch job</p> <pre><code>runai training pytorch bash [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_bash/#examples","title":"Examples","text":"<pre><code># Open a bash shell in the training pytorch's main worker\nrunai training pytorch bash pytorch-01\n\n# Open a bash shell in a specific training pytorch worker\nrunai training pytorch bash pytorch-01 --pod pytorch-01-worker-1\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_bash/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for bash\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_bash/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_bash/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training pytorch    - pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_delete/","title":"Runai training pytorch delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_delete/#runai-training-pytorch-delete","title":"runai training pytorch delete","text":"<p>delete pytorch training workload</p> <pre><code>runai training pytorch delete [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training pytorch    - pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_describe/","title":"Runai training pytorch describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_describe/#runai-training-pytorch-describe","title":"runai training pytorch describe","text":"<p>describe pytorch training</p> <pre><code>runai training pytorch describe [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_describe/#examples","title":"Examples","text":"<pre><code># Describe a pytorch training workload with a default project\nrunai training pytorch describe &lt;pytorch-name&gt;\n\n# Describe a pytorch training workload in a specific project\nrunai training pytorch describe &lt;pytorch-name&gt; -p &lt;project_name&gt;\n\n# Describe a pytorch training workload by UUID\nrunai training pytorch describe --uuid=&lt;pytorch_uuid&gt;\n\n# Describe a pytorch training workload with specific output format\nrunai training pytorch describe &lt;pytorch-name&gt; -o json\n\n# Describe a pytorch training workload with specific sections\nrunai training pytorch describe &lt;pytorch-name&gt; --general --compute --pods --events --networks\n\n# Describe a pytorch training workload with container details and custom limits\nrunai training pytorch describe &lt;pytorch-name&gt; --containers --pod-limit 20 --event-limit 100\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training pytorch    - pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_exec/","title":"Runai training pytorch exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_exec/#runai-training-pytorch-exec","title":"runai training pytorch exec","text":"<p>execute a command in a training pytorch job</p> <pre><code>runai training pytorch exec [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_exec/#examples","title":"Examples","text":"<pre><code># Execute bash in the training pytorch's main worker\nrunai training pytorch exec pytorch-01 --tty --stdin -- /bin/bash \n\n# Execute ls command in the training pytorch's main worker\nrunai training pytorch exec pytorch-01 -- ls\n\n# Execute a command in a specific training pytorch worker\nrunai training pytorch exec pytorch-01 --pod pytorch-01-worker-1 -- nvidia-smi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training pytorch    - pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_list/","title":"Runai training pytorch list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_list/#runai-training-pytorch-list","title":"runai training pytorch list","text":"<p>list pytorch training</p> <pre><code>runai training pytorch list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_list/#examples","title":"Examples","text":"<pre><code># List all pytorch training workloads\nrunai training pytorch list -A\n\n# List pytorch training workloads with default project\nrunai training pytorch list\n\n# List pytorch training workloads in a specific project\nrunai training pytorch list -p &lt;project_name&gt;\n\n# List all training pytorch workloads with a specific output format\nrunai training pytorch list -o wide\n\n# List pytorch training workloads with pagination\nrunai training pytorch list --limit 20 --offset 40\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_list/#options","title":"Options","text":"<pre><code>  -A, --all              list workloads from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list (default 50)\n      --no-headers       Output structure table without headers\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training pytorch    - pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_logs/","title":"Runai training pytorch logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_logs/#runai-training-pytorch-logs","title":"runai training pytorch logs","text":"<p>view logs of a pytorch training job</p> <pre><code>runai training pytorch logs [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_logs/#examples","title":"Examples","text":"<pre><code># Get logs for a pytorch training\nrunai training pytorch logs pytorch-01\n\n# Get logs for a specific pod in a pytorch training\nrunai training pytorch logs pytorch-01 --pod=pytorch-01-worker-0\n\n# Get logs for a specific container in a pytorch training\nrunai training pytorch logs pytorch-01 --container=pytorch-worker\n\n# Get the last 100 lines of logs\nrunai training pytorch logs pytorch-01 --tail=100\n\n# Get logs with timestamps\nrunai training pytorch logs pytorch-01 --timestamps\n\n# Follow the logs\nrunai training pytorch logs pytorch-01 --follow\n\n# Get logs for the previous instance of the training pytorch\nrunai training pytorch logs pytorch-01 --previous\n\n# Limit the logs to 1024 bytes\nrunai training pytorch logs pytorch-01 --limit-bytes=1024\n\n# Get logs since the last 5 minutes\nrunai training pytorch logs pytorch-01 --since=300s\n\n# Get logs since a specific timestamp\nrunai training pytorch logs pytorch-01 --since-time=2023-05-30T10:00:00Z\n\n# Wait up to 30 seconds for pytorch training to be ready for logs\nrunai training pytorch logs pytorch-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training pytorch    - pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_port-forward/","title":"Runai training pytorch port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_port-forward/#runai-training-pytorch-port-forward","title":"runai training pytorch port-forward","text":"<p>forward one or more local ports to a pytorch training job</p> <pre><code>runai training pytorch port-forward [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to training pytorch on port 8090:\nrunai training pytorch port-forward pytorch-01 --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to pytorch training on port 8080:\nrunai training pytorch port-forward pytorch-01 --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to pytorch training on port 8090 and from localhost:6443 to training pytorch on port 443:\nrunai training pytorch port-forward pytorch-01 --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training pytorch    - pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_submit/","title":"Runai training pytorch submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_submit/#runai-training-pytorch-submit","title":"runai training pytorch submit","text":"<p>submit pytorch training</p> <pre><code>runai training pytorch submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_submit/#examples","title":"Examples","text":"<pre><code># Submit a pytorch training job\nrunai training pytorch submit &lt;name&gt; -p &lt;project_name&gt; -i runai.jfrog.io/demo/quickstart-demo\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --attach                                         If true, wait for the pod to start running, and then attach to the pod as if 'runai attach' was called. Attach makes tty and stdin true by default. Defaults to false\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --master-args                                    Arguments to pass to the master pod container command. If used together with --master-command, overrides the image's entrypoint of the master pod container with the given command\n      --master-environment stringArray                 Set master environment variables in the container\n      --master-extended-resource stringArray           Request access to an extended resource. Use the format: resource_name=quantity\n      --master-gpu-devices-request int32               GPU units to allocate for the job (e.g. 1, 2)\n      --master-gpu-portion-limit float                 GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-gpu-portion-request float               GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-no-pvcs                                 Do not mount any persistent volumes in the master pod\n      --max-replicas int32                             Maximum number of replicas for an elastic PyTorch job\n      --mig-profile string                             [Deprecated] MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --min-replicas int32                             Minimum number of replicas for an elastic PyTorch job\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                NFS storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --no-master                                      Do not create a separate pod for the master\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --pod-running-timeout duration                   Pod check for running state timeout.\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-gid int                                 The group ID the container will run with\n      --run-as-uid int                                 The user ID the container will run with\n      --run-as-user                                    takes the uid, gid, and supplementary groups fields from the token, if all the fields do not exist, uses the local running terminal user credentials. if any of the fields exist take only the existing fields\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --stdin                                          Keep stdin open on the container(s) in the pod, even if nothing is attached\n      --supplemental-groups ints                       Comma seperated list of groups (IDs) that the user running the container belongs to\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n  -t, --tty                                            Allocate a TTY for the container\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --wait-for-submit duration                       Waiting duration for the workload to be created in the cluster. Defaults to 1 minute (1m)\n      --workers int32                                  the number of workers that will be allocated for running the workload\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_pytorch_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training pytorch    - pytorch management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_resume/","title":"Runai training resume","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_resume/#runai-training-resume","title":"runai training resume","text":"<p>resume standard training</p> <pre><code>runai training resume [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_resume/#options","title":"Options","text":"<pre><code>  -h, --help             help for resume\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_resume/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_resume/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard/","title":"Runai training standard","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_standard/#runai-training-standard","title":"runai training standard","text":"<p>standard training management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard/#options","title":"Options","text":"<pre><code>  -h, --help   help for standard\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> <li>runai training standard attach    - attach to a running container in a standard training job</li> <li>runai training standard bash    - open a bash shell in a training standard job</li> <li>runai training standard delete    - delete standard training workload</li> <li>runai training standard describe    - describe standard training</li> <li>runai training standard exec    - execute a command in a training standard job</li> <li>runai training standard list    - list standard training</li> <li>runai training standard logs    - view logs of a standard training job</li> <li>runai training standard port-forward    - forward one or more local ports to a standard training job</li> <li>runai training standard resume    - resume standard training</li> <li>runai training standard submit    - submit standard training</li> <li>runai training standard suspend  - suspend standard training</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_attach/","title":"Runai training standard attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_attach/#runai-training-standard-attach","title":"runai training standard attach","text":"<p>attach to a running container in a standard training job</p> <pre><code>runai training standard attach [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_attach/#examples","title":"Examples","text":"<pre><code># Attaching to the main worker of a standard training\nrunai training standard attach standard-01 --tty --stdin\n\n# Attaching to a specific pod of a standard training\nrunai training standard attach standard-01 --pod standard-01-worker-1 --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training standard  - standard training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_bash/","title":"Runai training standard bash","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_bash/#runai-training-standard-bash","title":"runai training standard bash","text":"<p>open a bash shell in a training standard job</p> <pre><code>runai training standard bash [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_bash/#examples","title":"Examples","text":"<pre><code># Open a bash shell in the training standard's main worker\nrunai training standard bash standard-01\n\n# Open a bash shell in a specific training standard worker\nrunai training standard bash standard-01 --pod standard-01-worker-1\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_bash/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for bash\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_bash/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_bash/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training standard  - standard training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_delete/","title":"Runai training standard delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_delete/#runai-training-standard-delete","title":"runai training standard delete","text":"<p>delete standard training workload</p> <pre><code>runai training standard delete [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training standard  - standard training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_describe/","title":"Runai training standard describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_describe/#runai-training-standard-describe","title":"runai training standard describe","text":"<p>describe standard training</p> <pre><code>runai training standard describe [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_describe/#examples","title":"Examples","text":"<pre><code># Describe a standard training workload with a default project\nrunai training standard describe &lt;standard-name&gt;\n\n# Describe a standard training workload in a specific project\nrunai training standard describe &lt;standard-name&gt; -p &lt;project_name&gt;\n\n# Describe a standard training workload by UUID\nrunai training standard describe --uuid=&lt;standard_uuid&gt;\n\n# Describe a standard training workload with specific output format\nrunai training standard describe &lt;standard-name&gt; -o json\n\n# Describe a standard training workload with specific sections\nrunai training standard describe &lt;standard-name&gt; --general --compute --pods --events --networks\n\n# Describe a standard training workload with container details and custom limits\nrunai training standard describe &lt;standard-name&gt; --containers --pod-limit 20 --event-limit 100\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training standard  - standard training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_exec/","title":"Runai training standard exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_exec/#runai-training-standard-exec","title":"runai training standard exec","text":"<p>execute a command in a training standard job</p> <pre><code>runai training standard exec [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_exec/#examples","title":"Examples","text":"<pre><code># Execute bash in the training standard's main worker\nrunai training standard exec standard-01 --tty --stdin -- /bin/bash \n\n# Execute ls command in the training standard's main worker\nrunai training standard exec standard-01 -- ls\n\n# Execute a command in a specific training standard worker\nrunai training standard exec standard-01 --pod standard-01-worker-1 -- nvidia-smi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training standard  - standard training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_list/","title":"Runai training standard list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_list/#runai-training-standard-list","title":"runai training standard list","text":"<p>list standard training</p> <pre><code>runai training standard list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_list/#examples","title":"Examples","text":"<pre><code># List all standard training workloads\nrunai training standard list -A\n\n# List standard training workloads with default project\nrunai training standard list\n\n# List standard training workloads in a specific project\nrunai training standard list -p &lt;project_name&gt;\n\n# List all training standard workloads with a specific output format\nrunai training standard list -o wide\n\n# List standard training workloads with pagination\nrunai training standard list --limit 20 --offset 40\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_list/#options","title":"Options","text":"<pre><code>  -A, --all              list workloads from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list (default 50)\n      --no-headers       Output structure table without headers\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training standard  - standard training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_logs/","title":"Runai training standard logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_logs/#runai-training-standard-logs","title":"runai training standard logs","text":"<p>view logs of a standard training job</p> <pre><code>runai training standard logs [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_logs/#examples","title":"Examples","text":"<pre><code># Get logs for a standard training\nrunai training standard logs standard-01\n\n# Get logs for a specific pod in a standard training\nrunai training standard logs standard-01 --pod=standard-01-worker-0\n\n# Get logs for a specific container in a standard training\nrunai training standard logs standard-01 --container=standard-worker\n\n# Get the last 100 lines of logs\nrunai training standard logs standard-01 --tail=100\n\n# Get logs with timestamps\nrunai training standard logs standard-01 --timestamps\n\n# Follow the logs\nrunai training standard logs standard-01 --follow\n\n# Get logs for the previous instance of the training standard\nrunai training standard logs standard-01 --previous\n\n# Limit the logs to 1024 bytes\nrunai training standard logs standard-01 --limit-bytes=1024\n\n# Get logs since the last 5 minutes\nrunai training standard logs standard-01 --since=300s\n\n# Get logs since a specific timestamp\nrunai training standard logs standard-01 --since-time=2023-05-30T10:00:00Z\n\n# Wait up to 30 seconds for standard training to be ready for logs\nrunai training standard logs standard-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training standard  - standard training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_port-forward/","title":"Runai training standard port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_port-forward/#runai-training-standard-port-forward","title":"runai training standard port-forward","text":"<p>forward one or more local ports to a standard training job</p> <pre><code>runai training standard port-forward [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to training standard on port 8090:\nrunai training standard port-forward standard-01 --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to standard training on port 8080:\nrunai training standard port-forward standard-01 --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to standard training on port 8090 and from localhost:6443 to training standard on port 443:\nrunai training standard port-forward standard-01 --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training standard  - standard training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_resume/","title":"Runai training standard resume","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_resume/#runai-training-standard-resume","title":"runai training standard resume","text":"<p>resume standard training</p> <pre><code>runai training standard resume [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_resume/#options","title":"Options","text":"<pre><code>  -h, --help             help for resume\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_resume/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_resume/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training standard  - standard training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_submit/","title":"Runai training standard submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_submit/#runai-training-standard-submit","title":"runai training standard submit","text":"<p>submit standard training</p> <pre><code>runai training standard submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_submit/#examples","title":"Examples","text":"<pre><code># Submit a standard training job\nrunai training standard submit &lt;name&gt; -p &lt;project_name&gt; -i runai.jfrog.io/demo/quickstart-demo\n# Submit a standard training jupiter notebook\nrunai training standard submit &lt;name&gt; -p &lt;project_name&gt; -i jupyter/scipy-notebook --gpu-devices-request 1 --external-url container=8888 --name-prefix jupyter --command -- start-notebook.sh --NotebookApp.base_url='/${RUNAI_PROJECT}/${RUNAI_JOB_NAME}' --NotebookApp.token='\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --attach                                         If true, wait for the pod to start running, and then attach to the pod as if 'runai attach' was called. Attach makes tty and stdin true by default. Defaults to false\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --completions int32                              Number of successful pods required for this job to be completed. Used with HPO\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --mig-profile string                             [Deprecated] MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                NFS storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --parallelism int32                              Number of pods to run in parallel at any given time. Used with HPO\n      --pod-running-timeout duration                   Pod check for running state timeout.\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-gid int                                 The group ID the container will run with\n      --run-as-uid int                                 The user ID the container will run with\n      --run-as-user                                    takes the uid, gid, and supplementary groups fields from the token, if all the fields do not exist, uses the local running terminal user credentials. if any of the fields exist take only the existing fields\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --stdin                                          Keep stdin open on the container(s) in the pod, even if nothing is attached\n      --supplemental-groups ints                       Comma seperated list of groups (IDs) that the user running the container belongs to\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n  -t, --tty                                            Allocate a TTY for the container\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --wait-for-submit duration                       Waiting duration for the workload to be created in the cluster. Defaults to 1 minute (1m)\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training standard  - standard training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_suspend/","title":"Runai training standard suspend","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_suspend/#runai-training-standard-suspend","title":"runai training standard suspend","text":"<p>suspend standard training</p> <pre><code>runai training standard suspend [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_suspend/#options","title":"Options","text":"<pre><code>  -h, --help             help for suspend\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_suspend/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_standard_suspend/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training standard  - standard training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_submit/","title":"Runai training submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_submit/#runai-training-submit","title":"runai training submit","text":"<p>submit standard training</p> <pre><code>runai training submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_submit/#examples","title":"Examples","text":"<pre><code># Submit a standard training job\nrunai training standard submit &lt;name&gt; -p &lt;project_name&gt; -i runai.jfrog.io/demo/quickstart-demo\n# Submit a standard training jupiter notebook\nrunai training standard submit &lt;name&gt; -p &lt;project_name&gt; -i jupyter/scipy-notebook --gpu-devices-request 1 --external-url container=8888 --name-prefix jupyter --command -- start-notebook.sh --NotebookApp.base_url='/${RUNAI_PROJECT}/${RUNAI_JOB_NAME}' --NotebookApp.token='\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --attach                                         If true, wait for the pod to start running, and then attach to the pod as if 'runai attach' was called. Attach makes tty and stdin true by default. Defaults to false\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --completions int32                              Number of successful pods required for this job to be completed. Used with HPO\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --mig-profile string                             [Deprecated] MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                NFS storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --parallelism int32                              Number of pods to run in parallel at any given time. Used with HPO\n      --pod-running-timeout duration                   Pod check for running state timeout.\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-gid int                                 The group ID the container will run with\n      --run-as-uid int                                 The user ID the container will run with\n      --run-as-user                                    takes the uid, gid, and supplementary groups fields from the token, if all the fields do not exist, uses the local running terminal user credentials. if any of the fields exist take only the existing fields\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --stdin                                          Keep stdin open on the container(s) in the pod, even if nothing is attached\n      --supplemental-groups ints                       Comma seperated list of groups (IDs) that the user running the container belongs to\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n  -t, --tty                                            Allocate a TTY for the container\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --wait-for-submit duration                       Waiting duration for the workload to be created in the cluster. Defaults to 1 minute (1m)\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_suspend/","title":"Runai training suspend","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_suspend/#runai-training-suspend","title":"runai training suspend","text":"<p>suspend standard training</p> <pre><code>runai training suspend [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_suspend/#options","title":"Options","text":"<pre><code>  -h, --help             help for suspend\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_suspend/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_suspend/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow/","title":"Runai training tensorflow","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow/#runai-training-tensorflow","title":"runai training tensorflow","text":"<p>tensorflow management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow/#options","title":"Options","text":"<pre><code>  -h, --help   help for tensorflow\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> <li>runai training tensorflow attach    - attach to a running container in a tf training job</li> <li>runai training tensorflow bash    - open a bash shell in a training tf job</li> <li>runai training tensorflow delete    - delete tf training workload</li> <li>runai training tensorflow describe    - describe tf training</li> <li>runai training tensorflow exec    - execute a command in a training tf job</li> <li>runai training tensorflow list    - list tf training</li> <li>runai training tensorflow logs    - view logs of a tf training job</li> <li>runai training tensorflow port-forward    - forward one or more local ports to a tf training job</li> <li>runai training tensorflow submit    - submit tf training</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_attach/","title":"Runai training tensorflow attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_attach/#runai-training-tensorflow-attach","title":"runai training tensorflow attach","text":"<p>attach to a running container in a tf training job</p> <pre><code>runai training tensorflow attach [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_attach/#examples","title":"Examples","text":"<pre><code># Attaching to the main worker of a tf training\nrunai training tf attach tf-01 --tty --stdin\n\n# Attaching to a specific pod of a tf training\nrunai training tf attach tf-01 --pod tf-01-worker-1 --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training tensorflow  - tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_bash/","title":"Runai training tensorflow bash","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_bash/#runai-training-tensorflow-bash","title":"runai training tensorflow bash","text":"<p>open a bash shell in a training tf job</p> <pre><code>runai training tensorflow bash [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_bash/#examples","title":"Examples","text":"<pre><code># Open a bash shell in the training tf's main worker\nrunai training tf bash tf-01\n\n# Open a bash shell in a specific training tf worker\nrunai training tf bash tf-01 --pod tf-01-worker-1\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_bash/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for bash\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_bash/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_bash/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training tensorflow  - tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_delete/","title":"Runai training tensorflow delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_delete/#runai-training-tensorflow-delete","title":"runai training tensorflow delete","text":"<p>delete tf training workload</p> <pre><code>runai training tensorflow delete [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training tensorflow  - tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_describe/","title":"Runai training tensorflow describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_describe/#runai-training-tensorflow-describe","title":"runai training tensorflow describe","text":"<p>describe tf training</p> <pre><code>runai training tensorflow describe [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_describe/#examples","title":"Examples","text":"<pre><code># Describe a tf training workload with a default project\nrunai training tf describe &lt;tf-name&gt;\n\n# Describe a tf training workload in a specific project\nrunai training tf describe &lt;tf-name&gt; -p &lt;project_name&gt;\n\n# Describe a tf training workload by UUID\nrunai training tf describe --uuid=&lt;tf_uuid&gt;\n\n# Describe a tf training workload with specific output format\nrunai training tf describe &lt;tf-name&gt; -o json\n\n# Describe a tf training workload with specific sections\nrunai training tf describe &lt;tf-name&gt; --general --compute --pods --events --networks\n\n# Describe a tf training workload with container details and custom limits\nrunai training tf describe &lt;tf-name&gt; --containers --pod-limit 20 --event-limit 100\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training tensorflow  - tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_exec/","title":"Runai training tensorflow exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_exec/#runai-training-tensorflow-exec","title":"runai training tensorflow exec","text":"<p>execute a command in a training tf job</p> <pre><code>runai training tensorflow exec [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_exec/#examples","title":"Examples","text":"<pre><code># Execute bash in the training tf's main worker\nrunai training tf exec tf-01 --tty --stdin -- /bin/bash \n\n# Execute ls command in the training tf's main worker\nrunai training tf exec tf-01 -- ls\n\n# Execute a command in a specific training tf worker\nrunai training tf exec tf-01 --pod tf-01-worker-1 -- nvidia-smi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training tensorflow  - tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_list/","title":"Runai training tensorflow list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_list/#runai-training-tensorflow-list","title":"runai training tensorflow list","text":"<p>list tf training</p> <pre><code>runai training tensorflow list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_list/#examples","title":"Examples","text":"<pre><code># List all tf training workloads\nrunai training tf list -A\n\n# List tf training workloads with default project\nrunai training tf list\n\n# List tf training workloads in a specific project\nrunai training tf list -p &lt;project_name&gt;\n\n# List all training tf workloads with a specific output format\nrunai training tf list -o wide\n\n# List tf training workloads with pagination\nrunai training tf list --limit 20 --offset 40\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_list/#options","title":"Options","text":"<pre><code>  -A, --all              list workloads from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list (default 50)\n      --no-headers       Output structure table without headers\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training tensorflow  - tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_logs/","title":"Runai training tensorflow logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_logs/#runai-training-tensorflow-logs","title":"runai training tensorflow logs","text":"<p>view logs of a tf training job</p> <pre><code>runai training tensorflow logs [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_logs/#examples","title":"Examples","text":"<pre><code># Get logs for a tf training\nrunai training tf logs tf-01\n\n# Get logs for a specific pod in a tf training\nrunai training tf logs tf-01 --pod=tf-01-worker-0\n\n# Get logs for a specific container in a tf training\nrunai training tf logs tf-01 --container=tf-worker\n\n# Get the last 100 lines of logs\nrunai training tf logs tf-01 --tail=100\n\n# Get logs with timestamps\nrunai training tf logs tf-01 --timestamps\n\n# Follow the logs\nrunai training tf logs tf-01 --follow\n\n# Get logs for the previous instance of the training tf\nrunai training tf logs tf-01 --previous\n\n# Limit the logs to 1024 bytes\nrunai training tf logs tf-01 --limit-bytes=1024\n\n# Get logs since the last 5 minutes\nrunai training tf logs tf-01 --since=300s\n\n# Get logs since a specific timestamp\nrunai training tf logs tf-01 --since-time=2023-05-30T10:00:00Z\n\n# Wait up to 30 seconds for tf training to be ready for logs\nrunai training tf logs tf-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training tensorflow  - tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_port-forward/","title":"Runai training tensorflow port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_port-forward/#runai-training-tensorflow-port-forward","title":"runai training tensorflow port-forward","text":"<p>forward one or more local ports to a tf training job</p> <pre><code>runai training tensorflow port-forward [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to training tf on port 8090:\nrunai training tf port-forward tf-01 --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to tf training on port 8080:\nrunai training tf port-forward tf-01 --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to tf training on port 8090 and from localhost:6443 to training tf on port 443:\nrunai training tf port-forward tf-01 --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training tensorflow  - tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_submit/","title":"Runai training tensorflow submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_submit/#runai-training-tensorflow-submit","title":"runai training tensorflow submit","text":"<p>submit tf training</p> <pre><code>runai training tensorflow submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_submit/#examples","title":"Examples","text":"<pre><code># Submit a tf training job\nrunai training tf submit &lt;name&gt; -p &lt;project_name&gt; -i runai.jfrog.io/demo/quickstart-demo\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --attach                                         If true, wait for the pod to start running, and then attach to the pod as if 'runai attach' was called. Attach makes tty and stdin true by default. Defaults to false\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --master-args                                    Arguments to pass to the master pod container command. If used together with --master-command, overrides the image's entrypoint of the master pod container with the given command\n      --master-environment stringArray                 Set master environment variables in the container\n      --master-extended-resource stringArray           Request access to an extended resource. Use the format: resource_name=quantity\n      --master-gpu-devices-request int32               GPU units to allocate for the job (e.g. 1, 2)\n      --master-gpu-portion-limit float                 GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-gpu-portion-request float               GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-no-pvcs                                 Do not mount any persistent volumes in the master pod\n      --max-replicas int32                             Maximum number of replicas for an elastic PyTorch job\n      --mig-profile string                             [Deprecated] MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --min-replicas int32                             Minimum number of replicas for an elastic PyTorch job\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                NFS storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --no-master                                      Do not create a separate pod for the master\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --pod-running-timeout duration                   Pod check for running state timeout.\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-gid int                                 The group ID the container will run with\n      --run-as-uid int                                 The user ID the container will run with\n      --run-as-user                                    takes the uid, gid, and supplementary groups fields from the token, if all the fields do not exist, uses the local running terminal user credentials. if any of the fields exist take only the existing fields\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --stdin                                          Keep stdin open on the container(s) in the pod, even if nothing is attached\n      --supplemental-groups ints                       Comma seperated list of groups (IDs) that the user running the container belongs to\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n  -t, --tty                                            Allocate a TTY for the container\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --wait-for-submit duration                       Waiting duration for the workload to be created in the cluster. Defaults to 1 minute (1m)\n      --workers int32                                  the number of workers that will be allocated for running the workload\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_tensorflow_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training tensorflow  - tensorflow management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost/","title":"Runai training xgboost","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost/#runai-training-xgboost","title":"runai training xgboost","text":"<p>xgboost management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost/#options","title":"Options","text":"<pre><code>  -h, --help   help for xgboost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training    - training management</li> <li>runai training xgboost attach  - attach to a running container in a xgboost training job</li> <li>runai training xgboost bash  - open a bash shell in a training xgboost job</li> <li>runai training xgboost delete  - delete xgboost training workload</li> <li>runai training xgboost describe  - describe xgboost training</li> <li>runai training xgboost exec  - execute a command in a training xgboost job</li> <li>runai training xgboost list  - list xgboost training</li> <li>runai training xgboost logs  - view logs of a xgboost training job</li> <li>runai training xgboost port-forward  - forward one or more local ports to a xgboost training job</li> <li>runai training xgboost submit  - submit xgboost training</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_attach/","title":"Runai training xgboost attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_attach/#runai-training-xgboost-attach","title":"runai training xgboost attach","text":"<p>attach to a running container in a xgboost training job</p> <pre><code>runai training xgboost attach [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_attach/#examples","title":"Examples","text":"<pre><code># Attaching to the main worker of a xgboost training\nrunai training xgboost attach xgboost-01 --tty --stdin\n\n# Attaching to a specific pod of a xgboost training\nrunai training xgboost attach xgboost-01 --pod xgboost-01-worker-1 --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training xgboost    - xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_bash/","title":"Runai training xgboost bash","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_bash/#runai-training-xgboost-bash","title":"runai training xgboost bash","text":"<p>open a bash shell in a training xgboost job</p> <pre><code>runai training xgboost bash [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_bash/#examples","title":"Examples","text":"<pre><code># Open a bash shell in the training xgboost's main worker\nrunai training xgboost bash xgboost-01\n\n# Open a bash shell in a specific training xgboost worker\nrunai training xgboost bash xgboost-01 --pod xgboost-01-worker-1\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_bash/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for bash\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_bash/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_bash/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training xgboost    - xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_delete/","title":"Runai training xgboost delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_delete/#runai-training-xgboost-delete","title":"runai training xgboost delete","text":"<p>delete xgboost training workload</p> <pre><code>runai training xgboost delete [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training xgboost    - xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_describe/","title":"Runai training xgboost describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_describe/#runai-training-xgboost-describe","title":"runai training xgboost describe","text":"<p>describe xgboost training</p> <pre><code>runai training xgboost describe [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_describe/#examples","title":"Examples","text":"<pre><code># Describe a xgboost training workload with a default project\nrunai training xgboost describe &lt;xgboost-name&gt;\n\n# Describe a xgboost training workload in a specific project\nrunai training xgboost describe &lt;xgboost-name&gt; -p &lt;project_name&gt;\n\n# Describe a xgboost training workload by UUID\nrunai training xgboost describe --uuid=&lt;xgboost_uuid&gt;\n\n# Describe a xgboost training workload with specific output format\nrunai training xgboost describe &lt;xgboost-name&gt; -o json\n\n# Describe a xgboost training workload with specific sections\nrunai training xgboost describe &lt;xgboost-name&gt; --general --compute --pods --events --networks\n\n# Describe a xgboost training workload with container details and custom limits\nrunai training xgboost describe &lt;xgboost-name&gt; --containers --pod-limit 20 --event-limit 100\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training xgboost    - xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_exec/","title":"Runai training xgboost exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_exec/#runai-training-xgboost-exec","title":"runai training xgboost exec","text":"<p>execute a command in a training xgboost job</p> <pre><code>runai training xgboost exec [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_exec/#examples","title":"Examples","text":"<pre><code># Execute bash in the training xgboost's main worker\nrunai training xgboost exec xgboost-01 --tty --stdin -- /bin/bash \n\n# Execute ls command in the training xgboost's main worker\nrunai training xgboost exec xgboost-01 -- ls\n\n# Execute a command in a specific training xgboost worker\nrunai training xgboost exec xgboost-01 --pod xgboost-01-worker-1 -- nvidia-smi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training xgboost    - xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_list/","title":"Runai training xgboost list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_list/#runai-training-xgboost-list","title":"runai training xgboost list","text":"<p>list xgboost training</p> <pre><code>runai training xgboost list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_list/#examples","title":"Examples","text":"<pre><code># List all xgboost training workloads\nrunai training xgboost list -A\n\n# List xgboost training workloads with default project\nrunai training xgboost list\n\n# List xgboost training workloads in a specific project\nrunai training xgboost list -p &lt;project_name&gt;\n\n# List all training xgboost workloads with a specific output format\nrunai training xgboost list -o wide\n\n# List xgboost training workloads with pagination\nrunai training xgboost list --limit 20 --offset 40\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_list/#options","title":"Options","text":"<pre><code>  -A, --all              list workloads from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list (default 50)\n      --no-headers       Output structure table without headers\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training xgboost    - xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_logs/","title":"Runai training xgboost logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_logs/#runai-training-xgboost-logs","title":"runai training xgboost logs","text":"<p>view logs of a xgboost training job</p> <pre><code>runai training xgboost logs [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_logs/#examples","title":"Examples","text":"<pre><code># Get logs for a xgboost training\nrunai training xgboost logs xgboost-01\n\n# Get logs for a specific pod in a xgboost training\nrunai training xgboost logs xgboost-01 --pod=xgboost-01-worker-0\n\n# Get logs for a specific container in a xgboost training\nrunai training xgboost logs xgboost-01 --container=xgboost-worker\n\n# Get the last 100 lines of logs\nrunai training xgboost logs xgboost-01 --tail=100\n\n# Get logs with timestamps\nrunai training xgboost logs xgboost-01 --timestamps\n\n# Follow the logs\nrunai training xgboost logs xgboost-01 --follow\n\n# Get logs for the previous instance of the training xgboost\nrunai training xgboost logs xgboost-01 --previous\n\n# Limit the logs to 1024 bytes\nrunai training xgboost logs xgboost-01 --limit-bytes=1024\n\n# Get logs since the last 5 minutes\nrunai training xgboost logs xgboost-01 --since=300s\n\n# Get logs since a specific timestamp\nrunai training xgboost logs xgboost-01 --since-time=2023-05-30T10:00:00Z\n\n# Wait up to 30 seconds for xgboost training to be ready for logs\nrunai training xgboost logs xgboost-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training xgboost    - xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_port-forward/","title":"Runai training xgboost port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_port-forward/#runai-training-xgboost-port-forward","title":"runai training xgboost port-forward","text":"<p>forward one or more local ports to a xgboost training job</p> <pre><code>runai training xgboost port-forward [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to training xgboost on port 8090:\nrunai training xgboost port-forward xgboost-01 --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to xgboost training on port 8080:\nrunai training xgboost port-forward xgboost-01 --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to xgboost training on port 8090 and from localhost:6443 to training xgboost on port 443:\nrunai training xgboost port-forward xgboost-01 --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training xgboost    - xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_submit/","title":"Runai training xgboost submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_submit/#runai-training-xgboost-submit","title":"runai training xgboost submit","text":"<p>submit xgboost training</p> <pre><code>runai training xgboost submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_submit/#examples","title":"Examples","text":"<pre><code># Submit a xgboost training job\nrunai training xgboost submit &lt;name&gt; -p &lt;project_name&gt; -i runai.jfrog.io/demo/quickstart-demo\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --attach                                         If true, wait for the pod to start running, and then attach to the pod as if 'runai attach' was called. Attach makes tty and stdin true by default. Defaults to false\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --master-args                                    Arguments to pass to the master pod container command. If used together with --master-command, overrides the image's entrypoint of the master pod container with the given command\n      --master-environment stringArray                 Set master environment variables in the container\n      --master-extended-resource stringArray           Request access to an extended resource. Use the format: resource_name=quantity\n      --master-gpu-devices-request int32               GPU units to allocate for the job (e.g. 1, 2)\n      --master-gpu-portion-limit float                 GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-gpu-portion-request float               GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-no-pvcs                                 Do not mount any persistent volumes in the master pod\n      --mig-profile string                             [Deprecated] MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                NFS storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --pod-running-timeout duration                   Pod check for running state timeout.\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-gid int                                 The group ID the container will run with\n      --run-as-uid int                                 The user ID the container will run with\n      --run-as-user                                    takes the uid, gid, and supplementary groups fields from the token, if all the fields do not exist, uses the local running terminal user credentials. if any of the fields exist take only the existing fields\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --stdin                                          Keep stdin open on the container(s) in the pod, even if nothing is attached\n      --supplemental-groups ints                       Comma seperated list of groups (IDs) that the user running the container belongs to\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n  -t, --tty                                            Allocate a TTY for the container\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --wait-for-submit duration                       Waiting duration for the workload to be created in the cluster. Defaults to 1 minute (1m)\n      --workers int32                                  the number of workers that will be allocated for running the workload\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_training_xgboost_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai training xgboost    - xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_upgrade/","title":"Runai upgrade","text":""},{"location":"Researcher/cli-reference/new-cli/runai_upgrade/#runai-upgrade","title":"runai upgrade","text":"<p>upgrades the CLI to the latest version</p> <pre><code>runai upgrade [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_upgrade/#options","title":"Options","text":"<pre><code>      --force   upgrade CLI without checking for new version\n  -h, --help    help for upgrade\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_upgrade/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_upgrade/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_version/","title":"Runai version","text":""},{"location":"Researcher/cli-reference/new-cli/runai_version/#runai-version","title":"runai version","text":"<p>show the current version of the CLI</p> <pre><code>runai version [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_version/#options","title":"Options","text":"<pre><code>  -h, --help   help for version\n      --wide   print full version details\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_version/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_version/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_whoami/","title":"Runai whoami","text":""},{"location":"Researcher/cli-reference/new-cli/runai_whoami/#runai-whoami","title":"runai whoami","text":"<p>show the current logged in user</p> <pre><code>runai whoami [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_whoami/#options","title":"Options","text":"<pre><code>  -h, --help   help for whoami\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_whoami/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_whoami/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload/","title":"Runai workload","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload/#runai-workload","title":"runai workload","text":"<p>workload management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_workload/#options","title":"Options","text":"<pre><code>  -h, --help                 help for workload\n      --interactive enable   set interactive mode (enabled|disabled)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai workload attach  - Attach to a process that is already running inside an existing container.</li> <li>runai workload describe  - Describe a workload</li> <li>runai workload exec  - exec management</li> <li>runai workload list  - List workloads</li> <li>runai workload logs  - logs management</li> <li>runai workload port-forward  - port forward management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_attach/","title":"Runai workload attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload_attach/#runai-workload-attach","title":"runai workload attach","text":"<p>Attach to a process that is already running inside an existing container.</p> <pre><code>runai workload attach WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_attach/#examples","title":"Examples","text":"<pre><code># Attaching to ubuntu workspace \nrunai workload attach ubuntu-wl --type workspace --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --type string                    The type of the workload (training, workspace)\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workload    - workload management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_describe/","title":"Runai workload describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload_describe/#runai-workload-describe","title":"runai workload describe","text":"<p>Describe a workload</p> <pre><code>runai workload describe WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --framework string    filter by workload framework\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --type string         The type of the workload (training, workspace)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workload    - workload management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_exec/","title":"Runai workload exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload_exec/#runai-workload-exec","title":"runai workload exec","text":"<p>exec management</p> <pre><code>runai workload exec WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_exec/#examples","title":"Examples","text":"<pre><code># Execute bash to workspace \nrunai workload exec jup --type workspace --tty --stdin -- /bin/bash \n\n# Execute ls to workload\nrunai workload exec jup --type workspace -- ls\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --type string                    The type of the workload (training, workspace)\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workload    - workload management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_list/","title":"Runai workload list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload_list/#runai-workload-list","title":"runai workload list","text":"<p>List workloads</p> <pre><code>runai workload list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_list/#options","title":"Options","text":"<pre><code>  -A, --all                list workloads from all projects\n      --framework string   filter by workload framework\n  -h, --help               help for list\n      --json               Output structure JSON\n      --limit int32        number of workload in list (default 50)\n      --no-headers         Output structure table without headers\n      --offset int32       offset number of limit, default 0 (first offset)\n  -p, --project string     Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string      filter by workload state\n      --table              Output structure table\n      --type string        filter by workload type\n      --yaml               Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workload    - workload management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_logs/","title":"Runai workload logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload_logs/#runai-workload-logs","title":"runai workload logs","text":"<p>logs management</p> <pre><code>runai workload logs WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_logs/#examples","title":"Examples","text":"<pre><code>  # Get logs for a workspace\n  runai workload logs workspace-01 --type=workspace\n\n  # Get logs for a specific pod in a workspace\n  runai workload logs workspace-01 --type=workspace --pod=workspace-01-0\n\n  # Get logs for a specific container in a workspace\n  runai workload logs workspace-01 --type=workspace --container=container-01\n\n  # Get the last 100 lines of logs\n  runai workload logs workspace-01 --type=workspace --tail=100\n\n  # Get logs with timestamps\n  runai workload logs workspace-01 --type=workspace --timestamps\n\n  # Follow the logs\n  runai workload logs workspace-01 --type=workspace --follow\n\n  # Get logs for the previous instance of the workspace\n  runai workload logs workspace-01 --type=workspace --previous\n\n  # GetLimit the logs to 1024 bytes\n  runai workload logs workspace-01 --type=workspace --limit-bytes=1024\n\n  # Get logs since the last 5 minutes\n  runai workload logs workspace-01 --type=workspace --since=5m\n\n  # Get logs since a specific timestamp\n  runai workload logs workspace-01 --type=workspace --since-time=2023-05-30T10:00:00Z\n\n  # Wait up to 30 seconds for workload to be ready for logs\n  runai workload logs workspace-01 --type=workspace --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --type string             The type of the workload (training, workspace)\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workload    - workload management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_port-forward/","title":"Runai workload port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workload_port-forward/#runai-workload-port-forward","title":"runai workload port-forward","text":"<p>port forward management</p> <pre><code>runai workload port-forward WORKLOAD_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to &lt;workload-name&gt; on port 8090:\nrunai workload port-forward &lt;workload-name&gt; --type=&lt;workload-type&gt; --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to &lt;job-name&gt; on port 8080:\nrunai workload port-forward &lt;workload-name&gt; --type=&lt;workload-type&gt; --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to &lt;workload-name&gt; on port 8090 and from localhost:6443 to &lt;workload-name&gt; on port 443:\nrunai workload port-forward &lt;workload-name&gt; --type=&lt;workload-type&gt; --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --type string                    The type of the workload (training, workspace)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workload_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workload    - workload management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace/","title":"Runai workspace","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace/#runai-workspace","title":"runai workspace","text":"<p>workspace management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace/#options","title":"Options","text":"<pre><code>  -h, --help                 help for workspace\n      --interactive enable   set interactive mode (enabled|disabled)\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai workspace attach    - Attach to a process that is already running inside an existing container.</li> <li>runai workspace bash    - open a bash shell in a job</li> <li>runai workspace delete    - delete workspace</li> <li>runai workspace describe    - Describe a training workload</li> <li>runai workspace exec    - exec management</li> <li>runai workspace list    - list workspace</li> <li>runai workspace logs    - logs management</li> <li>runai workspace port-forward    - port forward management</li> <li>runai workspace resume    - resume workspace</li> <li>runai workspace submit    - submit workspace</li> <li>runai workspace suspend  - suspend workspace</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_attach/","title":"Runai workspace attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_attach/#runai-workspace-attach","title":"runai workspace attach","text":"<p>Attach to a process that is already running inside an existing container.</p> <pre><code>runai workspace attach WORKSPACE_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_attach/#examples","title":"Examples","text":"<pre><code># Attaching to ubuntu workspace \nrunai workspace attach ubuntu-wl --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_bash/","title":"Runai workspace bash","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_bash/#runai-workspace-bash","title":"runai workspace bash","text":"<p>open a bash shell in a job</p> <pre><code>runai workspace bash WORKSPACE_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_bash/#examples","title":"Examples","text":"<pre><code># Open a bash shell in the main worker\nrunai workspace bash workspace-01\n\n# Open a bash shell in a specific worker\nrunai workspace bash workspace-01 --pod workspace-01-worker-1\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_bash/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for bash\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_bash/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_bash/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_delete/","title":"Runai workspace delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_delete/#runai-workspace-delete","title":"runai workspace delete","text":"<p>delete workspace</p> <pre><code>runai workspace delete [WORKSPACE_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_delete/#examples","title":"Examples","text":"<pre><code>runai workspace delete &lt;workspace_name&gt; (optional)-p=&lt;project_name&gt;\nrunai workspace delete --uuid=&lt;workload_uuid&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_describe/","title":"Runai workspace describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_describe/#runai-workspace-describe","title":"runai workspace describe","text":"<p>Describe a training workload</p> <pre><code>runai workspace describe WORKSPACE_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_exec/","title":"Runai workspace exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_exec/#runai-workspace-exec","title":"runai workspace exec","text":"<p>exec management</p> <pre><code>runai workspace exec WORKSPACE_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_exec/#examples","title":"Examples","text":"<pre><code># Execute bash to workspace \nrunai workspace exec jup --tty --stdin -- /bin/bash \n\n# Execute ls to workload\nrunai workspace exec jup -- ls\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_list/","title":"Runai workspace list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_list/#runai-workspace-list","title":"runai workspace list","text":"<p>list workspace</p> <pre><code>runai workspace list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_list/#examples","title":"Examples","text":"<pre><code>runai workspace list -A\nrunai workspace list --state=&lt;training_state&gt; --limit=20\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_list/#options","title":"Options","text":"<pre><code>  -A, --all              list workloads from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list (default 50)\n      --no-headers       Output structure table without headers\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_logs/","title":"Runai workspace logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_logs/#runai-workspace-logs","title":"runai workspace logs","text":"<p>logs management</p> <pre><code>runai workspace logs WORKSPACE_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_logs/#examples","title":"Examples","text":"<pre><code>  # Get logs for a workspace\n  runai workspace logs workspace-01\n\n  # Get logs for a specific pod in a workspace\n  runai workspace logs workspace-01 --pod=workspace-01-0\n\n  # Get logs for a specific container in a workspace\n  runai workspace logs workspace-01 --container=container-01\n\n  # Get the last 100 lines of logs\n  runai workspace logs workspace-01 --tail=100\n\n  # Get logs with timestamps\n  runai workspace logs workspace-01 --timestamps\n\n  # Follow the logs\n  runai workspace logs workspace-01 --follow\n\n  # Get logs for the previous instance of the workspace\n  runai workspace logs workspace-01 --previous\n\n  # GetLimit the logs to 1024 bytes\n  runai workspace logs workspace-01 --limit-bytes=1024\n\n  # Get logs since the last 5 minutes\n  runai workspace logs workspace-01 --since=300s\n\n  # Get logs since a specific timestamp\n  runai workspace logs workspace-01 --since-time=2023-05-30T10:00:00Z\n\n  # Wait up to 30 seconds for workspace to be ready for logs\n  runai workspace logs workspace-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_port-forward/","title":"Runai workspace port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_port-forward/#runai-workspace-port-forward","title":"runai workspace port-forward","text":"<p>port forward management</p> <pre><code>runai workspace port-forward WORKSPACE_NAME [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to &lt;workspace-name&gt; on port 8090:\nrunai workspace port-forward &lt;workspace-name&gt; --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to &lt;job-name&gt; on port 8080:\nrunai workspace port-forward &lt;workspace-name&gt; --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to &lt;workload-name&gt; on port 8090 and from localhost:6443 to &lt;workspace-name&gt; on port 443:\nrunai workspace port-forward &lt;workload-name&gt; --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_resume/","title":"Runai workspace resume","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_resume/#runai-workspace-resume","title":"runai workspace resume","text":"<p>resume workspace</p> <pre><code>runai workspace resume [WORKSPACE_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_resume/#examples","title":"Examples","text":"<pre><code>runai workspace resume &lt;workspace_name&gt; -p=&lt;project_name&gt;\nrunai workspace resume --uuid=&lt;workspace_uuid&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_resume/#options","title":"Options","text":"<pre><code>  -h, --help             help for resume\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_resume/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_resume/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_submit/","title":"Runai workspace submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_submit/#runai-workspace-submit","title":"runai workspace submit","text":"<p>submit workspace</p> <pre><code>runai workspace submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_submit/#examples","title":"Examples","text":"<pre><code>runai workspace submit &lt;workspace_name&gt; -p=&lt;project_name&gt; -i=runai.jfrog.io/demo/quickstart-demo\nrunai workspace submit --i jupyter/scipy-notebook --gpu-devices-request 1 --external-url container=8888 --name-prefix jupyter --command -- start-notebook.sh --NotebookApp.base_url='/${RUNAI_PROJECT}/${RUNAI_JOB_NAME}' --NotebookApp.token=''\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --attach                                         If true, wait for the pod to start running, and then attach to the pod as if 'runai attach' was called. Attach makes tty and stdin true by default. Defaults to false\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --mig-profile string                             [Deprecated] MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                NFS storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --pod-running-timeout duration                   Pod check for running state timeout.\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preemptible                                    Workspace preemptible workloads can be scheduled above guaranteed quota but may be reclaimed at any time\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-gid int                                 The group ID the container will run with\n      --run-as-uid int                                 The user ID the container will run with\n      --run-as-user                                    takes the uid, gid, and supplementary groups fields from the token, if all the fields do not exist, uses the local running terminal user credentials. if any of the fields exist take only the existing fields\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --stdin                                          Keep stdin open on the container(s) in the pod, even if nothing is attached\n      --supplemental-groups ints                       Comma seperated list of groups (IDs) that the user running the container belongs to\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n  -t, --tty                                            Allocate a TTY for the container\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --wait-for-submit duration                       Waiting duration for the workload to be created in the cluster. Defaults to 1 minute (1m)\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_suspend/","title":"Runai workspace suspend","text":""},{"location":"Researcher/cli-reference/new-cli/runai_workspace_suspend/#runai-workspace-suspend","title":"runai workspace suspend","text":"<p>suspend workspace</p> <pre><code>runai workspace suspend [WORKSPACE_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_suspend/#examples","title":"Examples","text":"<pre><code>runai workspace &lt;workspace_name&gt; -p=&lt;project_name&gt;\nrunai workspace suspend --uuid=&lt;workspace_uuid&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_suspend/#options","title":"Options","text":"<pre><code>  -h, --help             help for suspend\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_suspend/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_workspace_suspend/#see-also","title":"SEE ALSO","text":"<ul> <li>runai workspace  - workspace management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost/","title":"Runai xgboost","text":""},{"location":"Researcher/cli-reference/new-cli/runai_xgboost/#runai-xgboost","title":"runai xgboost","text":"<p>alias for xgboost management</p>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost/#options","title":"Options","text":"<pre><code>  -h, --help   help for xgboost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost/#see-also","title":"SEE ALSO","text":"<ul> <li>runai  - Run:ai Command-line Interface</li> <li>runai xgboost attach    - attach to a running container in a xgboost training job</li> <li>runai xgboost bash    - open a bash shell in a training xgboost job</li> <li>runai xgboost delete    - delete xgboost training workload</li> <li>runai xgboost describe    - describe xgboost training</li> <li>runai xgboost exec    - execute a command in a training xgboost job</li> <li>runai xgboost list    - list xgboost training</li> <li>runai xgboost logs    - view logs of a xgboost training job</li> <li>runai xgboost port-forward    - forward one or more local ports to a xgboost training job</li> <li>runai xgboost submit    - submit xgboost training</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_attach/","title":"Runai xgboost attach","text":""},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_attach/#runai-xgboost-attach","title":"runai xgboost attach","text":"<p>attach to a running container in a xgboost training job</p> <pre><code>runai xgboost attach [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_attach/#examples","title":"Examples","text":"<pre><code># Attaching to the main worker of a xgboost training\nrunai training xgboost attach xgboost-01 --tty --stdin\n\n# Attaching to a specific pod of a xgboost training\nrunai training xgboost attach xgboost-01 --pod xgboost-01-worker-1 --tty --stdin\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_attach/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for attach\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_attach/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_attach/#see-also","title":"SEE ALSO","text":"<ul> <li>runai xgboost  - alias for xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_bash/","title":"Runai xgboost bash","text":""},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_bash/#runai-xgboost-bash","title":"runai xgboost bash","text":"<p>open a bash shell in a training xgboost job</p> <pre><code>runai xgboost bash [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_bash/#examples","title":"Examples","text":"<pre><code># Open a bash shell in the training xgboost's main worker\nrunai training xgboost bash xgboost-01\n\n# Open a bash shell in a specific training xgboost worker\nrunai training xgboost bash xgboost-01 --pod xgboost-01-worker-1\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_bash/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for bash\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_bash/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_bash/#see-also","title":"SEE ALSO","text":"<ul> <li>runai xgboost  - alias for xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_delete/","title":"Runai xgboost delete","text":""},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_delete/#runai-xgboost-delete","title":"runai xgboost delete","text":"<p>delete xgboost training workload</p> <pre><code>runai xgboost delete [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_delete/#options","title":"Options","text":"<pre><code>  -h, --help             help for delete\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -u, --uuid string      The UUID of the workload\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_delete/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_delete/#see-also","title":"SEE ALSO","text":"<ul> <li>runai xgboost  - alias for xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_describe/","title":"Runai xgboost describe","text":""},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_describe/#runai-xgboost-describe","title":"runai xgboost describe","text":"<p>describe xgboost training</p> <pre><code>runai xgboost describe [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_describe/#examples","title":"Examples","text":"<pre><code># Describe a xgboost training workload with a default project\nrunai training xgboost describe &lt;xgboost-name&gt;\n\n# Describe a xgboost training workload in a specific project\nrunai training xgboost describe &lt;xgboost-name&gt; -p &lt;project_name&gt;\n\n# Describe a xgboost training workload by UUID\nrunai training xgboost describe --uuid=&lt;xgboost_uuid&gt;\n\n# Describe a xgboost training workload with specific output format\nrunai training xgboost describe &lt;xgboost-name&gt; -o json\n\n# Describe a xgboost training workload with specific sections\nrunai training xgboost describe &lt;xgboost-name&gt; --general --compute --pods --events --networks\n\n# Describe a xgboost training workload with container details and custom limits\nrunai training xgboost describe &lt;xgboost-name&gt; --containers --pod-limit 20 --event-limit 100\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_describe/#options","title":"Options","text":"<pre><code>      --compute             Show compute information (default true)\n      --containers          Include container information in pods\n      --event-limit int32   Limit the number of events displayed (-1 for no limit) (default 50)\n      --events              Show events information (default true)\n      --general             Show general information (default true)\n  -h, --help                help for describe\n      --networks            Show networks information (default true)\n  -o, --output string       Output format (table, json, yaml) (default \"table\")\n      --pod-limit int32     Limit the number of pods displayed (-1 for no limit) (default 10)\n      --pods                Show pods information (default true)\n  -p, --project string      Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_describe/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_describe/#see-also","title":"SEE ALSO","text":"<ul> <li>runai xgboost  - alias for xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_exec/","title":"Runai xgboost exec","text":""},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_exec/#runai-xgboost-exec","title":"runai xgboost exec","text":"<p>execute a command in a training xgboost job</p> <pre><code>runai xgboost exec [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_exec/#examples","title":"Examples","text":"<pre><code># Execute bash in the training xgboost's main worker\nrunai training xgboost exec xgboost-01 --tty --stdin -- /bin/bash \n\n# Execute ls command in the training xgboost's main worker\nrunai training xgboost exec xgboost-01 -- ls\n\n# Execute a command in a specific training xgboost worker\nrunai training xgboost exec xgboost-01 --pod xgboost-01-worker-1 -- nvidia-smi\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_exec/#options","title":"Options","text":"<pre><code>  -c, --container string               Container name for log extraction\n  -h, --help                           help for exec\n      --pod string                     Workload pod ID for log extraction, default: master (0-0)\n      --pod-running-timeout duration   Pod check for running state timeout.\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n  -i, --stdin                          Pass stdin to the container\n  -t, --tty                            Stdin is a TTY\n      --wait-timeout duration          Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_exec/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_exec/#see-also","title":"SEE ALSO","text":"<ul> <li>runai xgboost  - alias for xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_list/","title":"Runai xgboost list","text":""},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_list/#runai-xgboost-list","title":"runai xgboost list","text":"<p>list xgboost training</p> <pre><code>runai xgboost list [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_list/#examples","title":"Examples","text":"<pre><code># List all xgboost training workloads\nrunai training xgboost list -A\n\n# List xgboost training workloads with default project\nrunai training xgboost list\n\n# List xgboost training workloads in a specific project\nrunai training xgboost list -p &lt;project_name&gt;\n\n# List all training xgboost workloads with a specific output format\nrunai training xgboost list -o wide\n\n# List xgboost training workloads with pagination\nrunai training xgboost list --limit 20 --offset 40\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_list/#options","title":"Options","text":"<pre><code>  -A, --all              list workloads from all projects\n  -h, --help             help for list\n      --json             Output structure JSON\n      --limit int32      number of workload in list (default 50)\n      --no-headers       Output structure table without headers\n      --offset int32     offset number of limit, default 0 (first offset)\n  -p, --project string   Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --status string    filter by workload state\n      --table            Output structure table\n      --yaml             Output structure YAML\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_list/#see-also","title":"SEE ALSO","text":"<ul> <li>runai xgboost  - alias for xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_logs/","title":"Runai xgboost logs","text":""},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_logs/#runai-xgboost-logs","title":"runai xgboost logs","text":"<p>view logs of a xgboost training job</p> <pre><code>runai xgboost logs [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_logs/#examples","title":"Examples","text":"<pre><code># Get logs for a xgboost training\nrunai training xgboost logs xgboost-01\n\n# Get logs for a specific pod in a xgboost training\nrunai training xgboost logs xgboost-01 --pod=xgboost-01-worker-0\n\n# Get logs for a specific container in a xgboost training\nrunai training xgboost logs xgboost-01 --container=xgboost-worker\n\n# Get the last 100 lines of logs\nrunai training xgboost logs xgboost-01 --tail=100\n\n# Get logs with timestamps\nrunai training xgboost logs xgboost-01 --timestamps\n\n# Follow the logs\nrunai training xgboost logs xgboost-01 --follow\n\n# Get logs for the previous instance of the training xgboost\nrunai training xgboost logs xgboost-01 --previous\n\n# Limit the logs to 1024 bytes\nrunai training xgboost logs xgboost-01 --limit-bytes=1024\n\n# Get logs since the last 5 minutes\nrunai training xgboost logs xgboost-01 --since=300s\n\n# Get logs since a specific timestamp\nrunai training xgboost logs xgboost-01 --since-time=2023-05-30T10:00:00Z\n\n# Wait up to 30 seconds for xgboost training to be ready for logs\nrunai training xgboost logs xgboost-01 --wait-timeout=30s\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_logs/#options","title":"Options","text":"<pre><code>  -c, --container string        Container name for log extraction\n  -f, --follow                  Follow log output\n  -h, --help                    help for logs\n      --limit-bytes int         Limit the number of bytes returned from the server\n      --name string             Set workload name for log extraction\n      --pod string              Workload pod ID for log extraction, default: master (0-0)\n      --previous                Show previous pod log output\n  -p, --project string          Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --since duration          Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs\n      --since-time string       Return logs after a specific date (RFC3339)\n  -t, --tail int                Numer of tailed lines to fetch from the log, for no limit set to -1 (default -1)\n      --timestamps              Show timestamps in log output\n      --wait-timeout duration   Timeout for waiting for workload to be ready for log streaming\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_logs/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_logs/#see-also","title":"SEE ALSO","text":"<ul> <li>runai xgboost  - alias for xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_port-forward/","title":"Runai xgboost port forward","text":""},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_port-forward/#runai-xgboost-port-forward","title":"runai xgboost port-forward","text":"<p>forward one or more local ports to a xgboost training job</p> <pre><code>runai xgboost port-forward [WORKLOAD_NAME] [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_port-forward/#examples","title":"Examples","text":"<pre><code># Forward connections from localhost:8080 to training xgboost on port 8090:\nrunai training xgboost port-forward xgboost-01 --port 8080:8090 --address localhost\n\n# Forward connections from 0.0.0.0:8080 to xgboost training on port 8080:\nrunai training xgboost port-forward xgboost-01 --port 8080 --address 0.0.0.0 [requires privileges]\n\n# Forward multiple connections from localhost:8080 to xgboost training on port 8090 and from localhost:6443 to training xgboost on port 443:\nrunai training xgboost port-forward xgboost-01 --port 8080:8090 --port 6443:443 --address localhost\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_port-forward/#options","title":"Options","text":"<pre><code>      --address string                 --address [local-interface-ip\\host] --address localhost --address 0.0.0.0 [privileged] (default \"localhost\")\n  -h, --help                           help for port-forward\n      --pod string                     Workload pod ID for port-forward, default: distributed(master) otherwise(random)\n      --pod-running-timeout duration   Pod check for running state timeout.\n      --port stringArray               port\n  -p, --project string                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_port-forward/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_port-forward/#see-also","title":"SEE ALSO","text":"<ul> <li>runai xgboost  - alias for xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_submit/","title":"Runai xgboost submit","text":""},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_submit/#runai-xgboost-submit","title":"runai xgboost submit","text":"<p>submit xgboost training</p> <pre><code>runai xgboost submit [flags]\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_submit/#examples","title":"Examples","text":"<pre><code># Submit a xgboost training job\nrunai training xgboost submit &lt;name&gt; -p &lt;project_name&gt; -i runai.jfrog.io/demo/quickstart-demo\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_submit/#options","title":"Options","text":"<pre><code>      --allow-privilege-escalation                     Allow the job to gain additional privileges after starting\n      --annotation stringArray                         Set of annotations to populate into the container running the workspace\n      --attach                                         If true, wait for the pod to start running, and then attach to the pod as if 'runai attach' was called. Attach makes tty and stdin true by default. Defaults to false\n      --auto-deletion-time-after-completion duration   The length of time (like 5s, 2m, or 3h, higher than zero) after which a completed job is automatically deleted (default 0s)\n      --backoff-limit int                              The number of times the job will be retried before failing\n      --capability stringArray                         The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime.\n  -c, --command                                        If true, override the image's entrypoint with the command supplied after '--'\n      --configmap-map-volume stringArray               Mount ConfigMap as a volume. Use the fhe format name=CONFIGMAP_NAME,path=PATH\n      --cpu-core-limit float                           CPU core limit (e.g. 0.5, 1)\n      --cpu-core-request float                         CPU core request (e.g. 0.5, 1)\n      --cpu-memory-limit string                        CPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --cpu-memory-request string                      CPU memory to allocate for the job (e.g. 1G, 500M)\n      --create-home-dir                                Create a temporary home directory. Defaults to true when --run-as-user is set, false otherwise\n  -e, --environment stringArray                        Set environment variables in the container\n      --existing-pvc stringArray                       Mount an existing persistent volume. Use the format: claimname=CLAIM_NAME,path=PATH\n      --extended-resource stringArray                  Request access to an extended resource. Use the format: resource_name=quantity\n      --external-url stringArray                       Expose URL from the job container. Use the format: container=9443,url=https://external.runai.com,authusers=user1,authgroups=group1\n      --git-sync stringArray                           Specifies git repositories to mount into the container. Use the format: name=NAME,repository=REPO,path=PATH,secret=SECRET,rev=REVISION\n  -g, --gpu-devices-request int32                      GPU units to allocate for the job (e.g. 1, 2)\n      --gpu-memory-limit string                        GPU memory limit to allocate for the job (e.g. 1G, 500M)\n      --gpu-memory-request string                      GPU memory to allocate for the job (e.g. 1G, 500M)\n      --gpu-portion-limit float                        GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-portion-request float                      GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --gpu-request-type string                        GPU request type (portion|memory|migProfile)\n  -h, --help                                           help for submit\n      --host-ipc                                       Whether to enable host IPC. (Default: false)\n      --host-network                                   Whether to enable host networking. (Default: false)\n      --host-path stringArray                          Volumes to mount into the container. Use the format: path=PATH,mount=MOUNT,mount-propagation=None|HostToContainer,readwrite\n  -i, --image string                                   The image for the workload\n      --image-pull-policy string                       Set image pull policy. One of: Always, IfNotPresent, Never. Defaults to Always (default \"Always\")\n      --label stringArray                              Set of labels to populate into the container running the workspace\n      --large-shm                                      Request large /dev/shm device to mount\n      --master-args                                    Arguments to pass to the master pod container command. If used together with --master-command, overrides the image's entrypoint of the master pod container with the given command\n      --master-environment stringArray                 Set master environment variables in the container\n      --master-extended-resource stringArray           Request access to an extended resource. Use the format: resource_name=quantity\n      --master-gpu-devices-request int32               GPU units to allocate for the job (e.g. 1, 2)\n      --master-gpu-portion-limit float                 GPU portion limit, must be no less than the gpu-memory-request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-gpu-portion-request float               GPU portion request (between 0 and 1, e.g. 0.5, 0.2)\n      --master-no-pvcs                                 Do not mount any persistent volumes in the master pod\n      --mig-profile string                             [Deprecated] MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)\n      --name-prefix string                             Set defined prefix for the workload name and add index as a suffix\n      --new-pvc stringArray                            Mount a persistent volume, create it if it does not exist. Use the format: claimname=CLAIM_NAME,storageclass=STORAGE_CLASS,size=SIZE,path=PATH,accessmode-rwo,accessmode-rom,accessmode-rwm,ro,ephemeral\n      --nfs stringArray                                NFS storage details. Use the format: path=PATH,server=SERVER,mountpath=MOUNT_PATH,readwrite\n      --node-pools stringArray                         List of node pools to use for scheduling the job, ordered by priority\n      --node-type string                               Enforce node type affinity by setting a node-type label\n      --pod-running-timeout duration                   Pod check for running state timeout.\n      --port stringArray                               Expose ports from the job container. Use the format: service-type=NodePort,container=80,external=8080\n      --preferred-pod-topology-key string              If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values\n  -p, --project string                                 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use \u2018runai config project &lt;project name&gt;\u2019\n      --required-pod-topology-key string               Enforce scheduling pods of this job onto nodes that have a label with this key and identical values\n      --run-as-gid int                                 The group ID the container will run with\n      --run-as-uid int                                 The user ID the container will run with\n      --run-as-user                                    takes the uid, gid, and supplementary groups fields from the token, if all the fields do not exist, uses the local running terminal user credentials. if any of the fields exist take only the existing fields\n      --s3 stringArray                                 s3 storage details. Use the format: name=NAME,bucket=BUCKET,path=PATH,accesskey=ACCESS_KEY,url=URL\n      --seccomp-profile string                         Indicates which kind of seccomp profile will be applied to the container, options: RuntimeDefault|Unconfined|Localhost\n      --stdin                                          Keep stdin open on the container(s) in the pod, even if nothing is attached\n      --supplemental-groups ints                       Comma seperated list of groups (IDs) that the user running the container belongs to\n      --toleration stringArray                         Toleration details. Use the format: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n  -t, --tty                                            Allocate a TTY for the container\n      --user-group-source string                       Indicate the way to determine the user and group ids of the container, options: fromTheImage|fromIdpToken|fromIdpToken\n      --wait-for-submit duration                       Waiting duration for the workload to be created in the cluster. Defaults to 1 minute (1m)\n      --workers int32                                  the number of workers that will be allocated for running the workload\n      --working-dir string                             Set the container's working directory\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_submit/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --config-file string   config file name; can be set by environment variable RUNAI_CLI_CONFIG_FILE (default \"config.json\")\n      --config-path string   config path; can be set by environment variable RUNAI_CLI_CONFIG_PATH\n  -d, --debug                enable debug mode\n  -q, --quiet                enable quiet mode, suppress all output except error messages\n      --verbose              enable verbose mode\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/runai_xgboost_submit/#see-also","title":"SEE ALSO","text":"<ul> <li>runai xgboost  - alias for xgboost management</li> </ul>"},{"location":"Researcher/cli-reference/new-cli/guides/set-kubeconfig-with-oidc-parameters/","title":"Add Run:ai authorization to kubeconfig","text":"<p>The runai kubeconfig set command allows users to configure their kubeconfig file with Run:ai authorization token. This setup enables users to gain access to the Kubernetes (k8s) cluster seamlessly.</p> <p>Note</p> <p>Setting kubeconfig is not required in order to use the CLI. This command is used to enable third-party workloads under Run:ai authorization.</p>"},{"location":"Researcher/cli-reference/new-cli/guides/set-kubeconfig-with-oidc-parameters/#usage","title":"Usage","text":"<p>To set the token (will be fetched automatically) inside the kubeconfig file, run the following command:</p> <pre><code>runai kubeconfig set\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/guides/set-kubeconfig-with-oidc-parameters/#prerequisites","title":"Prerequisites","text":"<p>Before executing the command, ensure that</p> <ol> <li>Cluster authentication is configured and enabled.</li> <li>The user has a kubeconfig file configured.</li> <li>The user is logged in (use the runai login command).</li> </ol>"},{"location":"Researcher/cli-reference/new-cli/guides/set-kubeconfig-with-oidc-parameters/#cluster-configuration","title":"Cluster configuration","text":"<p>To enable cluster authentication, add the following flags to the Kubernetes server API of each cluster:</p> <pre><code>spec:\n  containers:\n  - command:\n    ...\n    - --oidc-client-id=&lt;OIDC_CLIENT_ID&gt;\n    - --oidc-issuer-url=url=https://&lt;HOST&gt;/auth/realms/&lt;REALM&gt;\n    - --oidc-username-prefix=-\n</code></pre>"},{"location":"Researcher/cli-reference/new-cli/guides/set-kubeconfig-with-oidc-parameters/#user-kubeconfig-configuration","title":"User Kubeconfig configuration","text":"<p>Add the following to the Kubernetes client configuration file (./kube/config). For the full command reference, see kubeconfig set.  </p> <ul> <li>Make sure to replace values with the actual cluster information and user credentials.  </li> <li>There can be multiple contexts in the kubeconfig file. The command will configure the current context.</li> </ul> <pre><code>apiVersion: v1\nkind: Config\npreferences:\n  colors: true\ncurrent-context: &lt;CONTEXT_NAME&gt;\ncontexts:\n- context:\n    cluster: &lt;CLUSTER_NAME&gt;\n    user: &lt;USER_NAME&gt;\n  name: &lt;CONTEXT_NAME&gt;\nclusters:\n- cluster:\n    server: &lt;CLUSTER_URL&gt;\n    certificate-authority-data: &lt;CLUSTER_CERT&gt;\n  name: &lt;CLUSTER_NAME&gt;\nusers:\n- name: &lt;USER_NAME&gt;\n</code></pre>"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/","title":"GPU Time Slicing Scheduler","text":""},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#new-time-slicing-scheduler-by-runai","title":"New Time-slicing scheduler by Run:ai","text":"<p>To provide customers with predictable and accurate GPU compute resources scheduling, Run:ai is introducing a new feature called Time-slicing GPU scheduler which adds fractional compute capabilities on top of other existing Run:ai memory fractions capabilities. Unlike the default NVIDIA GPU orchestrator which doesn\u2019t provide the ability to split or limit the runtime of each workload, Run:ai created a new mechanism that gives each workload exclusive access to the full GPU for a limited amount of time (lease time) in each scheduling cycle (plan time). This cycle repeats itself for the lifetime of the workload.</p> <p>Using the GPU runtime this way guarantees a workload is granted its requested GPU compute resources proportionally to its requested GPU fraction.</p> <p>Run:ai offers two new Time-slicing modes:</p> <ol> <li>Strict\u2014each workload gets its precise GPU compute fraction, which equals to its requested GPU (memory) fraction. In terms of official Kubernetes resource specification, this means:</li> </ol> <pre><code>gpu-compute-request = gpu-compute-limit = gpu-(memory-)fraction\n</code></pre> <ol> <li>Fair\u2014each workload is guaranteed at least its GPU compute fraction, but at the same time can also use additional GPU runtime compute slices that are not used by other idle workloads. Those excess time slices are divided equally between all workloads running on that GPU (after each got at least its requested GPU compute fraction). In terms of official Kubernetes resource specification, this means:</li> </ol> <pre><code>gpu-compute-request = gpu-(memory-)fraction\n\ngpu-compute-limit = 1.0\n</code></pre> <p>The figure below illustrates how Strict time-slicing mode is using the GPU from Lease (slice) and Plan (cycle) perspective:</p> <p></p> <p>The figure below illustrates how Fair time-slicing mode is using the GPU from Lease (slice) and Plan (cycle) perspective:</p> <p></p>"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#setting-the-time-slicing-scheduler-policy","title":"Setting the Time-slicing scheduler policy","text":"<p>Time-slicing is a cluster flag which changes the default behavior of Run:ai GPU fractions feature.</p> <p>Enable time-slicing by setting the following cluster flag in the <code>runaiconfig</code> file:</p> <pre><code>global: \n    core: \n        timeSlicing: \n            mode: fair/strict\n</code></pre> <p>If the <code>timeSlicing</code> flag is not set, the system continues to use the default NVidia GPU orchestrator to maintain backward compatability.</p>"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#time-slicing-plan-and-lease-times","title":"Time-slicing Plan and Lease Times","text":"<p>Each GPU scheduling cycle is a plan, the plan time is determined by the lease time and granularity (precision). By default, basic lease time is 250ms with 5% granularity (precision), which means the plan (cycle) time is: 250 / 0.05 = 5000ms (5 Sec). Using these values, a workload that asked to get gpu-fraction=0.5 gets 2.5s runtime out of 5s cycle time.</p> <p>Different workloads requires different SLA and precision, so it also possible to tune the lease time and precision for customizing the time-slicing capabilities to your cluster.</p> <p>Note</p> <p>Decreasing the lease time makes time-slicing less accurate. Increasing the lease time make the system more accurate, but each workload is less responsive.</p> <p>Once timeSlicing is enabled, all submitted GPU fraction or GPU memory workloads will have their gpu-compute-request\\limit set automatically by the system, depending on the annotation used on the timeSlicing mode:</p>"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#strict-compute-resources","title":"Strict Compute Resources","text":"Annotation Value GPU Compute Request GPU Compute Limit <code>gpu-fraction</code> x x x <code>gpu-memory</code> x 0 1.0"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#fair-compute-resources","title":"Fair Compute Resources","text":"Annotation Value GPU Compute Request GPU Compute Limit <code>gpu-fraction</code> x x 1.0 <code>gpu-memory</code> x 0 1.0 <p>Note</p> <p>The above tables show that when submitting a workload using gpu-memory annotation, the system will split the GPU compute time between the different workloads running on that GPU. This means the workload can get anything from very little compute time (&gt;0) to full GPU compute time (1.0).</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/","title":"Introduction","text":"<p>When we discuss the allocation of deep learning compute resources, the discussion tends to focus on GPUs as the most critical resource. But two additional resources are no less important:</p> <ul> <li>CPUs. Mostly needed for preprocessing and postprocessing tasks during a deep learning training run.</li> <li>Memory. Has a direct influence on the quantities of data a training run can process in batches.</li> </ul> <p>GPU servers tend to come installed with a significant amount of memory and CPUs.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#requesting-cpu-memory","title":"Requesting CPU &amp; Memory","text":"<p>When submitting a Job, you can request a guaranteed amount of CPUs and memory by using the --cpu and --memory flags in the runai submit command. For example:</p> <pre><code>runai submit job1 -i ubuntu --gpu 2 --cpu 12 --memory 1G\n</code></pre> <p>The system guarantees that if the Job is scheduled, you will be able to receive this amount of CPU and memory.</p> <p>For further details on these flags see: runai submit</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#cpu-over-allocation","title":"CPU over allocation","text":"<p>The number of CPUs your Job will receive is guaranteed to be the number defined using the <code>--cpu</code> flag. In practice, however, you may receive more CPUs than you have asked for:</p> <ul> <li>If you are currently alone on a node, you will receive all the node CPUs until such time when another workload has joined.</li> <li>However, when a second workload joins, each workload will receive a number of CPUs proportional to the number requested via the <code>--cpu</code> flag. For example, if the first workload asked for 1 CPU and the second for 3 CPUs, then on a node with 40 cpus, the workloads will receive 10 and 30 CPUs respectively. If the flag <code>--cpu</code> is not specified, it will be taken from the cluster default (see the section below)</li> </ul>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#memory-over-allocation","title":"Memory over allocation","text":"<p>The amount of Memory your Job will receive is guaranteed to be the number defined using the --memory flag. In practice, however, you may receive more memory than you have asked for. This is along the same lines as described with CPU over allocation above.</p> <p>It is important to note, however, that if you have used this memory over-allocation, and new workloads have joined, your Job may receive an out-of-memory exception and terminate.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#cpu-and-memory-limits","title":"CPU and Memory limits","text":"<p>You can limit your Job's allocation of CPU and memory by using the --cpu-limit and --memory-limit flags in the runai submit command. For example:</p> <pre><code>runai submit job1 -i ubuntu --gpu 2 --cpu 12 --cpu-limit 24 \\\n    --memory 1G --memory-limit 4G\n</code></pre> <p>The limit behavior is different for CPUs and memory.</p> <ul> <li>Your Job will never be allocated with more than the amount stated in the <code>--cpu-limit</code> flag</li> <li>If your Job tries to allocate more than the amount stated in the <code>--memory-limit</code> flag it will receive an out-of-memory exception.</li> </ul> <p>The limit (for both CPU and memory) overrides the cluster default described in the section below</p> <p>For further details on these flags see: runai submit</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#flag-defaults","title":"Flag Defaults","text":""},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-cpu-flag","title":"Defaults for --cpu flag","text":"<p>If your Job has not specified <code>--cpu</code>, the system will use a default. The default is cluster-wide and is defined as a ratio of GPUs to CPUs.</p> <p>If, for example, the default has been defined as 1:6 and your Job has specified <code>--gpu 2</code> and has not specified <code>--cpu</code>, then the implied <code>--cpu</code> flag value is 12 CPUs.</p> <p>The system comes with a cluster-wide default of 1:1. To change the ratio see below.</p> <p>If you didn't request any GPUs for your job and has not specified <code>--cpu</code>, the default is defined as a ratio of CPU limit to CPUs.</p> <p>If, for example, the default has been defined as 1:0.2 and your Job has specified <code>--cpu-limit 10</code> and has not specified <code>--cpu</code>, then the implied <code>--cpu</code> flag value is 2 CPUs.</p> <p>The system comes with a cluster-wide default of 1:0.1. To change the ratio see below.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-memory-flag","title":"Defaults for --memory flag","text":"<p>If your Job has not specified <code>--memory</code>, the system will use a default. The default is cluster-wide and is proportional to the number of requested GPUs.</p> <p>The system comes with a cluster-wide default of 100MiB of allocated CPU memory per GPU. To change the ratio see below.</p> <p>If you didn't request any GPUs for your job and has not specified <code>--memory</code>, the default is defined as a ratio of CPU Memory limit to CPU Memory Request.</p> <p>The system comes with a cluster-wide default of 1:0.1. To change the ratio see below.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-cpu-limit-flag","title":"Defaults for --cpu-limit flag","text":"<p>If your Job has not specified <code>--cpu-limit</code>, then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to CPUs. See below on how to change the ratio.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-memory-limit-flag","title":"Defaults for --memory-limit flag","text":"<p>If your Job has not specified <code>--memory-limit</code>, then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to Memory. See below on how to change the ratio.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#changing-the-ratios","title":"Changing the ratios","text":"<p>To change the cluster wide-ratio use the following process. The example shows:</p> <ul> <li>a CPU request with a default ratio of 2:1 CPUs to GPUs.</li> <li>a CPU Memory request with a default ratio of 200MB per GPU.</li> <li>a CPU limit with a default ratio of 4:1 CPU to GPU.</li> <li>a Memory limit with a default ratio of 2GB per GPU.</li> <li>a CPU request with a default ratio of 0.1 CPUs per 1 CPU limit.</li> <li>a CPU Memory request with a default ratio of 0.1:1 request per CPU Memory limit.</li> </ul> <p>You must edit the cluster installation values file:</p> <ul> <li>When installing the Run:ai cluster, edit the values file.</li> <li>On an existing installation, use the upgrade cluster instructions to modify the values file.</li> <li>You must specify at least the first 4 values as follows:</li> </ul> <pre><code>runai-operator:\n  config:\n    limitRange:\n      cpuDefaultRequestGpuFactor: 2\n      memoryDefaultRequestGpuFactor: 200Mi\n      cpuDefaultLimitGpuFactor: 4\n      memoryDefaultLimitGpuFactor: 2Gi\n      cpuDefaultRequestCpuLimitFactorNoGpu: 0.1\n      memoryDefaultRequestMemoryLimitFactorNoGpu: 0.1\n</code></pre>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#validating-cpu-memory-allocations","title":"Validating CPU &amp; Memory Allocations","text":"<p>To review CPU &amp; Memory allocations you need to look into Kubernetes. A Run:ai Job creates a Kubernetes pod. The pod declares its resource requests and limits. To see the memory and CPU consumption in Kubernetes:</p> <ul> <li>Get the pod name for the Job by running:</li> </ul> <p><code>runai describe job &lt;JOB_NAME&gt;</code></p> <p>the pod will appear under the <code>PODS</code> category.</p> <ul> <li>Run:</li> </ul> <p><code>kubectl describe pod &lt;POD_NAME&gt;</code></p> <p>The information will appear under <code>Requests</code> and <code>Limits</code>. For example:</p> <pre><code>Limits:\n    nvidia.com/gpu:  2\nRequests:\n    cpu:             1\n    memory:          104857600\n    nvidia.com/gpu:  2\n</code></pre>"},{"location":"Researcher/scheduling/dynamic-gpu-fractions/","title":"Dynamic GPU Fractions","text":""},{"location":"Researcher/scheduling/dynamic-gpu-fractions/#introduction","title":"Introduction","text":"<p>Many AI workloads are using GPU resources intermittently and sometimes these resources are not used at all. These AI workloads need these resources when they are running AI applications, or debugging a model in development. Other workloads such as Inference, might be using GPU resources at a lower utilization rate than requested, and may suddenly ask for higher guaranteed resources at peak utilization times.</p> <p>This pattern of resource request vs. actual resource utilization causes lower utilization of GPUs. This mainly happens if there are many workloads requesting resources to match their peak demand, even though the majority of the time they operate far below that peak.</p> <p>Run:ai has introduced Dynamic GPU fractions in v2.15 to cope with resource request vs. actual resource utilization which enables users to optimize GPU resource usage.</p> <p>Dynamic GPU fractions is part of Run:ai's core capabilities to enable workloads to optimize the use of GPU resources. This works by providing the ability to specify and consume GPU memory and compute resources dynamically by leveraging Kubernetes Request and Limit notations.</p> <p>Dynamic GPU fractions allow a workload to request a guaranteed fraction of GPU memory or GPU compute resource (similar to a Kubernetes request), and at the same time also request the ability to grow beyond that guaranteed request up to a specific limit (similar to a Kubernetes limit), if the resources are available.</p> <p>For example, with Dynamic GPU Fractions, a user can specify a workload with a GPU fraction Request of 0.25 GPU, and add the parameter <code>gpu-fraction-limit</code> of up to 0.80 GPU. The cluster/node-pool scheduler schedules the workload to a node that can provide the GPU fraction request (0.25), and then assigns the workload to a GPU. The GPU scheduler monitors the workload and allows it to occupy memory between 0 to 0.80 of the GPU memory (based on the parameter <code>gpu-fraction-limit</code>), where only 0.25 of the GPU memory is guaranteed to that workload. The rest of the memory (from 0.25 to 0.8) is \u201cloaned\u201d to the workload, as long as it is not needed by other workloads.</p> <p>Run:ai automatically manages the state changes between <code>request</code> and <code>Limit</code> as well as the reverse (when the balance need to be \"returned\"), updating the metrics and workloads' states and graphs.</p>"},{"location":"Researcher/scheduling/dynamic-gpu-fractions/#setting-fractional-gpu-memory-limit","title":"Setting Fractional GPU Memory Limit","text":"<p>With the fractional GPU memory limit, users can submit workloads using GPU fraction <code>Request</code> and <code>Limit</code>.</p> <p>You can either:</p> <ol> <li> <p>Use a GPU Fraction parameter (use the <code>gpu-fraction</code> annotation)</p> <p>or</p> </li> <li> <p>Use an absolute GPU Memory parameter (<code>gpu-memory</code> annotation)</p> </li> </ol> <p>When setting a GPU memory limit either as GPU fraction, or GPU memory size, the <code>Limit</code> must be equal or greater than the GPU fraction memory request.</p> <p>Both GPU fraction and GPU memory are translated into the actual requested memory size of the Request (guaranteed resources) and the Limit (burstable resources).</p> <p>To guarantee fair quality of service between different workloads using the same GPU, Run:ai developed an extendable GPU <code>OOMKiller</code> (Out Of Memory Killer) component that guarantees the quality of service using Kubernetes semantics for resources Request and Limit.</p> <p>The <code>OOMKiller</code> capability requires adding <code>CAP_KILL</code> capabilities to the Dynamic GPU fraction and to the Run:ai core scheduling module (toolkit daemon). This capability is disabled by default.</p> <p>To change the state of Dynamic GPU Fraction in the cluster, edit the <code>runaiconfig</code> file and set:</p> <pre><code>spec: \n  global: \n    core: \n      dynamicFraction: \n        enabled: true # Boolean field default is true.\n</code></pre> <p>To set the gpu memory limit per workload, add the <code>RUNAI_GPU_MEMORY_LIMIT</code> environment variable to the first container in the pod. This is the GPU consuming container.</p> <p>To use <code>RUNAI_GPU_MEMORY_LIMIT</code> environment variable:</p> <ol> <li> <p>Submit a workload yaml directly, and set the <code>RUNAI_GPU_MEMORY_LIMIT</code> environment variable.</p> </li> <li> <p>Create a policy, per Project or globally. For example, set all Interactive workloads of <code>Project=research_vision1</code> to always set the environment variable of <code>RUNAI_GPU_MEMORY_LIMIT</code> to 1.</p> </li> <li> <p>Pass the environment variable through the CLI or the UI.</p> </li> </ol> <p>The supported values depend on the label used. You can use them in either the UI or the CLI. Use only one of the variables in the following table (they cannot be mixed):</p> Variable Input format <code>gpu-fraction</code> A fraction value (for example: 0.25, 0.75). <code>gpu-memory</code> Kubernetes resources quantity which must be larger than <code>gpu-memory</code>. For example, 500000000, 2500M, 4G. NOTE: The <code>gpu-memory</code> label values are always in MB, unlike the env variable."},{"location":"Researcher/scheduling/dynamic-gpu-fractions/#compute-resources-ui-with-dynamic-fractions-support","title":"Compute Resources UI with Dynamic Fractions support","text":"<p>To enable the UI elements for Dynamic Fractions, press Settings, General, then open the Resources pane and toggle GPU Resource Optimization. This enables all the UI features related to GPU Resource Optimization for the whole tenant. There are other per cluster or per node-pool configurations that should be configured in order to use the capabilities of \u2018GPU Resource Optimization\u2019 See the documentation for each of these features. Once the \u2018GPU Resource Optimization\u2019 feature is enabled, you will be able to create Compute Resources with the GPU Portion (Fraction) Limit and GPU Memory Limit. In addition, you will be able to view the workloads\u2019 utilization vs. Request and Limit parameters in the Metrics pane for each workload.</p> <p></p> <p>Note</p> <p>To use Dynamic Fractions, GPU devices per pod must be equal to 1. If more than 1 GPU device is used per pod, or if a MIG profile is selected, Dynamic Fractions cannot be used for that Compute Resource (and any related pods).</p> <p>Note</p> <p>When setting a workload with Dynamic Fractions, (for example, when using it with GPU Request or GPU memory Limits), you practically make the workload burstable. This means it can use memory that is not guaranteed for that workload and is susceptible to an \u2018OOM Kill\u2019 signal if the actual owner of that memory requires it back. This applies to non-preemptive workloads as well. For that reason, its recommended that you use Dynamic Fractions with Interactive workloads running Notebooks. Notebook pods are not evicted when their GPU process is OOM Kill\u2019ed. This behavior is the same as standard Kubernetes burstable CPU workloads.</p>"},{"location":"Researcher/scheduling/fractions/","title":"Allocation of GPU Fractions","text":""},{"location":"Researcher/scheduling/fractions/#introduction","title":"Introduction","text":"<p>A single GPU has a significant amount of memory. Ranging from a couple of gigabytes in older generations and up to 80GB per GPU in the later models of the latest NVIDIA GPU technology. A single GPU also has a vast amount of computing power. </p> <p>This amount of memory and computing power is important for processing large amounts of data, such as in training deep learning models. However, there are quite a few applications that do not need this power. Examples can be inference workloads and the model-creation phase. It would thus be convenient if we could divide up a GPU between various workloads, thus achieving better GPU utilization. </p> <p>This article describes a Run:ai technology called Fractions that allow the division of GPUs and how to use them with Run:ai.</p>"},{"location":"Researcher/scheduling/fractions/#runai-fractions","title":"Run:ai Fractions","text":"<p>Run:ai provides the capability to allocate a container with a specific amount of GPU RAM. As a researcher, if you know that your code needs 4GB of RAM. You can submit a job using the flag <code>--gpu-memory 4G</code> to specify the exact portion of the GPU memory that you need. Run:ai will allocate your container that specific amount of GPU RAM. Attempting to reach beyond your allotted RAM will result in an out-of-memory exception. </p> <p>You can also use the flag <code>--gpu 0.2</code> to get 20% of the GPU memory on the GPU assigned for you. </p> <p>For more details on Run:ai fractions see the fractions quickstart.</p> <p>Limitation</p> <p>With the fraction technology all running workloads, which utilize the GPU, share the compute in parallel and on average get an even share of the compute. For example, assuming two containers, one with 0.25 GPU workload and the other with 0.75 GPU workload - both will get (on average) an equal part of the computation power. If one of the workloads does not utilize the GPU, the other workload will get the entire GPU's compute power.</p> <p>Info</p> <p>For interoperability with other Kubernetes schedulers, Run:ai creates special reservation pods. Once a workload has been allocated a fraction of a GPU, Run:ai will create a pod in a dedicated <code>runai-reservation</code> namespace with the full GPU as a resource. This would cause other schedulers to understand that the GPU is reserved.    </p>"},{"location":"Researcher/scheduling/fractions/#see-also","title":"See Also","text":"<ul> <li>Fractions quickstart.</li> </ul>"},{"location":"Researcher/scheduling/gpu-memory-swap/","title":"GPU Memory SWAP","text":""},{"location":"Researcher/scheduling/gpu-memory-swap/#introduction","title":"Introduction","text":"<p>To ensure efficient and effective usage of an organization\u2019s resources, Run:ai provides multiple features on multiple layers to help administrators and practitioners maximize their existing GPUs resource utilization.</p> <p>Run:ai\u2019s GPU memory swap feature helps administrators and AI practitioners to further increase the utilization of existing GPU hardware by improving GPU sharing between AI initiatives and stakeholders. This is done by expanding the GPU physical memory to the CPU memory which is typically an order of magnitude larger than that of the GPU.</p> <p>Expanding the GPU physical memory, helps the Run:ai system to put more workloads on the same GPU physical hardware, and to provide a smooth workload context switching between GPU memory and CPU memory, eliminating the need to kill workloads when the memory requirement is larger than what the GPU physical memory can provide.</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#benefits-of-gpu-memory-swap","title":"Benefits of GPU memory swap","text":"<p>There are several use cases where GPU memory swap can benefit and improve the user experience and the system's overall utilization:</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#sharing-a-gpu-between-multiple-interactive-workloads-notebooks","title":"Sharing a GPU between multiple interactive workloads (notebooks)","text":"<p>AI practitioners use notebooks to develop and test new AI models and to improve existing AI models. While developing or testing an AI model, notebooks use GPU resources intermittently, yet, required resources of the GPU\u2019s are pre-allocated by the notebook and cannot be used by other workloads after one notebook has already reserved them. To overcome this inefficiency, Run:ai introduced Dynamic Fractions and Node Level Scheduler.</p> <p>When one or more workloads require more than their requested GPU resources, there\u2019s a high probability not all workloads can run on a single GPU because the total memory required is larger than the physical size of the GPU memory.</p> <p>With GPU memory swap, several workloads can run on the same GPU, even if the sum of their used memory is larger than the size of the physical GPU memory. GPU memory swap can swap in and out workloads interchangeably, allowing multiple workloads to each use the full amount of GPU memory. The most common scenario is for one workload to run on the GPU (for example, an interactive notebook),while other notebooks are either idle or using the CPU to develop new code (while not using the GPU). From a user experience point of view, the swap in and out is a smooth process since the notebooks do not notice that they are being swapped in and out of the GPU memory. On rare occasions, when multiple notebooks need to access the GPU simultaneously, slower workload execution may be experienced.</p> <p>Notebooks typically use the GPU intermittently, therefore with high probability, only one workload (for example, an interactive notebook), will use the GPU at a time. The more notebooks the system puts on a single GPU, the higher the chances are that there will be more than one notebook requiring the GPU resources at the same time. Admins have a significant role here in fine tuning the number of notebooks running on the same GPU, based on specific use patterns and required SLAs. Using \u2018Node Level Scheduler\u2019 reduces GPU access contention between different interactive notebooks running on the same node.</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#sharing-a-gpu-between-inferenceinteractive-workloads-and-training-workloads","title":"Sharing a GPU between inference/interactive workloads and training workloads","text":"<p>A single GPU can be shared between an interactive or inference workload (for example, a Jupyter notebook, image recognition services, or an LLM service), and a training workload that is not time-sensitive or delay-sensitive. At times when the inference/interactive workload uses the GPU, both training and inference/interactive workloads share the GPU resources, each running part of the time swapped-in to the GPU memory, and swapped-out into the CPU memory the rest of the time.</p> <p>Whenever the inference/interactive workload stops using the GPU, the swap mechanism swaps out the inference/interactive workload GPU data to the CPU memory. Kubernetes wise, the POD is still alive and running using the CPU. This allows the training workload to run faster when the inference/interactive workload is not using the GPU, and slower when it does, thus sharing the same resource between multiple workloads, fully utilizing the GPU at all times, and maintaining uninterrupted service for both workloads.</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#serving-inference-warm-models-with-gpu-memory-swap","title":"Serving inference warm models with GPU memory swap","text":"<p>Running multiple inference models is a demanding task and you will need to ensure that your SLA is met. You need to provide high performance and low latency, while maximizing GPU utilization. This becomes even more challenging when the exact model usage patterns are unpredictable. You must plan for the agility of inference services and strive to keep models on standby in a ready state rather than an idle state.</p> <p>Run:ai\u2019s GPU memory swap feature enables you to load multiple models to a single GPU, where each can use up to the full amount GPU memory. Using an application load balancer, the administrator can control to which server each inference request is sent. Then the GPU can be loaded with multiple models, where the model in use is loaded into the GPU memory and the rest of the models are swapped-out to the CPU memory. The swapped models are stored as ready models to be loaded when required. GPU memory swap always maintains the context of the workload (model) on the GPU so it can easily and quickly switch between models. This is unlike industry standard model servers that load models from scratch into the GPU whenever required.</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#configuring-memory-swap","title":"Configuring memory swap","text":"<p>Perquisites\u2014before configuring the GPU Memory Swap the administrator must configure the Dynamic Fractions feature, and optionally configure the Node Level Scheduler feature. </p> <p>The first enables you to make your workloads burstable, and both features will maximize your workloads\u2019 performance and GPU utilization within a single node.</p> <p>To enable GPU memory swap in a Run:aAi cluster, the administrator must update the <code>runaiconfig</code> file with the following parameters:</p> <pre><code>spec: \n global: \n   core: \n     swap:\n       enabled: true\n       limits:\n         cpuRam: 100Gi\n</code></pre> <p>The example above uses <code>100Gi</code> as the size of the swap memory.</p> <p>You can also use the <code>patch</code> command from your terminal:</p> <pre><code>kubectl patch -n runai runaiconfigs.run.ai/runai --type='merge' --patch '{\"spec\":{\"global\":{\"core\":{\"swap\":{\"enabled\": true, \"limits\": {\"cpuRam\": \"100Gi\"}}}}}}'\n</code></pre> <p>To make a workload swappable, a number of conditions must be met:</p> <ol> <li> <p>The workload MUST use Dynamic Fractions. This means the workload\u2019s memory request is less than a full GPU, but it may add a GPU memory limit to allow the workload to effectively use the full GPU memory.</p> </li> <li> <p>The administrator must label each node that they want to provide GPU memory swap with a <code>run.ai/swap-enabled=true</code> this enables the feature on that node. Enabling the feature reserves CPU memory to serve the swapped GPU memory from all GPUs on that node. The administrator sets the size of the CPU reserved RAM memory using the runaiconfigs file.</p> </li> <li> <p>Optionally, configure Node Level Scheduler. Using node level scheduler can help in the following ways:</p> <ul> <li>The Node Level Scheduler automatically spreads workloads between the different GPUs on a node, ensuring maximum workload performance and GPU utilization.</li> <li>In scenarios where Interactive notebooks are involved, if the CPU reserved memory for the GPU swap is full, the Node Level Scheduler preempts the GPU process of that workload and potentially routes the workload to another GPU to run.</li> </ul> </li> </ol>"},{"location":"Researcher/scheduling/gpu-memory-swap/#configure-system-reserved-gpu-resources","title":"Configure <code>system reserved</code> GPU Resources","text":"<p>Swappable workloads require reserving a small part of the GPU for non-swappable allocations like binaries and GPU context. To avoid getting out-of-memory (OOM) errors due to non-swappable memory regions, the system reserves a 2GiB of GPU RAM memory by default, effectively truncating the total size of the GPU memory. For example, a 16GiB T4 will appear as 14GiB on a swap-enabled node. The exact reserved size is application-dependent, and 2GiB is a safe assumption for 2-3 applications sharing and swapping on a GPU. This value can be changed by editing the <code>runaiconfig</code> specification as follows:</p> <pre><code>spec: \n global: \n   core: \n     swap:\n       limits:\n         reservedGpuRam: 2Gi\n</code></pre> <p>You can also use the <code>patch</code> command from your terminal:</p> <pre><code>kubectl patch -n runai runaiconfigs.run.ai/runai --type='merge' --patch '{\"spec\":{\"global\":{\"core\":{\"swap\":{\"limits\":{\"reservedGpuRam\": &lt;quantity&gt;}}}}}}'\n</code></pre> <p>This configuration is in addition to the Dynamic Fractions configuration, and optional Node Level Scheduler configuration.</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#preventing-your-workloads-from-getting-swapped","title":"Preventing your workloads from getting swapped","text":"<p>If you prefer your workloads not to be swapped into CPU memory, you can specify on the pod an anti-affinity to <code>run.ai/swap-enabled=true</code> node label when submitting your workloads and the Scheduler will ensure not to use swap-enabled nodes.</p>"},{"location":"Researcher/scheduling/gpu-memory-swap/#known-limitations","title":"Known Limitations","text":"<ul> <li>A pod created before the GPU memory swap feature was eneabled in that cluster, cannot be scheduled to a swap-enabled node. A proper event is generated in case no matching node is found. Users must re-submit those pods to make them swap-enabled.</li> <li>GPU memory swap cannot be enabled if fairshare time-slicing or strict time-slicing is used, GPU memory swap can only be used with the default time-slicing mechanism.</li> <li>CPU RAM size cannot be decreased once GPU memory swap is enabled.</li> </ul>"},{"location":"Researcher/scheduling/gpu-memory-swap/#what-happens-when-the-cpu-reserved-memory-for-gpu-swap-is-exhausted","title":"What happens when the CPU reserved memory for GPU swap is exhausted?","text":"<p>CPU memory is limited, and since a single CPU serves multiple GPUs on a node, this number is usually between 2 to 8. For example, when using 80GB of GPU memory, each swapped workload consumes up to 80GB (but may use less) assuming each GPU is shared between 2-4 workloads. In this example, you can see how the swap memory can become very large. Therefore, we give administrators a way to limit the size of the CPU reserved memory for swapped GPU memory on each swap enabled node.</p> <p>Limiting the CPU reserved memory means that there may be scenarios where the GPU memory cannot be swapped out to the CPU reserved RAM. Whenever the CPU reserved memory for swapped GPU memory is exhausted, the workloads currently running will not be swapped out to the CPU reserved RAM, instead, Node Level Scheduler logic takes over and provides GPU resource optimization. See Node Level Scheduler.</p>"},{"location":"Researcher/scheduling/node-level-scheduler/","title":"Optimize performance with Node Level Scheduler","text":"<p>The Node Level Scheduler optimizes the performance of your pods and maximizes the utilization of GPUs by making optimal local decisions on GPU allocation to your pods. While the Cluster Scheduler chooses the specific node for a POD, but has no visibility to node\u2019s GPUs internal state, the Node Level Scheduler is aware of the local GPUs states and makes optimal local decisions such that it can optimize both the GPU utilization and pods\u2019 performance running on the node\u2019s GPUs.</p> <p>Node Level Scheduler applies to all workload types, but will best optimize the performance of burstable workloads, giving those more GPU memory than requested and up to the limit specified. Be aware, burstable workloads are always susceptible to an OOM Kill signal if the owner of the excess memory requires it back. This means that using the Node Level Scheduler with Inference or Training workloads may cause pod preemption. Interactive workloads that are using notebooks behave differently since the OOM Kill signal will cause the Notebooks' GPU process to exit but not the notebook itself. This keeps the Interactive pod running and retrying to attach a GPU again. This makes Interactive workloads with notebooks a great use case for burstable workloads and Node Level Scheduler.</p>"},{"location":"Researcher/scheduling/node-level-scheduler/#interactive-notebooks-use-case","title":"Interactive Notebooks Use Case","text":"<p>Consider the following example of a node with 2 GPUs and 2 interactive pods that are submitted and want GPU resources.</p> <p></p> <p>The Scheduler instructs the node to put the two pods on a single GPU, bin packing a single GPU and leaving the other free for a workload that might want a full GPU or more than half GPU. However that would mean GPU#2 is idle while the two notebooks can only use up to half a GPU, even if they temporarily need more.</p> <p></p> <p>However, with Node Level Scheduler enabled, the local decision will be to spread those two pods on two GPUs and allow them to maximize bot pods\u2019 performance and GPUs\u2019 utilization by bursting out up to the full GPU memory and GPU compute resources.</p> <p></p> <p>The Cluster Scheduler still sees a node with a full empty GPU. When a 3rd pod is scheduled, and it requires a full GPU (or more than 0.5 GPU), the scheduler will send it to that node, and Node Level Scheduler will move one of the Interactive workloads to run with the other pod in GPU#1, as was the Cluster Scheduler initial plan.</p> <p></p> <p>This is an example of one scenario that shows how Node Level Scheduler locally optimizes and maximizes GPU utilization and pods\u2019 performance.</p>"},{"location":"Researcher/scheduling/node-level-scheduler/#how-to-configure-node-level-scheduler","title":"How to configure Node Level Scheduler","text":"<p>Node Level Scheduler can be enabled per Node-Pool, giving the Administrator the option to decide which Node-Pools will be used with this new feature.</p> <p>To use Node Level Scheduler the Administrator should follow the steps:</p> <ol> <li> <p>Enable Node Level Scheduler at the cluster level (per cluster), edit the <code>runaiconfig</code> file and set:</p> <pre><code>spec: \n  global: \n      core: \n        nodeScheduler:\n          enabled: true\n</code></pre> <p>The Administrator can also use this patch command to perform the change:</p> <pre><code>kubectl patch -n runai runaiconfigs.run.ai/runai --type='merge' --patch '{\"spec\":{\"global\":{\"core\":{\"nodeScheduler\":{\"enabled\": true}}}}}'\n</code></pre> </li> <li> <p>To enable \u2018GPU resource optimization\u2019 on your tenant\u2019s, go to your tenant\u2019s UI and press Tools &amp; Settings, General, the open the Resources pane and toggle Resource Optimization to on.</p> </li> <li> <p>To enable \u2018Node Level Scheduler\u2019 on any of the Node Pools you want to use this feature, go to the tenant\u2019s UI \u2018Node Pools\u2019 tab (under \u2018Nodes\u2019), and either create a new Node-Pool or edit an existing Node-Pool. In the Node-Pool\u2019s form, under the \u2018Resource Utilization Optimization\u2019 tab, change the \u2018Number of workloads on each GPU\u2019 to any value other than \u2018Not Enforced\u2019 (i.e. 2, 3, 4, 5).</p> </li> </ol> <p>The Node Level Scheduler is now ready to be used on that Node-Pool.</p>"},{"location":"Researcher/scheduling/schedule-to-aws-groups/","title":"Scheduling workloads to AWS placement groups","text":"<p>Run:ai supports AWS placement groups when building and submitting a job. AWS Placement Groups are used to maximize throughput and performance of distributed training workloads.</p> <p>To enable and configure this feature:</p> <ol> <li>Press <code>Jobs | New job</code>.</li> <li>In <code>Scheduling and lifecycle</code> enable the <code>Topology aware scheduling</code>.</li> <li>In <code>Topology key</code>, enter the label of the topology of the node.</li> <li> <p>In <code>Scheduling rule</code> choose <code>Required</code> or <code>Preferred</code> from the drop down.</p> <ul> <li><code>Required</code>\u2014when enabled, all PODs must be scheduled to the same placement group.</li> <li><code>Preferred</code>\u2014when enabled, this is a best-effort, to place as many PODs on the same placement group.</li> </ul> </li> </ol>"},{"location":"Researcher/scheduling/the-runai-scheduler/","title":"The Run:ai Scheduler","text":"<p>Each time a user submits a workload via the Run:ai platform, through a 3rd party framework, or directly to Kubernetes APIs, the submitted workload goes to the selected Kubernetes cluster, and is handled by the Run:ai Scheduler.</p> <p>The Scheduler\u2019s main role is to find the best-suited node or nodes for each submitted workload. The nodes must match the resources and other characteristics requested by the workload, while adhering to the quota and fairness principles of the Run:ai platform. A workload can be a single pod running on a single node, or a distributed workload using multiple pods, each running on a node (or part of a node). It is not rare to find large training workloads using 128 nodes and even more, or inference workloads using many pods (replicas) and nodes. There are numerous types of workloads, some are Kubernetes native and some are 3rd party extensions on top of Kubernetes native pods. The Run:ai Scheduler schedules any Kubernetes native workloads, Run:ai workloads, or any other type of 3rd party workload.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#scheduler-basics","title":"Scheduler basics","text":"<p>Set out below are some basic terms and information regarding the Run:ai Scheduler.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#terminology","title":"Terminology","text":"<p>This section describes the terminology and building blocks of the Run:ai scheduler, it also explains some of the scheduling principles used by the Run:ai scheduler.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#workloads-and-pod-groups","title":"Workloads and Pod-Groups","text":"<p>The Run:ai scheduler attaches any newly created pod to a pod-group. A pod-group may contain one or more pods representing a workload. For example, if the submitted workload is a PyTorch distributed training with 32 workers, a single pod-group is created for the entire workload, and all pods are then attached to the pod-group with certain rules that may apply to the pod-group itself, for example, gang scheduling.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#scheduling-queue","title":"Scheduling queue","text":"<p>A scheduling queue (or simply a queue) represents a scheduler primitive that manages the scheduling of workloads based on different parameters. A queue is created for each project/node pool pair and department/node pool pair. The Run:ai scheduler supports hierarchical queueing, project queues are bound to department queues, per node pool. This allows an organization to manage quota, over-quota, and other characteristics for projects and their associated departments.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#priority-and-preemption","title":"Priority and Preemption","text":"<p>Run:ai supports scheduling workloads using different priorities and preemption policies. In the Run:ai scheduling system, higher priority workloads (pods) may preempt lower priority workloads (pods) within the same scheduling queue (project), according to their Preemption policy. Run:ai Scheduler implicitly assumes any PriorityClass &gt;= 100 is non-preemptible and any PriorityClass &lt; 100 is preemptible.</p> <p>Cross project and cross department workload preemptions are referred to as Resource reclaim and are based on fairness between queues rather than the priority of the workloads.</p> <p>To make it easier for users to submit AI workloads, Run:ai preconfigured several Kubernetes PriorityClass objects, the Run:ai preset PriorityClass objects have their preemptionPolicy always set to PreemptLowerPriority, regardless of their actual Run:ai preemption policy within the Run:ai platform.</p> PriorityClass Name PriorityClass Run:ai preemption policy K8S preemption policy Inference 125 Non-preemptible PreemptLowerPriority Build 100 Non-preemptible PreemptLowerPriority Interactive-preemptible 75 Preemptible PreemptLowerPriority Train 50 Preemptible PreemptLowerPriority"},{"location":"Researcher/scheduling/the-runai-scheduler/#quota","title":"Quota","text":"<p>Each project and department includes a set of guaranteed resource quotas per node pool per resource type. For example, Project LLM-Train/Node Pool NV-H100 quota parameters specify the number of GPUs, CPUs(cores), and the amount of CPU memory that this project guarantees for that node pool.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#over-quota","title":"Over-quota","text":"<p>Projects and departments can have a share in the unused resources of any node pool, beyond their quota of resources. We name these resources as over quota resources. The admin configures the over-quota parameters per node pool for each project and department.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#over-quota-priority","title":"Over quota priority","text":"<p>Projects can receive a share of the cluster/node pool unused resources when the over-quota priority setting is enabled, the part each Project receives depends on its over-quota priority value, and the total weights of all other projects\u2019 over-quota priorities. The admin configures the over-quota priority parameters per node pool for each project and department.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#fairshare-and-fairshare-balancing","title":"Fairshare and fairshare balancing","text":"<p>Run:ai Scheduler calculates a numerical value per project (or department) for each node-pool, representing the project\u2019s (department\u2019s) sum of guaranteed resources plus the portion of non-guaranteed resources in that node pool. We name this value fairshare.</p> <p>The scheduler strives to provide each project (or department) the resources they deserve using two main parameters - deserved quota and deserved fairshare (i.e. quota + over quota resources), this is done per node pool. If one project\u2019s node pool queue is below fairshare and another project\u2019s node pool queue is above fairshare, the scheduler shifts resources between queues to balance fairness; this may result in the preemption of some over-quota preemptible workloads.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#over-subscription","title":"Over-subscription","text":"<p>Over-subscription is a scenario where the sum of all guaranteed resource quotas surpasses the physical resources of the cluster or node pool. In this case, there may be scenarios in which the scheduler cannot find matching nodes to all workload requests, even if those requests were within the resource quota of their associated projects.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#gang-scheduling","title":"Gang scheduling","text":"<p>Gang scheduling describes a scheduling principle where a workload composed of multiple pods is either fully scheduled (i.e. all pods are scheduled and running) or fully pending (i.e. all pods are not running). Gang scheduling refers to a single pod group.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#fairness-fair-resource-distribution","title":"Fairness (fair resource distribution)","text":"<p>Fairness is a major principle within the Run:ai scheduling system. In essence, it means that the Run:ai Scheduler always respects certain resource splitting rules (fairness) between projects and between departments.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#preemption-of-lower-priority-workloads-within-a-project","title":"Preemption of lower priority workloads within a project","text":"<p>Workload priority is always respected within a project. This means higher priority workloads are scheduled before lower priority workloads, it also means that higher priority workloads may preempt lower priority workloads within the same project if the lower priority workloads are preemptible.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#reclaim-of-resources-between-projects-and-departments","title":"Reclaim of resources between projects and departments","text":"<p>Reclaim is an inter-project (and inter-department) scheduling action that takes back resources from one project (or department) that has used them as over-quota, back to a project (or department) that deserves those resources as part of its guaranteed quota, or to balance fairness between projects, each to its fairshare (i.e. sharing fairly the portion of the unused resources).</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#multi-level-quota-system","title":"Multi-Level quota system","text":"<p>Each project has a set of guaranteed resource quotas (GPUs, CPUs, and CPU memory) per node pool. Projects can go over-quota and get a share of the unused resources (over-quota) in a node pool beyond their guaranteed quota in that node pool. The same applies to Departments. The Scheduler balances the amount of over quota between departments, and then between projects.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#placement-strategy-bin-pack-and-spread","title":"Placement strategy - bin-pack and spread","text":"<p>The admin can set per node pool placement strategy of the scheduler for GPU based workloads and for CPU-only based workloads.</p> <p>Each type\u2019s strategy can be either bin-pack or spread.</p> <p>GPU workloads:</p> <ul> <li>Bin-pack means the Scheduler places as many workloads as possible in each GPU and each node to use fewer resources and maximize GPU and node vacancy.  </li> <li>Spread means the Scheduler spreads workloads across as many GPUs and nodes as possible to minimize the load and maximize the available resources per workload.  </li> <li>GPU workloads are workloads that request both GPU and CPU resources.</li> </ul> <p>CPU workloads:</p> <ul> <li>Bin-pack means the scheduler places as many workloads as possible in each CPU and node to use fewer resources and maximize CPU and node vacancy.  </li> <li>Spread means the scheduler spreads workloads across as many CPUs and nodes as possible to minimize the load and maximize the available resources per workload.  </li> <li>CPU workloads are workloads that request only CPU resources</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#scheduler-deep-dive","title":"Scheduler deep dive","text":""},{"location":"Researcher/scheduling/the-runai-scheduler/#allocation","title":"Allocation","text":"<p>When a user submits a workload, the workload controller creates a pod or pods (for distributed training workloads or a deployment based Inference). When the scheduler gets a submit request with the first pod, it creates a pod group and allocates all the relevant building blocks of that workload. The next pods of the same workload are attached to the same pod group.</p> <p>A workload, with its associated pod group, is queued in the appropriate queue. In every scheduling cycle, the Scheduler ranks the order of queues by calculating their precedence for scheduling.</p> <p>The next step is for the scheduler to find nodes for those pods, assign the pods to their nodes (bind operation), and bind other building blocks of the pods such as storage, ingress etc.</p> <p>If the pod-group has a gang scheduling rule attached to it, the scheduler either allocates and binds all pods together, or puts all of them into the pending state. It retries to schedule them all together in the next scheduling cycle.</p> <p>The scheduler also updates the status of the pods and their associate pod group, users are able to track the workload submission process both in the CLI or Run:ai UI.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#preemption","title":"Preemption","text":"<p>If the scheduler cannot find resources for the submitted workloads (and all of its associated pods), and the workload deserves resources either because it is under its queue quota or under its queue fairshare, the scheduler tries to reclaim resources from other queues; if this doesn\u2019t solve the resources issue, the scheduler tries to preempt lower priority preemptible workloads within the same queue.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#reclaim-preemption-between-projects-and-departments","title":"Reclaim preemption between projects (and departments)","text":"<p>Reclaim is an inter-project (and inter-department) resource balancing action that takes back resources from one project (or department) that has used them as an over-quota, back to a project (or department) that deserves those resources as part of its deserved quota, or to balance fairness between projects (or departments), so a project (or department) doesn\u2019t exceed its fairshare (portion of the unused resources).</p> <p>This mode of operation means that a lower priority workload submitted in one project (e.g. training) can reclaim resources from a project that runs a higher priority workload (e.g. preemptive workspace) if fairness balancing is required.</p> <p>Note</p> <p>Only preemptive workloads can go over-quota as they are susceptible to reclaim (cross-projects preemption) of the over-quota resources they are using. The amount of over-quota resources a project can gain depends on the over-quota priority or quota (if over-quota priority is disabled). Departments\u2019 over-quota is always proportional to its quota.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#priority-preemption-within-a-project","title":"Priority preemption within a project","text":"<p>Higher priority workloads may preempt lower priority preemptible workloads within the same project/node pool queue. For example, in a project that runs a training workload that exceeds the project quota for a certain node pool, a newly submitted workspace within the same project/node pool may stop (preempt) the training workload if there are not enough over-quota resources for the project within that node pool to run both workloads (e.g. workspace using in-quota resources and training using over-quota resources).</p> <p>There is no priority notion between workloads of different projects.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#quota-over-quota-and-fairshare","title":"Quota, over-quota, and fairshare","text":"<p>Run:ai scheduler strives to ensure fairness between projects and between departments, this means each department and project always strive to get their deserved quota, and unused resources are split between projects according to known rules (e.g. over-quota weights).</p> <p>If a project needs more resources even beyond its fairshare, and the scheduler finds unused resources that no other project needs, this project can consume resources even beyond its fairshare.</p> <p>Some scenarios can prevent the scheduler from fully providing the deserved quota and fairness promise, such as fragmentation or other scheduling constraints like affinities, taints etc.</p> <p>The example below illustrates a split of quota between different projects and departments, using several node pools:</p> <p>Legend:</p> <ul> <li>OQP = Over-quota priority</li> <li>OQ = Over-quota</li> </ul> <p></p> <p>The example below illustrates how fairshare is calculated per project/node pool and per department/node pool for the above example:</p> <p></p> <p>The Over quota (OQ) portion of each Project (per node pool) is calculated as:</p> <pre><code>[(OQ-Priority) / (\u03a3 Projects OQ-Priorities)] x (Unused Resource per node pool)\n</code></pre> <p>Fairshare(FS) is calculated as: the sum of Quota + Over-Quota</p> <p>Let\u2019s see how Project 2 over quota and fairshare are calculated:</p> <p>For this example, we assume that out of the 40 available GPUs in node pool A, 20 GPUs are currently unused (unused means either not part of any project\u2019s quota, or part of a project\u2019s quota but not used by any workloads of that project).</p> <p>Project 2 over quota share:</p> <pre><code>[(Project 2 OQ-Priority) / (\u03a3 all Projects OQ-Priorities)] x (Unused Resource within node pool A)\n\n[(3) / (2 + 3 + 1)] x (20) = (3/6) x 20 = 10 GPUs\n</code></pre> <p>Fairshare = deserved quota + over quota = 6 +10 = 16 GPUs</p> <p>Similarly, fairshare is also calculated for CPU and CPU memory.</p> <p>The scheduler can grant a project more resources than its fairshare if the scheduler finds resources not required by other projects that may deserve those resources.</p> <p>One can also see in the above illustration that Project 3 has no guaranteed quota, but it still has a share of the excess resources in node pool A. Run:ai Scheduler ensures that Project 3 receives its part of the unused resources for over quota, even if this results in reclaiming resources from other projects and preempting preemptible workloads.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#fairshare-balancing","title":"Fairshare balancing","text":"<p>The Scheduler constantly re-calculates the fairshare of each project and department (per node pool, represented in the scheduler as queues), resulting in the re-balancing of resources between projects and between departments. This means that a preemptible workload that was granted resources to run in one scheduling cycle, can find itself preempted and go back to the pending state waiting for resources on the next cycle.</p> <p>A queue, representing a scheduler-managed object for each Project or Department per node pool, can be in one of 3 states:</p> <ul> <li>__In-quota __    The queue\u2019s allocated resources \u2264 queue deserved quota  </li> <li>__Over-quota (but below fairshare) __    The queue\u2019s deserved quota &lt; queue\u2019s allocates resources &lt;= queue\u2019s fairshare  </li> <li>Over-Fairshare (and over-quota)   The queue\u2019s fairshare &lt; queue\u2019s allocated resources</li> </ul> <p></p> <p>The scheduler\u2019s first priority is to ensure each queue (representing a project/node pool or department/node pool scheduler object) receives its deserved quota. Then the scheduler tries to find and allocate more resources to queues that need resources beyond their deserved quota and up to their fairshare, finally, the scheduler tries to allocate resources to queues that need even more resources - beyond their fairshare.</p> <p>When re-balancing resources between queues of different projects and departments, the scheduler goes in the opposite direction, i.e. first take resources from over-fairshare queues, then from over-quota queues, and finally, in some scenarios, even from queues that are below their deserved quota.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#summary","title":"Summary","text":"<p>The scheduler\u2019s role is to bind any submitted pod to a node that satisfies the pod\u2019s requirements and constraints while adhering to the Run:ai quota and fairness system. In some scenarios, the scheduler finds a node for a pod (or nodes for a group of pods) immediately. In other scenarios, the scheduler has to preempt an already running workload to \u201cmake room\u201d, while sometimes a workload becomes pending until resources are released by other workloads (e.g. wait for other workloads to terminate), and only then it is scheduled and run.</p> <p>Other than scenarios where the requested resources or other constraints cannot be met within the cluster, either because the resources physically don\u2019t exist (e.g. a node with 16 GPUs, or a GPU with 200GB of memory), or a combination of constraints cannot be matched (e.g. a GPU with 80GB of memory together with a node with specific label or storage type), the scheduler eventually finds any workload its matching nodes to use, but this process may take some time.</p> <p>The Run:ai scheduler adheres to Kubernetes standard rules, but it also adds a layer of fairness between queues, queue hierarchy, node pools, and many more features, making the scheduling and Quota management more sophisticated, granular, and robust. The combination of these scheduler capabilities results in higher efficiency, scale, and maximization of cluster utilization.</p>"},{"location":"Researcher/tools/dev-jupyter/","title":"Use a Jupyter Notebook with a Run:ai Job","text":"<p>See the Jupyter Notebook Quickstart here.</p>"},{"location":"Researcher/tools/dev-pycharm/","title":"Use PyCharm with a Run:ai Job","text":"<p>Once you launch a workload using Run:ai, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook</p> <p>This document is about accessing the remote container created by Run:ai, from JetBrain's PyCharm. </p>"},{"location":"Researcher/tools/dev-pycharm/#submit-a-workload","title":"Submit a Workload","text":"<p>You will need your image to run an SSH server  (e.g OpenSSH). For the purposes of this document, we have created an image named <code>runai.jfrog.io/demo/pycharm-demo</code>. The image runs both python and ssh. Details on how to create the image are here. The image is configured to use the <code>root</code> user and password for SSH.</p> <p>Run the following command to connect to the container as if it were running locally:</p> <pre><code>runai submit build-remote -i runai.jfrog.io/demo/pycharm-demo --interactive  \\\n        --service-type=portforward --port 2222:22\n</code></pre> <p>The terminal will show the connection: </p> <pre><code>The job 'build-remote' has been submitted successfully\nYou can run `runai describe job build-remote -p team-a` to check the job status\nINFO[0007] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0045] Job started\nOpen access point(s) to service from localhost:2222\nForwarding from [::1]:2222 -&gt; 22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 2222</li> </ul> <p>Note</p> <pre><code>It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run:\n\n```\nrunai submit build-remote -i runai.jfrog.io/demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22\n```\n\n* The Job starts an sshd server on port 22.\n* The Job redirects the external port 30022 to port 22 and uses a [Node Port](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types){target=_blank} service type.\n* Run: `runai list worklaods`\n\n* Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222\n</code></pre>"},{"location":"Researcher/tools/dev-pycharm/#pycharm","title":"PyCharm","text":"<ul> <li>Under PyCharm | Preferences go to: Project | Python Interpreter </li> <li>Add a new SSH Interpreter. </li> <li>As Host, use the IP address above. Change the port to the above and use the Username <code>root</code></li> <li>You will be prompted for a password. Enter <code>root</code></li> <li>Apply settings and run the code via this interpreter. You will see your project uploaded to the container and running remotely. </li> </ul>"},{"location":"Researcher/tools/dev-tensorboard/","title":"Connecting to TensorBoard","text":"<p>Once you launch a Deep Learning workload using Run:ai, you may want to view its progress. A popular tool for viewing progress is TensorBoard.</p> <p>The document below explains how to use TensorBoard to view the progress or a Run:ai Job.</p>"},{"location":"Researcher/tools/dev-tensorboard/#emitting-tensorboard-logs","title":"Emitting TensorBoard Logs","text":"<p>When you submit a workload, your workload must save TensorBoard logs which can later be viewed. Follow this document on how to do this. You can also view the Run:ai sample code here.</p> <p>The code shows:</p> <ul> <li>A reference to a log directory:</li> </ul> <pre><code>log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n</code></pre> <ul> <li>A registered Keras callback for TensorBoard:</li> </ul> <pre><code>tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n\nmodel.fit(x_train, y_train,\n        ....\n        callbacks=[..., tensorboard_callback])\n</code></pre> <p>The <code>logs</code> directory must be saved on a Network File Server such that it can be accessed by the TensorBoard Job. For example, by running the Job as follows:</p> <pre><code>runai submit train-with-logs -i tensorflow/tensorflow:1.14.0-gpu-py3 \\\n  -v /mnt/nfs_share/john:/mydir -g 1  --working-dir /mydir --command -- ./startup.sh\n</code></pre> <p>Note the volume flag (<code>-v</code>) and working directory flag (<code>--working-dir</code>). The logs directory will be created on <code>/mnt/nfs_share/john/logs/fit</code>.</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-a-tensorboard-workload","title":"Submit a TensorBoard Workload","text":"<p>There are two ways to submit a TensorBoard Workload: via the Command-line interface or the user interface</p> User InterfaceCLI V1 <p>Browse to the provided Run:ai user interface and log in with your credentials.</p> <ul> <li>In the Run:ai UI select Workloads</li> <li>Select New Workload and then Workspace</li> <li>You should already have <code>Cluster</code>, <code>Project</code> and a <code>start from scratch</code> <code>Template</code> selected. Enter <code>tb</code> as the name and press CONTINUE.</li> <li>Under <code>Environment</code>,  select <code>jupyter-tensorboard</code>.</li> <li>Under <code>Compute Resource</code>, select <code>one-gpu</code>. </li> <li>Select CREATE WORKSPACE.</li> <li>In the workload list, add a column of <code>Connections</code></li> <li>When the workspace is running, you will see two connections:<ol> <li>Juypter</li> <li>TensorBoard </li> </ol> </li> </ul> <p>Run the following:</p> <pre><code>runai submit tb -i tensorflow/tensorflow:latest --interactive --service-type external-url --port 8888:8888  --working-dir /mydir  -v /mnt/nfs_share/john:/mydir  -- tensorboard --logdir logs/fit --port 8888 --host 0.0.0.0\n</code></pre> <p>The terminal will show the following: </p> <pre><code>The job 'tb' has been submitted successfully\nYou can run `runai describe job tb -p team-a` to check the job status\nINFO[0006] Waiting for job to start\nWaiting for job to start\nINFO[0014] Job started\nOpen access point(s) to service from localhost:8888\nForwarding from 127.0.0.1:8888 -&gt; 8888\nForwarding from [::1]:8888 -&gt; 8888\n</code></pre> <p>Browse to http://localhost:8888/ to view TensorBoard.</p> <p>Note</p> <p>A single TensorBoard Job can be used to view multiple deep learning Jobs, provided it has access to the logs directory for these Jobs. </p>"},{"location":"Researcher/tools/dev-vscode/","title":"Use Visual Studio Code with a Run:ai Job","text":"<p>Once you launch a workload using Run:ai, you will want to connect to it. You can do so via command line or via other tools such as a Jupyter Notebook</p> <p>Important</p> <p>This document is about accessing the remote container created by Run:ai, from the installed version of Visual Studio Code. If you want to use Visual Studio Code for web, please see Visual Studio Code Web Quickstart.</p>"},{"location":"Researcher/tools/dev-vscode/#submit-a-workload","title":"Submit a Workload","text":"<p>You will need your image to run an SSH server  (e.g OpenSSH). For this document, we have created an image named <code>runai.jfrog.io/demo/pycharm-demo</code>. The image runs both python and ssh. Details on how to create the image are here. The image is configured to use the <code>root</code> user and password for SSH.</p> <p>Run the following command to connect to the container as if it were running locally:</p> <pre><code>runai submit build-remote -i runai.jfrog.io/demo/pycharm-demo --interactive  \\\n        --service-type=portforward --port 2222:22\n</code></pre> <p>The terminal will show the connection: </p> <pre><code>The job 'build-remote' has been submitted successfully\nYou can run `runai describe job build-remote -p team-a` to check the job status\nINFO[0007] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0045] Job started\nOpen access point(s) to service from localhost:2222\nForwarding from [::1]:2222 -&gt; 22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 2222</li> </ul> <p>Note</p> <p>It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run:</p> <pre><code>runai submit build-remote -i runai.jfrog.io/demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The Job redirects the external port 30022 to port 22 and uses a Node Port service type.</li> <li> <p>Run: <code>runai list jobs</code></p> </li> <li> <p>Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222 </p> </li> </ul>"},{"location":"Researcher/tools/dev-vscode/#visual-studio-code","title":"Visual Studio Code","text":"<ul> <li>Under Visual Studio code install the Remote SSH extension.</li> <li>Create an ssh entry to the service by editing .ssh/config file or use the command Remote-SSH: Connect to Host... from the Command Palette.  Enter the IP address and port from above (e.g. ssh root@35.34.212.12 -p 30022 or ssh root@127.0.0.1 -p 2222). User and password are <code>root</code> </li> <li>Using VS Code, install the Python extension on the remote machine </li> <li>Write your first Python code and run it remotely.</li> </ul>"},{"location":"Researcher/tools/dev-x11forward-pycharm/","title":"Use PyCharm with X11 Forwarding and Run:ai","text":"<p>X11 is a window system for the Unix operating systems. X11 forwarding allows executing a program remotely through an SSH connection. Meaning, the executable file itself is hosted on a different machine than where the graphical interface is being displayed. The graphical windows are forwarded to your local machine through the SSH connection.</p> <p>This section is about setting up X11 forwarding from a Run:ai-based container to a PyCharm IDE on a remote machine.</p>"},{"location":"Researcher/tools/dev-x11forward-pycharm/#submit-a-workload","title":"Submit a Workload","text":"<p>You will need your image to run an SSH server  (e.g OpenSSH). For the purposes of this document, we have created an image named <code>runai.jfrog.io/demo/quickstart-x-forwarding</code>. The image runs:</p> <ul> <li>Python</li> <li>SSH Daemon configured for X11Forwarding </li> <li>OpenCV python library for image handling</li> </ul> <p>Details on how to create the image are here. The image is configured to use the <code>root</code> user and password for SSH.</p> <p>Run the following command to connect to the container as if it were running locally:</p> <pre><code>runai submit xforward-remote -i runai.jfrog.io/demo/quickstart-x-forwarding --interactive  \\\n        --service-type=portforward --port 2222:22\n</code></pre> <p>The terminal will show the connection:</p> <pre><code>The job 'xforward-remote' has been submitted successfully\nYou can run `runai describe job xforward-remote -p team-a` to check the job status\nINFO[0007] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0045] Job started\nOpen access point(s) to service from localhost:2222\nForwarding from [::1]:2222 -&gt; 22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 2222</li> </ul>"},{"location":"Researcher/tools/dev-x11forward-pycharm/#setup-the-x11-forwarding-tunnel","title":"Setup the X11 Forwarding Tunnel","text":"<p>Connect to the new Job by running:</p> <pre><code>ssh -X root@127.0.0.1 -p 2222\n</code></pre> <p>Note the <code>-X</code> flag. </p> <p>Run:</p> <p><pre><code>echo $DISPLAY\n</code></pre> Copy the value. It will be used as a PyCharm environment variable.</p> <p>Important</p> <p>The ssh terminal should remain active throughout the session.</p>"},{"location":"Researcher/tools/dev-x11forward-pycharm/#pycharm","title":"PyCharm","text":"<ul> <li>Under PyCharm | Preferences go to: Project | Python Interpreter</li> <li>Add a new SSH Interpreter.</li> <li>As Host, use <code>localhost</code>. Change the port to the above (<code>2222</code>) and use the Username <code>root</code>.</li> <li>You will be prompted for a password. Enter <code>root</code>.</li> <li>Make sure to set the correct path of the Python binary. In our case it's <code>/usr/local/bin/python</code>.</li> <li> <p>Apply your settings.</p> </li> <li> <p>Under PyCharm configuration set the following environment variables:</p> <ol> <li><code>DISPLAY</code> - set environment variable you copied before</li> <li><code>HOME</code> - In our case it's <code>/root</code>. This is required for the X11 authentication to work.</li> </ol> </li> </ul> <p>Run your code. You can use our sample code here.</p>"},{"location":"Researcher/workloads/inference-overview/","title":"Inference overview","text":""},{"location":"Researcher/workloads/inference-overview/#what-is-inference","title":"What is Inference","text":"<p>Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output.</p> <p>With Inference workloads, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time.</p>"},{"location":"Researcher/workloads/inference-overview/#inference-and-gpus","title":"Inference and GPUs","text":"<p>The Inference process is a subset of the original Training algorithm on a single datum (e.g. one sentence or one image), or a small batch. As such, GPU memory requirements are typically smaller than a full-blown Training process.</p> <p>Given that, Inference lends itself nicely to the usage of Run:ai Fractions. You can, for example, run 4 instances of an Inference server on a single GPU, each employing a fourth of the memory.</p>"},{"location":"Researcher/workloads/inference-overview/#inference-runai","title":"Inference @Run:ai","text":"<p>Run:ai provides Inference services as an equal part together with the other two Workload types: Train and Build.</p> <ul> <li> <p>Inference is considered a high-priority workload as it is customer-facing. Running an Inference workload (within the Project's quota) will preempt any Run:ai Workload marked as Training.</p> </li> <li> <p>Inference workloads will receive priority over Train and Build workloads during scheduling.</p> </li> <li> <p>Inference is implemented as a Kubernetes Deployment object with a defined number of replicas. The replicas are load-balanced by Kubernetes so adding more replicas will improve the overall throughput of the system.</p> </li> <li> <p>Multiple replicas will appear in Run:ai as a single Inference workload. The workload will appear in all Run:ai dashboards and views as well as the Command-line interface.</p> </li> <li> <p>Inference workloads can be submitted via Run:ai user interface as well as Run:ai API. Internally, spawning an Inference workload also creates a Kubernetes Service. The service is an end-point to which clients can connect.</p> </li> </ul>"},{"location":"Researcher/workloads/inference-overview/#autoscaling","title":"Autoscaling","text":"<p>To withstand SLA, Inference workloads are typically set with auto scaling. Auto-scaling is the ability to add more computing power (Kubernetes pods) when the load increases and shrink allocated resources when the system is idle. There are several ways to trigger autoscaling. Run:ai supports the following:</p> Metric Units Run:ai name Throughput requests/second throughput Concurrency concurrency <p>The Minimum and Maximum number of replicas can be configured as part of the autoscaling configuration.</p> <p>Autoscaling also supports a scale-to-zero policy with Throughput and Concurrency metrics, meaning that given enough time under the target threshold, the number of replicas will be scaled down to 0.</p> <p>This has the benefit of conserving resources at the risk of a delay from \"cold starting\" the model when traffic resumes.</p>"},{"location":"Researcher/workloads/inference-overview/#rolling-inference-updates","title":"Rolling inference updates","text":"<p>When deploying models and running inference workloads, it is relevant at times to update the workload configuration in a live manner, without impacting the important services that are provided by the workload.</p> <p>This means that an ML engineer can submit updates to an existing inference workload whether it is currently running, pending (or any other status).</p> <p>Following are a few examples of updates that can be implemented:</p> <ul> <li>Changing the container image to deploy a new version of the model  </li> <li>Changing different parameters (such as env variables)  </li> <li>Changing the compute resources to improve performance  </li> <li>Change the number of replicas and scale plan to adapt to requirement changes and scales</li> </ul> <p>As stated above, during the update and until its successful completion, the service that the workload provides is not jeopardized as these are production-grade workloads. Hence consumers can continue using the model (send prompts for example) during the updating process.</p> <p>During the update process of an inference workload, a new revision of pod(s) is created. This revision is the new desired specification of the workload. Although several updates can be submitted consecutively (even if the process of the previous update is not complete), the target goal (the desired specification) is always according to the last submitted update (the previous updates are ignored).</p> <p>Once the new revision is created completely (according to the desired spec) and up and running, the entire traffic of requests is navigated to the new revision, and the original workload is terminated. Then the update process is considered complete.</p> <p>It is important to note that:</p> <ul> <li> <p>To finish the inference workload update successfully, the project must have sufficient free GPU quota in favor of the update.   For example:  </p> <ul> <li> <p>Before the update: 3 replicas A running inference workload with 3 replicas (let's assume that each replica is equal to 1 GPU). This means the project is already using 3 GPUs of its quota. For the sake of simplicity, we will refer to this revision as revision #1.</p> </li> <li> <p>The update: 8 replicas This means, to complete the update, an additional 8 GPUs of free quota is needed. Only when the update is complete, the 3 GPUs used for the 1st revision are reclaimed.</p> </li> </ul> </li> <li> <p>The Workload grid in the user interface always displays the configuration of the desired specification (the latest submitted update). The status of the workload still represents the service status. For example, per the example described in point 1, during the update, the status of the workload is still \u201crunning\u201d as the service is still being provided to the consumers (using revision #1).</p> </li> <li> <p>The submission of inference updates is currently possible only via API. Following are the API fields that can be updated:  </p> <ul> <li>Command  </li> <li>Args  </li> <li>Image  </li> <li>imagePullPolicy  </li> <li>workingDir  </li> <li>createHomeDir  </li> <li>Probes  </li> <li>environmentVariables  </li> <li>Autoscaling</li> </ul> </li> <li> <p>As long as the update process is not completed, GPUs are not allocated to the replicas of the new revision. This prevents the allocation of idle GPUs so others will not be deprived using them.</p> </li> <li>If the update process is not completed within the default time limit of 10 minutes, it will automatically stop. At that point, all replicas of the new revision will be removed, and the original revision will continue to run normally.</li> <li>The default time limit for updates is configurable. Consider setting a longer duration if your workload requires extended time to pull the image due to its size, if the workload takes additional time to reach a 'READY' state due to a long initialization process, or if your cluster depends on autoscaling to allocate resources for new replicas. For example, to set the time limit to 30 minutes, you can run the following command: <pre><code>kubectl patch ConfigMap config-deployment -n knative-serving --type='merge' -p '{\"data\": {\"progress-deadline\": \"1800s\"}}'\n</code></pre></li> </ul>"},{"location":"Researcher/workloads/inference-overview/#inference-workloads-with-knative-new-behavior-in-v219","title":"Inference workloads with Knative new behavior in v2.19","text":"<p>Starting version 2.19, all pods of a single Knative revision are grouped under a single Pod-Group. This means that when a new Knative revision is created:</p> <ul> <li>It either succeeds in allocating the minimum number of pods; or </li> <li>It fails and moves into a pending state, to retry again later to allocate all pods with their resources. </li> </ul> <p>The resources (GPUs, CPUs) are not occupied by a new Knative revision until it succeeds in allocating all pods. The older revision pods are then terminated and release their resources (GPUs, CPUs) back to the cluster to be used by other workloads.</p>"},{"location":"Researcher/workloads/inference-overview/#see-also","title":"See Also","text":"<ul> <li>To set up Inference, see Cluster installation prerequisites.</li> <li>For running Inference see Inference quick-start.</li> <li>To run Inference using API see Workload overview.</li> </ul>"},{"location":"Researcher/workloads/trainings/","title":"Trainings","text":"<p>The Trainings interface provides a wizard to make submitting workloads easy.</p>"},{"location":"Researcher/workloads/trainings/#prerequisites","title":"Prerequisites","text":"<p>You must have:</p> <ul> <li>Workspaces enabled.</li> <li>At least one Project configured.</li> </ul> <p>Note</p> <p>See your system administrator to ensure the prerequisites are enabled and configured.</p>"},{"location":"Researcher/workloads/trainings/#adding-trainings","title":"Adding Trainings","text":"<p>Note</p> <p>Where there is a card gallery, use the search bar to find specific cards based on title or field values.</p> <p>To add a training:</p> <ol> <li>Press Tranings in the menu.</li> <li>In the Projects pane, select the destination project. Use the search box to find projects that are not listed. If you can't find the project, you can create your own, or see your system administrator.</li> <li>In the Multi-node pane, choose <code>Single node</code> for a single node training, or <code>Multi-node (distributed)</code> for distributed training. When you choose <code>Multi-node</code>, select a framework that is listed, then select the <code>multi-node</code> training configuration by selecting either <code>Workers &amp; master</code> or <code>Workers only</code>.</li> <li>In the Templates pane, select a template from the list. Use the search box to find templates that are not listed. If you can't find the specific template you need, see Creating a new template, or see your system administrator.</li> <li>In the Training name pane, enter a name for the Training, then press continue.</li> <li> <p>Select an environment from the tiles. If your environment is not listed, use the Search environments box to find it or press New environment to create a new environment. Press  to create an environment if needed. In the Set the connection for your tool(s), enter the URL of the tool if a custom URL has been enabled in the selected environment. Use the Private toggle to lock access to the tool to only the creator of the environment.</p> <p>In the Runtime Settings:</p> <ol> <li>Press Commands and Arguments to add special commands and arguments to your environment selection.</li> <li>Press Environment variable to add an environment variable. Press again if you need more environment variables.</li> </ol> </li> <li> <p>In the Compute resource pane:</p> <ol> <li>Select the number of workers for your training.</li> <li>Select Compute resources for your training or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> </ol> <p>Note</p> <p>The number of compute resources for the workers is based on the number of workers selected.</p> </li> <li> <p>(Optional) Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a *Volume persistency.</li> </ol> </li> <li> <p>(Optional) In the Data sources pane, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> </li> <li> <p>(Optional) In the General pane, add special settings for your training (optional):</p> <ol> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minutes, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails. (default = 30 days)</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>If you if selected  <code>Workers &amp; master</code> Press Continue to <code>Configure the master</code> and go to the next step. If not, then press Create training.</p> </li> <li> <p>If you do not want a different setup for the master, press Create training. If you would like to have a different setup for the master, toggle the switch to enable to enable a different setup.</p> <ol> <li>In the Environment pane select or create a new environment. Use the search box to find environments that are not listed. Press More settings to add an <code>Environment variable</code> or to edit the Command and Arguments field for the environment you selected.</li> <li>In the Compute resource pane, select a Compute resources for your training or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> <li> <p>(Optional) Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a *Volume persistency.</li> </ol> </li> <li> <p>(Optional) In the Data sources pane, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> </li> <li> <p>(Optional) In the General pane, add special settings for your training (optional):</p> <ol> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minutes, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails. (default = 30 days)</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> </ol> </li> <li> <p>When your training configuration is complete. press Create training.</p> </li> </ol>"},{"location":"Researcher/workloads/trainings/#managing-trainings","title":"Managing Trainings","text":"<p>The Trainings list contains a list of training jobs that you have created or have access to.</p> <p>To manage your trainings:</p> <ol> <li>Press Tranings in the left menu.</li> <li>Select a Training from the list.</li> <li>Choose from the following actions:<ul> <li>Activate\u2014activates the selected training job.</li> <li>Stop\u2014stops the selected training job.</li> <li>Connect\u2014connects to the training job's configured environment.</li> <li>Copy &amp; edit\u2014copies the details of the selected training job to a new training job.</li> <li>Delete\u2014deletes the current training session.</li> <li>Show details\u2014displays details about the training job.</li> </ul> </li> </ol>"},{"location":"Researcher/workloads/trainings/#training-details","title":"Training details","text":"<p>Training details are displayed using the Show details action. The details available per training job include;</p> <ul> <li>Event history\u2014a graph of the job's status over time along with a list of events found in the log.</li> <li> <p>Metrics\u2014a graph of available metrics for the job. Use the drop down select a date and a time slice. Metrics include:</p> </li> <li> <p>GPU utilization</p> </li> <li>GPU memory usage</li> <li>CPU usage</li> <li> <p>CPU memory usage</p> </li> <li> <p>Logs\u2014a log file of the current status. Use the download button to save the logs.</p> </li> </ul> <p>To hide the training details, press Hide details.</p>"},{"location":"Researcher/workloads/trainings/#download-trainings-table","title":"Download Trainings Table","text":"<p>You can download the Trainings table to a CSV file. Downloading a CSV can provide a snapshot history of your Trainings over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>To download the Trainings table to a CSV: 1. Open Trainings. 2. From the Columns icon, select the columns you would like to have displayed in the table. 3. Click on the ellipsis labeled More, and download the CSV.</p>"},{"location":"Researcher/workloads/assets/compute/","title":"Compute Resources","text":"<p>This article explains what compute resources are and how to create and use them.</p> <p>Compute resources are one type of workload asset. A compute resource is a template that simplifies how workloads are submitted and can be used by AI practitioners when they submit their workloads.</p> <p>A compute resource asset is a preconfigured building block that encapsulates all the specifications of compute requirements for the workload including:</p> <ul> <li>GPU devices and GPU memory  </li> <li>CPU memory and CPU compute</li> </ul>"},{"location":"Researcher/workloads/assets/compute/#compute-resource-table","title":"Compute resource table","text":"<p>The Compute resource table can be found under Compute resources in the Run:ai UI.</p> <p>The Compute resource table provides a list of all the compute resources defined in the platform and allows you to manage them.</p> <p></p> <p>The Compute resource table consists of the following columns:</p> Column Description Compute resource The name of the compute resource Description A description of the essence of the compute resource GPU devices request per pod The number of requested physical devices per pod of the workload that uses this compute resource GPU memory request per device The amount of GPU memory per requested device that is granted to each pod of the workload that uses this compute resource CPU memory request The minimum amount of CPU memory per pod of the workload that uses this compute resource CPU memory limit The maximum amount of CPU memory per pod of the workload that uses this compute resource CPU compute request The minimum number of CPU cores per pod of the workload that uses this compute resource CPU compute limit The maximum number of CPU cores per pod of the workload that uses this compute resource Scope The scope of this compute resource within the organizational tree. Click the name of the scope to view the organizational tree diagram Workload(s) The list of workloads associated with the compute resource Template(s) The list of workload templates that use this compute resource Created by The name of the user who created the compute resource Creation time The timestamp for when the rule was created Cluster The cluster that the compute resource is associated with"},{"location":"Researcher/workloads/assets/compute/#workloads-associated-with-the-compute-resource","title":"Workloads associated with the compute resource","text":"<p>Click one of the values in the Workload(s) column to view the list of workloads and their parameters.</p> Column Description Workload The workload that uses the compute resource Type (Workspace/Training/Inference) Status Represents the workload lifecycle. see the full list of workload status"},{"location":"Researcher/workloads/assets/compute/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table</li> </ul>"},{"location":"Researcher/workloads/assets/compute/#adding-new-compute-resource","title":"Adding new compute resource","text":"<p>To add a new compute resource:</p> <ol> <li>Go to the Compute resource table  </li> <li>Click +NEW COMPUTE RESOURCE </li> <li>Select under which cluster to create the compute resource  </li> <li>Select a scope </li> <li>Enter a name for the compute resource. The name must be unique.  </li> <li>Optional: Provide a description of the essence of the compute resource  </li> <li> <p>Set the resource types needed within a single node    (The Run:ai scheduler tries to match a single node that complies with the compute resource for each of the workload\u2019s pods)  </p> <ul> <li> <p>GPU </p> <ul> <li>GPU devices per pod The number of devices (physical GPUs) per pod    (for example, if you requested 3 devices per pod and the running workload using this compute resource consists of 3 pods, there are 9 physical GPU devices used in total)  </li> </ul> <p>Note</p> <ul> <li>You can insert a whole number of devices (0; 1; 2; 3; \u2026)  </li> <li>When setting it to zero, the workload using this computer resource neither requests or uses GPU resources while running  </li> <li>Only when setting it to 1, a fraction of a GPU memory can be requested  </li> <li>When setting a number higher than 1, the entire GPU memory of the devices is used by the running workloads  </li> </ul> <ul> <li>GPU memory per device <ul> <li>Select the memory request format  <ul> <li>% (of device) - Fraction of a GPU device\u2019s memory  </li> <li>MB (memory size) - An explicit GPU memory unit  </li> <li>GB (memory size) - An explicit GPU memory unit  </li> <li>Multi-instance GPU (MIG) - MIG profile (Deprecated)  </li> </ul> </li> <li>Set the memory Request - The minimum amount of GPU memory that is provisioned per device. This means that any pod of a running workload that uses this compute resource, receives this amount of GPU memory for each device(s) the pod utilizes  </li> <li>Optional: Set the memory Limit - The maximum amount of GPU memory that is provisioned per device. This means that any pod of a running workload that uses this compute resource, receives at most this amount of GPU memory for each device(s) the pod utilizes. To set a Limit, first enable the limit toggle. The limit value must be equal to or higher than the request.  </li> </ul> </li> </ul> <p>Note</p> <ul> <li>GPU memory limit is disabled by default. If you cannot see the Limit toggle in the compute resource form, then it must be enabled by your Administrator, under General Settings \u2192 Resources \u2192 GPU resource optimization  </li> <li>When a Limit is set and is bigger than the Request, the scheduler allows each pod to reach the maximum amount of GPU memory in an opportunistic manner (only upon availability).  </li> <li>If the GPU Memory Limit is bigger that the Request the pod is prone to be killed by the Run:ai toolkit (out of memory signal). The greater the difference between the GPU memory used and the request, the higher the risk of being killed  </li> <li>If GPU resource optimization is turned off, the minimum and maximum are in fact equal  </li> </ul> </li> <li> <p>CPU </p> <ul> <li>CPU compute per pod <ul> <li>Select the units for the CPU compute (Cores / Millicores)  </li> <li>Set the CPU compute Request - the minimum amount of CPU compute that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives this amount of CPU compute for each pod.  </li> <li>Optional: Set the CPU compute Limit - The maximum amount of CPU compute that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives at most this amount of CPU compute. To set a Limit, first enable the limit toggle. The limit value must be equal to or higher than the request. By default, the limit is set to \u201cUnlimited\u201d - which means that the pod may consume all the node's free CPU compute resources.  </li> </ul> </li> <li>CPU memory per pod <ul> <li>Select the units for the CPU memory (MB / GB)  </li> <li>Set the CPU memory Request - The minimum amount of CPU memory that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives this amount of CPU memory for each pod.  </li> <li>Optional: Set the CPU memory Limit - The maximum amount of CPU memory that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives at most this amount of CPU memory. To set a Limit, first enable the limit toggle. The limit value must be equal to or higher than the request. By default, the limit is set to \u201cUnlimited\u201d - Meaning that the pod may consume all the node's free CPU memory resources.  </li> </ul> </li> </ul> <p>Note</p> <p>If the CPU Memory Limit is bigger that the Request the pod is prone to be killed by the operating system (out of memory signal). The greater the difference between the CPU memory used and the request, the higher the risk of being killed.  </p> </li> </ul> </li> <li> <p>Optional: More settings  </p> <ul> <li>Increase shared memory size When enabled, the shared memory size available to the pod is increased from the default 64MB to the node's total available memory or the CPU memory limit, if set above.  </li> <li>Set extended resource(s) Click +EXTENDED RESOURCES to add resource/quantity pairs. For more information on how to set extended resources, see the Extended resources and Quantity guides  </li> </ul> </li> <li> <p>Click CREATE COMPUTE RESOURCE</p> <p>Note</p> <p>It is also possible to add compute resources directly when creating a specific Workspace, training or inference workload.</p> </li> </ol>"},{"location":"Researcher/workloads/assets/compute/#editing-a-compute-resource","title":"Editing a compute resource","text":"<p>To edit a compute resource:</p> <ol> <li>Select the compute resource from the table  </li> <li>Click RENAME to edit its name and description</li> </ol> <p>Note</p> <p>Additional fields can be edited using the API.</p>"},{"location":"Researcher/workloads/assets/compute/#copying-editing-a-compute-resource","title":"Copying &amp; editing a compute resource","text":"<p>To copy &amp; edit a compute resource:</p> <ol> <li>Select the compute resource you want to duplicate  </li> <li>Click COPY &amp; EDIT </li> <li>Update the compute resource and click CREATE COMPUTE RESOURCE</li> </ol>"},{"location":"Researcher/workloads/assets/compute/#deleting-a-compute-resource","title":"Deleting a compute resource","text":"<ol> <li>Select the compute resource you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion  </li> </ol> <p>Note</p> <p>It is not possible to delete a compute resource being used by an existing workload and template.</p>"},{"location":"Researcher/workloads/assets/compute/#using-api","title":"Using API","text":"<p>Go to the Compute resources API reference to view the available actions</p>"},{"location":"Researcher/workloads/assets/credentials/","title":"Credentials","text":"<p>This article explains what credentials are and how to create and use them.</p> <p>Credentials are a workload asset that simplify the complexities of Kubernetes secrets. They consist of and mask sensitive access information, such as passwords, tokens, and access keys, which are necessary for gaining access to various resources.</p> <p>Credentials are crucial for the security of AI workloads and the resources they require, as they restrict access to authorized users, verify identities, and ensure secure interactions. By enforcing the protection of sensitive data, credentials help organizations comply with industry regulations, fostering a secure environment overall.</p> <p>Essentially, credentials enable AI practitioners to access relevant protected resources, such as private data sources and Docker images, thereby streamlining the workload submission process.</p>"},{"location":"Researcher/workloads/assets/credentials/#credentials-table","title":"Credentials table","text":"<p>The Credentials table can be found under Credentials in the Run:ai User interface.</p> <p>The Credentials table provides a list of all the credentials defined in the platform and allows you to manage them.</p> <p></p> <p>The Credentials table comprises the following columns:</p> Column Description Credentials The name of the credentials Description A description of the credentials Type The type of credentials, e.g., Docker registry Status The different lifecycle phases and representation of the credentials\u2019 condition Data source(s) The private data source(s) that are accessed using the credentials Created by The user who created the credentials Scope The scope of this compute resource within the organizational treeClick the name of the scope to view the organizational tree diagram Creation time The timestamp of when the credentials were created Cluster The cluster with which the credentials are associated"},{"location":"Researcher/workloads/assets/credentials/#credentials-status","title":"Credentials status","text":"<p>The following table describes the credentials\u2019 condition and whether they were created successfully for the selected scope.</p> Status Description No issues found No issues were found while creating the credentials (this status may change while propagating the credentials to the selected scope) Issues found Issues found while propagating the credentials Issues found Failed to access the cluster Creating\u2026 Credentials are being created Deleting\u2026 Credentials are being deleted No status When the credentials\u2019 scope is an account, or the current version of the cluster is not up to date, the status cannot be displayed"},{"location":"Researcher/workloads/assets/credentials/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values</li> <li>Search - Click SEARCH and type the value to search by</li> <li>Sort - Click each column header to sort by</li> <li>Column selection - Click COLUMNS and select the columns to display in the table</li> <li>Refresh - Click REFRESH to update the table with the latest data</li> </ul>"},{"location":"Researcher/workloads/assets/credentials/#adding-new-credentials","title":"Adding new credentials","text":"<p>Creating credentials is limited to specific roles.</p> <p>To add a new credential:</p> <ol> <li>Go to the Credentials table:</li> <li>Click +NEW CREDENTIALS</li> <li>Select the credential type from the list     Follow the step-by-step guide for each credential type:</li> </ol>"},{"location":"Researcher/workloads/assets/credentials/#docker-registry","title":"Docker registry","text":"<p>These credentials allow users to authenticate and pull images from a Docker registry, enabling access to containerized applications and services.</p> <p>After creating the credentials, it is used automatically when pulling images.</p> <ol> <li>Select a scope.</li> <li>Enter a name for the credential. The name must be unique.</li> <li>Optional: Provide a description of the credentials</li> <li>Set how the credential is created<ul> <li>Existing secret (in the cluster)     This option applies when the purpose is to create credentials based on an existing secret<ul> <li>Select a secret from the list (The list is empty if no secrets were created in advance)</li> </ul> </li> <li>New secret (recommended)     A new secret is created together with the credentials. New secrets are not added to the list of existing secrets.<ul> <li>Enter the username, password, and Docker registry URL</li> </ul> </li> </ul> </li> <li>Click CREATE CREDENTIALS</li> </ol> <p>After the credentials are created, check their status to monitor their proper creation across the selected scope.</p>"},{"location":"Researcher/workloads/assets/credentials/#access-key","title":"Access key","text":"<p>These credentials are unique identifiers used to authenticate and authorize access to cloud services or APIs, ensuring secure communication between applications. They typically consist of two parts:</p> <ul> <li>An access key ID</li> <li>A secret access key</li> </ul> <p>The purpose of this credential type is to allow access to restricted data.</p> <ol> <li>Select a scope.</li> <li>Enter a name for the credential. The name must be unique.</li> <li>Optional: Provide a description of the credential</li> <li>Set how the credential is created<ul> <li>Existing secret (in the cluster)     This option applies when the purpose is to create credentials based on an existing secret<ul> <li>Select a secret from the list (The list is empty if no secrets were created in advance)</li> </ul> </li> <li>New secret (recommended)     A new secret is created together with the credentials. New secrets are not added to the list of existing secrets.<ul> <li>Enter the Access key and Access secret</li> </ul> </li> </ul> </li> <li>Click CREATE CREDENTIALS</li> </ol> <p>After the credentials are created, check their status to monitor their proper creation across the selected scope.</p>"},{"location":"Researcher/workloads/assets/credentials/#username-password","title":"Username &amp; password","text":"<p>These credentials require a username and corresponding password to access various resources, ensuring that only authorized users can log in.</p> <p>The purpose of this credential type is to allow access to restricted data.</p> <ol> <li>Select a scope</li> <li>Enter a name for the credential. The name must be unique.</li> <li>Optional: Provide a description of the credentials</li> <li>Set how the credential is created<ul> <li>Existing secret (in the cluster)     This option applies when the purpose is to create credentials based on an existing secret<ul> <li>Select a secret from the list (The list is empty if no secrets were created in advance)</li> </ul> </li> <li>New secret (recommended)     A new secret is created together with the credentials. New secrets are not added to the list of existing secrets.<ul> <li>Enter the username and password</li> </ul> </li> </ul> </li> <li>Click CREATE CREDENTIALS</li> </ol> <p>After the credentials are created, check their status to monitor their proper creation across the selected scope.</p>"},{"location":"Researcher/workloads/assets/credentials/#generic-secret","title":"Generic secret","text":"<p>These credentials are a flexible option that consists of multiple keys &amp; values and can store various sensitive information, such as API keys or configuration data, to be used securely within applications.</p> <p>The purpose of this credential type is to allow access to restricted data.</p> <ol> <li>Select a scope</li> <li>Enter a name for the credential. The name must be unique.</li> <li>Optional: Provide a description of the credentials</li> <li>Set how the credential is created<ul> <li>Existing secret (in the cluster)     This option applies when the purpose is to create credentials based on an existing secret<ul> <li>Select a secret from the list (The list is empty if no secrets were created in advance)</li> </ul> </li> <li>New secret (recommended)     A new secret is created together with the credentials. New secrets are not added to the list of existing secrets.<ul> <li>Click +KEY &amp; VALUE - to add key/value pairs to store in the new secret</li> </ul> </li> </ul> </li> <li>Click CREATE CREDENTIALS</li> </ol>"},{"location":"Researcher/workloads/assets/credentials/#editing-credentials","title":"Editing credentials","text":"<p>To rename a credential:</p> <ol> <li>Select the credential from the table</li> <li>Click Rename to edit its name and description</li> </ol>"},{"location":"Researcher/workloads/assets/credentials/#deleting-credentials","title":"Deleting credentials","text":"<p>To delete a credential:</p> <ol> <li>Select the credential you want to delete</li> <li>Click DELETE</li> <li>In the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>Credentials cannot be deleted if they are being used by a workload and template.</p>"},{"location":"Researcher/workloads/assets/credentials/#using-credentials","title":"Using credentials","text":"<p>You can use credentials (secrets) in various ways within the system</p>"},{"location":"Researcher/workloads/assets/credentials/#access-private-data-sources","title":"Access private data sources","text":"<p>To access private data sources, attach credentials to data sources of the following types: Git, S3 Bucket</p>"},{"location":"Researcher/workloads/assets/credentials/#use-directly-within-the-container","title":"Use directly within the container","text":"<p>To use the secret directly from within the container, you can choose between the following options</p> <ol> <li>Get the secret mounted to the file system by using the Generic secret data source</li> <li>Get the secret as an environment variable injected into the container. There are two equivalent ways to inject the environment variable.       a. By adding it to the Environment asset.        b. By adding it ad-hoc as part of the workload.</li> </ol>"},{"location":"Researcher/workloads/assets/credentials/#creating-secrets-in-advance","title":"Creating secrets in advance","text":"<p>Add secrets in advance to be used when creating credentials via the Run:ai UI.</p> <p>Follow the steps below for each required scope:</p> Cluster scopeDepartment scopeProject scope <ol> <li>Create the secret in the Run:ai namespace (runai)</li> <li>To authorize Run:ai to use the secret, label it: <code>run.ai/cluster-wide: \"true\"</code></li> <li>Label the secret with the correct credential type:<ol> <li>Docker registry - <code>run.ai/resource: \"docker-registry\"</code></li> <li>Access key - <code>run.ai/resource: \"access-key\"</code></li> <li>Username and password - <code>run.ai/resource: \"password\"</code></li> <li>Generic secret - <code>run.ai/resource: \"generic\"</code> \u05bf</li> </ol> </li> </ol> <ol> <li>Create the secret in the Run:ai namespace (runai)</li> <li>To authorize Run:ai to use the secret, label it: <code>run.ai/department: \"&lt;department id&gt;\"</code></li> <li>Label the secret with the correct credential type:<ol> <li>Docker registry - <code>run.ai/resource: \"docker-registry\"</code></li> <li>Access key - <code>run.ai/resource: \"access-key\"</code></li> <li>Username and password - <code>run.ai/resource: \"password\"</code></li> <li>Generic secret - <code>run.ai/resource: \"generic\"</code></li> </ol> </li> </ol> <ol> <li>Create the secret in the project\u2019s namespace</li> <li>Label the secret with the correct credential type:<ol> <li>Docker registry - <code>run.ai/resource: \"docker-registry\"</code></li> <li>Access key - <code>run.ai/resource: \"access-key\"</code></li> <li>Username and password - <code>run.ai/resource: \"password\"</code></li> <li>Generic secret - <code>run.ai/resource: \"generic\"</code></li> </ol> </li> </ol> <p>The secret is now displayed for that scope in the list of existing secrets.</p>"},{"location":"Researcher/workloads/assets/credentials/#using-api","title":"Using API","text":"<p>To view the available actions, go to the Credentials API reference</p>"},{"location":"Researcher/workloads/assets/data-volumes/","title":"Data Volumes","text":"<p>Data volumes offer a powerful solution for storing, managing, and sharing AI training data within the Run:ai platform. They promote collaboration, simplify data access control, and streamline the AI development lifecycle.</p> <p>Data volumes are snapshots of datasets stored in Kubernetes Persistent Volume Claims (PVCs). They act as a central repository for training data.</p>"},{"location":"Researcher/workloads/assets/data-volumes/#why-use-a-data-volume","title":"Why use a data volume?","text":"<ol> <li>Sharing with multiple scopes     Unlike other Run:ai data sources, data volumes can be shared across projects, departments, or clusters, encouraging data reuse and collaboration within the organization.</li> <li>Storage saving     A single copy of the data can be used across multiple scopes</li> </ol>"},{"location":"Researcher/workloads/assets/data-volumes/#typical-use-cases","title":"Typical use cases","text":"<ol> <li>Sharing large data sets     In large organizations, the data is often stored in a remote location, which can be a barrier for large model training. Even if the data is transferred into the cluster, sharing it easily with multiple users is still challenging. Data volumes can help share the data seamlessly, with maximum security and control.</li> <li>Sharing data with colleagues     When sharing training results, generated data sets, or other artifacts with team members is needed, data volumes can help make the data available easily.</li> </ol>"},{"location":"Researcher/workloads/assets/data-volumes/#prerequisites","title":"Prerequisites","text":"<p>To create a data volume, there must be a project with a PVC in its namespace.</p> <p>Working with data volumes is currently available using the API. To view the available actions, go to the Data volumes API reference.</p>"},{"location":"Researcher/workloads/assets/data-volumes/#adding-a-new-data-volume","title":"Adding a new data volume","text":"<p>Data volume creation is limited to specific roles</p>"},{"location":"Researcher/workloads/assets/data-volumes/#adding-scopes-for-a-data-volume","title":"Adding scopes for a data volume","text":"<p>Data volume sharing (adding scopes) is limited to specific roles</p> <p>Once created, the data volume is available to its originating project (see the prerequisites above).</p> <p>Data volumes can be shared with additional scopes in the organization.</p>"},{"location":"Researcher/workloads/assets/data-volumes/#who-can-use-a-data-volume","title":"Who can use a data volume?","text":"<p>Data volumes are used when submitting workloads. Any user, application or SSO group with a role that has permissions to create workloads can also use data volumes.</p> <p>Researchers can list available data volumes within their permitted scopes for easy selection.</p>"},{"location":"Researcher/workloads/assets/datasources/","title":"Data Sources","text":"<p>This article explains what data sources are and how to create and use them.</p> <p>Data sources are a type of workload asset and represent a location where data is actually stored. They may represent a remote data location, such as NFS, Git, or S3, or a Kubernetes local resource, such as PVC, ConfigMap, HostPath, or Secret.</p> <p>This configuration simplifies the mapping of the data into the workload\u2019s file system and handles the mounting process during workload creation for reading and writing. These data sources are reusable and can be easily integrated and used by AI practitioners while submitting workloads across various scopes.</p>"},{"location":"Researcher/workloads/assets/datasources/#data-sources-table","title":"Data sources table","text":"<p>The data sources table can be found under Data sources in the Run:ai platform.</p> <p>The data sources table provides a list of all the data sources defined in the platform and allows you to manage them.</p> <p></p> <p>The data sources table comprises the following columns:</p> Column Description Data source The name of the data source Description A description of the data source Type The type of data source connected \u2013 e.g., S3 bucket, PVC, or others Status The different lifecycle phases and representation of the data source condition Scope The scope of the data source within the organizational tree. Click the scope name to view the organizational tree diagram Workload(s) The list of existing workloads that use the data source Template(s) The list of workload templates that use the data source Created by The user who created the data source Creation time The timestamp for when the data source was created Cluster The cluster that the data source is associated with"},{"location":"Researcher/workloads/assets/datasources/#data-sources-status","title":"Data sources status","text":"<p>The following table describes the data sources' condition and whether they were created successfully for the selected scope.</p> Status Description No issues found No issues were found while creating the data source Issues found Issues were found while propagating the data source credentials Issues found The cluster could not be accessed Creating\u2026 The data source is being created No status / \u201c-\u201d When the data source\u2019s scope is an account, the current version of the cluster is not up to date, or the asset is not a cluster-syncing entity, the status can\u2019t be displayed"},{"location":"Researcher/workloads/assets/datasources/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values</li> <li>Search - Click SEARCH and type the value to search by</li> <li>Sort - Click each column header to sort by</li> <li>Column selection - Click COLUMNS and select the columns to display in the table</li> <li>Download table - Click MORE and then click \u2018Download as CSV\u2019</li> <li>Refresh - Click REFRESH to update the table with the latest data</li> </ul>"},{"location":"Researcher/workloads/assets/datasources/#adding-a-new-data-source","title":"Adding a new data source","text":"<p>To create a new data source:</p> <ol> <li>Click +NEW DATA SOURCE</li> <li>Select the data source type from the list. Follow the step-by-step guide for each data source type:</li> </ol>"},{"location":"Researcher/workloads/assets/datasources/#nfs","title":"NFS","text":"<p>A Network File System (NFS) is a Kubernetes concept used for sharing storage in the cluster among different pods. Like a PVC, the NFS volume\u2019s content remains preserved, even outside the lifecycle of a single pod. However, unlike PVCs, which abstract storage management, NFS provides a method for network-based file sharing. The NFS volume can be pre-populated with data and can be mounted by multiple pod writers simultaneously. At Run:ai, an NFS-type data source is an abstraction that is mapped directly to a Kubernetes NFS volume. This integration allows multiple workloads under various scopes to mount and present the NFS data source.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Set the data origin<ul> <li>Enter the NFS server (host name or host IP)</li> <li>Enter the NFS path</li> </ul> </li> <li>Set the data target location<ul> <li>Container path</li> </ul> </li> <li>Optional: Restrictions<ul> <li>Prevent data modification - When enabled, the data will be mounted with read-only permissions</li> </ul> </li> <li>Click CREATE DATA SOURCE</li> </ol>"},{"location":"Researcher/workloads/assets/datasources/#pvc","title":"PVC","text":"<p>A Persistent Volume Claim (PVC) is a Kubernetes concept used for managing storage in the cluster, which can be provisioned by an administrator or dynamically by Kubernetes using a StorageClass. PVCs allow users to request specific sizes and access modes (read/write once, read-only many). At Run:ai, a PVC-type data source is an abstraction that is mapped directly to a Kubernetes PVC. By leveraging PVCs as data sources, Run:ai enables access to persistent storage for workloads, ensuring that data remains consistent and accessible across various scopes and workloads, beyond the lifecycle of individual pods. This ensures that data generated by AI workloads is not lost when pods are rescheduled or updated, providing a seamless and efficient storage solution that can handle the large datasets typically associated with AI projects.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Select PVC:<ul> <li>Existing PVC     This option is relevant when the purpose is to create a PVC-type data source based on an existing PVC in the cluster<ul> <li>Select a PVC from the list - (The list is empty if no existing PVCs were created in advance)</li> </ul> </li> <li>New PVC - creates a new PVC in the cluster. New PVCs are not added to the Existing PVCs list.     When creating a PVC-type data source and selecting the \u2018New PVC\u2019 option, the PVC is immediately created in the cluster (even if no workload has requested this PVC).</li> </ul> </li> <li>Select the storage class<ul> <li>None - Proceed without defining a storage class</li> <li>Custom storage class - This option applies when selecting a storage class based on existing storage classes.     To add new storage classes to the storage class list, and for additional information, check Kubernetes storage classes</li> </ul> </li> <li>Select the access mode(s) (multiple modes can be selected)<ul> <li>Read-write by one node - The volume can be mounted as read-write by a single node.</li> <li>Read-only by many nodes - The volume can be mounted as read-only by many nodes.</li> <li>Read-write by many nodes - The volume can be mounted as read-write by many nodes.</li> </ul> </li> <li>Set the claim size and its units</li> <li>Select the volume mode</li> <li>Set the data target location<ul> <li>container path</li> </ul> </li> <li>Optional: Prevent data modification - When enabled, the data will be mounted with read-only permission.</li> <li>Click CREATE DATA SOURCE</li> </ol> <p>After the data source is created, check its status to monitor its proper creation across the selected scope.</p>"},{"location":"Researcher/workloads/assets/datasources/#s3-bucket","title":"S3 Bucket","text":"<p>The S3 bucket data source enables the mapping of a remote S3 bucket into the workload\u2019s file system. Similar to a PVC, this mapping remains accessible across different workload executions, extending beyond the lifecycle of individual pods. However, unlike PVCs, data stored in an S3 bucket resides remotely, which may lead to decreased performance during the execution of heavy machine learning workloads. As part of the Run:ai connection to the S3 bucket, you can create credentials in order to access and map private buckets.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Set the data origin<ul> <li>Set the S3 service URL</li> <li>Select the credentials<ul> <li>None - for public buckets</li> <li>Credential names - This option is relevant for private buckets based on existing credentials that were created for the scope.     To add new credentials to the credentials list, and for additional information, check the Credentials article.</li> </ul> </li> <li>Enter the bucket name</li> </ul> </li> <li>Set the data target location<ul> <li>container path</li> </ul> </li> <li>Click CREATE DATA SOURCE</li> </ol> <p>After a private data source is created, check its status to monitor its proper creation across the selected scope.</p>"},{"location":"Researcher/workloads/assets/datasources/#git","title":"Git","text":"<p>A Git-type data source is a Run:ai integration, that enables code to be copied from a Git branch into a dedicated folder in the container. It is mainly used to provide the workload with the latest code repository. As part of the integration with Git, in order to access private repositories, you can add predefined credentials to the data source mapping.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Set the data origin<ul> <li>Set the Repository URL</li> <li>Set the Revision (branch, tag, or hash)- If left empty, it will use the 'HEAD' (latest)</li> <li>Select the credentials<ul> <li>None - for public repositories</li> <li>Credential names - This option applies to private repositories based on existing credentials that were created for the scope.     To add new credentials to the credentials list, and for additional information, check the Credentials article.</li> </ul> </li> </ul> </li> <li>Set the data target location<ul> <li>container path</li> </ul> </li> <li>Click CREATE DATA SOURCE</li> </ol> <p>After a private data source is created, check its status to monitor its proper creation across the selected scope.</p>"},{"location":"Researcher/workloads/assets/datasources/#host-path","title":"Host path","text":"<p>A Host path volume is a Kubernetes concept that enables mounting a host path file or a directory on the workload\u2019s file system. Like a PVC, the host path volume\u2019s data persists across workloads under various scopes. It also enables data serving from the hosting node.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Set the data origin<ul> <li>host path</li> </ul> </li> <li>Set the data target location<ul> <li>container path</li> </ul> </li> <li>Optional: Prevent data modification - When enabled, the data will be mounted with read-only permissions.</li> <li>Click CREATE DATA SOURCE</li> </ol>"},{"location":"Researcher/workloads/assets/datasources/#configmap","title":"ConfigMap","text":"<p>A ConfigMap data source is a Run:ai abstraction for the Kubernetes ConfigMap concept. The ConfigMap is used mainly for storage that can be mounted on the workload container for non-confidential data. It is usually represented in key-value pairs (e.g., environment variables, command-line arguments etc.). It allows you to decouple environment-specific system configurations from your container images, so that your applications are easily portable. ConfigMaps must be created on the cluster prior to being used within the Run:ai system.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Set the data origin<ul> <li>Select the ConfigMap name (The list is empty if no existing ConfigMaps were created in advance).</li> </ul> </li> <li>Set the data target location<ul> <li>container path</li> </ul> </li> <li>Click CREATE DATA SOURCE</li> </ol>"},{"location":"Researcher/workloads/assets/datasources/#secret","title":"Secret","text":"<p>A secret-type data source enables the mapping of a credential into the workload\u2019s file system. Credentials are a workload asset that simplify the complexities of Kubernetes Secrets. The credentials mask sensitive access information, such as passwords, tokens, and access keys, which are necessary for gaining access to various resources.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Set the data origin<ul> <li>Select the credentials     To add new credentials, and for additional information, check the Credentials article.</li> </ul> </li> <li>Set the data target location<ul> <li>container path</li> </ul> </li> <li>Click CREATE DATA SOURCE</li> </ol> <p>After the data source is created, check its status to monitor its proper creation across the selected scope.</p> <p>Note</p> <p>It is also possible to add data sources directly when creating a specific workspace, training or inference workload</p>"},{"location":"Researcher/workloads/assets/datasources/#editing-a-data-source","title":"Editing a data source","text":"<p>To edit a data source:</p> <ol> <li>Select the data source from the table</li> <li>Click Rename to provide it with a new name</li> <li>Click Copy &amp; Edit to make any changes to the data source</li> </ol>"},{"location":"Researcher/workloads/assets/datasources/#deleting-a-data-source","title":"Deleting a data source","text":"<p>To delete a data source:</p> <ol> <li>Select the data source you want to delete</li> <li>Click DELETE</li> <li>Confirm you want to delete the data source</li> </ol> <p>Note</p> <p>It is not possible to delete an environment being used by an existing workload or template.</p>"},{"location":"Researcher/workloads/assets/datasources/#using-api","title":"Using API","text":"<p>To view the available actions, go to the Data sources API reference.</p>"},{"location":"Researcher/workloads/assets/environments/","title":"Environments","text":"<p>This article explains what environments are and how to create and use them.</p> <p>Environments are one type of workload asset. An environment consists of a configuration that simplifies how workloads are submitted and can be used by AI practitioners when they submit their workloads.</p> <p>An environment asset is a preconfigured building block that encapsulates aspects for the workload such as:</p> <ul> <li>Container image and container configuration  </li> <li>Tools and connections  </li> <li>The type of workload it serves</li> </ul>"},{"location":"Researcher/workloads/assets/environments/#environments-table","title":"Environments table","text":"<p>The Environments table can be found under Environments in the Run:ai platform.</p> <p>The Environment table provides a list of all the environment defined in the platform and allows you to manage them.</p> <p></p> <p>The Environments table consists of the following columns:</p> Column Description Environment The name of the environment Description A description of the essence of the environment Scope The scope of this environment within the organizational tree. Click the name of the scope to view the organizational tree diagram Image The application or service to be run by the workload Workload Architecture This can be either standard for running workloads on a single node or distributed for running distributed workloads on a multiple nodes Tool(s) The tools and connection types the environment exposes Workload(s) The list of existing workloads that use the environment Workload types The workload types that can use the environment Template(s) The list of workload templates that use this environment Created by The user who created the environment. By default Run:ai UI comes with preinstalled environments created by Run:ai Creation time The timestamp for when the environment was created Cluster The cluster that the environment is associated with"},{"location":"Researcher/workloads/assets/environments/#tools-associated-with-the-environment","title":"Tools associated with the environment","text":"<p>Click one of the values in the tools column to view the list of tools and their connection type.</p> Column Description Tool name The name of the tool or application AI practitioner can set up within the environment. Connection type The method by which you can access and interact with the running workload. It's essentially the \"doorway\" through which you can reach and use the tools the workload provide. (E.g node port, external URL, etc)"},{"location":"Researcher/workloads/assets/environments/#workloads-associated-with-the-environment","title":"Workloads associated with the environment","text":"<p>Click one of the values in the Workload(s) column to view the list of workloads and their parameters.</p> Column Description Workload The workload that uses the environment Type The workload type (Workspace/Training/Inference) Status Represents the workload lifecycle. see the full list of workload status"},{"location":"Researcher/workloads/assets/environments/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"Researcher/workloads/assets/environments/#environments-created-by-runai","title":"Environments created by Run:ai","text":"<p>When installing Run:ai, you automatically get the environment created by Run:ai to ease up the onboarding process and support different use cases out of the box. These environments are created at the scope of the account.</p> Environment Image Jupiter-lab jupyter/scipy-notebook jupyter-tensorboard gcr.io/run-ai-demo/jupyter-tensorboard tensorboard tensorflow/tensorflow:latest llm-server runai.jfrog.io/core-llm/runai-vllm:v0.5.5-0.5.0 chatbot-ui runai.jfrog.io/core-llm/llm-app gpt2 runai.jfrog.io/core-llm/quickstart-inference:gpt2-cpu"},{"location":"Researcher/workloads/assets/environments/#adding-a-new-environment","title":"Adding a new environment","text":"<p>Environment creation is limited to specific roles</p> <p>To add a new environment:</p> <ol> <li>Go to the Environments table  </li> <li>Click +NEW ENVIRONMENT </li> <li>Select under which cluster to create the environment  </li> <li>Select a scope </li> <li>Enter a name for the environment. The name must be unique.  </li> <li>Optional: Provide a description of the essence of the environment  </li> <li>Enter the Image URL    If a token or secret is required to pull the image, it is possible to create it via credentials of type docker registry. These credentials are automatically used once the image is pulled (which happens when the workload is submitted)  </li> <li>Set the image pull policy - the condition for when to pull the image from the registry  </li> <li>Set the workload architecture:  <ul> <li>Standard Only standard workloads can use the environment. A standard workload consists of a single process.  </li> <li>Distributed Only distributed workloads can use the environment. A distributed workload consists of multiple processes working together. These processes can run on different nodes.  </li> <li>Select a framework from the list.  </li> </ul> </li> <li>Set the workload type:  <ul> <li>Workspace </li> <li>Training </li> <li>Inference </li> <li>When inference is selected, define the endpoint of the model by providing both the protocol and the container\u2019s serving port  </li> </ul> </li> <li>Optional: Set the connection for your tool(s). The tools must be configured in the image. When submitting a workload using the environment, it is possible to connect to these tools  <ul> <li>Select the tool from the list (the available tools varies from IDE, experiment tracking, and more, including a custom tool for your choice)  </li> <li>Select the connection type  <ul> <li>External URL <ul> <li>Auto generate   A unique URL is automatically created for each workload using the environment  </li> <li>Custom URL   The URL is set manually  </li> </ul> </li> <li>Node port <ul> <li>Auto generate   A unique port is automatically exposed for each workload using the environment  </li> <li>Custom URL   Set the port manually  </li> </ul> </li> <li>Set the container port </li> </ul> </li> </ul> </li> <li>Optional: Set a command and arguments for the container running the pod  <ul> <li>When no command is added, the default command of the image is used (the image entrypoint)  </li> <li>The command can be modified while submitting a workload using the environment  </li> <li>The argument(s) can be modified while submitting a workload using the environment  </li> </ul> </li> <li>Optional: Set the environment variable(s) <ul> <li>The environment variable(s) are added to the default environment variables that are already set within the image  </li> <li>The environment variables can be modified and new variables can be added while submitting a workload using the environment</li> <li>You can configure a new Environment variable from your credentials (of type generic secret, access key or username &amp; password). When selecting an environment variable source from credentials, the predefined name for the credential assets are displayed as an option. In addition, you can select the type of the credential to be used (username / password or access key / access secret).</li> </ul> </li> <li>Optional: Set the container\u2019s working directory to define where the container\u2019s process starts running. When left empty, the default directory is used.  </li> <li>Optional: Set where the UID, GID and supplementary groups are taken from, this can be:  <ul> <li>From the image </li> <li>From the IdP token (only available in an SSO installations)  </li> <li>Custom (manually set) - decide whether the submitter can modify these value upon submission.  </li> </ul> </li> <li>Optional: Select Linux capabilities - Grant certain privileges to a container without granting all the privileges of the root user. </li> <li>Click CREATE ENVIRONMENT</li> </ol> <p>Note</p> <p>It is also possible to add environments directly when creating a specific workspace, training or inference workload</p>"},{"location":"Researcher/workloads/assets/environments/#editing-an-environment","title":"Editing an environment","text":"<p>To edit an environment:</p> <ol> <li>Select the environment from the table  </li> <li>Click Rename to edit its name and description</li> </ol> <p>Note</p> <p>Additional fields can be edited using the API</p>"},{"location":"Researcher/workloads/assets/environments/#copying-editing-an-environment","title":"Copying &amp; Editing an environment","text":"<p>To copy &amp; edit an environment:</p> <ol> <li>Select the project you want to duplicate  </li> <li>Click COPY &amp; EDIT. </li> <li>Update the environment and click SAVE.</li> </ol>"},{"location":"Researcher/workloads/assets/environments/#deleting-an-environment","title":"Deleting an environment","text":"<p>To delete an environment:</p> <ol> <li>Select the environment you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>It is not possible to delete an environment being used by an existing workload and template.</p>"},{"location":"Researcher/workloads/assets/environments/#using-api","title":"Using API","text":"<p>Go to the Environment API reference to view the available actions</p>"},{"location":"Researcher/workloads/assets/overview/","title":"Overview","text":"<p>Workload assets enable organizations to:</p> <ul> <li>Create and reuse preconfigured setup for code, data, storage and resources to be used by AI practitioners to simplify the process of submitting workloads  </li> <li>Share the preconfigured setup with a wide audience of AI practitioners with similar needs</li> </ul> <p>Note</p> <ul> <li>The creation of assets is possible only via API and the Run:ai UI  </li> <li>The submission of workloads using assets, is possible only via the Run:ai UI</li> </ul>"},{"location":"Researcher/workloads/assets/overview/#workload-asset-types","title":"Workload asset types","text":"<p>There are four workload asset types used by the workload:</p> <ul> <li>Environments   The container image, tools and connections for the workload  </li> <li>Data sources   The type of data, its origin and the target storage location such as PVCs or cloud storage buckets where datasets are stored  </li> <li>Compute resources   The compute specification, including GPU and CPU compute and memory  </li> <li>Credentials   The secrets to be used to access sensitive data, services, and applications such as docker registry or S3 buckets</li> </ul>"},{"location":"Researcher/workloads/assets/overview/#asset-scope","title":"Asset scope","text":"<p>When a workload asset is created, a scope is required. The scope defines who in the organization can view and/or use the asset.</p> <p>Note</p> <p>When an asset is created via API, the scope can be the entire account, this is currently an experimental feature.</p>"},{"location":"Researcher/workloads/assets/overview/#who-can-create-an-asset","title":"Who can create an asset?","text":"<p>Any subject (user, application, or SSO group) with a role that has permissions to Create an asset, can do so within their scope.</p>"},{"location":"Researcher/workloads/assets/overview/#who-can-use-an-asset","title":"Who can use an asset?","text":"<p>Assets are used when submitting workloads. Any subject (user, application or SSO group) with a role that has permissions to Create workloads, can also use assets.</p>"},{"location":"Researcher/workloads/assets/overview/#who-can-view-an-asset","title":"Who can view an asset?","text":"<p>Any subject (user, application, or SSO group) with a role that has permission to View an asset, can do so within their scope.  </p>"},{"location":"Researcher/workloads/assets/templates/","title":"Templates","text":"<p>This article explains the procedure to manage templates.</p> <p>A template is a pre-set configuration that is used to quickly configure and submit workloads using existing assets. A template consists of all the assets a workload needs, allowing researchers to submit a workload in a single click, or make subtle adjustments to differentiate them from each other.</p>"},{"location":"Researcher/workloads/assets/templates/#workspace-templates-table","title":"Workspace templates table","text":"<p>Access to the Templates table can be found on the left-hand menu in the Run:ai platform.</p> <p>The Templates table provides a list of all the templates defined in the platform, and allows you to manage them.</p> <p>Flexible Management</p> <p>It is also possible to manage templates directly for a specific user, application, project, or department.</p> <p></p> <p>The Templates table consists of the following columns:</p> Column Description Scope The scope to which the subject has access. Click the name of the scope to see the scope and its subordinates Environment The name of the environment related to the workspace template Compute resource The name of the compute resource connected to the workspace template Data source(s) The name of the data source(s) connected to the workspace template Created by The subject that created the template Creation time The timestamp for when the template was created Cluster The cluster name containing the template"},{"location":"Researcher/workloads/assets/templates/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV  </li> <li>Refresh (optional) - Click REFRESH to update the table with the latest data  </li> <li>Show/Hide details (optional) - Click to view additional information on the selected row</li> </ul>"},{"location":"Researcher/workloads/assets/templates/#adding-a-new-workspace-template","title":"Adding a new workspace template","text":"<p>To add a new template:</p> <ol> <li>Click +NEW TEMPLATE </li> <li>Set the scope for the template  </li> <li>Enter a name for the template  </li> <li>Select the environment for your workload  </li> <li> <p>Select the node resources needed to run your workload     - or -    Click +NEW COMPUTE RESOURCE</p> </li> <li> <p>Set the volume needed for your workload  </p> </li> <li>Create a new data source  </li> <li>Set auto-deletion, annotations and labels, as required  </li> <li>Click CREATE TEMPLATE</li> </ol>"},{"location":"Researcher/workloads/assets/templates/#editing-a-template","title":"Editing a template","text":"<p>To edit a template:</p> <ol> <li>Select the template from the table  </li> <li>Click Rename to provide it with a new name  </li> <li>Click Copy &amp; Edit to make any changes to the template</li> </ol>"},{"location":"Researcher/workloads/assets/templates/#deleting-a-template","title":"Deleting a template","text":"<p>To delete a template:</p> <ol> <li>Select the template you want to delete  </li> <li>Click DELETE </li> <li>Confirm you want to delete the template</li> </ol>"},{"location":"Researcher/workloads/assets/templates/#using-api","title":"Using API**","text":"<p>Go to the Workload template API reference to view the available actions  </p>"},{"location":"Researcher/workloads/overviews/managing-workloads/","title":"Managing Workloads","text":"<p>This article explains the procedure for managing workloads.</p>"},{"location":"Researcher/workloads/overviews/managing-workloads/#workloads-table","title":"Workloads table","text":"<p>The Workloads table can be found under Workloads in the Run:ai platform.</p> <p>The workloads table provides a list of all the workloads scheduled on the run:ai Scheduler, and allows you to manage them.</p> <p></p> <p>The Workloads table consists of the following columns:</p> Column Description Workload The name of the workload Type The workload type Preemptible Is the workload preemptible Status The different phases in a workload life cycle. Project The project in which the workload runs. Department The department that the workload is associated with. this column is visible only if the department toggle is enabled by your administrator. Created by The user who created the workload Running/requested pods The number of running pods out of the requested Creation time The timestamp for when the workload was created Completion time The timestamp the workload reached a terminal state (failed/completed) Connection(s) The method by which you can access and interact with the running workload. It's essentially the \"doorway\" through which you can reach and use the tools the workload provide. (E.g node port, external URL, etc). Click one of the values in the column to view the list of connections and their parameters Data source(s) Data resources used by the workload Environment The environment used by the workload Workload architecture Standard or distributed. A standard workload consists of a single process. A distributed workload consists of multiple processes working together. These processes can run on different nodes. GPU compute request Amount of GPU devices Requested GPU compute allocation Amount of GPU devices allocated GPU memory request Amount of GPU memory Requested GPU memory allocation Amount of GPU memory allocated CPU compute request Amount of CPU cores requested CPU compute allocation Amount of CPU cores allocated CPU memory request Amount of CPU memory requested CPU memory allocation Amount of CPU memory allocated Cluster The cluster that the workload is associated with"},{"location":"Researcher/workloads/overviews/managing-workloads/#workload-status","title":"Workload status","text":"<p>The following table describes the different phases in a workload life cycle.</p> Status Description Entry Condition Exit Condition Creating Workload setup is initiated in the cluster. Resources and pods are now provisioning. A workload is submitted. A multi-pod group is created. Pending Workload is queued and awaiting resource allocation. A pod group exists. All pods are scheduled. Initializing Workload is retrieving images, starting containers, and preparing pods. All pods are scheduled. All pods are initialized or a failure to initialize is detected. Running Workload is currently in progress with all pods operational. All pods initialized (all containers in pods are ready). Workload completion or failure. Degraded Pods may not align with specifications, network services might be incomplete, or persistent volumes may be detached. Check your logs for specific details. Pending - All pods are running but have issues. Running - All pods are running with no issues. Running - All resources are OK. Completed - Workload finished with fewer resources. Failed - Workload failure or user-defined rules. Deleting Workload and its associated resources are being decommissioned from the cluster. Deleting the workload. Resources are fully deleted. Stopped Workload is on hold and resources are intact but inactive. Stopping the workload without deleting resources. Transitioning back to the initializing phase or proceeding to deleting the workload. Failed Image retrieval failed or containers experienced a crash. Check your logs for specific details. An error occurs preventing the successful completion of the workload. Terminal state. Completed Workload has successfully finished its execution. The workload has finished processing without errors. Terminal state."},{"location":"Researcher/workloads/overviews/managing-workloads/#pods-associated-with-workload","title":"Pods Associated with Workload","text":"<p>Click one of the values in the Running/requested pods column, to view the list of pods and their parameters.</p> Column Description Pod Pod name Status Pod lifecycle stages Node The node on which the pod resides Node pool The node pool in which the pod resides (applicable if node pools are enabled) Image The pod\u2019s main image GPU compute allocation Amount of GPU devices allocated for the pod GPU memory allocation Amount of GPU memory allocated for the pod"},{"location":"Researcher/workloads/overviews/managing-workloads/#connections-associated-with-workload","title":"Connections Associated with Workload","text":"<p>A connection refers to the method by which you can access and interact with the running workloads. It is essentially the \"doorway\" through which you can reach and use the applications (tools) these workloads provide.</p> <p>Click one of the values in the Connection(s) column, to view the list of connections and their parameters. Connections are network interfaces that communicate with the application running in the workload. Connections are either the URL the application exposes or the IP and the port of the node that the workload is running on.</p> Column Description Name The name of the application running on the workload Connection type The network connection type selected for the workload Access Who is authorized to use this connection (everyone, specific groups/users) Address The connection URL Copy button Copy URL to clipboard Connect button Enabled only for supported tools"},{"location":"Researcher/workloads/overviews/managing-workloads/#data-sources-associated-with-workload","title":"Data Sources Associated with Workload","text":"<p>Click one of the values in the Data source(s) column, to view the list of data sources and their parameters.</p> Column Description Data source The name of the data source mounted to the workload Type The data source type"},{"location":"Researcher/workloads/overviews/managing-workloads/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV  </li> <li>Refresh - Click REFRESH to update the table with the latest data  </li> <li>Show/Hide details - Click to view additional information on the selected row</li> </ul>"},{"location":"Researcher/workloads/overviews/managing-workloads/#showhide-details","title":"Show/Hide details","text":"<p>Click a row in the Workloads table and then click the SHOW DETAILS button at the upper-right side of the action bar. The details pane appears, presenting the following tabs:</p>"},{"location":"Researcher/workloads/overviews/managing-workloads/#event-history","title":"Event History","text":"<p>Displays the workload status over time. It displays events describing the workload lifecycle and alerts on notable events. Use the filter to search through the history for specific events.</p>"},{"location":"Researcher/workloads/overviews/managing-workloads/#metrics","title":"Metrics","text":"<ul> <li>GPU utilization   Per GPU graph and an average of all GPUs graph, all on the same chart, along an adjustable period allows you to see the trends of all GPUs compute utilization (percentage of GPU compute) in this node.  </li> <li>GPU memory utilization   Per GPU graph and an average of all GPUs graph, all on the same chart, along an adjustable period allows you to see the trends of all GPUs memory usage (percentage of the GPU memory) in this node.  </li> <li>CPU compute utilization   The average of all CPUs\u2019 cores compute utilization graph, along an adjustable period allows you to see the trends of CPU compute utilization (percentage of CPU compute) in this node.  </li> <li>CPU memory utilization   The utilization of all CPUs memory in a single graph, along an adjustable period allows you to see the trends of CPU memory utilization (percentage of CPU memory) in this node.  </li> <li> <p>CPU memory usage   The usage of all CPUs memory in a single graph, along an adjustable period allows you to see the trends of CPU memory usage (in GB or MB of CPU memory) in this node.</p> </li> <li> <p>For GPUs charts - Click the GPU legend on the right-hand side of the chart, to activate or deactivate any of the GPU lines.  </p> </li> <li>You can click the date picker to change the presented period  </li> <li>You can use your mouse to mark a sub-period in the graph for zooming in, and use Reset zoom to go back to the preset period  </li> <li>Changes in the period affect all graphs on this screen.</li> </ul>"},{"location":"Researcher/workloads/overviews/managing-workloads/#logs","title":"Logs","text":"<p>Workload events are ordered in chronological order. The logs contain events from the workload\u2019s lifecycle to help monitor and debug issues.</p>"},{"location":"Researcher/workloads/overviews/managing-workloads/#adding-new-workload","title":"Adding new workload","text":"<p>Before starting, make sure you have created a project or have one created for you to work with workloads.</p> <p>To create a new workload:</p> <ol> <li>Click +NEW WORKLOAD </li> <li>Select a workload type - Follow the links below to view the step-by-step guide for each workload type:  <ul> <li>Workspace. Used for data preparation and model-building tasks.  </li> <li>Training. Used for training tasks of all sorts  </li> <li>Inference. Used for inference and serving tasks  </li> <li>Job (legacy). This type is displayed only if enabled by your Administrator, under General Settings \u2192 Workloads \u2192 Workload policies  </li> </ul> </li> <li>Click CREATE WORKLOAD</li> </ol>"},{"location":"Researcher/workloads/overviews/managing-workloads/#stopping-a-workload","title":"Stopping a workload","text":"<p>Stopping a workload kills the workload pods and releases the workload resources.</p> <ol> <li>Select the workload you want to stop  </li> <li>Click STOP</li> </ol>"},{"location":"Researcher/workloads/overviews/managing-workloads/#running-a-workload","title":"Running a workload","text":"<p>Running a workload spins up new pods and resumes the workload work after it was stopped.</p> <ol> <li>Select the workload you want to run again  </li> <li>Click RUN</li> </ol>"},{"location":"Researcher/workloads/overviews/managing-workloads/#connecting-to-a-workload","title":"Connecting to a workload","text":"<p>To connect to an application running in the workload (for example, Jupyter Notebook)</p> <ol> <li>Select the workload you want to connect  </li> <li>Click CONNECT </li> <li>Select the tool from the drop-down list  </li> <li>The selected tool is opened in a new tab on your browser</li> </ol>"},{"location":"Researcher/workloads/overviews/managing-workloads/#deleting-a-workload","title":"Deleting a workload","text":"<ol> <li>Select the workload you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion  </li> </ol> <p>Note</p> <p>Once a workload is deleted you can view it in the Deleted tab in the workloads view. This tab is displayed only if enabled by your Administrator, under General Settings \u2192 Workloads \u2192 Deleted workloads</p> <ol> <li>Select the workload you want to copy and edit  </li> <li>Click COPY &amp; EDIT </li> <li>Update the workload and click CREATE WORKLOAD</li> </ol>"},{"location":"Researcher/workloads/overviews/managing-workloads/#using-api","title":"Using API","text":"<p>Go to the Workloads API reference to view the available actions</p>"},{"location":"Researcher/workloads/overviews/managing-workloads/#troubleshooting","title":"Troubleshooting","text":"<p>To understand the condition of the workload, review the workload status in the Workload table. For more information, see check the workload\u2019s event history.</p> <p>Listed below are a number of known issues when working with workloads and how to fix them:</p> Issue Mediation Cluster connectivity issues (there are issues with your connection to the cluster error message) Verify that you are on a network that has been granted access to the cluster.  Reach out to your cluster admin for instructions on verifying this.  If you are an admin, see the troubleshooting section in the cluster documentation Workload in \u201cInitializing\u201d status for some time Check that you have access to the Container image registry.  Check the statuses of the pods in the pods\u2019 modal.  Check the event history for more details Workload has been pending for some time Check that you have the required quota.  Check the project\u2019s available quota in the project dialog.  Check that all services needed to run are bound to the workload.  Check the event history for more details. PVCs created using the K8s API or <code>kubectl</code> are not visible or mountable in Run:ai. This is by design.  - Create a new data source of type PVC in the Run:ai UI  - In the Data mount section, select Existing PVC  - Select the PVC you created via the K8S API  You are now able to select and mount this PVC in your Run:ai submitted workloads. Workload is not visible in the UI. Check that the workload hasn\u2019t been deleted.  See the \u201cDeleted\u201d tab in the workloads view"},{"location":"Researcher/workloads/overviews/workload-support/","title":"Workload Support","text":"<p>Workloads are the basic unit of work in Run:ai. Researchers and Engineers use workloads for every stage in their AI Project lifecycle. Workloads can be used to build, train, or deploy a model. Run:ai supports all types of Kubernetes workloads. Researchers can work with any workload in their organization but will get the largest value working with Run:ai native workloads.</p> <p>Run:ai offers three native types of workloads:</p> <ul> <li>Workspace. Used for data preparation and model-building tasks.  </li> <li>Training. Used for training tasks.  </li> <li>Inference. Used for inference and model serving tasks  </li> </ul> <p>Run:ai native workloads can be created via the Run:ai User interface, API or Command-line interface.</p>"},{"location":"Researcher/workloads/overviews/workload-support/#levels-of-support","title":"Levels of support","text":"<p>Different types of workloads have different levels of support. Understanding what capabilities are needed before selecting the workload type to work with is important. The table below details the level of support for each workload type in Run:ai. The Run:ai native workloads are fully supported with all of Run:ai advanced features and capabilities. While third-party workloads are partially supported. The list of capabilities can change between different Run:ai versions.</p> Functionality Workload Type Run:ai workloads Third-party workloads Training - Standard Workspace Inference Training - distributed All K8s workloads Fairness v v v v v Priority and preemption v v v v v Over quota v v v v v Node pools v v v v v Bin packing / Spread v v v v v Fractions v v v v v Dynamic fractions v v v v v Node level scheduler v v v v v GPU swap v v v v v Elastic scaling NA NA v v v Gang scheduling v v v v v Monitoring v v v v v RBAC v v v v Workload awareness v v v v Workload submission v v v v Workload actions (stop/run) v v v Policies v v v v Scheduling rules v v v <p>Note</p> <p>Workload awareness</p> <p>Specific workload-aware visibility, so that different pods are identified and treated as a single workload (for example GPU utilization, workload view, dashboards).</p>"},{"location":"Researcher/workloads/overviews/workload-support/#workload-scopes","title":"Workload scopes","text":"<p>Workloads must be created under a project. A project is the fundamental organization unit in the Run:ai account. To manage workloads, it\u2019s required to first create a project or have one created by the administrator.</p>"},{"location":"Researcher/workloads/overviews/workload-support/#policies-and-rules","title":"Policies and rules","text":"<p>Policies and rules empower administrators to establish default values and implement restrictions on workloads allowing enhanced control, assuring compatibility with organizational policies, and optimizing resource usage and utilization.</p>"},{"location":"Researcher/workloads/overviews/workload-support/#workload-statuses","title":"Workload statuses","text":"<p>The following table describes the different phases in a workload life cycle.</p> Phase Description Entry condition Exit condition Creating Workload setup is initiated in the Cluster. Resources and pods are now provisioning A workload is submitted A multi-pod group is created Pending Workload is queued and awaiting resource allocation. A pod group exists All pods are scheduled Initializing Workload is retrieving images, starting containers, and preparing pods All pods are scheduled All pods are initialized or a failure to initialize is detected Running Workload is currently in progress with all pods operational All pods initialized (all containers in pods are ready) workload completion or failure Degraded Pods may not align with specifications, network services might be incomplete, or persistent volumes may be detached. Check your logs for specific details. Pending: All pods are running but with issues Running: All pods are running with no issues. Running: All resources are OK Completed: Workload finished with fewer resources Failed: Workload failure or user-defined rules Deleting Workload and its associated resources are being decommissioned from the cluster Deleting the workload. Resources are fully deleted Stopped The workload is on hold and resources are intact but inactive Stopping the workload without deleting resources Transitioning back to the initializing phase or proceeding to deleting the workload Failed Image retrieval failed or containers experienced a crash. Check your logs for specific details. An error occurs preventing the successful completion of the workload Terminal State Completed Workload has successfully finished its execution The workload has finished processing without errors Terminal State"},{"location":"Researcher/workloads/workspaces/overview/","title":"Getting familiar with workspaces","text":"<p>Workspace is a simplified tool for researchers to conduct experiments, build AI models, access standard MLOps tools, and collaborate with their peers.</p> <p>Run:ai workspaces abstract complex concepts related to running containerized workloads in a Kubernetes environment. Aspects such as networking, storage, and secrets, are built from predefined abstracted setups, that ease and streamline the researcher's AI model development.</p> <p>A workspace consists of all the setup and configuration needed for the research, including container images, data sets, resource requests, as well as all required tools for the research, in a single place.  This setup is set to facilitate the research needs and yet to ensure infrastructure owners keep control and efficiency when supporting the various needs.</p> <p>A workspace is associated with a specific Run:ai project (internally: a Kubernetes namespace). A researcher can create multiple workspaces under a specific project.</p> <p>Researchers can only view and use workspaces that are created under projects they are assigned to.</p> <p></p> <p>Workspaces can be created with just a few clicks of a button. See Workspace creation.  </p> <p>Workspaces can be stopped and started to save expensive resources without losing complex environment configurations.</p> <p>Only when a workspace is in status active (see also Workspace Statuses) does it consume resources. </p> <p>When the workspace is active it exposes the connections to the tools (for example, a Jupyter notebook) within the workspace. </p> <p></p> <p>An active workspace is a Run:ai interactive workload. The interactive workload starts when the workspace is started and stops when the workspace is stopped. </p> <p>Workspaces can be used via the user interface or programmatically via the Run:ai Admin API. Workspaces are not supported via the command line interface. You can still run an interactive workload via the command line. </p>"},{"location":"Researcher/workloads/workspaces/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Workspaces are made from building blocks. Read about the various building blocks</li> <li>See how to create a Workspace.  </li> </ul>"},{"location":"Researcher/workloads/workspaces/workspace-v2/","title":"Running workspaces","text":"<p>This article explains how to create a workspace via the Run:ai UI.</p> <p>A workspace contains the setup and configuration needed for building your model, including the container, images, data sets, and resource requests, as well as the required tools for the research, all in a single place.</p> <p>The workspace is assigned to a project and is affected by the project\u2019s quota just like any other workload. For a list of supported features and capabilities for workspaces, check the Workloads in Run:ai article.</p> <p></p>"},{"location":"Researcher/workloads/workspaces/workspace-v2/#creating-a-new-workspace","title":"Creating a new Workspace","text":"<p>Before starting, make sure you have a project.</p> <p>To add a new workspace:</p> <ol> <li>Go to the Workload manager \u2192 Workloads</li> <li>Click +NEW WORKLOAD and select Workspace     within the new workspace form:</li> <li>Select under which cluster to create the workload</li> <li>Select the project in which your workspace will run</li> <li>Select a preconfigured template or select Start from scratch to launch a new workspace quickly</li> <li>Enter a unique name for the workspace (if the name already exists in the project\u2019s namespace, you will need to choose a different one)</li> <li>Click CONTINUE     In the next step:</li> <li> <p>Select the environment for your workspace</p> <ul> <li>Select an environment or click +NEW ENVIRONMENT to add a new environment to the gallery.     For a step-by-step guide on adding environments to the gallery, check the Environments article. Once created, the new environment will be automatically selected.</li> <li>Set the connection for your tool(s). The tools are configured as part of the environment.<ul> <li>External URL<ul> <li>Custom URL<ul> <li>Set the URL</li> </ul> </li> <li>Optional: modify who can access the tool:<ul> <li>All authenticated users (default)     Everyone within the organization\u2019s account</li> <li>Specific group(s)<ul> <li>Click +GROUP</li> <li>Enter group names as they appear in your identity provider. You must be a member of one of the groups listed to have access to the tool.</li> </ul> </li> <li>Specific user(s)<ul> <li>Click +USER</li> <li>Enter a valid email address or username. If you remove yourself, you will lose access to the tool.</li> </ul> </li> </ul> </li> </ul> </li> <li>Node port<ul> <li>Custom port<ul> <li>Set the node port (enter a port between 30000 and 32767; if the node port is already in use, the workload will fail and display an error message)</li> </ul> </li> </ul> </li> </ul> </li> <li>Set the User ID (UID), Group ID (GID) and the supplementary groups that can run commands in the container<ul> <li>Enter UID</li> <li>Enter GID</li> <li>Add Supplementary groups (multiple groups can be added, separated by commas).</li> </ul> </li> <li>Optional: Set the command and arguments for the container running the workload     When no command is added, the default command of the image is used (the image entry-point).<ul> <li>Modify the existing command or click +COMMAND &amp; ARGUMENTS to add a new command.</li> <li>Set multiple arguments separated by spaces, using the following format (e.g.: <code>--arg1=val1</code>).</li> </ul> </li> <li>Optional: Set the environment variable(s)     The environment variable(s) are added to the environment variables already configured in the environment.<ul> <li>Click +ENVIRONMENT VARIABLE</li> <li>Enter a name</li> <li>Set a value</li> </ul> </li> </ul> </li> <li> <p>Select the compute resource for your workspace</p> <ul> <li>Select a compute resource or click +NEW COMPUTE RESOURCE to add a new compute resource to the gallery.     For a step-by-step guide on adding compute resources to the gallery, check the compute resources article. Once created, the new compute resource will be automatically selected.</li> <li>Optional: Set the order of priority for the node pools on which the scheduler tries to run the workload.     When a workload is created, the scheduler will try to run it on the first node pool on the list. If the node pool doesn't have free resources, the scheduler will move on to the next one until it finds one that's available.<ul> <li>Drag and drop them to change the order, remove unwanted ones, or reset to the default order defined in the project.</li> <li>Click +NODE POOL to add a new node pool from the list of node pools that were defined on the cluster.     To configure a new node pool and for additional information, check the node pools article.</li> </ul> </li> <li>Select a node affinity to schedule the workload on a specific node type.     If the administrator added a \u2018node type (affinity)\u2019 scheduling rule to the project/department, then this field is mandatory.     Otherwise entering a node type (affinity) is optional. Nodes must be tagged with a label that matches the node type key and value.     Optional: Set toleration(s) to let the workload be scheduled on a node with a matching taint<ul> <li>Click +TOLERATION</li> <li>Enter a key</li> <li>Select the operator<ul> <li>Exists - If the key exists on the node, the effect will be applied.</li> <li>Equals - if the key and the value set below matches to the value on the node, the effect will be applied<ul> <li>Enter a value matching the value on the node</li> </ul> </li> </ul> </li> <li>Select the effect for the toleration<ul> <li>NoExecute - Pods that do not tolerate this taint are evicted immediately.</li> <li>NoSchedule - No new pods will be scheduled on the tainted node unless they have a matching toleration. Pods currently running on the node will not be evicted.</li> <li>PreferNoSchedule - The control plane will try to avoid placing a pod that does not tolerate the taint on the node, but it is not guaranteed.</li> <li>Any - All effects above match.</li> </ul> </li> </ul> </li> </ul> </li> <li>Optional: Set the volume needed for your workload     A volume allocates storage space to your workload that is persistent across restarts.<ul> <li>Click +VOLUME</li> <li>Select the storage class<ul> <li>None - Proceed without defining a storage class.</li> <li>Custom storage class - This option applies when selecting a storage class based on existing storage classes.     To add new storage classes to the storage class list, and for additional information, check Kubernetes storage classes</li> </ul> </li> <li>Select the access mode(s) (multiple modes can be selected)<ul> <li>Read-write by one node - The volume can be mounted as read-write by a single node.</li> <li>Read-only by many nodes - The volume can be mounted as read-only by many nodes.</li> <li>Read-write by many nodes - The volume can be mounted as read-write by many nodes.</li> </ul> </li> <li>Set the claim size and its units</li> <li>Select the volume mode<ul> <li>File system (default) - This allows the volume to be mounted as a file system, enabling the usage of directories and files.</li> <li>Block - This exposes the volume as a block storage, which can be formatted or used directly by applications without a file system.</li> </ul> </li> <li>Set the volume target location<ul> <li>Container path</li> </ul> </li> <li>Set the volume persistency<ul> <li>Persistent - The volume and its data will be deleted only when the workload is deleted.</li> <li>Ephemeral - The volume and its data will be deleted every time the workload\u2019s status changes to \u201cStopped.\u201d</li> </ul> </li> </ul> </li> <li>Optional: Select data sources for your workspace     Select a data source or click +NEW DATA SOURCE to add a new data source to the gallery. If there are issues with the connectivity to the cluster, or if there were issues while creating the data source, the data source won't be available for selection.     For a step-by-step guide on adding data sources to the gallery, check the data sources article.     Once created, the new data source will be automatically selected.<ul> <li>Optional: Modify the data target location for the selected data source(s).</li> </ul> </li> <li>Optional - General settings:<ul> <li>Allow the workload to exceed the project quota (Workloads running over quota may be preempted and stop at any time).</li> <li>Set the backoff limit before workload failure. The backoff limit is the maximum number of retry attempts for failed workloads. After reaching the limit, the workload status will change to \"Failed.\" (Enter a value between 1 and 100.)</li> <li>Set the timeframe for auto-deletion after workload completion or failure (the time after which a completed or failed workload is deleted; if this field is set to 0 seconds, the workload will be deleted automatically).</li> <li>Set annotations(s)     Kubernetes annotations are key-value pairs attached to the workload. They are used for storing additional descriptive metadata to enable documentation, monitoring and automation.<ul> <li>Click +ANNOTATION</li> <li>Enter a name</li> <li>Enter a value</li> </ul> </li> <li>Set labels(s)     Kubernetes labels are key-value pairs attached to the workload. They are used for categorizing to enable querying.<ul> <li>Enter a name</li> <li>Enter a value</li> </ul> </li> </ul> </li> <li>Click CREATE WORKSPACE</li> </ol>"},{"location":"Researcher/workloads/workspaces/workspace-v2/#workload-policies","title":"Workload Policies","text":"<p>When creating a new workload, fields and assets may have limitations or defaults. These rules and defaults are derived from a policy your administrator set.</p> <p>Policies let you control, standardize, and simplify the workload submission process. For additional information, check Workload Policies and Rules.</p> <p>The effects of the policy are reflected in the workspace creation form:</p> <ul> <li>Defaults derived from the policy will be displayed automatically for specific fields.</li> <li>Disabled actions or values must be within a certain range.</li> <li>Rules and defaults for entire sections (such as environments, compute resources, or data sources) may prevent selection and will appear on the entire library card with an option for additional information via an external modal.</li> </ul>"},{"location":"Researcher/workloads/workspaces/workspace-v2/#managing-and-monitoring","title":"Managing and monitoring","text":"<p>After the workspace is created, it is added to the Workloads table, where it can be managed and monitored.</p>"},{"location":"Researcher/workloads/workspaces/workspace-v2/#using-cli","title":"Using CLI","text":"<p>To view the available actions on workspaces, please visit the Workspaces CLI v2 reference or the CLI v1 reference.</p>"},{"location":"Researcher/workloads/workspaces/workspace-v2/#using-api","title":"Using API","text":"<p>To view the available actions on workspaces, please visit the Workspaces API reference.</p>"},{"location":"admin/overview-administrator/","title":"Overview: Infrastructure Administrator","text":"<p>The Infrastructure Administrator is an IT person, responsible for the installation, setup and IT maintenance of the Run:ai product. </p> <p>As part of the Infrastructure Administrator documentation you will find:</p> <ul> <li>Install Run:ai <ul> <li>Understand the Run:ai installation</li> <li>Set up a Run:ai Cluster.</li> <li>Set up Researchers to work with Run:ai.</li> </ul> </li> <li>IT Configuration of the Run:ai system</li> <li>Connect Run:ai to an identity provider.</li> <li>Maintenance &amp; monitoring of the Run:ai system</li> <li>Troubleshooting.</li> </ul>"},{"location":"admin/authentication/accessrules/","title":"Access Rules","text":"<p>This article explains the procedure to manage Access rules.</p> <p>Access rules provide users, groups, or applications privileges to system entities.</p> <p>An access rule is the assignment of a role to a subject in a scope: <code>&lt;Subject&gt;</code> is a <code>&lt;Role&gt;</code> in a <code>&lt;Scope&gt;</code>.</p> <p>For example, user user@domain.com is a department admin in department A.</p>"},{"location":"admin/authentication/accessrules/#access-rules-table","title":"Access rules table","text":"<p>The Access rules table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The Access rules table provides a list of all the access rules defined in the platform and allows you to manage them.</p> <p>Note</p> <p>Flexible management</p> <p>It is also possible to manage access rules directly for a specific user, application, project, or department.</p> <p></p> <p>The Access rules table consists of the following columns:</p> Column Description Type The type of subject assigned to the access rule (user, SSO group, or application). Subject The user, SSO group, or application assigned with the role Role The role assigned to the subject Scope The scope to which the subject has access. Click the name of the scope to see the scope and its subordinates Authorized by The user who granted the access rule Creation time The timestamp for when the rule was created Last updated The last time the access rule was updated"},{"location":"admin/authentication/accessrules/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"admin/authentication/accessrules/#adding-new-access-rules","title":"Adding new access rules","text":"<p>To add a new access rule:</p> <ol> <li>Click +NEW ACCESS RULE </li> <li>Select a subject User, SSO Group, or Application </li> <li>Select or enter the subject identifier:  <ul> <li>User Email for a local user created in Run:ai or for SSO user as recognized by the IDP  </li> <li>Group name as recognized by the IDP  </li> <li>Application name as created in Run:ai  </li> </ul> </li> <li>Select a role  </li> <li>Select a scope  </li> <li>Click SAVE RULE</li> </ol> <p>Note</p> <p>An access rule consists of a single subject with a single role in a single scope. To assign multiple roles or multiple scopes to the same subject, multiple access rules must be added.</p>"},{"location":"admin/authentication/accessrules/#editing-an-access-rule","title":"Editing an access rule","text":"<p>Access rules cannot be edited. To change an access rule, you must delete the rule, and then create a new rule to replace it.</p>"},{"location":"admin/authentication/accessrules/#deleting-an-access-rule","title":"Deleting an access rule","text":"<ol> <li>Select the access rule you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol>"},{"location":"admin/authentication/accessrules/#using-api","title":"Using API","text":"<p>Go to the Access rules API reference to view the available actions</p>"},{"location":"admin/authentication/applications/","title":"Applications","text":"<p>This article explains the procedure to manage applications and it\u2019s permissions.</p> <p>Applications are used for API integrations with Run:ai. An application contains a secret key. Using the secret key you can obtain a token and use it within subsequent API calls.</p> <p>Applications are managed locally and assigned with Access Rules to manage its permissions.</p> <p>For example, application ci-pipeline-prod assigned with a Researcher role in Cluster: A.</p>"},{"location":"admin/authentication/applications/#applications-table","title":"Applications table","text":"<p>The Applications table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The Applications table provides a list of all the applications defined in the platform, and allows you to manage them.</p> <p></p> <p>The Applications table consists of the following columns:</p> Column Description Application The name of the application Status The status of the application Access rule(s) The access rules assigned to the application Last login The timestamp for the last time the user signed in Created by The user who created the application Creation time The timestamp for when the application was created Last updated The last time the application was updated"},{"location":"admin/authentication/applications/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"admin/authentication/applications/#creating-an-application","title":"Creating an application","text":"<p>To create an application:</p> <ol> <li>Click +NEW APPLICATION </li> <li>Enter the application\u2019s Name </li> <li>Click CREATE </li> <li>Copy the credentials and store it securely:  <ul> <li>Application name </li> <li>Secret key </li> </ul> </li> <li>Click DONE</li> </ol> <p>Note</p> <p>The secret key is visible only at the time of creation, it cannot be recovered but can be regenerated.</p>"},{"location":"admin/authentication/applications/#adding-an-access-rule-to-an-application","title":"Adding an access rule to an application","text":"<p>To create an access rule:</p> <ol> <li>Select the application you want to add an access rule for  </li> <li>Click ACCESS RULES </li> <li>Click +ACCESS RULE </li> <li>Select a role  </li> <li>Select a scope  </li> <li>Click SAVE RULE </li> <li>Click CLOSE</li> </ol>"},{"location":"admin/authentication/applications/#deleting-an-access-rule-from-an-application","title":"Deleting an access rule from an application","text":"<p>To delete an access rule:</p> <ol> <li>Select the application you want to remove an access rule from  </li> <li>Click ACCESS RULES </li> <li>Find the access rule assigned to the user you would like to delete  </li> <li>Click on the trash icon  </li> <li>Click CLOSE</li> </ol>"},{"location":"admin/authentication/applications/#regenerating-key","title":"Regenerating key","text":"<p>To regenerate an application\u2019s key:</p> <ol> <li>Select the application you want to regenerate it\u2019s secret key  </li> <li>Click REGENERATE KEY </li> <li>Click REGENERATE </li> <li>Review the user\u2019s credentials and store it securely:  <ul> <li>Application name  </li> <li>Secret key </li> </ul> </li> <li>Click DONE</li> </ol> <p>Warning</p> <p>Regenerating an application key revokes its previous key.</p>"},{"location":"admin/authentication/applications/#deleting-an-application","title":"Deleting an application","text":"<ol> <li>Select the application you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol>"},{"location":"admin/authentication/applications/#using-api","title":"Using API","text":"<p>Go to the Applications, Access rules API reference to view the available actions</p>"},{"location":"admin/authentication/authentication-overview/","title":"Authentication &amp; Authorization","text":"<p>Run:ai Authentication &amp; Authorization enables a streamlined experience for the user with precise controls covering the data each user can see and the actions each user can perform in the Run:ai platform.</p> <p>Authentication verifies user identity during login, and Authorization assigns the user with specific permissions according to the assigned access rules.</p> <p>Authenticated access is required to use all aspects of the Run:ai interfaces, including the Run:ai platform, the Run:ai Command Line Interface (CLI) and APIs.</p>"},{"location":"admin/authentication/authentication-overview/#authentication","title":"Authentication","text":"<p>There are multiple methods to authenticate and access Run:ai.</p>"},{"location":"admin/authentication/authentication-overview/#single-sign-on-sso","title":"Single Sign-On (SSO)","text":"<p>Single Sign-On (SSO) is the preferred authentication method by large organizations, as it avoids the need to manage duplicate sets of user identities.</p> <p>Run:ai offers SSO integration, enabling users to utilize existing organizational credentials to access Run:ai without requiring dedicated credentials.</p> <p>Run:ai supports three methods to set up SSO:</p> <ul> <li>SAML </li> <li>OpenID Connect (OIDC) </li> <li>OpenShift</li> </ul> <p>When using SSO, it is highly recommended to manage at least one local user, as a breakglass account (an emergency account), in case access to SSO is not possible.</p>"},{"location":"admin/authentication/authentication-overview/#username-and-password","title":"Username and password","text":"<p>Username and password access can be used when SSO integration is not possible.</p>"},{"location":"admin/authentication/authentication-overview/#secret-key-for-application-programmatic-access","title":"Secret key (for Application programmatic access)","text":"<p>A Secret is the authentication method for Applications. Applications use the Run:ai APIs to perform automated tasks including scripts and pipelines based on their assigned access rules.</p>"},{"location":"admin/authentication/authentication-overview/#authorization","title":"Authorization","text":"<p>The Run:ai platform uses Role Base Access Control (RBAC) to manage authorization.</p> <p>Once a user or an application is authenticated, they can perform actions according to their assigned access rules.</p>"},{"location":"admin/authentication/authentication-overview/#role-based-access-control-rbac-in-runai","title":"Role Based Access Control (RBAC) in Run:ai","text":"<p>While Kubernetes RBAC is limited to a single cluster, Run:ai expands the scope of Kubernetes RBAC, making it easy for administrators to manage access rules across multiple clusters.</p> <p>RBAC at Run:ai is configured using access rules.</p> <p>An access rule is the assignment of a role to a subject in a scope: <code>&lt;Subject&gt;</code> is a <code>&lt;Role&gt;</code> in a <code>&lt;Scope&gt;</code>.</p> <ul> <li>Subject </li> <li>A user, a group, or an application assigned with the role  </li> <li>Role </li> <li>A set of permissions that can be assigned to subjects  </li> <li>A permission is a set of actions (view, edit, create and delete) over a Run:ai entity (e.g. projects, workloads, users)  <ul> <li>For example, a role might allow a user to create and read Projects, but not update or delete them  </li> <li>Roles at Run:ai are system defined and cannot be created, edited or deleted  </li> </ul> </li> <li>Scope </li> <li>A scope is part of an organization in which a set of permissions (roles) is effective. Scopes include Projects, Departments, Clusters, Account (all clusters).</li> </ul> <p>Below is an example of an access rule: username@company.com is a Department admin in Department: A</p> <p></p>"},{"location":"admin/authentication/non-root-containers/","title":"User Identity in Container","text":"<p>The identity of the user in the container determines its access to resources. For example, network file storage solutions typically use this identity to determine the container's access to network volumes. This document explains multiple ways for propagating the user identity into the container.</p>"},{"location":"admin/authentication/non-root-containers/#the-default-root-access","title":"The Default: Root Access","text":"<p>In docker, as well as in Kubernetes, the default for running containers is running as root. The implication of running as root is that processes running within the container have enough permissions to change anything in the container, and if propagated to network resources - can have permissions outside the container as well.</p> <p>This gives a lot of power to the Researcher but does not sit well with modern security standards of enterprise security.</p> <p>By default, if you run:</p> <p><pre><code>runai submit -i ubuntu --attach --interactive -- bash\n</code></pre> then run <code>id</code>, you will see the root user.</p>"},{"location":"admin/authentication/non-root-containers/#use-runai-flags-to-limit-root-access","title":"Use Run:ai flags to limit root access","text":"<p>There are two [runai submit flags that control user identity at the Researcher level:</p> <ul> <li>The flag <code>--run-as-user</code> starts the container with a specific user. The user is the current Linux user (see below for other behaviors if used in conjunction with Single sign-on).</li> <li>The flag <code>--prevent-privilege-escalation</code> prevents the container from elevating its own privileges into <code>root</code> (e.g. running <code>sudo</code> or changing system files.).</li> </ul> <p>Equivalent flags exist in the Researcher User Interface.</p>"},{"location":"admin/authentication/non-root-containers/#run-as-current-user","title":"Run as Current User","text":"<p>From a Linux/Mac box, run:</p> <pre><code>runai submit -i ubuntu --attach --interactive --run-as-user -- bash\n</code></pre> <p>then run <code>id</code>, you will see the users and groups of the box you have been using to launch the Job.</p>"},{"location":"admin/authentication/non-root-containers/#prevent-escalation","title":"Prevent Escalation","text":"<p>From a Linux/Mac box, run:</p> <pre><code>runai submit -i ubuntu --attach --interactive --run-as-user \\\n  --prevent-privilege-escalation  -- bash\n</code></pre> <p>then verify that you cannot run <code>su</code> to become root within the container.</p>"},{"location":"admin/authentication/non-root-containers/#setting-a-cluster-wide-default","title":"Setting a Cluster-Wide Default","text":"<p>The two flags are voluntary. They are not enforced by the system. It is however possible to enforce them using Policies. Policies allow an Administrator to force compliance on both the User Interface and Command-line interface.</p>"},{"location":"admin/authentication/non-root-containers/#passing-user-identity","title":"Passing user identity","text":""},{"location":"admin/authentication/non-root-containers/#passing-user-identity-from-identity-provider","title":"Passing user identity from Identity Provider","text":"<p>A best practice is to store the user identifier (UID) and the group identifier (GID) in the organization's directory. Run:ai allows you to pass these values to the container and use them as the container identity.</p> <p>To perform this, you must:</p> <ul> <li>Set up single sign-on. Perform the steps for UID/GID integration.</li> <li>Run: <code>runai login</code> and enter your credentials</li> <li>Use the flag --run-as-user</li> </ul> <p>Running <code>id</code> should show the identifier from the directory.</p>"},{"location":"admin/authentication/non-root-containers/#passing-user-identity-explicitly-via-the-researcher-ui","title":"Passing user identity explicitly via the Researcher UI","text":"<p>Via the Researcher User Interface, it is possible to explicitly provide the user id and group id:</p> <p></p>"},{"location":"admin/authentication/non-root-containers/#using-openshift-or-gatekeeper-to-provide-cluster-level-controls","title":"Using OpenShift or Gatekeeper to provide Cluster Level Controls","text":"<p>Run:ai supports OpenShift as a Kubernetes platform. In OpenShift the system will provide a random UID to containers. The flags <code>--run-as-user</code> and <code>--prevent-privilege-escalation</code> are disabled on OpenShift. It is possible to achieve a similar effect on Kubernetes systems that are not OpenShift. A leading tool is Gatekeeper. Gatekeeper similarly enforces non-root on containers at the system level.</p>"},{"location":"admin/authentication/non-root-containers/#creating-a-temporary-home-directory","title":"Creating a Temporary Home Directory","text":"<p>When containers run as a specific user, the user needs to have a pre-created home directory within the image. Otherwise, when running a shell, you will not have a home directory:</p> <pre><code>runai submit -i ubuntu --attach --interactive --run-as-user -- bash\nThe job 'job-0' has been submitted successfully\nYou can run `runai describe job job-0 -p team-a` to check the job status\nWaiting for pod to start running...\nINFO[0007] Job started\nConnecting to pod job-0-0-0\nIf you don't see a command prompt, try pressing enter.\nI have no name!@job-0-0-0:/$ \n</code></pre> <p>Adding home directories to an image per user is not a viable solution. To overcome this, Run:ai provides an additional flag <code>--create-home-dir</code>. Adding this flag creates a temporary home directory for the user within the container.  </p> <p>Notes</p> <ul> <li>Data saved in this directory will not be saved when the container exits.</li> <li>This flag is set by default to true when the <code>--run-as-user</code> flag is used, and false if not.</li> </ul>"},{"location":"admin/authentication/researcher-authentication/","title":"Setup Researcher Access Control","text":""},{"location":"admin/authentication/researcher-authentication/#introduction","title":"Introduction","text":"<p>The following instructions explain how to complete the configuration of access control for Researchers. This requires several steps:</p> <ul> <li>(Mandatory) Modify the Kubernetes entry point (called the <code>Kubernetes API server</code>) to validate the credentials of incoming requests against the Run:ai Authentication authority.</li> <li>(Command-line Interface usage only) Modify the Kubernetes profile to prompt the Researcher for credentials when running <code>runai login</code> (or <code>oc login</code> for OpenShift).</li> </ul> <p>Important</p> <ul> <li>As of Run:ai version 2.16, you only need to perform these steps when accessing Run:ai from the command-line interface or sending YAMLs directly to Kubernetes</li> <li>As of Run:ai version 2.18, you only need to perform these steps when if using the older command-line interface or sending YAMLs directly to Kubernetes.</li> </ul>"},{"location":"admin/authentication/researcher-authentication/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<p>You must direct the Kubernetes API server to authenticate via Run:ai. This requires adding flags to the Kubernetes API Server. The flags show in the Run:ai user interface under <code>Settings</code> | <code>General</code> | <code>Researcher Authentication</code> | <code>Server configuration</code>.</p> <p>Modifying the API Server configuration differs between Kubernetes distributions:</p> Vanilla KubernetesOpenShiftRKERKE2GKEEKSBCMAKSOther <ul> <li>Locate the Kubernetes API Server configuration file. The file's location may differ between different Kubernetes distributions. The location for vanilla Kubernetes is <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code></li> <li>Edit the document, under the <code>command</code> tag, add the server configuration text described above.</li> <li>Verify that the <code>kube-apiserver-&lt;master-node-name&gt;</code> pod in the <code>kube-system</code> namespace has been restarted and that changes have been incorporated. Run the below and verify that the oidc flags you have added:</li> </ul> <pre><code>kubectl get pods -n kube-system kube-apiserver-&lt;master-node-name&gt; -o yaml\n</code></pre> <p>No configuration is needed. Instead, Run:ai assumes that an Identity Provider has been defined at the OpenShift level and that the Run:ai Cluster installation has set the <code>OpenshiftIdp</code> flag to true. For more information see the Run:ai OpenShift control-plane setup.</p> <p>Edit Rancher <code>cluster.yml</code> (with Rancher UI, follow this). Add the following:</p> cluster.yml<pre><code>kube-api:\n    always_pull_images: false\n    extra_args:\n        oidc-client-id: runai  # (1)\n        ...\n</code></pre> <ol> <li>These are example parameters. Copy the actual parameters from <code>Settings | General | Researcher Authentication</code> as described above.</li> </ol> <p>You can verify that the flags have been incorporated into the RKE cluster by following the instructions here and running <code>docker inspect &lt;kube-api-server-container-id&gt;</code>, where <code>&lt;kube-api-server-container-id&gt;</code> is the container ID of api-server via obtained in the Rancher document. </p> <p>If working via the RKE2 Quickstart, edit <code>/etc/rancher/rke2/config.yaml</code>. Add the parameters provided in the server configuration section as described above in the following fashion:</p> /etc/rancher/rke2/config.yaml<pre><code>kube-apiserver-arg:\n- \"oidc-client-id=runai\" # (1)\n...\n</code></pre> <ol> <li>These are example parameters. Copy the actual parameters from <code>Settings | General | Researcher Authentication</code> as described above.</li> </ol> <p>If working via Rancher UI, need to add the flag as part of the cluster provisioning. </p> <p>Under <code>Cluster Management | Create</code>, turn on RKE2 and select a platform. Under <code>Cluster Configuration | Advanced | Additional API Server Args</code>. Add the Run:ai flags as <code>&lt;key&gt;=&lt;value&gt;</code> (e.g. <code>oidc-username-prefix=-</code>).</p> <p>Install Anthos identity service by running:</p> <pre><code>gcloud container clusters update &lt;gke-cluster-name&gt; \\\n    --enable-identity-service --project=&lt;gcp-project-name&gt; --zone=&lt;gcp-zone-name&gt;\n</code></pre> <p>Install the yq utility and run:</p> <p>For username-password authentication, run:</p> <pre><code>kubectl get clientconfig default -n kube-public -o yaml &gt; login-config.yaml\nyq -i e \".spec +={\\\"authentication\\\":[{\\\"name\\\":\\\"oidc\\\",\\\"oidc\\\":{\\\"clientID\\\":\\\"runai\\\",\\\"issuerURI\\\":\\\"$OIDC_ISSUER_URL\\\",\\\"kubectlRedirectURI\\\":\\\"http://localhost:8000/callback\\\",\\\"userClaim\\\":\\\"sub\\\",\\\"userPrefix\\\":\\\"-\\\"}}]}\" login-config.yaml\nkubectl apply -f login-config.yaml\n</code></pre> <p>For single-sign-on, run:</p> <pre><code>kubectl get clientconfig default -n kube-public -o yaml &gt; login-config.yaml\nyq -i e \".spec +={\\\"authentication\\\":[{\\\"name\\\":\\\"oidc\\\",\\\"oidc\\\":{\\\"clientID\\\":\\\"runai\\\",\\\"issuerURI\\\":\\\"$OIDC_ISSUER_URL\\\",\\\"groupsClaim\\\":\\\"groups\\\",\\\"kubectlRedirectURI\\\":\\\"http://localhost:8000/callback\\\",\\\"userClaim\\\":\\\"email\\\",\\\"userPrefix\\\":\\\"-\\\"}}]}\" login-config.yaml\nkubectl apply -f login-config.yaml\n</code></pre> <p>Where the <code>OIDC</code> flags are provided in the Run:ai server configuration section as described above. </p> <p>Then update runaiconfig with  the Anthos endpoint - gke-oidc-envoy. Get the externel IP of the service in the Anthos namespace.</p> <pre><code>kubectl get svc -n anthos-identity-service\nNAME               TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)              AGE\ngke-oidc-envoy     LoadBalancer   10.37.3.111   39.201.319.10   443:31545/TCP        12h\n</code></pre> <p>Add the IP to runaiconfig </p> <pre><code>kubectl -n runai patch runaiconfig runai -p '{\"spec\": {\"researcher-service\": {\"args\": {\"gkeOidcEnvoyHost\": \"35.236.229.19\"}}}}'  --type=\"merge\"\n</code></pre> <p>To create a kubeconfig profile for Researchers run:</p> <pre><code>kubectl oidc login --cluster=CLUSTER_NAME --login-config=login-config.yaml \\\n    --kubeconfig=developer-kubeconfig\n</code></pre> <p>(this will require installing the kubectl oidc plug-in as described in the Anthos document above <code>gcloud components install kubectl-oidc</code>)</p> <p>Then modify the <code>developer-kubeconfig</code> file as described in the Command-line Inteface Access section below.</p> <ul> <li>In the AWS Console, under EKS, find your cluster.</li> <li>Go to <code>Configuration</code> and then to <code>Authentication</code>.</li> <li>Associate a new <code>identity provider</code>. Use the parameters provided in the server configuration section as described above. The process can take up to 30 minutes.</li> </ul> <p>Please follow the \"Vanilla Kubernetes\" instructions</p> <p>Please contact Run:ai customer support.</p> <p>See specific instructions in the documentation of the Kubernetes distribution.  </p>"},{"location":"admin/authentication/researcher-authentication/#command-line-interface-access","title":"Command-line Interface Access","text":"<p>To control access to Run:ai (and Kubernetes) resources, you must modify the Kubernetes configuration file. The file is distributed to users as part of the Command-line interface installation.</p> <p>When making changes to the file, keep a copy of the original file to be used for cluster administration. After making the modifications, distribute the modified file to Researchers.</p> <ul> <li>Under the <code>~/.kube</code> directory edit the <code>config</code> file, remove the administrative user, and replace it with text from <code>Settings | General | Researcher Authentication</code> | <code>Client Configuration</code>.</li> <li>Under <code>contexts | context | user</code> change the user to <code>runai-authenticated-user</code>.</li> </ul> <p>Important</p> <ul> <li>After adding the new user, ensure to delete the following fields from the kubeconfig file to prevent unauthorized access: - Delete: <code>client-certificate-data</code>- Delete: <code>client-key-data</code>- Remove: Any references to the <code>admin</code> user.</li> </ul>"},{"location":"admin/authentication/researcher-authentication/#test-via-command-line-interface","title":"Test via Command-line interface","text":"<ul> <li>Run: <code>runai login</code> (in OpenShift environments use <code>oc login</code> rather than <code>runai login</code>).</li> <li>You will be prompted for a username and password. In a single sign-on flow, you will be asked to copy a link to a browser, log in and return a code.</li> <li>Once login is successful, submit a Job.</li> <li>If the Job was submitted with a Project to which you have no access, your access will be denied.</li> <li>If the Job was submitted with a Project to which you have access, your access will be granted.</li> </ul> <p>You can also submit a Job from the Run:ai User interface and verify that the new job shows on the job list with your user name.</p>"},{"location":"admin/authentication/researcher-authentication/#test-via-user-interface","title":"Test via User Interface","text":"<ul> <li>Open the Run:ai user interface, go to <code>Workloads</code>.</li> <li>On the top-right, select <code>Submit Workload</code>.</li> </ul>"},{"location":"admin/authentication/roles/","title":"Roles","text":"<p>This article explains the available roles in the Run:ai platform.</p> <p>A role is a set of permissions that can be assigned to a subject in a scope.</p> <p>A permission is a set of actions (View, Edit, Create and Delete) over a Run:ai entity (e.g. projects, workloads, users).Roles table</p> <p>The Roles table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The Roles table displays a list of predefined roles available to users in the Run:ai platform. It is not possible to create additional rules or edit or delete existing rules.</p> <p></p> <p>The Roles table consists of the following columns:</p> Column Description Role The name of the role Created by The name of the role creator Creation time The timestamp when the role was created"},{"location":"admin/authentication/roles/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"admin/authentication/roles/#reviewing-a-role","title":"Reviewing a role","text":"<ol> <li>To review a role click the role name on the table  </li> <li>In the role form review the following:  <ul> <li>Role name   The name of the role  </li> <li>Entity  A system-managed object that can be viewed, edited, created or deleted by a user based on their assigned role and scope  </li> <li>Actions  The actions that the role assignee is authorized to perform for each entity  <ul> <li>View If checked, an assigned user with this role can view instances of this type of entity within their defined scope  </li> <li>Edit If checked, an assigned user with this role can change the settings of an instance of this type of entity within their defined scope  </li> <li>Create If checked, an assigned user with this role can create new instances of this type of entity within their defined scope  </li> <li>Delete If checked, an assigned user with this role can delete instances of this type of entity within their defined scope</li> </ul> </li> </ul> </li> </ol>"},{"location":"admin/authentication/roles/#roles-in-runai","title":"Roles in Run:ai","text":"<p>Run:ai supports the following roles and their permissions:  Under each role is a detailed list of the actions that the role assignee is authorized to perform for each entity.</p> Compute resource administrator <p></p> Data source administrator <p></p> Data volume administrator <p></p> Department administrator <p></p> Department viewer <p></p> Editor <p></p> Environment administrator <p></p> L1 researcher <p></p> L2 researcher <p></p> ML engineer <p></p> Research manager <p></p> System administrator <p></p> Template administrator <p></p> Viewer <p></p>"},{"location":"admin/authentication/roles/#permitted-workloads","title":"Permitted workloads","text":"<p>When assigning a role with either one, all or any combination of the View, Edit, Create and Delete permissions for workloads, the subject has permissions to manage not only Run:ai native workloads (Workspace, Training, Inference), but also a list of 3rd party workloads:</p> <ul> <li>k8s: StatefulSet</li> <li>k8s: ReplicaSet</li> <li>k8s: Pod</li> <li>k8s: Deployment</li> <li>batch: Job</li> <li>batch: CronJob</li> <li>machinelearning.seldon.io: SeldonDeployment</li> <li>kubevirt.io: VirtualMachineInstance</li> <li>kubeflow.org: TFJob</li> <li>kubeflow.org: PyTorchJob</li> <li>kubeflow.org: XGBoostJob</li> <li>kubeflow.org: MPIJob</li> <li>kubeflow.org: MPIJob</li> <li>kubeflow.org: Notebook</li> <li>kubeflow.org: ScheduledWorkflow</li> <li>amlarc.azureml.com: AmlJob</li> <li>serving.knative.dev: Service</li> <li>workspace.devfile.io: DevWorkspace</li> <li>ray.io: RayCluster</li> <li>ray.io: RayJob</li> <li>ray.io: RayService</li> <li>ray.io: RayCluster</li> <li>ray.io: RayJob</li> <li>ray.io: RayService</li> <li>tekton.dev: TaskRun</li> <li>tekton.dev: PipelineRun</li> <li>argoproj.io: Workflow</li> </ul>"},{"location":"admin/authentication/roles/#using-api","title":"Using API","text":"<p>Go to the Roles API reference to view the available actions.</p>"},{"location":"admin/authentication/users/","title":"Users","text":"<p>This article explains the procedure to manage users and their permissions.</p> <p>Users can be managed locally, or via the Identity provider, while assigned with Access Rules to manage its permissions.</p> <p>For example, user user@domain.com is a department admin in department A.</p>"},{"location":"admin/authentication/users/#users-table","title":"Users table","text":"<p>The Users table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The users table provides a list of all the users in the platform. You can manage local users and manage user permissions (access rules) for both local and SSO users.</p> <p>Note</p> <p>Single Sign-On users</p> <p>SSO users are managed by the identity provider and appear once they have signed in to Run:ai</p> <p></p> <p>The Users table consists of the following columns:</p> Column Description User The unique identity of the user (email address) Type The type of the user - SSO / local Last login The timestamp for the last time the user signed in Access rule(s) The access rules assigned to the user Created By The user who created the user Creation time The timestamp for when the user was created Last updated The last time the user was updated"},{"location":"admin/authentication/users/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"admin/authentication/users/#creating-a-local-user","title":"Creating a local user","text":"<p>To create a local user:</p> <ol> <li>Click +NEW LOCAL USER </li> <li>Enter the user\u2019s Email address </li> <li>Click CREATE </li> <li>Review and copy the user\u2019s credentials:  <ul> <li>User Email </li> <li>Temporary password to be used on first sign-in  </li> </ul> </li> <li>Click DONE</li> </ol> <p>Note</p> <p>The temporary password is visible only at the time of user\u2019s creation, and must be changed after the first sign-in</p>"},{"location":"admin/authentication/users/#adding-an-access-rule-to-a-user","title":"Adding an access rule to a user","text":"<p>To create an access rule:</p> <ol> <li>Select the user you want to add an access rule for  </li> <li>Click ACCESS RULES </li> <li>Click +ACCESS RULE </li> <li>Select a role  </li> <li>Select a scope  </li> <li>Click SAVE RULE </li> <li>Click CLOSE</li> </ol>"},{"location":"admin/authentication/users/#deleting-users-access-rule","title":"Deleting user\u2019s access rule","text":"<p>To delete an access rule:</p> <ol> <li>Select the user you want to remove an access rule from  </li> <li>Click ACCESS RULES </li> <li>Find the access rule assigned to the user you would like to delete  </li> <li>Click on the trash icon  </li> <li>Click CLOSE</li> </ol>"},{"location":"admin/authentication/users/#resetting-a-user-password","title":"Resetting a user password","text":"<p>To reset a user\u2019s password:</p> <ol> <li>Select the user you want to reset it\u2019s password  </li> <li>Click RESET PASSWORD </li> <li>Click RESET </li> <li>Review and copy the user\u2019s credentials:  <ul> <li>User Email </li> <li>Temporary password to be used on next sign-in  </li> </ul> </li> <li>Click DONE</li> </ol>"},{"location":"admin/authentication/users/#deleting-a-user","title":"Deleting a user","text":"<ol> <li>Select the user you want to delete  </li> <li>Click DELETE </li> <li>In the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>To ensure administrative operations are always available, at least one local user with System Administrator role should exist.</p>"},{"location":"admin/authentication/users/#using-api","title":"Using API","text":"<p>Go to the Users, Access rules API reference to view the available actions</p>"},{"location":"admin/authentication/sso/openidconnect/","title":"Setup SSO with OpenID Connect","text":"<p>Single Sign-On (SSO) is an authentication scheme, allowing users to log-in with a single pair of credentials to multiple, independent software systems.</p> <p>This article explains the procedure to configure single sign-on to Run:ai using the OpenID Connect protocol.</p>"},{"location":"admin/authentication/sso/openidconnect/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have the following available from your identity provider:</p> <ul> <li>Discovery URL - the OpenID server where the content discovery information is published.  </li> <li>ClientID - the ID used to identify the client with the Authorization Server.  </li> <li>Client Secret - a secret password that only the Client and Authorization server know.  </li> <li>Optional: Scopes - a set of user attributes to be used during authentication to authorize access to a user's details.</li> </ul>"},{"location":"admin/authentication/sso/openidconnect/#setup","title":"Setup","text":"<p>Follow the steps below to setup SSO with OpenID Connect.</p>"},{"location":"admin/authentication/sso/openidconnect/#adding-the-identity-provider","title":"Adding the identity provider","text":"<ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section and click +IDENTITY PROVIDER </li> <li>Select Custom OpenID Connect </li> <li>Enter the Discovery URL, Client ID, and Client Secret </li> <li>Copy the Redirect URL to be used in your identity provider  </li> <li>Optional: Add the OIDC scopes  </li> <li>Optional: Enter the user attributes and their value in the identity provider (see the user attributes table below)  </li> <li>Click SAVE    User attributes</li> </ol> Attribute Default value in Run:ai Description User role groups GROUPS If it exists in the IDP, it allows you to assign Run:ai role groups via the IDP. The IDP attribute must be a list of strings. Linux User ID UID If it exists in the IDP, it allows Researcher containers to start with the Linux User UID. Used to map access to network resources such as file systems to users. The IDP attribute must be of type integer. Linux Group ID GID If it exists in the IDP, it allows Researcher containers to start with the Linux Group GID. The IDP attribute must be of type integer. Supplementary Groups SUPPLEMENTARYGROUPS If it exists in the IDP, it allows Researcher containers to start with the relevant Linux supplementary groups. The IDP attribute must be a list of integers. Email email Defines the user attribute in the IDP holding the user's email address, which is the user identifier in Run:ai User first name firstName Used as the user\u2019s first name appearing in the Run:ai user interface User last name lastName Used as the user\u2019s last name appearing in the Run:ai user interface"},{"location":"admin/authentication/sso/openidconnect/#testing-the-setup","title":"Testing the setup","text":"<ol> <li>Log-in to the Run:ai platform as an admin  </li> <li>Add Access Rules to an SSO user defined in the IDP  </li> <li>Open the Run:ai platform in an incognito browser tab  </li> <li>On the sign-in page click CONTINUE WITH SSO    You are redirected to the identity provider sign in page  </li> <li>In the identity provider sign-in page, log in with the SSO user who you granted with access rules  </li> <li>If you are unsuccessful signing-in to the identity provider, follow the Troubleshooting section below</li> </ol>"},{"location":"admin/authentication/sso/openidconnect/#editing-the-identity-provider","title":"Editing the identity provider","text":"<p>You can view the identity provider details and edit its configuration:</p> <ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider box, click Edit identity provider </li> <li>You can edit either the Discovery URL, Client ID, Client Secret, OIDC scopes, or the User attributes</li> </ol> <p>### Removing the identity provider</p> <p>You can remove the identity provider configuration:</p> <ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider card, click Remove identity provider </li> <li>In the dialog, click REMOVE to confirm the action</li> </ol> <p>Note</p> <p>To avoid losing access, removing the identity provider must be carried out by a local user.</p>"},{"location":"admin/authentication/sso/openidconnect/#troubleshooting","title":"Troubleshooting","text":"<p>If testing the setup was unsuccessful, try the different troubleshooting scenarios according to the error you received.</p>"},{"location":"admin/authentication/sso/openidconnect/#troubleshooting-scenarios","title":"Troubleshooting scenarios","text":"403 - Sorry, we can\u2019t let you see this page. Something about permissions\u2026 <p>Description: The authenticated user is missing permissions</p> <p>Mitigation:</p> <ol> <li>Validate either the user or its related group/s are assigned with access rules </li> <li>Validate groups attribute is available in the configured OIDC Scopes  </li> <li>Validate the user\u2019s groups attribute is mapped correctly</li> </ol> <p>Advanced:</p> <ol> <li>Open the Chrome DevTools: Right-click on page \u2192 Inspect \u2192 Console tab  </li> <li>Run the following command to retrieve and paste the user\u2019s token: <code>localStorage.token;</code> </li> <li>Paste in https://jwt.io </li> <li>Under the Payload section validate the values of the user\u2019s attributes</li> </ol> 401 - We\u2019re having trouble identifying your account because your email is incorrect or can\u2019t be found. <p>Description: Authentication failed because email attribute was not found.</p> <p>Mitigation:</p> <ol> <li>Validate email attribute is available in the configured OIDC Scopes  </li> <li>Validate the user\u2019s email attribute is mapped correctly</li> </ol> Unexpected error when authenticating with identity provider <p></p> <p>Description: User authentication failed</p> <p>Mitigation:</p> <ol> <li>Validate that the configured OIDC Scopes exist and match the Identity Provider\u2019s available scopes</li> </ol> <p>Advanced:</p> <ol> <li>Look for the specific error message in the URL address</li> </ol> Unexpected error when authenticating with identity provider (SSO sign-in is not available) <p></p> <p>Description: User authentication failed</p> <p>Mitigation:</p> <ol> <li>Validate that the configured OIDC scope exists in the Identity Provider  </li> <li>Validate the configured Client Secret match the Client Secret in the Identity Provider</li> </ol> <p>Advanced:</p> <ol> <li>Look for the specific error message in the URL address</li> </ol> Client not found <p>Description: OIDC Client ID was not found in the Identity Provider</p> <p>Mitigation:</p> <ol> <li>Validate that the configured Client ID matches the Identity Provider Client ID  </li> </ol>"},{"location":"admin/authentication/sso/openshift/","title":"Setup SSO with OpenShift","text":"<p>Single Sign-On (SSO) is an authentication scheme, allowing users to log-in with a single pair of credentials to multiple, independent software systems.</p> <p>This article explains the procedure to configure single sign-on to Run:ai using the OpenID Connect protocol in OpenShift V4.</p>"},{"location":"admin/authentication/sso/openshift/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have the following available from your OpenShift cluster:</p> <ul> <li>OpenShift OAuth client - see Registering an additional OAuth client </li> <li>ClientID - the ID used to identify the client with the Authorization Server.  </li> <li>Client Secret - a secret password that only the Client and Authorization Server know.  </li> <li>Base URL - the OpenShift API Server endpoint (example: https://api.&lt;cluster-url&gt;:6443)</li> </ul>"},{"location":"admin/authentication/sso/openshift/#setup","title":"Setup","text":"<p>Follow the steps below to setup SSO with OpenShift.</p>"},{"location":"admin/authentication/sso/openshift/#adding-the-identity-provider","title":"Adding the identity provider","text":"<ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section and click +IDENTITY PROVIDER </li> <li>Select OpenShift V4 </li> <li>Enter the Base URL, Client ID, and Client Secret from your OpenShift OAuth client.  </li> <li>Copy the Redirect URL to be used in your OpenShift OAuth client  </li> <li>Optional: Enter the user attributes and their value in the identity provider (see the user attributes table below)  </li> <li>Click SAVE    User attributes</li> </ol> Attribute Default value in Run:ai Description User role groups GROUPS If it exists in the IDP, it allows you to assign Run:ai role groups via the IDP. The IDP attribute must be a list of strings. Linux User ID UID If it exists in the IDP, it allows researcher containers to start with the Linux User UID. Used to map access to network resources such as file systems to users. The IDP attribute must be of type integer. Linux Group ID GID If it exists in the IDP, it allows researcher containers to start with the Linux Group GID. The IDP attribute must be of type integer. Supplementary Groups SUPPLEMENTARYGROUPS If it exists in the IDP, it allows researcher containers to start with the relevant Linux supplementary groups. The IDP attribute must be a list of integers. Email email Defines the user attribute in the IDP holding the user's email address, which is the user identifier in Run:ai User first name firstName Used as the user\u2019s first name appearing in the Run:ai platform User last name lastName Used as the user\u2019s last name appearing in the Run:ai platform"},{"location":"admin/authentication/sso/openshift/#testing-the-setup","title":"Testing the setup","text":"<ol> <li>Open the Run:ai platform as an admin  </li> <li>Add Access Rules to an SSO user defined in the IDP  </li> <li>Open the Run:ai platform in an incognito browser tab  </li> <li>On the sign-in page click CONTINUE WITH SSO    You are redirected to the OpenShift IDP sign-in page  </li> <li>In the identity provider sign-in page, log-in with the SSO user who you granted with access rules  </li> <li>If you are unsuccessful signing-in to the identity provider, follow the Troubleshooting section below</li> </ol>"},{"location":"admin/authentication/sso/openshift/#editing-the-identity-provider","title":"Editing the identity provider","text":"<p>You can view the identity provider details and edit its configuration:</p> <ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider box, click Edit identity provider </li> <li>You can edit either the Base URL, Client ID, Client Secret, or the User attributes</li> </ol>"},{"location":"admin/authentication/sso/openshift/#removing-the-identity-provider","title":"Removing the identity provider","text":"<p>You can remove the identity provider configuration:</p> <ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider card, click Remove identity provider </li> <li>In the dialog, click REMOVE to confirm the action</li> </ol> <p>Note</p> <p>To avoid losing access, removing the identity provider must be carried out by a local user.</p>"},{"location":"admin/authentication/sso/openshift/#troubleshooting","title":"Troubleshooting","text":"<p>If testing the setup was unsuccessful, try the different troubleshooting scenarios according to the error you received.</p>"},{"location":"admin/authentication/sso/openshift/#troubleshooting-scenarios","title":"Troubleshooting scenarios","text":"403 - Sorry, we can\u2019t let you see this page. Something about permissions\u2026 <p>Description: The authenticated user is missing permissions</p> <p>Mitigation:</p> <ol> <li>Validate either the user or its related group/s are assigned with access rules </li> <li>Validate groups attribute is available in the configured OIDC Scopes  </li> <li>Validate the user\u2019s groups attribute is mapped correctly</li> </ol> <p>Advanced:</p> <ol> <li>Open the Chrome DevTools: Right-click on page \u2192 Inspect \u2192 Console tab  </li> <li>Run the following command to retrieve and copy the user\u2019s token: <code>localStorage.token;</code> </li> <li>Paste in https://jwt.io </li> <li>Under the Payload section validate the value of the user\u2019s attributes</li> </ol> 401 - We\u2019re having trouble identifying your account because your email is incorrect or can\u2019t be found. <p>Description: Authentication failed because e-mail attribute was not found.</p> <p>Mitigation:</p> <ol> <li>Validate email attribute is available in the configured OIDC Scopes  </li> <li>Validate the user\u2019s email attribute is mapped correctly</li> </ol> Unexpected error when authenticating with identity provider <p></p> <p>Description: User authentication failed</p> <p>Mitigation:</p> <ol> <li>Validate the the configured OIDC Scopes exist and match the Identity Provider\u2019s available scopes</li> </ol> <p>Advanced:</p> <ol> <li>Look for the specific error message in the URL address</li> </ol> Unexpected error when authenticating with identity provider (SSO sign-in is not available) <p></p> <p>Description: User authentication failed</p> <p>Mitigation:</p> <ol> <li>Validate that the configured OIDC scope exists in the Identity Provider  </li> <li>Validate that the configured Client Secret matches the Client Secret value in the OAuthclient Kubernetes object.</li> </ol> <p>Advanced:</p> <ol> <li>Look for the specific error message in the URL address</li> </ol> unauthorized_client <p></p> <p>Description: OIDC Client ID was not found in the OpenShift IDP</p> <p>Mitigation:</p> <ol> <li>Validate that the configured Client ID matches the value in the OAuthclient Kubernetes object.  </li> </ol>"},{"location":"admin/authentication/sso/saml/","title":"Setup SSO with SAML","text":"<p>Single Sign-On (SSO) is an authentication scheme, allowing users to log-in with a single pair of credentials to multiple, independent software systems.</p> <p>This article explains the procedure to configure SSO to Run:ai using the SAML 2.0 protocol.</p>"},{"location":"admin/authentication/sso/saml/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following available from your identity provider:</p> <ul> <li>SAML XML Metadata</li> </ul>"},{"location":"admin/authentication/sso/saml/#setup","title":"Setup","text":"<p>Follow the steps below to setup SSO with SAML.</p>"},{"location":"admin/authentication/sso/saml/#adding-the-identity-provider","title":"Adding the identity provider","text":"<ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section and click +IDENTITY PROVIDER </li> <li>Select Custom SAML 2.0 </li> <li>Select either From computer or From URL <ul> <li>From computer - click the Metadata XML file field, then select your file for upload  </li> <li>From URL - in the Metadata XML URL field, enter the URL to the XML Metadata file  </li> </ul> </li> <li>Copy the Redirect URL and Entity ID to be used in your identity provider  </li> <li>Optional: Enter the user attributes and their value in the identity provider (see the user attributes table below)  </li> </ol> Attribute Default value in Run:ai Description User role groups GROUPS If it exists in the IDP, it allows you to assign Run:ai role groups via the IDP. The IDP attribute must be a list of strings. Linux User ID UID If it exists in the IDP, it allows Researcher containers to start with the Linux User UID. Used to map access to network resources such as file systems to users. The IDP attribute must be of type integer. Linux Group ID GID If it exists in the IDP, it allows Researcher containers to start with the Linux Group GID. The IDP attribute must be of type integer. Supplementary Groups SUPPLEMENTARYGROUPS If it exists in the IDP, it allows Researcher containers to start with the relevant Linux supplementary groups. The IDP attribute must be a list of integers. Email email Defines the user attribute in the IDP holding the user's email address, which is the user identifier in Run:ai. User first name firstName Used as the user\u2019s first name appearing in the Run:ai platform. User last name lastName Used as the user\u2019s last name appearing in the Run:ai platform. <ol> <li>Click SAVE </li> </ol>"},{"location":"admin/authentication/sso/saml/#testing-the-setup","title":"Testing the setup","text":"<ol> <li>Open the Run:ai platform as an admin  </li> <li>Add Access Rules to an SSO user defined in the IDP  </li> <li>Open the Run:ai platform in an incognito browser tab  </li> <li>On the sign-in page click CONTINUE WITH SSO.     You are redirected to the identity provider sign in page  </li> <li>In the identity provider sign-in page, log-in with the SSO user who you granted with access rules  </li> <li>If you are unsuccessful signing-in to the identity provider, follow the Troubleshooting section below</li> </ol>"},{"location":"admin/authentication/sso/saml/#editing-the-identity-provider","title":"Editing the identity provider","text":"<p>You can view the identity provider details and edit its configuration:</p> <ol> <li>Go Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider box, click Edit identity provider </li> <li>You can edit either the metadata file or the user attributes  </li> <li>You can view the identity provider URL, identity provider entity ID, and the certificate expiration date</li> </ol>"},{"location":"admin/authentication/sso/saml/#removing-the-identity-provider","title":"Removing the identity provider","text":"<p>You can remove the identity provider configuration:</p> <ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider card, click Remove identity provider </li> <li>In the dialog, click REMOVE to confirm the action</li> </ol> <p>Note</p> <p>To avoid losing access, removing the identity provider must be carried out by a local user.</p>"},{"location":"admin/authentication/sso/saml/#downloading-the-xml-metadata-file","title":"Downloading the XML metadata file","text":"<p>You can download the XML file to view the identity provider settings:</p> <ol> <li>Go to Tools &amp; Settings \u2192 General </li> <li>Open the Security section  </li> <li>On the identity provider card, click Download metadata XML file</li> </ol>"},{"location":"admin/authentication/sso/saml/#troubleshooting","title":"Troubleshooting","text":"<p>If testing the setup was unsuccessful, try the different troubleshooting scenarios according to the error you received. If an error still occurs, check the advanced troubleshooting section.</p>"},{"location":"admin/authentication/sso/saml/#troubleshooting-scenarios","title":"Troubleshooting scenarios","text":"Invalid signature in response from identity provider <p>Description: After trying to log-in, the following message is received in the RunLai log-in page.   Mitigation:   1. Go to the Tools &amp; Settings menu   2. Click General   3. Open the Security section   4. In the identity provider box, check for a \"Certificate expired\u201d error   5. If it is expired, update the SAML metadata file to include a valid certificate</p> 401 - We\u2019re having trouble identifying your account because your email is incorrect or can\u2019t be found. <p>Description: Authentication failed because email attribute was not found.</p> <p>Mitigation:</p> <ol> <li>Validate the user\u2019s email attribute is mapped correctly</li> </ol> 403 - Sorry, we can\u2019t let you see this page. Something about permissions\u2026 <p>Description: The authenticated user is missing permissions</p> <p>Mitigation:</p> <ol> <li>Validate either the user or its related group/s are assigned with access rules </li> <li>Validate the user\u2019s groups attribute is mapped correctly</li> </ol> <p>Advanced:</p> <ol> <li>Open the Chrome DevTools: Right-click on page \u2192 Inspect \u2192 Console tab  </li> <li>Run the following command to retrieve and paste the user\u2019s token: <code>localStorage.token;</code> </li> <li>Paste in https://jwt.io </li> <li>Under the Payload section validate the values of the user\u2019s attributes</li> </ol>"},{"location":"admin/authentication/sso/saml/#advanced-troubleshooting","title":"Advanced Troubleshooting","text":"Validating the SAML request <p>The SAML login flow can be separated into two parts:</p> <ul> <li>Run:ai redirects to the IDP for log-ins using a SAML Request  </li> <li>On successful log-in, the IDP redirects back to Run:ai with a SAML Response</li> </ul> <p>Validate the SAML Request to ensure the SAML flow works as expected:</p> <ol> <li>Go to the Run:ai login screen  </li> <li>Open the Chrome Network inspector: Right-click \u2192 Inspect on the page \u2192 Network tab  </li> <li>On the sign-in page click CONTINUE WITH SSO.  </li> <li>Once redirected to the Identity Provider, search in the Chrome network inspector for an HTTP request showing the SAML Request. Depending on the IDP url, this would be a request to the IDP domain name. For example, <code>accounts.google.com/idp?1234</code>.  </li> <li>When found, go to the Payload tab and copy the value of the SAML Request  </li> <li>Paste the value into a SAML decoder (e.g. https://www.samltool.com/decode.php)  </li> <li>Validate the request:  <ul> <li>The content of the <code>&lt;saml:Issuer&gt;</code> tag is the same as <code>Entity ID</code> given when adding the identity provider  </li> <li>The content of the <code>AssertionConsumerServiceURL</code> is the same as the <code>Redirect URI</code> given when adding the identity provider  </li> </ul> </li> <li>Validate the response:  <ul> <li>The user email under the <code>&lt;saml2:Subject&gt;</code> tag is the same as the logged-in user  </li> <li>Make sure that under the <code>&lt;saml2:AttributeStatement&gt;</code> tag, there is an Attribute named <code>email</code> (lowercase). This attribute is mandatory.  </li> <li>If other, optional user attributes (<code>groups</code>, <code>firstName</code>, <code>lastName</code>, <code>uid</code>, <code>gid</code>) are mapped make sure they also exist under <code>&lt;saml2:AttributeStatement&gt;</code> along with their respective values.</li> </ul> </li> </ol>"},{"location":"admin/config/access-roles/","title":"Understand the Kubernetes Cluster Access provided to Run:ai","text":"<p>Run:ai has configuration flags that control specific behavioral aspects of Run:ai. Specifically, those which require additional permissions. Such as automatic namespace/project creation, secret propagation, and more.</p> <p>The purpose of this document is to provide security officers with the ability to review what cluster-wide access Run:ai requires, and verify that it is in line with organizational policy, before installing the Run:ai cluster. </p>"},{"location":"admin/config/access-roles/#review-cluster-access-roles","title":"Review Cluster Access Roles","text":"<p>Run the following:</p> <pre><code>helm repo add runai https://run-ai-charts.storage.googleapis.com\nhelm repo update\nhelm install runai-cluster runai/runai-cluster -n runai -f runai-&lt;cluster-name&gt;.yaml \\\n        --dry-run &gt; cluster-all.yaml\n</code></pre> <p>The file <code>cluster-all.yaml</code> can be then be reviewed. You can use the internal filenames (provided in comments within the file) to gain more understanding according to the table below:</p> Folder File Purpose <code>clusterroles</code> <code>base.yaml</code> Mandatory Kubernetes Cluster Roles and Cluster Role Bindings <code>clusterroles</code> <code>project-controller-ns-creation.yaml</code> Automatic Project Creation and Maintenance. Provides Run:ai with the ability to create Kubernetes namespaces when the Run:ai administrator creates new Projects. Can be turned on/off via flag <code>clusterroles</code> <code>project-controller-rb-creation.yaml</code> Automatically assign Users to Projects. Can be turned on/off via flag <code>clusterroles</code> <code>project-controller-limit-range.yaml</code> Disables the usage of the Kubernetes Limit Range feature. Can be turned on/off via flag <code>ocp</code> <code>scc.yaml</code> OpenShift-specific Security Contexts <code>priorityclasses</code> 4 files Folder contains a list of Priority Classes used by Run:ai"},{"location":"admin/config/admin-messages/","title":"Administrator Messages","text":"<p>System administrators can use Administrator messages to make announcements to users once they have logged in. These messages typically are used to keep user informed about different aspects of the platform.</p> <p>To configure an Administrator message:</p> <ol> <li>Press <code>Settings | General</code>.</li> <li>Expand the Message from administrator pane.</li> <li>Press Message.</li> <li>Enter your message in the text box. Use the formatting tools in the toolbar to add special formatting or links to the message.</li> <li>Enable the <code>Display \"Don't show this again\" checkbox on message to users</code> to allow the users to see the message only once.</li> <li>Press Publish when complete.</li> </ol>"},{"location":"admin/config/advanced-cluster-config/","title":"Advanced Cluster Configuration","text":"<p>Advanced cluster configurations can be used to tailor your Run:ai cluster deployment to meet specific operational requirements and optimize resource management. By fine-tuning these settings, you can enhance functionality, ensure compatibility with organizational policies, and achieve better control over your cluster environment. This article provides guidance on implementing and managing these configurations to adapt the Run:ai cluster to your unique needs.</p> <p>After the Run:ai cluster is installed, you can adjust various settings to better align with your organization's operational needs and security requirements.</p>"},{"location":"admin/config/advanced-cluster-config/#edit-cluster-configurations","title":"Edit cluster configurations","text":"<p>Advanced cluster configurations are managed through the runaiconfig Kubernetes Custom Resource. To modify the cluster configurations, use the following command:</p> <pre><code>kubectl edit runaiconfig runai -n runai\n</code></pre>"},{"location":"admin/config/advanced-cluster-config/#configurations","title":"Configurations","text":"<p>The following configurations allow you to enable or disable features, control permissions, and customize the behavior of your Run:ai cluster:</p> Key Description Default spec.project-controller.createNamespaces (boolean) Allows Kubernetes namespace creation for new projects true spec.mps-server.enabled (boolean) Enabled when using NVIDIA MPS false spec.global.subdomainSupport (boolean) Allows the creation of subdomains for ingress endpoints, enabling access to workloads via unique subdomains on the Fully Qualified Domain Name (FQDN). For details, see External Access to Container false spec.runai-container-toolkit.enabled (boolean) Allows workloads to use GPU fractions true spec.prometheus.spec.retention (string) Defines how long Prometheus retains Run:ai metrics locally, which is useful in case of potential connectivity issues. For more information, see Prometheus Storage 2h spec.prometheus.spec.retentionSize (string) Allocates storage space for Run:ai metrics in Prometheus, which is useful in case of potential connectivity issues. For more information, see Prometheus Storage \"\" spec.prometheus.logLevel (string) Sets the Prometheus log levelPossible values: [debug, info, warn, error] \u201cinfo\" spec.global.schedulingServices (object) Defines resource constraints uniformly for the entire set of Run:ai scheduling services. For more information, see Resource requests and limits of Pod and container <code>{resources: {}}</code> spec.global.syncServices (object) Defines resource constraints uniformly for the entire set of Run:ai sync services. For more information, see Resource requests and limits of Pod and container <code>{resources: {}}</code> spec.global.workloadServices (object) Defines resource constraints uniformly for the entire set of Run:ai workload services. For more information, see Resource requests and limits of Pod and container <code>{resources: {}}</code> global.nodeAffinity.restrictScheduling (boolean) Enables setting node roles and restricting workload scheduling to designated nodes false spec.runai-container-toolkit.logLevel (boolean) Specifies the run:ai-container-toolkit logging level: either 'SPAM', 'DEBUG', 'INFO', 'NOTICE', 'WARN', or 'ERROR' INFO node-scale-adjuster.args.gpuMemoryToFractionRatio (object) A scaling-pod requesting a single GPU device will be created for every 1 to 10 pods requesting fractional GPU memory (1/gpuMemoryToFractionRatio). This value represents the ratio (0.1-0.9) of fractional GPU memory (any size) to GPU fraction (portion) conversion. 0.1 spec.global.core.dynamicFractions.enabled (boolean) Enables dynamic GPU fractions true spec.global.core.swap.enabled (boolean) Enables memory swap for GPU workloads false spec.global.core.swap.limits.cpuRam (string) Sets the CPU memory size used to swap GPU workloads 100Gi spec.global.core.swap.limits.reservedGpuRam (string) Sets the reserved GPU memory size used to swap GPU workloads 2Gi spec.global.core.nodeScheduler.enabled  (boolean) Enables the node-level scheduler false spec.limitRange.cpuDefaultRequestCpuLimitFactorNoGpu  (string) Sets a default ratio between the CPU request and the limit for workloads without GPU requests 0.1 spec.limitRange.memoryDefaultRequestMemoryLimitFactorNoGpu  (string) Sets a default ratio between the memory request and the limit for workloads without GPU requests 0.1 spec.limitRange.cpuDefaultRequestGpuFactor (string) Sets a default amount of CPU allocated per GPU when the CPU is not specified spec.limitRange.cpuDefaultLimitGpuFactor (int) Sets a default CPU limit based on the number of GPUs requested when no CPU limit is specified NO DEFAULT spec.limitRange.memoryDefaultRequestGpuFactor (string) Sets a default amount of memory allocated per GPU when the memory is not specified 100Mi spec.limitRange.memoryDefaultLimitGpuFactor (string) Sets a default memory limit based on the number of GPUs requested when no memory limit is specified NO DEFAULT global.core.timeSlicing.mode (string) Sets the GPU time-slicing mode.Possible values:<code>timesharing</code> - all pods on a GPU share the GPU compute time evenly.\u2018strict\u2019 - each pod gets an exact time slice according to its memory fraction value.<code>fair</code> - each pod gets an exact time slice according to its memory fraction value and any unused GPU compute time is split evenly between the running pods. timesharing runai-scheduler.fullHierarchyFairness (boolean) Enables fairness between departments, on top of projects fairness true pod-grouper.args.gangSchedulingKnative (boolean) Enables gang scheduling for inference workloads.For backward compatibility with versions earlier than v2.19, change the value to false true runai-scheduler.args.defaultStalenessGracePeriod Sets the timeout in seconds before the scheduler evicts a stale pod-group (gang) that went below its min-members in running state: 0s - Immediately (no timeout) -1 - Never 60s runai-scheduler.args.verbosity (int) Configures the level of detail in the logs generated by the scheduler service 4 <p>Tip</p> <p>To view the full runaiconfig object structure, use the following command:  <pre><code>kubectl get crds/runaiconfigs.run.ai -n runai -o yaml\n</code></pre></p>"},{"location":"admin/config/allow-external-access-to-containers/","title":"External access to Containers","text":""},{"location":"admin/config/allow-external-access-to-containers/#introduction","title":"Introduction","text":"<p>Researchers working with containers may at times need to remotely access the container. Some examples:</p> <ul> <li>Using a Jupyter notebook that runs within the container</li> <li>Using PyCharm to run python commands remotely.</li> <li>Using TensorBoard to view machine learning visualizations</li> </ul> <p>This requires exposing container ports. When using docker, the way Researchers expose ports is by declaring them when starting the container. Run:ai has similar syntax.</p> <p>Run:ai is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers several options:</p> Method Description Prerequisites Port Forwarding Simple port forwarding allows access to the container via local and/or remote port. None NodePort Exposes the service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service from outside the cluster by requesting <code>&lt;NODE-IP&gt;:&lt;NODE-PORT&gt;</code> regardless of which node the container actually resides in. None LoadBalancer Exposes the service externally using a cloud provider\u2019s load balancer. Only available with cloud providers <p>See https://kubernetes.io/docs/concepts/services-networking/service for further details on these options.</p>"},{"location":"admin/config/allow-external-access-to-containers/#workspaces-configuration","title":"Workspaces configuration","text":"<p>Workspaces allow the Researcher to build AI models interactively. </p> <p>Workspaces allow the Researcher to launch tools such as Visual Studio code, TensorFlow, TensorBoard etc. These tools require access to the container. Access is provided via URLs. </p> <p>Run:ai uses the Cluster URL provided to dynamically create SSL-secured URLs for researchers\u2019 workspaces in the format of <code>https://&lt;CLUSTER_URL&gt;/project-name/workspace-name</code>.</p> <p>While this form of path-based routing conveniently works with applications like Jupyter Notebooks, it may often not be compatible with other applications. These applications assume running at the root file system, so hardcoded file paths and settings within the container may become invalid when running at a path other than the root. For instance, if the container is expecting to find a file at <code>/etc/config.json</code> but is running at <code>/project-name/workspace-name</code>, the file will not be found. This can cause the container to fail or not function as intended.</p> <p>To address this issue, Run:ai provides support for host-based routing. When enabled, Run:ai creates workspace URLs in a subdomain format (<code>https://project-name-workspace-name.&lt;CLUSTER_URL&gt;/</code>), which allows all workspaces to run at the root path and function properly. </p> <p>To enable host-based routing you must perform the following steps:</p> <p>Note</p> <p>For OpenShift, editing the Runaiconfig command is the only step required to generate workspace URLs. Refer to the last step below.</p> <ol> <li>Create a second DNS entry (A record) for <code>*.&lt;CLUSTER_URL&gt;</code>, pointing to the same IP as the cluster Fully Qualified Domain Name (FQDN)</li> <li> <p>Obtain a wildcard SSL certificate for this DNS.</p> </li> <li> <p>Add the certificate as a secret:</p> </li> </ol> <pre><code>kubectl create secret tls runai-cluster-domain-star-tls-secret -n runai \\ \n    --cert /path/to/fullchain.pem --key /path/to/private.pem\n</code></pre> <ol> <li>Create the following ingress rule:</li> </ol> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: runai-cluster-domain-star-ingress\n  namespace: runai\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: '*.&lt;CLUSTER_URL&gt;'\n  tls:\n  - hosts:\n    - '*.&lt;CLUSTER_URL&gt;'\n    secretName: runai-cluster-domain-star-tls-secret\n</code></pre> <p>Replace <code>&lt;CLUSTER_URL&gt;</code> as described above and run: <code>kubectl apply -f &lt;filename&gt;</code>.</p> <ol> <li>Edit Runaiconfig to generate the URLs correctly:</li> </ol> <pre><code>kubectl patch RunaiConfig runai -n runai --type=\"merge\" \\\n    -p '{\"spec\":{\"global\":{\"subdomainSupport\": true}}}' \n</code></pre> <p>Once these requirements have been met, all workspaces will automatically be assigned a secured URL with a subdomain, ensuring full functionality for all researcher applications.</p>"},{"location":"admin/config/allow-external-access-to-containers/#see-also","title":"See Also","text":"<ul> <li>To learn how to use port forwarding see the Quickstart document:  Launch an Interactive Build Workload with Connected Ports.</li> <li>See CLI command runai submit.</li> </ul>"},{"location":"admin/config/cli-admin-install/","title":"Administrator CLI","text":"<p>The Run:ai Administrator (<code>runai-adm</code>) is a lightweight tool designed to support infrastructure administrators by simplifying two key tasks:</p> <ul> <li>Collecting logs for troubleshooting and sharing with Run:ai support.</li> <li>Configuring node roles in the cluster for optimal performance and reliability.</li> </ul> <p>This article outlines the installation and usage of the Run:ai Administrator CLI to help you get started quickly.</p>"},{"location":"admin/config/cli-admin-install/#prerequisites","title":"Prerequisites","text":"<p>Before installing the CLI, review the following:</p> <ul> <li>Operating system: The CLI is supported on Mac and Linux.</li> <li>Kubectl: The Kubernetes command-line interface must be installed and configured to access your cluster. Follow the official guide.</li> <li>Cluster administrative permissions: The CLI requires a Kubernetes profile with administrative privileges.</li> </ul>"},{"location":"admin/config/cli-admin-install/#installation","title":"Installation","text":"<p>To install the Run:ai Administrator CLI, ensure that the CLI version matches the version of your Run:ai cluster. You can either install the latest version or a specific version from the list.</p>"},{"location":"admin/config/cli-admin-install/#installing-the-latest-version","title":"Installing the latest version","text":"<p>Use the following commands to download and install the latest version of the CLI:</p> MacLinux <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/darwin # (1) \nchmod +x runai-adm  \nsudo mv runai-adm /usr/local/bin/runai-adm\n</code></pre> <ol> <li>In self-hosted environment, use the control-plane URL instead of <code>app.run.ai</code> </li> </ol> <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/linux # (1) \nchmod +x runai-adm  \nsudo mv runai-adm /usr/local/bin/runai-adm\n</code></pre> <ol> <li>In self-hosted environment, use the control-plane URL instead of <code>app.run.ai</code> </li> </ol>"},{"location":"admin/config/cli-admin-install/#installing-a-specific-version","title":"Installing a specific version","text":"<p>To install a specific version of the Administrator CLI that matches your Run:ai cluster version, append the version number to the download URL. Refer to the list of available versions linked above for the correct version number.</p> MacLinux <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/&lt;version&gt;/darwin # Replace &lt;version&gt; with the desired version in the format vX.X.X (e.g., v2.19.5) \nchmod +x runai-adm  \nsudo mv runai-adm /usr/local/bin/runai-adm\n</code></pre> <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/&lt;version&gt;/linux # Replace &lt;version&gt; with the desired version in the format vX.X.X (e.g., v2.19.5)\nchmod +x runai-adm  \nsudo mv runai-adm /usr/local/bin/runai-adm\n</code></pre>"},{"location":"admin/config/cli-admin-install/#verifying-installation","title":"Verifying installation","text":"<p>Verify your installation completed successfully by running the following command:</p> <pre><code>runai-adm version\n</code></pre>"},{"location":"admin/config/cli-admin-install/#reference","title":"Reference","text":""},{"location":"admin/config/cli-admin-install/#node-roles","title":"Node roles","text":"<p>To set or remove node rules using the <code>runai-adm</code> tool, run the following:</p> <pre><code>runai-adm set node-role [--runai-system-worker | --gpu-worker | --cpu-worker] &lt;node-name&gt;\n</code></pre> <pre><code>runai-adm remove node-role [--runai-system-worker | --gpu-worker | --cpu-worker] &lt;node-name&gt;\n</code></pre> <p>Note</p> <p>Use the <code>--all</code> flag to set or remove a role to all nodes.</p>"},{"location":"admin/config/cli-admin-install/#collect-logs","title":"Collect logs","text":"<p>To collect logs using the <code>runai-adm</code> tool:</p> <ol> <li> <p>Run the following command:</p> <pre><code>runai-adm collect-logs\n</code></pre> </li> <li> <p>Locate the generated compressed log file.</p> </li> </ol>"},{"location":"admin/config/cluster-wide-pvc/","title":"Cluster wide PVCs","text":"<p>A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes. For more information about PVCs, see Persistent Volumes.</p> <p>PVCs are namespace-specific. If your PVC relates to all run:ai Projects, do the following to propagate the PVC to all Projects:</p> <p>Create a PVC within the run:ai namespace, then run the following once to propagate the PVC to all run:ai Projects:</p> <pre><code>kubectl label persistentvolumeclaims -n runai &lt;PVC_NAME&gt; runai/cluster-wide=true\n</code></pre> <p>To delete a PVC from all run:ai Projects, run:</p> <pre><code>kubectl label persistentvolumeclaims -n runai &lt;PVC_NAME&gt; runai/cluster-wide-\n</code></pre> <p>You can add a PVC to a job using the <code>New job</code> form.</p> <p>To add a PVC to a new job:</p> <ol> <li>On the <code>New job</code> form, press <code>Storage</code>.</li> <li>In <code>Persistent Volume Claims</code> press <code>Add</code>.</li> <li>Enable <code>Existing PVC</code>.</li> <li>Enter the name (claim name) of the PVC.</li> <li>Enter the storage class. (Optional)</li> <li>Enter the size.</li> <li>Enable / disable access modes.</li> </ol>"},{"location":"admin/config/clusters/","title":"Clusters","text":"<p>This article explains the procedure to view and manage Clusters.</p> <p>The Cluster table provides a quick and easy way to see the status of your cluster.</p> <p></p>"},{"location":"admin/config/clusters/#clusters-table","title":"Clusters table","text":"<p>The Clusters table can be found under Clusters in the Run:ai platform.</p> <p>The clusters table provides a list of the clusters added to Run:ai platform, along with their status.</p> <p>The clusters table consists of the following columns:</p> Column Description Cluster The name of the cluster Status The status of the cluster. For more information see the table below. Hover over the information icon for a short description and links to troubleshooting Creation time The timestamp when the cluster was created URL The URL that was given to the cluster Run:ai cluster version The Run:ai version installed on the cluster Kubernetes distribution The flavor of Kubernetes distribution Kubernetes version The version of Kubernetes installed Run:ai cluster UUID The unique ID of the cluster"},{"location":"admin/config/clusters/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"admin/config/clusters/#cluster-status","title":"Cluster status","text":"Status Description Waiting to connect The cluster has never been connected. Disconnected There is no communication from the cluster to the {{glossary.Control plane}}. This may be due to a network issue. See the troubleshooting scenarios. Missing prerequisites Some prerequisites are missing from the cluster. As a result, some features may be impacted. See the troubleshooting scenarios. Service issues At least one of the services is not working properly. You can view the list of nonfunctioning services for more information. See the troubleshooting scenarios. Connected The Run:ai cluster is connected, and all Run:ai services are running."},{"location":"admin/config/clusters/#adding-a-new-cluster","title":"Adding a new cluster","text":"<p>To add a new cluster see the installation guide.</p>"},{"location":"admin/config/clusters/#removing-a-cluster","title":"Removing a cluster","text":"<ol> <li>Select the cluster you want to remove  </li> <li>Click REMOVE</li> <li>A dialog appears: Make sure to carefully read the message before removing  </li> <li>Click REMOVE to confirm the removal.</li> </ol>"},{"location":"admin/config/clusters/#using-the-api","title":"Using the API","text":"<p>Go to the Clusters API reference to view the available actions</p>"},{"location":"admin/config/clusters/#troubleshooting","title":"Troubleshooting","text":"<p>Before starting, make sure you have the following:</p> <ul> <li>Access to the Kubernetes cluster where Run:ai is deployed with the necessary permissions  </li> <li>Access to the Run:ai Platform</li> </ul>"},{"location":"admin/config/clusters/#troubleshooting-scenarios","title":"Troubleshooting scenarios","text":"Cluster disconnected <p>Description: When the cluster's status is \u2018disconnected\u2019, there is no communication from the cluster services reaching the Run:ai Platform. This may be due to networking issues or issues with Run:ai services.</p> <p>Mitigation:</p> <ol> <li> <p>Check Run:ai\u2019s services status: </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permission to view pods  </li> <li>Copy and paste the following command to verify that Run:ai\u2019s services are running:  </li> </ul> <p><pre><code>kubectl get pods -n runai | grep -E 'runai-agent|cluster-sync|assets-sync'\n</code></pre> * If any of the services are not running, see the \u2018cluster has service issues\u2019 scenario.  </p> </li> <li> <p>Check the network connection  </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to create pods  </li> <li>Copy and paste the following command to create a connectivity check pod:  </li> </ul> <pre><code>kubectl run control-plane-connectivity-check -n runai --image=wbitt/network-multitool \\\n    --command -- /bin/sh -c 'curl -sSf &lt;control-plane-endpoint&gt; &gt; /dev/null &amp;&amp; echo \"Connection Successful\" \\\n    || echo \"Failed connecting to the Control Plane\"'\n</code></pre> <ul> <li>Replace <code>&lt;control-plane-endpoint&gt;</code> with the URL of the Control Plane in your environment. If the pod fails to connect to the Control Plane, check for potential network policies </li> </ul> </li> <li> <p>Check and modify the network policies</p> <ul> <li>Open your terminal  </li> <li> <p>Copy and paste the following command to check the existence of network policies: <pre><code>kubectl get networkpolicies -n runai\n</code></pre></p> </li> <li> <p>Review the policies to ensure that they allow traffic from the Run:ai namespace to the Control Plane. If necessary, update the policies to allow the required traffic. Example of allowing traffic:</p> </li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: allow-control-plane-traffic\nnamespace: runai\nspec:\npodSelector:\n    matchLabels:\n    app: runai\npolicyTypes:\n    - Ingress\n    - Egress\negress:\n    - to:\n        - ipBlock:\n            cidr: &lt;control-plane-ip-range&gt;\n    ports:\n        - protocol: TCP\n        port: &lt;control-plane-port&gt;\ningress:\n    - from:\n        - ipBlock:\n            cidr: &lt;control-plane-ip-range&gt;\n    ports:\n        - protocol: TCP\n        port: &lt;control-plane-port&gt;\n</code></pre> <ul> <li> <p>Check infrastructure-level configurations: </p> <ul> <li>Ensure that firewall rules and security groups allow traffic between your Kubernetes cluster and the Control Plane  </li> <li>Verify required ports and protocols:  <ul> <li>Ensure that the necessary ports and protocols for Run:ai\u2019s services are not blocked by any firewalls or security groups  </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Check Run:ai services logs  </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to view logs  </li> <li>Copy and paste the following commands to view the logs of the Run:ai services: </li> </ul> <pre><code>kubectl logs deployment/runai-agent -n runai\nkubectl logs deployment/cluster-sync -n runai\nkubectl logs deployment/assets-sync -n runai\n</code></pre> <ul> <li>Try to identify the problem from the logs. If you cannot resolve the issue, continue to the next step. </li> </ul> </li> <li> <p>Contact Run:ai\u2019s support  </p> <ul> <li>If the issue persists, contact Run:ai\u2019s support for assistance.</li> </ul> </li> </ol> Cluster has service issues <p>Description: When a cluster's status is Has service issues, it means that one or more Run:ai services running in the cluster are not available.</p> <p>Mitigation:</p> <ol> <li> <p>Verify non-functioning services  </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to view the <code>runaiconfig</code> resource  </li> <li>Copy and paste the following command to determine which services are not functioning:  </li> </ul> <pre><code>kubectl get runaiconfig -n runai runai -ojson | jq -r '.status.conditions | map(select(.type == \"Available\"))'\n</code></pre> </li> <li> <p>Check for Kubernetes events </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to view events  </li> <li>Copy and paste the following command to get all Kubernetes events:  </li> </ul> </li> <li> <p>Inspect resource details  </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to describe resources  </li> <li>Copy and paste the following command to check the details of the required resource:  </li> </ul> <pre><code>kubectl describe &lt;resource_type&gt; &lt;name&gt;\n</code></pre> </li> <li> <p>Contact Run:ai\u2019s Support  </p> <ul> <li>If the issue persists, contact contact Run:ai\u2019s support for assistance.</li> </ul> </li> </ol> Cluster is waiting to connect <p>Description: When the cluster's status is \u2018waiting to connect\u2019, it means that no communication from the cluster services reaches the Run:ai Platform. This may be due to networking issues or issues with Run:ai services.</p> <p>Mitigation:</p> <ol> <li> <p>Check Run:ai\u2019s services status </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to view pods  </li> <li>Copy and paste the following command to verify that Run:ai\u2019s services are running:  </li> </ul> <pre><code>kubectl get pods -n runai | grep -E 'runai-agent|cluster-sync|assets-sync'\n</code></pre> <ul> <li>If any of the services are not running, see the \u2018cluster has service issues\u2019 scenario. </li> </ul> </li> <li> <p>Check the network connection </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permissions to create pods  </li> <li>Copy and paste the following command to create a connectivity check pod:  </li> </ul> <pre><code>kubectl run control-plane-connectivity-check -n runai --image=wbitt/network-multitool --command -- /bin/sh -c 'curl -sSf &lt;control-plane-endpoint&gt; &gt; /dev/null &amp;&amp; echo \"Connection Successful\" || echo \"Failed connecting to the Control Plane\"'\n</code></pre> <ul> <li>Replace <code>&lt;control-plane-endpoint&gt;</code> with the URL of the Control Plane in your environment. If the pod fails to connect to the Control Plane, check for potential network policies:  </li> </ul> </li> <li> <p>Check and modify the network policies  </p> <ul> <li>Open your terminal  </li> <li>Copy and paste the following command to check the existence of network policies:  </li> </ul> <pre><code>kubectl get networkpolicies -n runai\n</code></pre> <ul> <li>Review the policies to ensure that they allow traffic from the Run:ai namespace to the Control Plane. If necessary, update the policies to allow the required traffic.  Example of allowing traffic:  </li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: allow-control-plane-traffic\nnamespace: runai\nspec:\n  podSelector:\n    matchLabels:\n    app: runai\n  policyTypes:\n    - Ingress\n    - Egress\n  egress:\n    - to:\n        - ipBlock:\n            cidr: &lt;control-plane-ip-range&gt;\n    ports:\n        - protocol: TCP\n        port: &lt;control-plane-port&gt;\n  ingress:\n    - from:\n        - ipBlock:\n            cidr: &lt;control-plane-ip-range&gt;\n    ports:\n        - protocol: TCP\n        port: &lt;control-plane-port&gt;\n</code></pre> <ul> <li>Check infrastructure-level configurations:  </li> <li>Ensure that firewall rules and security groups allow traffic between your Kubernetes cluster and the Control Plane  </li> <li>Verify required ports and protocols:  <ul> <li>Ensure that the necessary ports and protocols for Run:ai\u2019s services are not blocked by any firewalls or security groups </li> </ul> </li> </ul> </li> <li> <p>Check Run:ai services logs  </p> <ul> <li>Open your terminal  </li> <li>Make sure you have access to the Kubernetes cluster with permission to view logs  </li> <li>Copy and paste the following commands to view the logs of the Run:ai services:  </li> </ul> <pre><code>kubectl logs deployment/runai-agent -n runai\nkubectl logs deployment/cluster-sync -n runai\nkubectl logs deployment/assets-sync -n runai\n</code></pre> <ul> <li>Try to identify the problem from the logs. If you cannot resolve the issue, continue to the next step  </li> </ul> </li> <li> <p>Contact Run:ai\u2019s support  </p> <ul> <li>If the issue persists, contact Run:ai\u2019s support for assistance.</li> </ul> </li> </ol> Cluster is missing prerequisites <p>Description: When a cluster's status displays Missing prerequisites, it indicates that at least one of the Mandatory Prerequisites has not been fulfilled. In such cases, Run:ai services may not function properly.</p> <p>Mitigation:</p> <p>If you have ensured that all prerequisites are installed and the status still shows missing prerequisites, follow these steps:</p> <ol> <li>Check the message in the Run:ai platform for further details regarding the missing prerequisites.  </li> <li> <p>Inspect the <code>runai-public</code> ConfigMap:  </p> <ul> <li>Open your terminal. In the terminal, type the following command to list all ConfigMaps in the <code>runai</code> namespace: </li> </ul> <pre><code>kubectl get configmap -n runai\n</code></pre> </li> <li> <p>Describe the ConfigMap  </p> <ul> <li>Locate the ConfigMap named <code>runai-public</code> from the list  </li> <li>To view the detailed contents of this ConfigMap, type the following command:  </li> </ul> <pre><code>kubectl describe configmap runai-public -n runai\n</code></pre> </li> <li> <p>Find Missing Prerequisites  </p> <ul> <li>In the output displayed, look for a section labeled <code>dependencies.required</code> </li> <li>This section provides detailed information about any missing resources or prerequisites. Review this information to identify what is needed  </li> </ul> </li> <li> <p>Contact Run:ai\u2019s support  </p> <ul> <li>If the issue persists, contact Run:ai\u2019s support for assistance.</li> </ul> </li> </ol>"},{"location":"admin/config/create-k8s-assets-in-advance/","title":"Creating Kubernetes Assets in Advance","text":"<p>The article describe how to mark Kubernetes assets for use by Run:ai</p>"},{"location":"admin/config/create-k8s-assets-in-advance/#creating-pvcs-in-advance","title":"Creating PVCs in advance","text":"<p>Add PVCs in advance to be used when creating a PVC-type data source via the Run:ai UI.</p> <p>Follow the steps below for each required scope:</p>"},{"location":"admin/config/create-k8s-assets-in-advance/#cluster-scope","title":"Cluster scope","text":"<ol> <li>Locate the PVC in the Run:ai namespace (runai)</li> <li>To authorize Run:ai to use the PVC, label it: <code>run.ai/cluster-wide: \"true\u201d</code>     The PVC is now displayed for that scope in the list of existing PVCs.</li> </ol>"},{"location":"admin/config/create-k8s-assets-in-advance/#department-scope","title":"Department scope","text":"<ol> <li>Locate the PVC in the Run:ai namespace (runai)</li> <li>To authorize Run:ai to use the PVC, label it: <code>run.ai/department: \"id\"</code>     The PVC is now displayed for that scope in the list of existing PVCs.</li> </ol>"},{"location":"admin/config/create-k8s-assets-in-advance/#project-scope","title":"Project scope","text":"<ol> <li>Locate the PVC in the project\u2019s namespace     The PVC is now displayed for that scope in the list of existing PVCs.</li> </ol>"},{"location":"admin/config/create-k8s-assets-in-advance/#creating-configmaps-in-advance","title":"Creating ConfigMaps in advance","text":"<p>Add ConfigMaps in advance to be used when creating a ConfigMap-type data source via the Run:ai UI.</p>"},{"location":"admin/config/create-k8s-assets-in-advance/#cluster-scope_1","title":"Cluster scope","text":"<ol> <li>Locate the ConfigMap in the Run:ai namespace (runai)</li> <li>To authorize Run:ai to use the ConfigMap, label it: <code>run.ai/cluster-wide: \"true\u201d</code></li> <li> <p>The ConfigMap must have a label of <code>run.ai/resource: &lt;resource-name&gt;</code></p> <p>The ConfigMap is now displayed for that scope in the list of existing ConfigMaps.</p> </li> </ol>"},{"location":"admin/config/create-k8s-assets-in-advance/#department-scope_1","title":"Department scope","text":"<ol> <li>Locate the ConfigMap in the Run:ai namespace (runai)</li> <li>To authorize Run:ai to use the ConfigMap, label it: <code>run.ai/department: \"&lt;department-id&gt;\"</code> </li> <li> <p>The ConfigMap must have a label of <code>run.ai/resource: &lt;resource-name&gt;</code></p> <p>The ConfigMap is now displayed for that scope in the list of existing ConfigMaps.</p> </li> </ol>"},{"location":"admin/config/create-k8s-assets-in-advance/#project-scope_1","title":"Project scope","text":"<ol> <li>Locate the ConfigMap in the project\u2019s namespace</li> <li> <p>The ConfigMap must have a label of <code>run.ai/resource: &lt;resource-name&gt;</code></p> <p>The ConfigMap is now displayed for that scope in the list of existing ConfigMaps.</p> </li> </ol>"},{"location":"admin/config/default-scheduler/","title":"Setting Run:ai as the default scheduler per Namespace (Project)","text":""},{"location":"admin/config/default-scheduler/#introduction","title":"Introduction","text":"<p>Kubernetes has a default scheduler that makes decisions on where to place Kubernetes Pods. Run:ai has implemented a different scheduler called the <code>runai-scheduler</code>. By default, Run:ai uses its own scheduler</p> <p>You can decide to use the Run:ai scheduler for other, non-Run:ai, workloads by adding the following to the workload's YAML file:</p> <pre><code>schedulerName: runai-scheduler\n</code></pre>"},{"location":"admin/config/default-scheduler/#making-runai-the-default-scheduler","title":"Making Run:ai the default scheduler","text":"<p>There may be cases where you cannot change the YAML file but still want to use the Run:ai Scheduler to schedule those workloads. </p> <p>For such cases, another option is to configure the Run:ai Scheduler as the default scheduler for a specific namespace. This will now make any workload type that is submitted to that namespace (equivalent to a Run:ai Project) use the Run:ai scheduler. </p> <p>To configure this, add the following annotation to the namespace itself:</p> <p><code>runai/enforce-scheduler-name: true</code></p>"},{"location":"admin/config/default-scheduler/#example","title":"Example","text":"<p>To annotate a project named <code>proj-a</code>, use the following command:</p> <pre><code>kubectl annotate ns runai-proj-a runai/enforce-scheduler-name=true\n</code></pre> <p>Verify the namespace in YAML format to see the annotation:</p> <pre><code>kubectl get ns runai-proj-a -o yaml\n</code></pre> <p>Output: </p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  annotations:\n    runai/enforce-scheduler-name: \"true\"\n  creationTimestamp: \"2024-04-09T08:15:50Z\"\n  labels:\n    kubernetes.io/metadata.name: runai-proj-a\n    runai/namespace-version: v2\n    runai/queue: proj-a\n  name: runai-proj-a\n  resourceVersion: \"388336\"\n  uid: c53af666-7989-43df-9804-42bf8965ce83\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Active\n</code></pre>"},{"location":"admin/config/dr/","title":"Backup &amp; Restore","text":""},{"location":"admin/config/dr/#runai-cluster-restore","title":"Run:ai Cluster Restore","text":"<p>This article explains how to restore a Run:ai cluster on a different Kubernetes environment.</p> <p>In the event of a critical Kubernetes failure or alternatively, if you want to migrate a Run:ai cluster to a new Kubernetes environment, simply reinstall the Run:ai cluster. Once you have reinstalled and reconnected the cluster - projects, workloads and other cluster data is synced automatically.</p> <p>The restoration or back-up of Run:ai cluster Advanced features and Customized deployment configurations which are stored locally on the Kubernetes cluster is optional and they can be restored and backed-up separately.</p>"},{"location":"admin/config/dr/#backup","title":"Backup","text":"<p>As back-up of data is not required, the backup procedure is optional for advanced deployments, as explained above.</p>"},{"location":"admin/config/dr/#backup-cluster-configurations","title":"Backup cluster configurations","text":"<p>To backup Run:ai cluster configurations:</p> <ol> <li>Run the following command in your terminal: <pre><code>kubectl get runaiconfig runai -n runai -o yaml -o=jsonpath='{.spec}' &gt; runaiconfig_backup.yaml\n</code></pre></li> <li>Once the <code>runaiconfig_back.yaml</code> back-up file is created, save the file externally, so that it can be retrieved later.</li> </ol>"},{"location":"admin/config/dr/#restore","title":"Restore","text":"<p>Follow the steps below to restore the Run:ai cluster on a new Kubernetes environment.</p>"},{"location":"admin/config/dr/#prerequisites","title":"Prerequisites","text":"<p>Before restoring the Run:ai cluster, it is essential to validate that it is both disconnected and uninstalled.</p> <ol> <li>If the Kubernetes cluster is still available, uninstall the Run:ai cluster - make sure not to remove the cluster from the Control Plane  </li> <li>Navigate to the Cluster page in the Run:ai platform  </li> <li>Search for the cluster, and make sure its status is Disconnected</li> </ol>"},{"location":"admin/config/dr/#re-installing-runai-cluster","title":"Re-installing Run:ai Cluster","text":"<ol> <li>Follow the Run:ai cluster installation instructions and ensure all prerequisites are met  </li> <li>If you have a back-up of the cluster configurations, reload it once the installation is complete <pre><code>kubectl apply -f runaiconfig_backup.yaml -n runai\n</code></pre></li> <li>Navigate to the Cluster page in the Run:ai platform  </li> <li>Search for the cluster, and make sure its status is Connected</li> </ol>"},{"location":"admin/config/dr/#runai-control-plane","title":"Run:ai Control Plane","text":"<p>The self-hosted variant of Run:ai also installs the control-plane at the customer site. As such, it becomes the responsibility of the IT organization to verify that the system is configured for proper backup and learn how to recover the data when needed.</p>"},{"location":"admin/config/dr/#database-storage","title":"Database Storage","text":"<p>Run:ai uses an internal PostgreSQL database. The database is stored on a Kubernetes Persistent Volume (PV). You must provide a backup solution for the database. Some options:</p> <ul> <li>Backing up of PostgreSQL itself. Example: <code>kubectl -n runai-backend exec -it runai-backend-postgresql-0 -- env  PGPASSWORD=password pg_dump -U postgres   backend   &gt; cluster_name_db_backup.sql</code></li> <li>Backing up the persistent volume holding the database storage.</li> <li>Using third-party backup solutions.</li> </ul> <p>Run:ai also supports an external PostgreSQL database. For details see external PostgreSQL database</p>"},{"location":"admin/config/dr/#metrics-storage","title":"Metrics Storage","text":"<p>Run:ai stores metric history using Thanos. Thanos is configured to store data on a persistent volume. The recommendation is to back up the PV.</p>"},{"location":"admin/config/dr/#backing-up-control-plane-configuration","title":"Backing up Control-Plane Configuration","text":"<p>The installation of the Run:ai control plane can be configured. The configuration is provided as <code>--set</code> command in the helm installation. These changes will be preserved on upgrade, but will not be preserved on uninstall or upon damage to Kubernetes. Thus, it is best to back up these customizations. For a list of customizations used during the installation, run:</p> <p><code>helm get values runai-backend -n runai-backend</code></p>"},{"location":"admin/config/dr/#recovery","title":"Recovery","text":"<p>To recover Run:ai</p> <ul> <li>Re-create the Kubernetes/OpenShift cluster.</li> <li>Recover the persistent volumes for metrics and database.</li> <li>Re-install the Run:ai control plane. Use the additional configuration previously saved and connect to the restored PostgreSQL PV. Connect Prometheus to the stored metrics PV.</li> <li>Re-install the cluster. Add additional configuration post-install.  </li> <li>If the cluster is configured such that Projects do not create a namespace automatically, you will need to re-create namespaces and apply role bindings as discussed in Kubernetes or OpenShift.</li> </ul>"},{"location":"admin/config/ha/","title":"High Availability","text":"<p>The purpose of this document is to configure Run:ai such that it will continue to provide service even if parts of the system are down.</p> <p>A frequent fail scenario is a physical node in the system becoming non-responsive due to physical problems or lack of resources. In such a case, Kubernetes will attempt to relocate the running pods, but the process may take time, during which Run:ai will be down.</p> <p>A different scenario is a high transaction load, leading to system overload. To address such a scenario, please review the article: scaling the Run:ai system.</p>"},{"location":"admin/config/ha/#runai-control-plane","title":"Run:ai Control Plane","text":""},{"location":"admin/config/ha/#runai-system-workers","title":"Run:ai system workers","text":"<p>The Run:ai control plane allows the optional gathering of Run:ai pods into specific nodes. When this feature is used, it is important to set more than one node as a Run:ai system worker. Otherwise, the horizontal scaling described below will not span multiple nodes, and the system will remain with a single point of failure.  </p>"},{"location":"admin/config/ha/#horizontal-scalability-of-runai-services","title":"Horizontal Scalability of Run:ai services","text":"<p>Horizontal scalability is about instructing the system to create more pods to dynamically scale according to incoming load and downsize when the load subsides.</p> <p>The Run:ai control plane is running on a single Kubernetes namespace named <code>runai-backend</code>. The namespace contains various Kubernetes Deployments and StatefulSets. Each of these services can be scaled horizontally.</p>"},{"location":"admin/config/ha/#deployments","title":"Deployments","text":"<p>Each of the Run:ai deployments can be set to scale up, by adding a helm settings on install/upgrade. E.g. <code>--set frontend.autoscaling.enabled=true</code>. For a full list of settings, please contact Run:ai customer support.</p>"},{"location":"admin/config/ha/#statefulsets","title":"StatefulSets","text":"<p>Run:ai uses three third parties which are managed as Kubernetes StatefulSets:</p> <ul> <li>Keycloak\u2014Stores the Run:ai authentication configuration as well as user identities. To scale Keycloak, use the Run:ai control-plane helm flag <code>--set keycloakx.autoscaling.enabled=true</code>. By default, Keycloak sets a minimum of 3 pods and will scale to more on transaction load.</li> <li>PostgreSQL\u2014It is not possible to configure an internal PostgreSQL to scale horizontally. If this is of importance, please contact Customer Support to understand how to connect Run:ai to an external PostgreSQL service which can be configured for high availability.</li> <li>Thanos\u2014To enable Thanos autoscaling, use the following Run:ai control-plane helm flags:</li> </ul> <pre><code>--set thanos.query.autoscaling.enabled=true  \n--set thanos.query.autoscaling.maxReplicas=2\n--set thanos.query.autoscaling.minReplicas=2 \n</code></pre>"},{"location":"admin/config/ha/#runai-cluster","title":"Run:ai Cluster","text":""},{"location":"admin/config/ha/#runai-system-workers_1","title":"Run:ai system workers","text":"<p>The Run:ai cluster allows the mandatory gathering of Run:ai pods into specific nodes. When this feature is used, it is important to set more than one node as a Run:ai system worker. Otherwise, the horizontal scaling described below may not span multiple nodes, and the system will remain with a single point of failure.  </p>"},{"location":"admin/config/ha/#prometheus","title":"Prometheus","text":"<p>The default Prometheus installation uses a single pod replica. If the node running the pod is unresponsive, metrics will not be scraped from the cluster and will not be sent to the Run:ai control-plane.</p> <p>Prometheus supports high availability by allowing to run multiple instances. The tradeoff of this approach is that all instances will scrape and send the same data. The Run:ai control plane will identify duplicate metric series and ignore them. This approach will thus increase network, CPU and memory consumption.</p> <p>To change the number of Prometheus instances, edit the <code>runaiconfig</code> as described under customizing the Run:ai cluster. Under <code>prometheus.spec</code>, set <code>replicas</code> to 2.</p>"},{"location":"admin/config/large-clusters/","title":"Scaling the Run:ai system","text":"<p>The purpose of this document is to provide information on how to scale the Run:ai cluster and the Run:ai control-plane to withstand large transaction loads</p>"},{"location":"admin/config/large-clusters/#scaling-the-runai-control-plane","title":"Scaling the Run:ai Control Plane","text":"<p>The Control plane deployments which may encounter load are:</p> Name Kubernetes Deployment name Purpose Backend runai-backend-backend Main control-plane service Frontend runai-backend-frontend Serving of the Run:ai console Grafana runai-backend-grafana Serving of the Run:ai metrics inside the Run:ai console <p>To increase the number of replicas, run:</p> <p>To increase the number of replicas, use the following Run:ai control-plane helm flags</p> <pre><code>--set backend.autoscaling.enabled=true \n--set frontend.autoscaling.enabled=true\n--set grafana.autoscaling.enabled=true --set grafana.autoscaling.minReplicas=2\n</code></pre> <p>Important</p> <p>If you have chosen to mark some of the nodes as Run:ai System Workers, the new replicas will attempt to use these nodes first. Thus, for high availability purposes, you will want to mark more than one node as a Run:ai System Worker.  </p>"},{"location":"admin/config/large-clusters/#thanos","title":"Thanos","text":"<p>Thanos is the 3rd party used by Run:ai to store metrics Under a significant user load, we would also need to increase resources for the Thanos query function. Use the following Run:ai control-plane helm flags:</p> <pre><code>--set thanos.query.resources.limits.memory=3G\n--set thanos.query.resources.requests.memory=3G\n--set thanos.query.resources.limits.cpu=1\n--set thanos.query.resources.requests.cpu=1\n\n--set thanos.receive.resources.limits.memory=6G \n--set thanos.receive.resources.requests.memory=6G\n--set thanos.receive.resources.limits.cpu=1 \n--set thanos.receive.resources.requests.cpu=1\n</code></pre>"},{"location":"admin/config/large-clusters/#scaling-the-runai-cluster","title":"Scaling the Run:ai Cluster","text":""},{"location":"admin/config/large-clusters/#cpu-memory-resources","title":"CPU &amp; Memory Resources","text":"<p>Under Kubernetes, each of the Run:ai containers, has default resource requirements that reflect an average customer load. With significantly larger cluster loads, certain Run:ai services will require more CPU and memory resources. Run:ai now supports the ability to configure these resources and to do so for each Run:ai service group separately.</p>"},{"location":"admin/config/large-clusters/#service-groups","title":"Service Groups","text":"<p>Run:ai supports setting requests and limits configurations for CPU and memory for Run:ai containers. The configuration is set per service group. Each service group reflects a certain load type:</p> Service Group Description Run:ai containers SchedulingServices Containers associated with the Run:ai scheduler Scheduler, StatusUpdater, MetricsExporter, PodGrouper, PodGroupAssigner, Binder SyncServices Containers associated with syncing updates between the Run:ai cluster and the Run:ai control plane Agent, ClusterSync, AssetsSync WorkloadServices Containers associated with submitting Run:ai Workloads WorkloadController, JobController"},{"location":"admin/config/large-clusters/#configuration-steps","title":"Configuration Steps","text":"<p>To configure resource requirements for a group of services, update the RunaiConfig. Set the <code>spec.global.&lt;service-group&gt;.</code> resources section. The following example shows the configuration of scheduling services resource requirements:</p> <pre><code>apiVersion: run.ai/v1\nkind: RunaiConfig\nmetadata:\nspec:\n global:\n   schedulingServices:\n     resources:\n       limits:\n         cpu: 1000m\n         memory: 1Gi\n       requests:\n         cpu: 100m\n         memory: 512Mi\n</code></pre> <p>Use <code>syncServices</code> and <code>workloadServices</code> for the other two service groups.</p>"},{"location":"admin/config/large-clusters/#recommended-resource-specifications-for-large-clusters","title":"Recommended Resource Specifications For Large Clusters","text":"<p>In large clusters (100 nodes or 1500 GPUs or more), we recommend the following configuration for SchedulingServices and SyncServices groups:</p> <pre><code>resources:\n requests:\n   cpu: 1\n   memory: 1Gi\n limits:\n   cpu: 2\n   memory: 2Gi\n</code></pre>"},{"location":"admin/config/large-clusters/#sending-metrics","title":"Sending Metrics","text":"<p>Run:ai uses Prometheus to scrape metrics from the Run:ai cluster and to send them to the Run:ai control plane. The number of metrics is a function of the number of Nodes, Jobs and Projects which the system contains. When reaching hundreds of Nodes and Projects, the system will be sending large quantities of metrics which, in turn, will create a strain on the network as well as the receiving side in the control plane (SaaS or self-hosted).</p> <p>To reduce this strain, we suggest to configure Prometheus to send information in larger bulks and reduce the number of network connections:</p> <ul> <li>Edit the <code>runaiconfig</code> as described under customizing the cluster.</li> <li>Under <code>prometheus.remoteWrite</code> add the following:</li> </ul> <pre><code>queueConfig:\n  capacity: 5000\n  maxSamplesPerSend: 1000\n  maxShards: 100\n</code></pre> <p>This article provides additional details and insight.</p> <p>Also, note that this configuration enlarges the Prometheus queues and thus increases the required memory. It is hence suggested to reduce the metrics retention period as described here</p>"},{"location":"admin/config/limit-to-node-group/","title":"Group Nodes","text":""},{"location":"admin/config/limit-to-node-group/#why","title":"Why?","text":"<p>In some business scenarios, you may want to direct the Run:ai scheduler to schedule a Workload to a specific node or a node group. For example, in some academic institutions, Hardware is bought using a specific grant and thus \"belongs\" to a specific research group. Another example is an inference workload that is optimized to a specific GPU type and must have dedicated resources reserved to ensure enough capacity.</p> <p>Run:ai provides two methods to designate, and group, specific resources:</p> <ul> <li>Node Pools: Run:ai allows administrators to group specific nodes into a node pool. A node pool is a group of nodes identified by a given name (node pool name) and grouped by any label (key and value combination). The label can be chosen by the administrator or can be an existing, pre-set, label (such as an NVIDIA GPU type label).</li> <li>Node Affinity: Run:ai allows this \"taint\" by labeling a node, or a set of nodes and then during scheduling, using the flag <code>--node-type &lt;label&gt;</code> to force this allocation.</li> </ul> <p>Important</p> <p>One can set and use both node pool and node affinity combined as a prerequisite to the scheduler, for example, if a researcher wants to use a T4 node with an Infiniband card - he or she can use a node pool of T4 and from that group, choose only the nodes with Infiniband card (node-type = infiniband).</p> <p>There is a tradeoff in place when allowing Researchers to designate specific nodes. Overuse of this feature limits the scheduler in finding an optimal resource and thus reduces overall cluster utilization.</p>"},{"location":"admin/config/limit-to-node-group/#configuring-node-groups","title":"Configuring Node Groups","text":"<p>To configure a node pool:</p> <ul> <li>Find the label key &amp; value you want to use for Run:ai to create the node pool.</li> <li>Check that the nodes you want to group as a pool have a unique label to use, otherwise you should mark those nodes with your own uniquely identifiable label.</li> <li>Get the names of the nodes you want Run:ai to group together. To get a list of nodes, run:</li> </ul> <pre><code>kubectl get nodes\nKubectl get nodes --show-labels\n</code></pre> <ul> <li>If you chose to set your own label, run the following:</li> </ul> <pre><code>kubectl label node &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;\n</code></pre> <p>The same value can be set to a single node or multiple nodes. Node Pool can only use one label (key &amp; value) at a time.</p> <ul> <li>To create a node pool use the create node pool Run:ai API.</li> </ul> <p>To configure a node affinity:</p> <ul> <li>Get the names of the nodes where you want to limit Run:ai. To get a list of nodes, run:</li> </ul> <pre><code>kubectl get nodes\n</code></pre> <ul> <li>For each node run the following:</li> </ul> <pre><code>kubectl label node &lt;node-name&gt; run.ai/type=&lt;label&gt;\n</code></pre> <p>The same value can be set to a single node, or for multiple nodes. A node can only be set with a single value.</p>"},{"location":"admin/config/limit-to-node-group/#using-node-groups-via-the-cli","title":"Using Node Groups via the CLI","text":"<p>To use Run:ai node pool with a workload, use Run:ai CLI command \u2018node-pool\u2019: </p> <pre><code>runai submit job1 ... --node-pools \"my-pool\" ...\n</code></pre> <p>To use multiple node pools with a workload, use the Run:ai CLI command:</p> <pre><code>runai submit job1 ... --node-pools \"my-pool my-pool2 my-pool3\" ...\n</code></pre> <p>With multiple node pools, the researcher creates a list of prioritized node pools and lets the scheduler try and choose from any of the node pools in the list, according to the given priority. </p> <p>To use node affinity, use the node type label with the <code>--node-type</code> flag:</p> <pre><code>runai submit job1 ... --node-type \"my-nodes\"\n</code></pre> <p>A researcher may combine the two flags to select both a node pool and a specific set of nodes out of that node pool (e.g. gpu-type=t4 and node-type=infiniband):</p> <pre><code>runai submit job1 ... --node-pool-name \u201cmy pool\u201d --node-type \"my-nodes\"\n</code></pre> <p>Note</p> <p>When submitting a workload, if you choose a node pool label and a node affinity (node type) label which does not intersect, the Run:ai scheduler will not be able to schedule that workload as it represents an empty nodes group.</p> <p>See the runai submit documentation for further information.</p>"},{"location":"admin/config/limit-to-node-group/#assigning-node-groups-to-a-project","title":"Assigning Node Groups to a Project","text":"<p>Node Pools are automatically assigned to all Projects and Departments with zero resource allocation as default. Allocating resources to a node pool can be done for each Project and Department. Submitting a workload to a node pool that has zero allocation for a specific project (or department) results in that workload running as an over-quota workload.</p> <p>To assign and configure specific node affinity groups or node pools to a Project see working with Projects.</p> <p>When the command-line interface flag is used in conjunction with Project-based affinity, the flag is used to refine the list of allowable node groups set in the Project.</p>"},{"location":"admin/config/node-affinity-with-cloud-node-pools/","title":"Node affinity with cloud node pools","text":"<p>Run:ai allows for node affinity. Node affinity is the ability to assign a Project to run on specific nodes. To use the node affinity feature, You will need to label the target nodes with the label  <code>run.ai/node-type</code>. Most cloud clusters allow configuring node labels for the node pools in the cluster. This guide shows how to apply this configuration to different cloud providers.</p> <p>To make the node affinity work with node pools on various cloud providers, we need to make sure the node pools are configured with the appropriate Kubernetes label (<code>run.ai/type=&lt;TYPE_VALUE&gt;</code>).</p>"},{"location":"admin/config/node-affinity-with-cloud-node-pools/#setting-node-labels-while-creating-a-new-cluster","title":"Setting node labels while creating a new cluster","text":"<p>You can configure node-pool labels at cluster creation time</p> GKEAKSEKS <ul> <li>At the first creation screen, you will see a menu on the left side named <code>node-pools</code>.</li> <li>Expand the node pool you want to label.</li> <li>Click on <code>Metadata</code>.</li> <li>Near the bottom, you will find the Kubernetes <code>label</code> section. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>When creating AKS cluster at the node-pools page click on create new node-pool.</li> <li>Go to the <code>labels</code> section and add key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>Create a regular EKS cluster.</li> <li>Click on <code>compute</code>.</li> <li>Click on <code>Add node group</code>.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul>"},{"location":"admin/config/node-affinity-with-cloud-node-pools/#setting-node-labels-for-a-new-node-pool","title":"Setting node labels for a new node pool","text":"GKEAKSEKS <ul> <li>At the node pool creation screen, go to the <code>metadata</code> section.</li> <li>Near the bottom, you will find the Kubernetes <code>label</code> section. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>Go to your AKS page at Azure.</li> <li>On the left menu click the <code>node-pools</code> button.</li> <li>Click on <code>Add Node Pool</code>.</li> <li>In the new Node Pool page go to <code>Optional settings</code>.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>Go to <code>Add node group</code> screen.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul>"},{"location":"admin/config/node-affinity-with-cloud-node-pools/#editing-node-labels-for-an-existing-node-pool","title":"Editing node labels for an existing node pool","text":"GKEAKSEKS <ul> <li>Go to the <code>Google Kubernetes Engine</code> page in the Google Cloud console.</li> <li>Go to <code>Google Kubernetes Engine</code>.</li> <li>In the cluster list, click the name of the cluster you want to modify.</li> <li>Click the <code>Nodes</code> tab</li> <li>Under <code>Node Pools</code>, click the name of the node pool you want to modify, then click <code>Edit</code>.</li> <li>Near the bottom, you will find the Kubernetes <code>label</code> section. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <p>To update an existing node pool label you must use the azure cli. Run the following command:</p> <pre><code>az aks nodepool update \\\n    --resource-group [RESOURCE GROUP] \\\n    --cluster-name [CLUSTER NAME] \\\n    --name labelnp \\\n    --labels run.ai/type=[TYPE_VALUE] \\\n    --no-wait\n</code></pre> <ul> <li>Go to the <code>node group</code> page and click on <code>Edit</code>.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul>"},{"location":"admin/config/node-roles/","title":"Node roles","text":"<p>This article explains how to designate specific node roles in a Kubernetes cluster to ensure optimal performance and reliability in production deployments.</p> <p>For optimal performance in production clusters, it is essential to avoid extensive CPU usage on GPU nodes where possible. This can be done by ensuring the following:</p> <ul> <li>Run:ai system-level services run on dedicated CPU-only nodes.</li> <li>Workloads that do not request GPU resources (e.g. Machine Learning jobs) are executed on CPU-only nodes.</li> </ul> <p>The Run:ai cluster applies Kubernetes Node Affinity using node labels to manage scheduling for cluster services (system) and DaemonSets (worker).</p>"},{"location":"admin/config/node-roles/#prerequisites","title":"Prerequisites","text":"<p>To perform these tasks, make sure to install the Run:ai Administrator CLI.</p>"},{"location":"admin/config/node-roles/#configure-node-roles","title":"Configure Node Roles","text":"<p>The following node roles can be configured on the cluster:</p> <ul> <li>System node: Reserved for Run:ai system-level services.</li> <li>GPU Worker node: Dedicated for GPU-based workloads.</li> <li>CPU Worker node: Used for CPU-only workloads.</li> </ul>"},{"location":"admin/config/node-roles/#system-nodes","title":"System nodes","text":"<p>Run:ai system nodes run system-level services required to operate. This can be done via the Run:ai Administrator CLI.</p> <p>Recommendation</p> <p>To ensure high availability and prevent a single point of failure, it is recommended to configure at least three system nodes in your cluster.</p> <p>To set a system role for a node in your Kubernetes cluster, follow these steps:</p> <ol> <li>Run the <code>kubectl get nodes</code> command to list all the nodes in your cluster and identify the name of the node you want to modify.</li> <li>Run one of the following commands to set or remove a node\u2019s role:     <pre><code>runai-adm set node-role --runai-system-worker &lt;node-name&gt;\nrunai-adm remove node-role --runai-system-worker &lt;node-name&gt;\n</code></pre></li> </ol> <p>The <code>runai-adm</code> CLI will label the node and set relevant cluster configurations.</p> <p>Warning</p> <p>Do not assign a system node role to the Kubernetes master node. This may disrupt Kubernetes functionality, particularly if the Kubernetes API Server is configured to use port 443 instead of the default 6443.</p>"},{"location":"admin/config/node-roles/#worker-nodes","title":"Worker nodes","text":"<p>Run:ai worker nodes run user-submitted workloads and system-level DeamonSets required to operate. This can be managed via the Run:ai Administrator CLI, or Kubectl. </p>"},{"location":"admin/config/node-roles/#runai-administrator-cli","title":"Run:ai Administrator CLI","text":"<p>To set worker role for a node in your Kubernetes cluster via Run:ai Administrator CLI, follow these steps:</p> <ol> <li>Use the <code>kubectl get nodes</code> command to list all the nodes in your cluster and identify the name of the node you want to modify.</li> <li>Run one of the following commands to set or remove a node\u2019s role:    <pre><code> runai-adm set node-role [--gpu-worker | --cpu-worker] &lt;node-name&gt;\n runai-adm remove node-role [--gpu-worker | --cpu-worker] &lt;node-name&gt;\n</code></pre></li> </ol> <p>The <code>runai-adm</code> CLI will label the node and set relevant cluster configurations.</p> <p>Tip</p> <p>Use the --all flag to set or remove a role to all nodes.</p>"},{"location":"admin/config/node-roles/#kubectl","title":"Kubectl","text":"<p>To set a worker role for a node in your Kubernetes cluster using Kubectl, follow these steps:</p> <ol> <li>Validate the <code>global.nodeAffinity.restrictScheduling</code> is set to true in the cluster\u2019s Configurations.</li> <li>Use the <code>kubectl get nodes</code> command to list all the nodes in your cluster and identify the name of the node you want to modify.</li> <li>Run one of the following commands to label the node with its role:    <pre><code>kubectl label nodes &lt;node-name&gt; [node-role.kubernetes.io/runai-gpu-worker=true | node-role.kubernetes.io/runai-cpu-worker=true]\nkubectl label nodes &lt;node-name&gt; [node-role.kubernetes.io/runai-gpu-worker=false | node-role.kubernetes.io/runai-cpu-worker=false]\n</code></pre></li> </ol>"},{"location":"admin/config/notifications/","title":"Notifications System","text":""},{"location":"admin/config/notifications/#email-notifications-for-data-scientists","title":"Email Notifications for Data Scientists","text":"<p>Managing numerous data science workloads requires monitoring various stages, including submission, scheduling, initialization, execution, and completion. Additionally, handling suspensions and failures is crucial for ensuring timely workload completion. Email Notifications address this need by sending alerts for critical workload life cycle changes. This empowers data scientists to take necessary actions and prevent delays.</p>"},{"location":"admin/config/notifications/#setting-up-email-notifications","title":"Setting Up Email Notifications","text":"<p>Important</p> <p>The system administrator needs to enable and setup email notifications so that users are kept informed about different system statuses.</p> <p>To enable email notifications for the system:</p> <ol> <li> <p>Press Tools &amp; Settings, then select Notifications.</p> <p>Note</p> <p>For SaaS deployments, use the Enable email notifications toggle.</p> </li> <li> <p>In the SMTP Host field, enter the SMTP server address and in the SMTP port field the port number.</p> </li> <li>Select an Authentication type Plain or Login. Enter a username and password to be used for authentication.</li> <li>Enter the From email address and the Display name.</li> <li>Press Verify to ensure that the email configuration is working.</li> <li>Press Save when complete.</li> </ol>"},{"location":"admin/config/notifications/#system-notifications","title":"System Notifications","text":"<p>Administrators can set system wide notifications for all the users in order to alert them of important information. System notifications allows administrators the ability to update users with events that may be occurring within the Run:ai platform. The system notification will appear at each login or after the message has changed for users who are already logged in.</p> <p>To configure system notifications:</p> <ol> <li>Press Tools &amp; Settings, then select Notifications.</li> <li>In the System notification pane, press +MESSAGE.</li> <li>Enter your message in the text box. Use the formatting tool bar to add special formats to your message text.</li> <li>Enable the \"Don't show this again\" option to allow users to opt out of seeing the message multiple times.</li> <li>When complete, press Save &amp; Publish.</li> </ol>"},{"location":"admin/config/org-cert/","title":"Working with a Local Certificate Authority","text":"<p>Run:ai can be installed in an isolated network. In this air-gapped configuration, the organization will not be using an established root certificate authority. Instead, the organization creates a local certificate which serves as the root certificate for the organization. The certificate is installed in all browsers within the organization. </p> <p>In the context of Run:ai, the cluster and control-plane need to be aware of this certificate for consumers to be able to connect to the system.</p>"},{"location":"admin/config/org-cert/#preparation","title":"Preparation","text":"<p>You will need to have the public key of the local certificate authority. </p>"},{"location":"admin/config/org-cert/#control-plane-installation","title":"Control-Plane Installation","text":"<ul> <li>Create the <code>runai-backend</code> namespace if it does not exist. </li> <li> <p>Add the public key to the <code>runai-backend</code> namespace: <pre><code>kubectl -n runai-backend create secret generic runai-ca-cert \\ \n    --from-file=runai-ca.pem=&lt;ca_bundle_path&gt;\n</code></pre></p> </li> <li> <p>As part of the installation instructions, you need to create a secret for runai-backend-tls. Use the local certificate authority instead.</p> </li> <li>Install the control plane, add the following flag to the helm command <code>--set global.customCA.enabled=true</code></li> </ul>"},{"location":"admin/config/org-cert/#cluster-installation","title":"Cluster Installation","text":"<ul> <li>Create the <code>runai</code> namespace if it does not exist. </li> <li>Add the public key to the <code>runai</code> namespace: <pre><code>kubectl -n runai create secret generic runai-ca-cert \\\n    --from-file=runai-ca.pem=&lt;ca_bundle_path&gt;\n</code></pre></li> <li>In case you're using Openshift, add the public key to the <code>openshift-monitoring</code> namespace: <pre><code>kubectl -n openshift-monitoring create secret generic runai-ca-cert \\\n    --from-file=runai-ca.pem=&lt;ca_bundle_path&gt;\n</code></pre></li> <li>Install the Run:ai operator, add the following flag to the helm command <code>--set global.customCA.enabled=true</code></li> </ul>"},{"location":"admin/config/overview/","title":"Run:ai Configuration Articles","text":"<p>This section provides a list of installation-related articles dealing with a wide range of subjects:</p> Article Purpose Designating Specific Role Nodes Set one or more designated Run:ai system nodes or limit Run:ai monitoring and scheduling to specific nodes in the cluster. Create and Troubleshoot Clusters Create new clusters, view properties and status, and troubleshoot cluster connectivity related issues. Set Default Scheduler Set the default scheduler for a specific namespace Review Kubernetes Access provided to Run:ai In Restrictive Kubernetes environments such as when using OpenShift, understand and control what Kubernetes roles are provided to Run:ai External access to Containers Understand the available options for Researchers to access containers from the outside Install the Run:ai Administrator Command-line Interface The Administrator command-line is useful in a variety of flows such as cluster upgrade, node setup etc. Set Node affinity with cloud node pools Set node affinity when using a cloud provider for your cluster Local Certificate Authority For self-hosted Run:ai environments, specifically air-gapped installation, setup a local certificate authority to allow customers to safely connect to Run:ai Backup &amp; Restore For self-hosted Run:ai environments, set up a scheduled backup of Run:ai data High Availability Configure Run:ai such that it will continue to provide service even if parts of the system are down. Scaling Scale the Run:ai cluster and the Run:ai control-plane to withstand large transaction loads Emails and system notification Configure e-mail notification"},{"location":"admin/config/secure-cluster/","title":"Secure your cluster","text":"<p>This article details the security considerations for deploying Run:ai. It is intended to help administrators and security officers understand the specific permissions required by Run:ai.</p>"},{"location":"admin/config/secure-cluster/#access-to-the-kubernetes-cluster","title":"Access to the Kubernetes cluster","text":"<p>Run:ai integrates with Kubernetes clusters and requires specific permissions to successfully operate. These are permissions are controlled with configuration flags that dictate how Run:ai interacts with cluster resources. Prior to installation, security teams can review the permissions and ensure it aligns with their organization\u2019s policies.</p>"},{"location":"admin/config/secure-cluster/#permissions-and-their-related-use-case","title":"Permissions and their related use-case","text":"<p>Run:ai provides various security-related permissions that can be customized to fit specific organizational needs. Below are brief descriptions of the key use cases for these customizations:</p> Permission Use case Automatic Namespace creation Controls whether Run:ai automatically creates Kubernetes namespaces when new projects are created. Useful in environments where namespace creation must be strictly managed. Automatic user assignment Decides if users are automatically assigned to projects within Run:ai. Helps manage user access more tightly in certain compliance-driven environments. Secret propagation Determines whether Run:ai should propagate secrets across the cluster. Relevant for organizations with specific security protocols for managing sensitive data. Disabling Kubernetes limit range Chooses whether to disable the Kubernetes Limit Range feature. May be adjusted in environments with specific resource management needs. <p>Note</p> <p>These security customizations allow organizations to tailor Run:ai to their specific needs. All changes should be modified cautiously and only when necessary to meet particular security, compliance or operational requirements.</p>"},{"location":"admin/config/secure-cluster/#secure-installation","title":"Secure installation","text":"<p>Many organizations enforce IT compliance rules for Kubernetes, with strict access control for installing and running workloads. OpenShift uses Security Context Constraints (SCC) for this purpose. Run:ai fully supports SCC, ensuring integration with OpenShift's security requirements.</p>"},{"location":"admin/config/secure-cluster/#security-vulnerabilities","title":"Security vulnerabilities","text":"<p>The platform is actively monitored for security vulnerabilities, with regular scans conducted to identify and address potential issues. Necessary fixes are applied to ensure that the software remains secure and resilient against emerging threats, providing a safe and reliable experience.</p>"},{"location":"admin/config/shared-storage/","title":"Shared Storage","text":"<p>Shared storage is a critical component in AI and machine learning workflows, particularly in scenarios involving distributed training and shared datasets. In AI and ML environments, data must be readily accessible across multiple nodes, especially when training large models or working with vast datasets. Shared storage enable seamless access to data, ensuring that all nodes in a distributed training setup can read and write to the same datasets simultaneously. This setup not only enhances efficiency but is also crucial for maintaining consistency and speed in high-performance computing environments.</p> <p>While Run:ai Platform supports a variety of remote data sources, such as Git and S3, it is often more efficient to keep data close to the compute resources. This proximity is typically achieved through the use of shared storage, accessible to multiple nodes in your Kubernetes cluster.</p>"},{"location":"admin/config/shared-storage/#shared-storage","title":"Shared storage","text":"<p>When implementing shared storage in Kubernetes, there are two primary approaches:</p> <ul> <li>Utilizing the Kubernetes Storage Classes of your storage provider; or  </li> <li>Using a direct NFS (Network File System) mount</li> </ul> <p>Storage Classes being the recommended option.</p> <p>Run:ai Data Sources support both direct NFS mount and Kubernetes Storage Classes.</p>"},{"location":"admin/config/shared-storage/#kubernetes-storage-classes","title":"Kubernetes storage classes","text":"<p>Storage classes in Kubernetes defines how storage is provisioned and managed. This allows you to select storage types optimized for AI workloads. For example, you can choose storage with high IOPS (Input/Output Operations Per Second) for rapid data access during intensive training sessions, or tiered storage options to balance cost and performance-based on your organization\u2019s requirements. This approach supports dynamic provisioning, enabling storage to be allocated on-demand as required by your applications.</p> <p>Run:ai data sources such as Persistent Volume Claims (PVC) and Data Volumes leverage storage class to manage and allocate storage efficiently. This ensures that the most suitable storage option is always accessible, contributing to the efficiency and performance of AI workloads.</p> <p>Note</p> <p>Run:ai lists all available storage classes in the Kubernetes cluster, making it easy for users to select the appropriate storage. Additionally, policies can be set to restrict or enforce the use of specific storage classes, to helpl maintain compliance with organizational standards and optimize resource utilization.</p> Kubernetes 1.23 (old) <p>When using Kubernetes 1.23, Data Source of PVC type does not work using a Storage Class with the property <code>volumeBindingMode</code> equals to <code>WaitForFirstConsumer</code></p>"},{"location":"admin/config/shared-storage/#direct-nfs-mount","title":"Direct NFS mount","text":"<p>Direct NFS allows you to mount a shared file system directly across multiple nodes in your Kubernetes cluster. This method provides a straightforward way to share data among nodes and is often used for simple setups or when a dedicated NFS server is available.</p> <p>However, using NFS can present challenges related to security and control. Direct NFS setups might lack the fine-grained control and security features available with storage class.</p>"},{"location":"admin/config/workload-ownership-protection/","title":"Workload Deletion Protection","text":""},{"location":"admin/config/workload-ownership-protection/#workload-deletion-protection","title":"Workload Deletion Protection","text":"<p>Workload deletion protection in Run:ai ensures that only users who created a workload can delete or modify them. This feature is designed to safeguard important jobs and configurations from accidental or unauthorized modifications by users who did not originally create the workload.</p> <p>By enforcing ownership rules, Run:ai helps maintain the integrity and security of your machine learning operations. This additional layer of security ensures that only users with the appropriate permissions can delete and suspend workloads.</p> <p>The protection feature is implemented at the cluster level.</p> <p>To enable deletion protection run the following command:</p> <pre><code>kubectl patch -n runai runaiconfigs.run.ai/runai --type='merge' --patch '{\"spec\":{\"global\":{\"enableWorkloadOwnershipProtection\":true}}}'\n</code></pre>"},{"location":"admin/maintenance/alert-monitoring/","title":"System Monitoring","text":"<p>This article explains how to configure Run:ai to generate health alerts and to connect these alerts to alert-management systems within your organization. Alerts are generated for Run:ai clusters.</p>"},{"location":"admin/maintenance/alert-monitoring/#alert-infrastructure","title":"Alert infrastructure","text":"<p>Run:ai uses Prometheus for externalizing metrics and providing visibility to end-users. The Run:ai Cluster installation includes Prometheus or can connect to an existing Prometheus instance used in your organization. The alerts are based on the Prometheus AlertManager. Once installed, it is enabled by default.</p> <p>This document explains how to:</p> <ul> <li>Configure alert destinations - triggered alerts send data to specified destinations  </li> <li>Understand the out-of-the-box cluster alerts, provided by Run:ai  </li> <li>Add additional custom alerts</li> </ul>"},{"location":"admin/maintenance/alert-monitoring/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster with the necessary permissions  </li> <li>Up and running Run:ai environment, including Prometheus Operator  </li> <li>kubectl command-line tool installed and configured to interact with the cluster</li> </ul>"},{"location":"admin/maintenance/alert-monitoring/#set-up","title":"Set-up","text":"<p>Use the steps below to set up monitoring alerts.</p>"},{"location":"admin/maintenance/alert-monitoring/#validating-prometheus-operator-installed","title":"Validating Prometheus operator installed","text":"<ol> <li>Verify that the Prometheus Operator Deployment is running    Copy the following command and paste it in your terminal, where you have access to the Kubernetes cluster: <code>kubectl get deployment kube-prometheus-stack-operator -n monitoring</code>    In your terminal, you can see an output indicating the deployment's status, including the number of replicas and their current state.  </li> <li>Verify that Prometheus instances are running    Copy the following command and paste it in your terminal: <code>kubectl get prometheus -n runai</code>    You can see the Prometheus instance(s) listed along with their status.</li> </ol>"},{"location":"admin/maintenance/alert-monitoring/#enabling-prometheus-alertmanager","title":"Enabling Prometheus AlertManager","text":"<p>In each of the steps in this section, copy the content of the code snippet to a new YAML file (e.g., <code>step1.yaml</code>).</p> <ul> <li>Copy the following command to your terminal, to apply the YAML file to the cluster:</li> </ul> <p>kubectl apply -f step1.yaml Copy the following command to your terminal to create the AlertManager CustomResource, to enable AlertManager: </p> <pre><code>apiVersion: monitoring.coreos.com/v1  \nkind: Alertmanager  \nmetadata:  \n   name: runai  \n   namespace: runai  \nspec:  \n   replicas: 1  \n   alertmanagerConfigSelector:  \n      matchLabels:\n         alertmanagerConfig: runai \n</code></pre> <ul> <li>Copy the following command to your terminal to validate that the AlertManager instance has started: <code>kubectl get alertmanager -n runai</code> </li> <li>Copy the following command to your terminal to validate that the Prometheus operator has created a Service for AlertManager: <code>kubectl get svc alertmanager-operated -n runai</code></li> </ul>"},{"location":"admin/maintenance/alert-monitoring/#configuring-prometheus-to-send-alerts","title":"Configuring Prometheus to send alerts","text":"<ol> <li>Open the terminal on your local machine or another machine that has access to your Kubernetes cluster  </li> <li> <p>Copy and paste the following command in your terminal to edit the Prometheus configuration for the <code>runai</code> Namespace: <pre><code>kubectl edit prometheus runai -n runai\n</code></pre> This command opens the Prometheus configuration file in your default text editor (usually <code>vi</code> or <code>nano</code>).</p> </li> <li> <p>Copy and paste the following text to your terminal to change the configuration file: <pre><code>alerting:  \n   alertmanagers:  \n      - namespace: runai  \n        name: alertmanager-operated  \n        port: web\n</code></pre></p> </li> <li>Save the changes and exit the text editor.  </li> </ol> <p>Note</p> <p>To save changes using <code>vi</code>, type <code>:wq</code> and press Enter.   The changes are applied to the Prometheus configuration in the cluster.</p>"},{"location":"admin/maintenance/alert-monitoring/#alert-destinations","title":"Alert destinations","text":"<p>Set out below are the various alert destinations.</p>"},{"location":"admin/maintenance/alert-monitoring/#configuring-alertmanager-for-custom-email-alerts","title":"Configuring AlertManager for custom email alerts","text":"<p>In each step, copy the contents of the code snippets to a new file and apply it to the cluster using <code>kubectl apply -f</code>.</p> <p>Add your smtp password as a secret: </p> <pre><code>apiVersion: v1  \nkind: Secret  \nmetadata:  \n   name: alertmanager-smtp-password  \n   namespace: runai  \nstringData:\n   password: \"your_smtp_password\"\n</code></pre> <p>Replace the relevant smtp details with your own, then apply the <code>alertmanagerconfig</code> using <code>kubectl apply</code>.  </p> <pre><code> apiVersion: monitoring.coreos.com/v1alpha1  \n kind: AlertmanagerConfig  \n metadata:  \n   name: runai  \n   namespace: runai  \n labels:  \n    alertmanagerConfig: runai  \n spec:  \n    route:  \n       continue: true  \n       groupBy:   \n       - alertname\n\n       groupWait: 30s  \n       groupInterval: 5m  \n       repeatInterval: 1h\n\n    matchers:  \n    - matchType: =~  \n      name: alertname  \n      value: Runai.*\n\n    receiver: email\n\n receivers:  \n - name: 'email'  \n   emailConfigs:  \n   - to: '&lt;destination_email_address&gt;'  \n     from: '&lt;from_email_address&gt;'  \n     smarthost: 'smtp.gmail.com:587'  \n     authUsername: '&lt;smtp_server_user_name&gt;'  \n     authPassword:  \n       name: alertmanager-smtp-password\n         key: password  \n</code></pre> <p>Save and exit the editor. The configuration is automatically reloaded.</p>"},{"location":"admin/maintenance/alert-monitoring/#third-party-alert-destinations","title":"Third-party alert destinations","text":"<p>Prometheus AlertManager provides a structured way to connect to alert-management systems. There are built-in plugins for popular systems such as PagerDuty and OpsGenie, including a generic Webhook.</p>"},{"location":"admin/maintenance/alert-monitoring/#example-integrating-runai-with-a-webhook","title":"Example: Integrating Run:ai with a Webhook","text":"<ol> <li>Use webhook.site to get a unique URL.  </li> <li>Use the upgrade cluster instructions to modify the values file:     Edit the values file to add the following, and replace <code>&lt;WEB-HOOK-URL&gt;</code> with the URL from webhook.site.</li> </ol> <p><pre><code>codekube-prometheus-stack:  \n  ...  \n  alertmanager:  \n    enabled: true  \n    config:  \n      global:  \n        resolve_timeout: 5m  \n      receivers:  \n      - name: \"null\"  \n      - name: webhook-notifications  \n        webhook_configs:  \n          - url: &lt;WEB-HOOK-URL&gt;  \n            send_resolved: true  \n      route:  \n        group_by:  \n        - alertname  \n        group_interval: 5m  \n        group_wait: 30s  \n        receiver: 'null'  \n        repeat_interval: 10m  \n        routes:  \n        - receiver: webhook-notifications\n</code></pre> 3. Verify that you are receiving alerts on the webhook.site, in the left pane:  </p> <p></p>"},{"location":"admin/maintenance/alert-monitoring/#built-in-alerts","title":"Built-in alerts","text":"<p>A Run:ai cluster comes with several built-in alerts. Each alert notifies on a specific functionality of a Run:ai\u2019s entity. There is also a single, inclusive alert: <code>Run:ai Critical Problems</code>, which aggregates all component-based alerts into a single cluster health test.</p> <p>Runai agent cluster info push rate low</p> Meaning The <code>cluster-sync</code> Pod in the <code>runai</code> namespace might not be functioning properly Impact Possible impact - no info/partial info from the cluster is being synced back to the control-plane Severity Critical Diagnosis <code>kubectl get pod -n runai</code> to see if the <code>cluster-sync</code> pod is running Troubleshooting/Mitigation To diagnose issues with the <code>cluster-sync</code> pod, follow these steps: Paste the following command to your terminal, to receive detailed information about the <code>cluster-sync</code> deployment:<code>kubectl describe deployment cluster-sync -n runai</code> Check the Logs: Use the following command to view the logs of the <code>cluster-sync</code> deployment:<code>kubectl logs deployment/cluster-sync -n runai</code> Analyze the Logs and Pod Details: From the information provided by the logs and the deployment details, attempt to identify the reason why the <code>cluster-sync</code> pod is not functioning correctly Check Connectivity: Ensure there is a stable network connection between the cluster and the Run:ai Control Plane. A connectivity issue may be the root cause of the problem. Contact Support: If the network connection is stable and you are still unable to resolve the issue, contact Run:ai support for further assistance <p>Runai cluster sync handling rate low</p> <p>| Meaning | The <code>cluster-sync</code> Pod in the <code>runai</code> namespace might be functioning slowly |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | | :---- |:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Impact | Possible impact - info from the cluster is being synced back to the control-plane with a slow rate                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | | Severity | Warning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | | Diagnosis | <code>kubectl logs deployment/cluster-sync -n runai</code> to see if the <code>cluster-sync</code> pod is running properly                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | | Troubleshooting/Mitigation | To diagnose issues with the <code>cluster-sync</code> pod, follow these steps: Check the Logs: Use the following command to view the logs of the <code>cluster-sync</code> deployment:<code>kubectl logs deployment/cluster-sync -n runai</code> Analyze the Logs and Pod Details: From the information provided by the logs and the deployment details, attempt to identify the reason why the <code>cluster-sync</code> pod is not functioning correctly Check Connectivity: Ensure there is a stable network connection between the cluster and the Run:ai Control Plane. A connectivity issue may be the root cause of the problem. Contact Support: If the network connection is stable and you are still unable to resolve the issue, contact Run:ai support for further assistance |</p> <p>Runai agent pull rate low</p> Meaning The <code>runai-agent</code> pod may be too loaded, is slow in processing data (possible in very big clusters), or the <code>runai-agent</code> pod itself in the <code>runai</code> namespace may not be functioning properly. Impact Possible impact - no info/partial info from the control-plane is being synced in the cluster Severity Critical Diagnosis Run: <code>kubectl get pod -n runai</code> And see if the <code>runai-agent</code> pod is running. Troubleshooting/Mitigation To diagnose issues with the <code>runai-agent</code> pod, follow these steps: Describe the Deployment: Run the following command to get detailed information about the <code>runai-agent</code> deployment:<code>kubectl describe deployment runai-agent -n runai</code> Check the Logs: Use the following command to view the logs of the <code>runai-agent</code> deployment:<code>kubectl logs deployment/runai-agent -n runai</code> Analyze the Logs and Pod Details: From the information provided by the logs and the deployment details, attempt to identify the reason why the <code>runai-agent</code> pod is not functioning correctly. There may be a connectivity issue with the control plane. Check Connectivity: Ensure there is a stable network connection between the <code>runai-agent</code> and the control plane. A connectivity issue may be the root cause of the problem. Consider Cluster Load: If the <code>runai-agent</code> appears to be functioning properly but the cluster is very large and heavily loaded, it may take more time for the agent to process data from the control plane. Adjust Alert Threshold: If the cluster load is causing the alert to fire, you can adjust the threshold at which the alert triggers. The default value is 0.05. You can try changing it to a lower value (e.g., 0.045 or 0.04).To edit the value, paste the following in your terminal:<code>kubectl edit runaiconfig -n runai</code>In the editor, navigate to:spec:   prometheus:     agentPullPushRateMinForAlert: If the <code>agentPullPushRateMinForAlert</code> value does not exist, add it under <code>spec -&gt; prometheus</code> <p>Runai container memory usage critical</p> Meaning <code>Runai</code> container is using more than 90% of its Memory limit Impact The container might run out of memory and crash. Severity Critical Diagnosis Calculate the memory usage, this is performed by pasting the following to your terminal: <code>container_memory_usage_bytes{namespace=~\"runai|runai-backend\"}</code> Troubleshooting/Mitigation Add more memory resources to the container. If the issue persists, contact Run:ai <p>Runai container memory usage warning</p> Meaning Runai container is using more than 80% of its memory limit Impact The container might run out of memory and crash Severity Warning Diagnosis Calculate the memory usage, this can be done by pasting the following to your terminal: <code>container_memory_usage_bytes{namespace=~\"runai|runai-backend\"}</code> Troubleshooting/Mitigation Add more memory resources to the container. If the issue persists, contact Run:ai <p>Runai container restarting</p> Meaning <code>Runai</code> container has restarted more than twice in the last 10 min Impact The container might become unavailable and impact the Run:ai system Severity Warning Diagnosis To diagnose the issue and identify the problematic pods, paste this into your terminal: <code>kubectl get pods -n runai kubectl get pods -n runai-backend</code>One or more of the pods have a restart count &gt;= 2. Troubleshooting/Mitigation Paste this into your terminal:<code>kubectl logs -n NAMESPACE POD_NAME</code>Replace <code>NAMESPACE</code> and <code>POD_NAME</code> with the relevant pod information from the previous step. Check the logs for any standout issues and verify that the container has sufficient resources. If you need further assistance, contact Run:ai <p>Runai CPU usage warning</p> Meaning <code>runai</code> container is using more than 80% of its CPU limit Impact This might cause slowness in the operation of certain Run:ai features. Severity Warning Diagnosis Paste the following query to your terminal in order to calculate the CPU usage: <code>rate(container_cpu_usage_seconds_total{namespace=~\"runai|runai-backend\"}[2m])</code> Troubleshooting/Mitigation Add more CPU resources to the container. If the issue persists, please contact Run:ai. <p>Runai critical problem</p> Meaning One of the critical Run:ai alerts is currently active Impact Impact is based on the active alert Severity Critical Diagnosis Check Run:ai alerts in Prometheus to identify any active critical alerts <p>Runai daemonSet rollout stuck / Runai DaemonSet unavailable on nodes</p> Meaning There are currently 0 available pods for the <code>runai</code> daemonset on the relevant node Impact No fractional GPU workloads support Severity Critical Diagnosis Paste the following command to your terminal: <code>kubectl get daemonset -n runai-backend</code> In the result of this command, identify the daemonset(s) that don\u2019t have any running pods Troubleshooting/Mitigation Paste the following command to your terminal, where <code>daemonsetX</code> is the problematic daemonset from the pervious step: <code>kubectl describe daemonsetX -n runai</code> on the relevant deamonset(s) from the previous step. The next step is to look for the specific error which prevents it from creating pods. Possible reasons might be:Node Resource Constraints: The nodes in the cluster may lack sufficient resources (CPU, memory, etc.) to accommodate new pods from the daemonset. Node Selector or Affinity Rules: The daemonset may have node selector or affinity rules that are not matching with any nodes currently available in the cluster, thus preventing pod creation. <p>Runai deployment insufficient replicas / Runai deployment no available replicas /RunaiDeploymentUnavailableReplicas</p> Meaning <code>Runai</code> deployment has one or more unavailable pods Impact When this happens, there may be scale issues. Additionally, new versions cannot be deployed, potentially resulting in missing features. Severity Critical Diagnosis Paste the following commands to your terminal, in order to get the status of the deployments in the <code>runai</code> and <code>runai-backend</code> namespaces:<code>kubectl get deployment -n runai kubectl get deployment -n runai-backend</code>Identify any deployments that have missing pods. Look for discrepancies in the <code>DESIRED</code> and <code>AVAILABLE</code> columns. If the number of <code>AVAILABLE</code> pods is less than the <code>DESIRED</code> pods, it indicates that there are missing pods. Troubleshooting/Mitigation Paste the following commands to your terminal, to receive detailed information about the problematic deployment:<code>kubectl describe deployment &lt;DEPLOYMENT_NAME&gt; -n runai kubectl describe deployment &lt;DEPLOYMENT_NAME&gt; -n runai-backend</code> Paste the following commands to your terminal, to check the replicaset details associated with the deployment:<code>kubectl describe replicaset &lt;REPLICASET_NAME&gt; -n runai kubectl describe replicaset &lt;REPLICASET_NAME&gt; -n runai-backend</code> Paste the following commands to your terminal to retrieve the logs for the deployment to identify any errors or issues:<code>kubectl logs deployment/&lt;DEPLOYMENT_NAME&gt; -n runai kubectl logs deployment/&lt;DEPLOYMENT_NAME&gt; -n runai-backend</code> From the logs and the detailed information provided by the <code>describe</code> commands, analyze the reasons why the deployment is unable to create pods. Look for common issues such as: Resource constraints (CPU, memory) Misconfigured deployment settings or replicasets Node selector or affinity rules preventing pod schedulingIf the issue persists, contact Run:ai. <p>Runai project controller reconcile failure</p> Meaning The <code>project-controller</code> in <code>runai</code> namespace had errors while reconciling projects Impact Some projects might not be in the \u201cReady\u201d state. This means that they are not fully operational and may not have all the necessary components running or configured correctly. Severity Critical Diagnosis Retrieve the logs for the <code>project-controller</code> deployment by pasting the following command in your terminal:<code>kubectl logs deployment/project-controller -n runai</code> Carefully examine the logs for any errors or warning messages. These logs help you understand what might be going wrong with the project controller. Troubleshooting/Mitigation Once errors in the log have been identified, follow these steps to mitigate the issue: The error messages in the logs should provide detailed information about the problem. Read through them to understand the nature of the issue. If the logs indicate which project failed to reconcile, you can further investigate by checking the status of that specific project. Run the following command, replacing <code>&lt;PROJECT_NAME&gt;</code> with the name of the problematic project:<code>kubectl get project &lt;PROJECT_NAME&gt; -o yaml</code> Review the status section in the YAML output. This section describes the current state of the project and provide insights into what might be causing the failure.If the issue persists, contact Run:ai. <p>Runai StatefulSet insufficient replicas / Runai StatefulSet no available replicas</p> Meaning <code>Runai</code> statefulset has no available pods Impact Absence of Metrics Database Unavailability Severity Critical Diagnosis To diagnose the issue, follow these steps: Check the status of the stateful sets in the <code>runai-backend</code> namespace by running the following command:<code>kubectl get statefulset -n runai-backend</code> Identify any stateful sets that have no running pods. These are the ones that might be causing the problem. Troubleshooting/Mitigation Once you've identified the problematic stateful sets, follow these steps to mitigate the issue: Describe the stateful set to get detailed information on why it cannot create pods. Replace <code>X</code> with the name of the stateful set:<code>kubectl describe statefulset X -n runai-backend</code> Review the description output to understand the root cause of the issue. Look for events or error messages that explain why the pods are not being created. If you're unable to resolve the issue based on the information gathered, contact Run:ai support for further assistance."},{"location":"admin/maintenance/alert-monitoring/#adding-a-custom-alert","title":"Adding a custom alert","text":"<p>You can add additional alerts from Run:ai. Alerts are triggered by using the Prometheus query language with any Run:ai metric.</p> <p>To create an alert, follow these steps using Prometheus query language with Run:ai Metrics:</p> <ul> <li>Modify Values File: Use the upgrade cluster instructions to modify the values file.  </li> <li>Add Alert Structure: Incorporate alerts according to the structure outlined below. Replace placeholders <code>&lt;ALERT-NAME&gt;</code>, <code>&lt;ALERT-SUMMARY-TEXT&gt;</code>, <code>&lt;PROMQL-EXPRESSION&gt;</code>, <code>&lt;optional: duration s/m/h&gt;</code>, and <code>&lt;critical/warning&gt;</code> with appropriate values for your alert, as described below.</li> </ul> <p><pre><code>kube-prometheus-stack:  \n   additionalPrometheusRulesMap:  \n     custom-runai:  \n       groups:  \n       - name: custom-runai-rules  \n         rules:  \n         - alert: &lt;ALERT-NAME&gt;  \n           annotations:  \n             summary: &lt;ALERT-SUMMARY-TEXT&gt;  \n           expr:  &lt;PROMQL-EXPRESSION&gt;  \n           for: &lt;optional: duration s/m/h&gt;  \n           labels:  \n             severity: &lt;critical/warning&gt;\n</code></pre> * <code>&lt;ALERT-NAME&gt;</code>: Choose a descriptive name for your alert, such as <code>HighCPUUsage</code> or <code>LowMemory</code>. <code>&lt;ALERT-SUMMARY-TEXT&gt;</code>: Provide a brief summary of what the alert signifies, for example, <code>High CPU usage detected</code> or <code>Memory usage below threshold</code>. <code>&lt;PROMQL-EXPRESSION&gt;</code>: Construct a Prometheus query (PROMQL) that defines the conditions under which the alert should trigger. This query should evaluate to a boolean value (<code>1</code> for alert, <code>0</code> for no alert). <code>&lt;optional: duration s/m/h&gt;</code>: Optionally, specify a duration in seconds (<code>s</code>), minutes (<code>m</code>), or hours (<code>h</code>) that the alert condition should persist before triggering an alert. If not specified, the alert triggers as soon as the condition is met. <code>&lt;critical/warning&gt;</code>: Assign a severity level to the alert, indicating its importance. Choose between <code>critical</code> for severe issues requiring immediate attention, or <code>warning</code> for less critical issues that still need monitoring.</p> <p>You can find an example in the Prometheus documentation.</p>"},{"location":"admin/maintenance/audit-log/","title":"Audit Log","text":"<p>This article provides details about Run:ai\u2019s Audit log. The Run:ai control plane provides the audit log API and event history table in the Run:ai UI . Both reflect the same information regarding changes to business objects: clusters, projects and assets etc.</p>"},{"location":"admin/maintenance/audit-log/#events-history-table","title":"Events history table","text":"<p>The Events history table can be found under Event history in the Run:ai UI.</p> <p></p> <p>The Event history table consists of the following columns:</p> Column Description Subject The name of the subject Subject type The user or application assigned with the role Source IP The IP address of the subject Date &amp; time The exact timestamp at which the event occurred. Format dd/mm/yyyy for date and hh:mm am/pm for time. Event The type of the event. Possible values: Create, Update, Delete, Login Event ID Internal event ID, can be used for support purposes Status The outcome of the logged operation. Possible values: Succeeded, Failed Entity type The type of the logged business object. Entity name The name of logged business object. Entity ID The system's internal id of the logged business object. URL The endpoint or address that was accessed during the logged event. HTTP Method The HTTP operation method used for the request. Possible values include standard HTTP methods such as <code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, indicating what kind of action was performed on the specified URL."},{"location":"admin/maintenance/audit-log/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV or Download as JSON</li> </ul>"},{"location":"admin/maintenance/audit-log/#using-the-event-history-date-selector","title":"Using the event history date selector","text":"<p>The Event history table saves events for the last 90 days. However, the table itself presents up to the last 30 days of information due to the potentially very high number of operations that might be logged during this period.</p> <p></p> <p>To view older events, or to refine your search for more specific results or fewer results, use the time selector and change the period you search for. You can also refine your search by clicking and using ADD FILTER accordingly.</p>"},{"location":"admin/maintenance/audit-log/#using-api","title":"Using API","text":"<p>Go to the Audit log API reference to view the available actions. Since the amount of data is not trivial, the API is based on paging. It retrieves a specified number of items for each API call. You can get more data by using subsequent calls.</p>"},{"location":"admin/maintenance/audit-log/#limitations","title":"Limitations","text":"<ul> <li>User login auditing: User login events are not currently audited. This means, there is no record of when users log in or out of the system.  </li> <li>Workload submission auditing: Submissions of workloads are not audited. As a result, the system does not track or log details of workload submissions, such as timestamps or user activity.</li> </ul>"},{"location":"admin/maintenance/node-downtime/","title":"Node Maintenance","text":"<p>This article provides detailed instructions on how to manage both planned and unplanned node downtime in a Kubernetes cluster that is running Run:ai. It covers all the steps to maintain service continuity and ensure the proper handling of workloads during these events.</p>"},{"location":"admin/maintenance/node-downtime/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to Kubernetes cluster   Administrative access to the Kubernetes cluster, including permissions to run <code>kubectl</code> commands  </li> <li>Basic knowledge of Kubernetes   Familiarity with Kubernetes concepts such as nodes, taints, and workloads  </li> <li>Run:ai installation   The Run:ai software installed and configured within your Kubernetes cluster   </li> <li>Node naming conventions   Know the names of the nodes within your cluster, as these are required when executing the commands</li> </ul>"},{"location":"admin/maintenance/node-downtime/#node-types","title":"Node types","text":"<p>This article distinguishes between two types of nodes within a Run:ai installation:</p> <ul> <li>Worker nodes. Nodes on which AI practitioners can submit and run workloads</li> <li>Run:ai system nodes. Nodes on which the Run:ai software runs, managing the cluster's operations</li> </ul>"},{"location":"admin/maintenance/node-downtime/#worker-nodes","title":"Worker nodes","text":"<p>Worker Nodes are responsible for running workloads. When a worker node goes down, either due to planned maintenance or unexpected failure, workloads ideally migrate to other available nodes or wait in the queue to be executed when possible.</p>"},{"location":"admin/maintenance/node-downtime/#training-vs-interactive-workloads","title":"Training vs. Interactive workloads","text":"<p>The following workload types can run on worker nodes: </p> <ul> <li> <p>Training workloads. These are long-running processes that, in case of node downtime, can automatically move to another node.</p> </li> <li> <p>Interactive workloads. These are short-lived, interactive processes that require manual intervention to be relocated to another node.</p> </li> </ul> <p>Note</p> <p>While training workloads can be automatically migrated, it is recommended to plan maintenance and manually manage this process for a faster response, as it may take time for Kubernetes to detect a node failure,</p>"},{"location":"admin/maintenance/node-downtime/#planned-maintenance","title":"Planned maintenance","text":"<p>Before stopping a worker node for maintenance, perform the following steps:</p> <ol> <li> <p>Prevent new workloads on the node    To stop the Kubernetes Scheduler from assigning new workloads to the node and to safely remove all existing workloads, copy the following command to your terminal:  </p> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute\n</code></pre> <p>Explanation: </p> <ul> <li><code>&lt;node-name&gt;</code>     Replace this placeholder with the actual name of the node you want to drain  </li> <li><code>kubectl taint nodes</code>     This command is used to add a taint to the node, which prevents any new pods from being scheduled on it  </li> <li><code>runai=drain:NoExecute</code>     This specific taint ensures that all existing pods on the node are evicted and rescheduled on other available nodes, if possible. </li> </ul> <p>Result: The node stops accepting new workloads, and existing workloads either migrate to other nodes or are placed in a queue for later execution. </p> </li> <li> <p>Shut down and perform maintenance    After draining the node, you can safely shut it down and perform the necessary maintenance tasks. </p> </li> <li> <p>Restart the node     Once maintenance is complete and the node is back online, remove the taint to allow the node to resume normal operations. Copy the following command to your terminal:  </p> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute-\n</code></pre> <p>Explanation: </p> <ul> <li><code>runai=drain:NoExecute-</code>   The <code>-</code> at the end of the command indicates the removal of the taint. This allows the node to start accepting new workloads again.</li> </ul> <p>Result: The node rejoins the cluster's pool of available resources, and workloads can be scheduled on it as usual</p> </li> </ol>"},{"location":"admin/maintenance/node-downtime/#unplanned-downtime","title":"Unplanned downtime","text":"<p>In the event of unplanned downtime:</p> <ol> <li>Automatic Restart     If a node fails but immediately restarts, all services and workloads automatically resume.  </li> <li> <p>Extended Downtime    If the node remains down for an extended period, drain the node to migrate workloads to other nodes. Copy the following command to your terminal:  </p> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute\n</code></pre> <p>Explanation: The command works the same as in the planned maintenance section, ensuring that no workloads remain scheduled on the node while it is down.  </p> </li> <li> <p>Reintegrate the Node     Once the node is back online, remove the taint to allow it to rejoin the cluster's operations. Copy the following command to your terminal:  </p> <p><pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute-\n</code></pre> Result: This action reintegrates the node into the cluster, allowing it to accept new workloads.  </p> </li> <li> <p>Permanent Shutdown     If the node is to be permanently decommissioned, remove it from Kubernetes with the following command:  </p> <p><pre><code>kubectl delete node &lt;node-name&gt;\n</code></pre> Explanation: </p> <ul> <li><code>kubectl delete node</code>   This command completely removes the node from the cluster  </li> <li><code>&lt;node-name&gt;</code>   Replace this placeholder with the actual name of the node  </li> </ul> <p>Result: The node is no longer part of the Kubernetes cluster. If you plan to bring the node back later, it must be rejoined to the cluster using the steps outlined in the next section.</p> </li> </ol>"},{"location":"admin/maintenance/node-downtime/#runai-system-nodes","title":"Run:ai System nodes","text":"<p>In a production environment, the services responsible for scheduling, submitting and managing Run:ai workloads operate on one or more Run:ai system nodes. It is recommended to have more than one system node to ensure high availability. If one system node goes down, another can take over, maintaining continuity. If a second system node does not exist, you must designate another node in the cluster as a temporary Run:ai system node to maintain operations.</p> <p>The protocols for handling planned maintenance and unplanned downtime are identical to those for worker nodes. Refer to the above section for detailed instructions. </p>"},{"location":"admin/maintenance/node-downtime/#rejoining-a-node-into-the-kubernetes-cluster","title":"Rejoining a node into the Kubernetes cluster","text":"<p>To rejoin a node to the Kubernetes cluster, follow these steps:</p> <ol> <li> <p>Generate a join command on the master node    On the master node, copy the following command to your terminal:  </p> <pre><code>kubeadm token create --print-join-command\n</code></pre> <p>Explanation: </p> <ul> <li><code>kubeadm token create</code>     This command generates a token that can be used to join a node to the Kubernetes cluster.  </li> <li><code>--print-join-command</code>     This option outputs the full command that needs to be run on the worker node to rejoin it to the cluster.</li> </ul> <p>Result: The command outputs a <code>kubeadm join</code> command. </p> </li> <li> <p>Run the Join Command on the Worker Node    Copy the <code>kubeadm join</code> command generated from the previous step and run it on the worker node that needs to rejoin the cluster.</p> <p>Explanation: </p> <ul> <li>The <code>kubeadm join</code> command re-enrolls the node into the cluster, allowing it to start participating in the cluster's workload scheduling. </li> </ul> </li> <li> <p>Verify Node Rejoining     Verify that the node has successfully rejoined the cluster by running:  </p> <pre><code>kubectl get nodes\n</code></pre> <p>Explanation:  </p> <p>This command lists all nodes currently part of the Kubernetes cluster, along with their status  </p> <p>Result: The rejoined node should appear in the list with a status of Ready </p> </li> <li> <p>Re-label Nodes     Once the node is back online, ensure it is labeled according to its role within the cluster</p> </li> </ol>"},{"location":"admin/maintenance/overview/","title":"Monitoring and maintenance Overview","text":"<p>Deploying Run:ai in mission-critical environments requires proper monitoring and maintenance of resources to ensure workloads run and are deployed as expected.</p> <p>Details on how to monitor different parts of the physical resources in your Kubernetes system, including clusters and nodes, can be found in the monitoring and maintenance section. Adjacent configuration and troubleshooting sections also cover high availability, restoring and securing clusters, collecting logs, and reviewing audit logs to meet compliance requirements.</p> <p>In addition to monitoring Run:ai resources, it is also highly recommended to monitor Run:ai runs on Kubernetes, which manages containerized applications. In particular, focus on three main layers:</p>"},{"location":"admin/maintenance/overview/#runai-control-plane-and-cluster-services","title":"Run:ai Control Plane and cluster services","text":"<p>This is the highest layer and includes the parts of Run:ai pods, which run in containers managed by Kubernetes.</p>"},{"location":"admin/maintenance/overview/#kubernetes-cluster","title":"Kubernetes cluster","text":"<p>This layer includes the main Kubernetes system that runs and manages Run:ai components. Important elements to monitor include:</p> <ul> <li>The health of the cluster and nodes (machines in the cluster).</li> <li>The status of key Kubernetes services, such as the API server. For detailed information on managing clusters, see the official Kubernetes documentation.</li> </ul>"},{"location":"admin/maintenance/overview/#host-infrastructure","title":"Host infrastructure","text":"<p>This is the base layer, representing the actual machines (virtual or physical) that make up the cluster IT teams need to handle:</p> <ul> <li>Managing CPU, memory, and storage</li> <li>Keeping the operating system updated</li> <li>Setting up the network and balancing the load</li> </ul> <p>Run:ai does not require any special configurations at this level.</p> <p>The articles below explain how to monitor these layers, maintain system security and compliance, and ensure the reliable operation of Run:ai in critical environments.</p>"},{"location":"admin/researcher-setup/cli-install/","title":"Install the Run:ai V1 Command-line Interface","text":"<p>The Run:ai Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc.</p> <p>The instructions below will guide you through the process of installing the CLI. The Run:ai CLI runs on Mac, Linux and Windows. </p>"},{"location":"admin/researcher-setup/cli-install/#researcher-authentication","title":"Researcher Authentication","text":"<p>When enabled, Researcher authentication requires additional setup when installing the CLI. To configure authentication see Setup Project-based Researcher Access Control. Use the modified Kubernetes configuration file described in the article.</p>"},{"location":"admin/researcher-setup/cli-install/#prerequisites","title":"Prerequisites","text":"<ul> <li>When installing the command-line interface, it is worth considering future upgrades:<ul> <li>Install the CLI on a dedicated Jumpbox machine. Researchers will connect to the Jumpbox from which they can submit Run:ai commands</li> <li>Install the CLI on a shared directory that is mounted on Researchers' machines.  </li> </ul> </li> <li>A Kubernetes configuration file.</li> </ul>"},{"location":"admin/researcher-setup/cli-install/#setup","title":"Setup","text":""},{"location":"admin/researcher-setup/cli-install/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<ul> <li>In the Researcher's root folder, create a directory .kube. Copy the Kubernetes configuration file into the directory. Each Researcher should have a separate copy of the configuration file. The Researcher should have write access to the configuration file as it stores user defaults.</li> <li>If you choose to locate the file at a different location than <code>~/.kube/config</code>, you must create a shell variable to point to the configuration file as follows:</li> </ul> <pre><code>export KUBECONFIG=&lt;Kubernetes-config-file&gt;\n</code></pre> <ul> <li>Test the connection by running:</li> </ul> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#install-runai-cli","title":"Install Run:ai CLI","text":"<ul> <li>Go to the Run:ai user interface. On the top right select <code>Researcher Command Line Interface</code>.</li> <li>Select <code>Mac</code>, <code>Linux</code> or <code>Windows</code>.</li> <li>Download directly using the button or copy the file to run it on a remote machine</li> </ul> Mac or LinuxWindows <p>Run:</p> <pre><code>chmod +x runai\nsudo mv runai /usr/local/bin/runai\n</code></pre> <p>Rename the downloaded file to have a <code>.exe</code> extension and move the file to a folder that is a part of the <code>PATH</code>.</p> <p>Note</p> <p>An alternative way of downloading the CLI is provided under the CLI Troubleshooting section.</p> <p>To verify the installation run:</p> <pre><code>runai list jobs\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#install-command-auto-completion","title":"Install Command Auto-Completion","text":"<p>It is possible to configure your Linux/Mac shell to complete Run:ai CLI commands. This feature works on bash and zsh shells only.</p> ZshBash <p>Edit the file <code>~/.zshrc</code>. Add the lines:</p> <pre><code>autoload -U compinit; compinit -i\nsource &lt;(runai completion zsh)\n</code></pre> <p>Install the bash-completion package:</p> <ul> <li>Mac: <code>brew install bash-completion</code></li> <li>Ubuntu/Debian: <code>sudo apt-get install bash-completion</code></li> <li>Fedora/Centos: <code>sudo yum install bash-completion</code></li> </ul> <p>Edit the file <code>~/.bashrc</code>. Add the lines:</p> <pre><code>[[ -r \u201c/usr/local/etc/profile.d/bash_completion.sh\u201d ]] &amp;&amp; . \u201c/usr/local/etc/profile.d/bash_completion.sh\u201d\nsource &lt;(runai completion bash)\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#troubleshoot-the-cli-installation","title":"Troubleshoot the CLI Installation","text":"<p>See Troubleshooting a CLI installation</p>"},{"location":"admin/researcher-setup/cli-install/#update-the-runai-cli","title":"Update the Run:ai CLI","text":"<p>To update the CLI to the latest version perform the same install process again.</p>"},{"location":"admin/researcher-setup/cli-install/#delete-the-runai-cli","title":"Delete the Run:ai CLI","text":"<p>If you have installed using the default path, run:</p> <pre><code>sudo rm /usr/local/bin/runai\n</code></pre>"},{"location":"admin/researcher-setup/docker-to-runai/","title":"From Docker to Run:ai","text":""},{"location":"admin/researcher-setup/docker-to-runai/#dockers-images-and-kubernetes","title":"Dockers, Images, and Kubernetes","text":"<p>Researchers are typically proficient in working with Docker. Docker is an isolation level above the operating system which allows creating your own bundle of the operating system + deep learning environment and packaging it within a single file. The file is called a docker image.</p> <p>You create a container by starting a docker image on a machine.</p> <p>Run:ai is based on Kubernetes. At its core, Kubernetes is an orchestration software above Docker: Among other things, it allows location abstraction as to where the actual container is running. This calls for some adaptation to the Researcher's workflow as follows.</p>"},{"location":"admin/researcher-setup/docker-to-runai/#image-repository","title":"Image Repository","text":"<p>If your Kubernetes cluster contains a single GPU node (machine), then your image can reside on the node itself (in which case, when runai submit workloads, the Researcher must use the flag <code>--local-image</code>).</p> <p>If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the image can no longer reside on the node itself.  It must be relocated to an image repository. There are quite a few repository-as-a-service, most notably Docker hub. Alternatively, the organization can install a private repository on-prem.</p> <p>Day-to-day work with the image located remotely is almost identical to local work. The image name now contains its location. For example, <code>nvcr.io/nvidia/pytorch:19.12-py_3</code> is a PyTorch image that is located in nvcr.io. This is the Nvidia image repository as found on the web. </p>"},{"location":"admin/researcher-setup/docker-to-runai/#data","title":"Data","text":"<p>Deep learning is about data. It can be your code, the training data, saved checkpoints, etc.</p> <p>If your Kubernetes cluster contains a single GPU node (machine), then your data can reside on the node itself.</p> <p>If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the data must sit outside the machine, typically on network storage. The storage must be uniformly mapped to your container when it starts (using the -v command).</p>"},{"location":"admin/researcher-setup/docker-to-runai/#working-with-containers","title":"Working with Containers","text":"<p>Starting a container using docker usually involves a single command-line with multiple flags. A typical example: </p> <pre><code>docker run --runtime=nvidia --shm-size 16G -it --rm -e HOSTNAME='hostname' \\\n    -v /raid/public/my_datasets:/root/dataset:ro   -i  nvcr.io/nvidia/pytorch:19.12-py3\n</code></pre> <p>The docker command <code>docker run</code> should be replaced with a Run:ai command <code>runai submit</code>. The flags are usually the same but some adaptation is required. A complete list of flags can be found here: runai submit. </p> <p>There are similar commands to get a shell into the container (runai bash), get the container logs (runai logs), and more. For a complete list see the Run:ai CLI reference. </p>"},{"location":"admin/researcher-setup/docker-to-runai/#schedule-an-onboarding-session","title":"Schedule an Onboarding Session","text":"<p>It is highly recommended to schedule an onboarding session for Researchers with a Run:ai customer success professional. Run:ai can help with the above transition, but adding to that, we at Run:ai have also acquired a large body of knowledge on data science best practices which can help streamline  Researchers' work as well as save money for the organization.</p>"},{"location":"admin/researcher-setup/new-cli-install/","title":"Installing the V2 Command-line interface","text":"<p>This article explains the procedure for installing and configuring the new researcher Command Line Interface (CLI). </p> <p>Important</p> <p>This document refers to the new CLI which only works with clusters of version 2.18 and up.    The installation instructions for the older CLI are here.</p>"},{"location":"admin/researcher-setup/new-cli-install/#enabling-the-v2-cli","title":"Enabling the V2 CLI","text":"<p>Under Tools &amp; Settings \u2192 General settings \u2192 Workloads, enable the flag <code>Improved command line interface</code></p>"},{"location":"admin/researcher-setup/new-cli-install/#installing-the-cli","title":"Installing the CLI","text":"<ol> <li>Click the Help (?) icon in the top right corner  </li> <li>Select Researcher Command Line Interface </li> <li>Select the cluster you want the CLI to communicate with  </li> <li>Select your computer\u2019s operating system </li> <li>Copy the installer command and run it in the terminal or download the binary file for Windows OS</li> <li>Follow the installation process instructions  </li> <li>Click <code>Enter</code> to use the default values (recommended)</li> </ol>"},{"location":"admin/researcher-setup/new-cli-install/#testing-the-installation","title":"Testing the installation","text":"<p>To verify the CLI client was installed properly</p> <ol> <li>Open the terminal  </li> <li>Run the command <code>runai version</code></li> </ol>"},{"location":"admin/researcher-setup/new-cli-install/#configuring-the-cli","title":"Configuring the CLI","text":"<p>Follow the steps below to configure the CLI.</p>"},{"location":"admin/researcher-setup/new-cli-install/#setting-the-control-plane-url","title":"Setting the Control plane URL","text":"<p>The following step is required for Windows users only. Linux and Mac clients are configured via the installation script automatically</p> <p>Run the command <code>runai config set --cp-url &lt;CONTROL_PLANE_URL&gt;</code>.  This will also create the <code>config.json</code> file in the default path.</p>"},{"location":"admin/researcher-setup/new-cli-install/#authenticating-the-cli","title":"Authenticating the CLI","text":"<p>After installation, sign in to the Run:ai platform to authenticate the CLI:</p> <ol> <li>Open the terminal on your local machine. </li> <li>Run <code>runai login</code>.</li> <li>Enter your username and password on the Run:ai platform's sign-in page. </li> <li>Return to the terminal window to use the CLI.</li> </ol>"},{"location":"admin/researcher-setup/new-cli-install/#setting-the-default-cluster","title":"Setting the default cluster","text":"<p>If only one cluster is connected to the account, it is set as the default cluster when you first sign in.  If there are multiple clusters, you must follow the steps below to set your preferred cluster for workload submission:</p> <ol> <li>Open the terminal on your local machine.  </li> <li>Run <code>runai cluster</code> and select the desired cluster from the interactive menu.</li> </ol> <p>Alternatively:  </p> <ol> <li>Open the terminal on your local machine.  </li> <li>Run <code>runai cluster list</code> to find the desired cluster name.  </li> <li>Run the following command <code>runai cluster set &lt;CLUSTER_NAME&gt;</code>.</li> </ol>"},{"location":"admin/researcher-setup/new-cli-install/#setting-a-default-project","title":"Setting a default project","text":"<p>Set a default working project, to easily submit workloads without mentioning the project name in every command.</p> <ol> <li>Open the terminal on your local machine.  </li> <li>Run <code>runai project</code> and select the desired cluster from the interactive menu.</li> </ol> <p>alernativly  </p> <ol> <li>Open the terminal on your local machine.  </li> <li>Run <code>runai cluster list</code> to find the desired project name.  </li> <li>Run the following command <code>runai project set &lt;PROJECT_NAME&gt;</code> </li> <li>If successful, the following message is returned <code>project &lt;PROJECT_NAME&gt; configured successfully</code> </li> </ol>"},{"location":"admin/researcher-setup/new-cli-install/#validating-the-configuration","title":"Validating the configuration","text":"<p>To view the current configuration run <code>runai config generate --json</code></p>"},{"location":"admin/researcher-setup/new-cli-install/#installing-command-auto-completion","title":"Installing command auto-completion","text":"<p>Auto-completion assists with completing the command syntax automatically for ease of use. Auto-completion is installed automatically.  The interfaces below require manual installation:</p> ZshBashWindows <ol> <li>Edit the file <code>~/.zshrc</code> </li> <li>Add the following code:</li> </ol> <pre><code>autoload -U compinit; compinit -i\nsource &lt;(runai completion zsh)\n</code></pre> <ol> <li>Install the bash-completion package  </li> <li>Choose your operating system:      Mac: <code>brew install bash-completion</code>      Ubuntu/Debian: <code>sudo apt-get install bash-completion</code>      Fedora/Centos: <code>sudo yum install bash-completion</code> </li> <li>Edit the file <code>~/.bashrc</code> and add the following lines:</li> </ol> <pre><code>[[ $PS1 &amp;&amp; -f /usr/share/bash-completion/bash_completion ]] &amp;&amp; . /usr/share/bash-completion/bash_completion\nsource &lt;(runai completion bash)\n</code></pre> <p>Add the following code in the powershell profile: <pre><code>runai.exe completion powershell | Out-String | Invoke-Expression\nSet-PSReadLineKeyHandler -Key Tab -Function MenuComplete\n</code></pre> For more completion modes options, see Powershell completions.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/","title":"Researcher Setup Overview","text":"<p>Following is a step-by-step guide for getting a new Researcher up to speed with Run:ai and Kubernetes.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#change-of-paradigms-from-docker-to-kubernetes","title":"Change of Paradigms: from Docker to Kubernetes","text":"<p>As part of Run:ai, the organization is typically moving from Docker-based workflows to Kubernetes. This document is an attempt to help the Researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:ai CLI.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#setup-the-runai-command-line-interface","title":"Setup the Run:ai Command-Line Interface","text":"<p>Run:ai CLI needs to be installed on the Researcher's machine. This document provides step by step instructions.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#provide-the-researcher-with-a-gpu-quota","title":"Provide the Researcher with a GPU Quota","text":"<p>To submit workloads with Run:ai, the Researcher must be provided with a Project that contains a GPU quota. Please see Working with Projects document on how to create Projects and set a quota.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#provide-access-to-the-runai-user-interface","title":"Provide access to the Run:ai User Interface","text":"<p>See Setting up users for further information on how to provide access to users.  </p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#schedule-an-onboarding-session","title":"Schedule an Onboarding Session","text":"<p>It is highly recommended to schedule an onboarding session for Researchers with a Run:ai customer success professional. Run:ai can help with the above transition, but adding to that, we at Run:ai have also acquired a large body of knowledge on data science best practices which can help streamline the Researchers' work as well as save money for the organization. </p>"},{"location":"admin/runai-setup/installation-types/","title":"Installation Types","text":"<p>Run:ai consists of two components:</p> <ul> <li>The Run:ai Cluster. One or more data-science GPU clusters hosted by the customer (on-prem or cloud).</li> <li>The Run:ai Control plane. A single entity that monitors clusters, sets priorities, and business policies.</li> </ul> <p>There are two main installation options:</p> Installation Type Description Classic (SaaS) Run:ai is installed on the customer's data science GPU clusters. The cluster connects to the Run:ai control plane on the cloud (https://<code>&lt;tenant-name&gt;</code>.run.ai).  With this installation, the cluster requires an outbound connection to the Run:ai cloud. Self-hosted The Run:ai control plane is also installed in the customer's data center <p>The self-hosted option is for organizations that cannot use a SaaS solution due to data leakage concerns. The self-hosted installation is priced differently. For further information please talk to Run:ai sales.</p> <p></p>"},{"location":"admin/runai-setup/installation-types/#self-hosted-installation","title":"Self-hosted Installation","text":"<p>Run:ai self-hosting comes with two variants:</p> Self-hosting Type Description Connected The organization can freely download from the internet (though upload is not allowed) Air-gapped The organization has no connection to the internet"},{"location":"admin/runai-setup/installation-types/#self-hosting-with-kubernetes-vs-openshift","title":"Self-hosting with Kubernetes vs OpenShift","text":"<p>Kubernetes has many Certified Kubernetes Providers. Run:ai has been certified with several of them (see the Kubernetes distribution section). The OpenShift installation is different from the rest. As such, the Run:ai self-hosted installation instructions are divided into two separate sections:</p> <ul> <li>OpenShift-based installation. See Run:ai OpenShift installation.</li> <li>Kubernetes-based installation. See Run:ai Kubernetes installation.</li> </ul>"},{"location":"admin/runai-setup/installation-types/#secure-installation","title":"Secure Installation","text":"<p>In many organizations, Kubernetes is governed by IT compliance rules. In this scenario, there are strict access control rules during the installation and running of workloads:</p> <ul> <li>OpenShift is secured using Security Context Constraints (SCC). The Run:ai installation supports SCC.</li> <li>Run:ai provides limited support for Kubernetes Pod Security Admission (PSA). For more information see Kubernetes prerequisites.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-delete/","title":"Cluster Uninstall","text":"<p>This article explains how to uninstall Run:ai Cluster installation from the Kubernetes cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-delete/#unistall-runai-cluster","title":"Unistall Run:ai cluster","text":"<p>Uninstall of Run:ai cluster from the Kubernetes cluster does not delete existing projects, departments or workloads submitted by users.</p> <p>To uninstall the Run:ai cluster, run the following helm command in your terminal:</p> <pre><code>helm uninstall runai-cluster -n runai\n</code></pre> <p>To delete the Run:ai cluster from the Run:ai Platform, see Removing a cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/","title":"Cluster Install","text":"<p>This article explains the steps required to install the Run:ai cluster on a Kubernetes cluster using Helm.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#before-installation","title":"Before installation","text":"<p>There are a number of matters to consider prior to installing using Helm.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#system-and-network-requirements","title":"System and network requirements","text":"<p>Before installing the Run:ai cluster, validate that the system requirements and network requirements are met.</p> <p>Once all the requirements are met, it is highly recommend to use the Run:ai cluster preinstall diagnostics tool to:</p> <ul> <li>Test the below requirements in addition to failure points related to Kubernetes, NVIDIA, storage, and networking  </li> <li>Look at additional components installed and analyze their relevance to a successful installation</li> </ul> <p>To run the preinstall diagnostics tool, download the latest version, and run:</p> SaaSSelf-hostedAirgap <ul> <li>On EKS deployments, run <code>aws configure</code> prior to execution</li> </ul> <pre><code>chmod +x ./preinstall-diagnostics-&lt;platform&gt; &amp;&amp; \\\n./preinstall-diagnostics-&lt;platform&gt; \\\n  --domain ${COMPANY_NAME}.run.ai \\\n  --cluster-domain ${CLUSTER_FQDN}\n</code></pre> <pre><code>chmod +x ./preinstall-diagnostics-&lt;platform&gt; &amp;&amp; \\ \n./preinstall-diagnostics-&lt;platform&gt; \\\n  --domain ${CONTROL_PLANE_FQDN} \\\n  --cluster-domain ${CLUSTER_FQDN} \\\n#if the diagnostics image is hosted in a private registry\n  --image-pull-secret ${IMAGE_PULL_SECRET_NAME} \\\n  --image ${PRIVATE_REGISTRY_IMAGE_URL}    \n</code></pre> <p>In an air-gapped deployment, the diagnostics image is saved, pushed, and pulled manually from the organization's registry.</p> <pre><code>#Save the image locally\ndocker save --output preinstall-diagnostics.tar gcr.io/run-ai-lab/preinstall-diagnostics:${VERSION}\n#Load the image to the organization's registry\ndocker load --input preinstall-diagnostics.tar\ndocker tag gcr.io/run-ai-lab/preinstall-diagnostics:${VERSION} ${CLIENT_IMAGE_AND_TAG} \ndocker push ${CLIENT_IMAGE_AND_TAG}\n</code></pre> <p>Run the binary with the <code>--image</code> parameter to modify the diagnostics image to be used:</p> <pre><code>chmod +x ./preinstall-diagnostics-darwin-arm64 &amp;&amp; \\\n./preinstall-diagnostics-darwin-arm64 \\\n  --domain ${CONTROL_PLANE_FQDN} \\\n  --cluster-domain ${CLUSTER_FQDN} \\\n  --image-pull-secret ${IMAGE_PULL_SECRET_NAME} \\\n  --image ${PRIVATE_REGISTRY_IMAGE_URL}    \n</code></pre> <p>For more information see preinstall diagnostics.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#helm","title":"Helm","text":"<p>Run:ai cluster requires Helm 3.14 or above. To install Helm, see Helm Install.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#permissions","title":"Permissions","text":"<p>A Kubernetes user with the <code>cluster-admin</code> role is required to ensure a successful installation, for more information see Using RBAC authorization.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#runai-namespace","title":"Run:ai namespace","text":"<p>Run:ai cluster must be installed in a namespace named <code>runai</code>. Create the namespace by running:</p> <pre><code>kubectl create ns runai\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#tls-certificates","title":"TLS certificates","text":"<p>A TLS private and public keys are required for HTTP access to the cluster. Create a Kubernetes Secret named <code>runai-cluster-domain-tls-secret</code> in the <code>runai</code> namespace with the cluster\u2019s Fully Qualified Domain Name (FQDN) private and public keys, by running the following:</p> <pre><code>kubectl create secret tls runai-cluster-domain-tls-secret -n runai \\\n    --cert /path/to/fullchain.pem  \\ # Replace /path/to/fullchain.pem with the actual path to your TLS certificate\n    --key /path/to/private.pem # Replace /path/to/private.pem with the actual path to your private key\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#installation","title":"Installation","text":"<p>Follow these instructions to install using Helm.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#adding-a-new-cluster","title":"Adding a new cluster","text":"<p>Follow the steps below to add a new cluster.</p> <p>Note</p> <p>When adding a cluster for the first time, the New Cluster form automatically opens when you log-in to the Run:ai platform. Other actions are prevented, until the cluster is created.</p> <p>If this is your first cluster and you have completed the New Cluster form, start at step 3. Otherwise, start at step 1.</p> <ol> <li>In the Run:ai platform, go to Clusters </li> <li>Click +NEW CLUSTER </li> <li>Enter a unique name for your cluster  </li> <li>Optional: Chose the Run:ai cluster version (latest, by default)  </li> <li>Enter the Cluster URL . For more information see Domain Name Requirement </li> <li>Click Continue</li> </ol>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#installing-runai-cluster","title":"Installing Run:ai cluster","text":"<p>In the next Section, the Run:ai cluster installation steps will be presented.</p> <ol> <li>Follow the installation instructions and run the commands provided on your Kubernetes cluster.  </li> <li>Click DONE</li> </ol> <p>The cluster is displayed in the table with the status Waiting to connect, once installation is complete, the cluster status changes to Connected</p> <p>Note</p> <p>To customize the installation based on your environment, see Customize cluster installation.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter an issue with the installation, try the troubleshooting scenario below.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#installation_1","title":"Installation","text":"<p>If the Run:ai cluster installation failed, check the installation logs to identify the issue. Run the following script to print the installation logs:</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/run-ai/public/main/installation/get-installation-logs.sh\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#cluster-status","title":"Cluster status","text":"<p>If the Run:ai cluster installation completed, but the cluster status did not change its status to Connected, check the cluster troubleshooting scenarios</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/","title":"System Requirements","text":"<p>The Run:ai Cluster is a Kubernetes application.</p> <p>This article explains the required hardware and software system requirements for the Run:ai cluster.</p> <p>Set out below are the system requirements for the Run:ai cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#hardware-requirements","title":"Hardware Requirements","text":"<p>The following hardware requirements are for the Kubernetes Cluster nodes\u2019. By default, all Run:ai cluster services run on all available nodes. For production deployments, you may want to Set Node Roles, to separate between system and worker nodes, reduce downtime and save CPU cycles on expensive GPU Machines.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#runai-cluster-system-nodes","title":"Run:ai Cluster - system nodes","text":"<p>This configuration is the minimum requirement you need to install and use Run:ai Cluster.</p> Component Required Capacity CPU 10 cores Memory 20GB Disk space 50GB"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#runai-cluster-worker-nodes","title":"Run:ai Cluster - Worker nodes","text":"<p>The Run:ai Cluster supports x86 CPUs and NVIDIA GPUs from the T, V, A, L, and H architecture families. For the list of supported GPU models, see Supported NVIDIA Data Center GPUs and Systems.</p> <p>The following configuration represents the minimum hardware requirements for installing and operating the Run:ai cluster on worker nodes. Each node must meet these specifications:</p> Component Required Capacity CPU 2 cores Memory 4GB"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#shared-storage","title":"Shared storage","text":"<p>Run:ai workloads must be able to access data from any worker node in a uniform way, to access training data and code as well as save checkpoints, weights, and other machine-learning-related artifacts.</p> <p>Typical protocols are Network File Storage (NFS) or Network-attached storage (NAS). Run:ai Cluster supports both, for more information see Shared storage.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#software-requirements","title":"Software requirements","text":"<p>The following software requirements must be fulfilled on the Kubernetes cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#operating-system","title":"Operating system","text":"<ul> <li>Any Linux operating system supported by both Kubernetes and NVIDIA GPU Operator  </li> <li>Run:ai cluster on Google Kubernetes Engine (GKE) supports both Ubuntu and Container Optimized OS (COS). COS is supported only with NVIDIA GPU Operator 24.6 or newer, and Run:ai cluster version 2.19 or newer.</li> <li>Internal tests are being performed on Ubuntu 22.04 and CoreOS for OpenShift.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#kubernetes-distribution","title":"Kubernetes distribution","text":"<p>Run:ai Cluster requires Kubernetes. The following Kubernetes distributions are supported:</p> <ul> <li>Vanilla Kubernetes  </li> <li>OpenShift Container Platform (OCP)  </li> <li>NVIDIA Base Command Manager (BCM)  </li> <li>Elastic Kubernetes Engine (EKS)  </li> <li>Google Kubernetes Engine (GKE)  </li> <li>Azure Kubernetes Service (AKS)</li> <li>Oracle Kubernetes Engine (OKE)</li> <li>Rancher Kubernetes Engine (RKE1)  </li> <li>Rancher Kubernetes Engine 2 (RKE2)</li> </ul> <p>Contact Run:ai customer support for up-to-date support details for:</p> <ul> <li>Ezmeral Runtime Enterprise  </li> <li>Tanzu Platform</li> </ul> <p>Important</p> <p>The latest release of the Run:ai cluster supports Kubernetes 1.28 to 1.31 and OpenShift 4.12 to 4.17</p> <p>For existing Kubernetes clusters, see the following Kubernetes version support matrix for the latest Run:ai cluster releases:</p> Run:ai version Supported Kubernetes versions Supported OpenShift versions v2.13 1.23 to 1.28 4.10 to 4.13 v2.16 1.26 to 1.28 4.11 to 4.14 v2.17 1.27 to 1.29 4.12 to 4.15 v2.18 1.28 to 1.30 4.12 to 4.16 v2.19 (latest) 1.28 to 1.31 4.12 to 4.17 <p>For information on supported versions of managed Kubernetes, it's important to consult the release notes provided by your Kubernetes service provider. There, you can confirm the specific version of the underlying Kubernetes platform supported by the provider, ensuring compatibility with Run:ai. For an up-to-date end-of-life statement see Kubernetes Release History or OpenShift Container Platform Life Cycle Policy.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#kubernetes-pod-security-admission","title":"Kubernetes Pod Security Admission","text":"<p>Run:ai v2.15 and above supports <code>restricted</code> policy for Pod Security Admission (PSA) on OpenShift only. Other Kubernetes distributions are only supported with <code>privileged</code> policy.</p> <p>For Run:ai on OpenShift to run with PSA <code>restricted</code> policy:</p> <ul> <li>Label the <code>runai</code> namespace as described in Pod Security Admission with the following labels:</li> </ul> <pre><code>pod-security.kubernetes.io/audit=privileged\npod-security.kubernetes.io/enforce=privileged\npod-security.kubernetes.io/warn=privileged\n</code></pre> <ul> <li>The workloads submitted through Run:ai should comply with the restrictions of PSA restricted policy, This can be enforced using Policies.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#kubernetes-ingress-controller","title":"Kubernetes Ingress Controller","text":"<p>Run:ai cluster requires Kubernetes Ingress Controller to be installed on the Kubernetes cluster.</p> <ul> <li>OpenShift, RKE and RKE2 come pre-installed ingress controller.  </li> <li>Internal tests are being performed on NGINX, Rancher NGINX, OpenShift Router, and Istio.  </li> <li>Make sure that a default ingress controller is set.</li> </ul> <p>There are many ways to install and configure different ingress controllers. A simple example to install and configure NGINX ingress controller using helm:</p> Vanilla KubernetesManaged Kubernetes (EKS, GKE, AKS)Oracle Kubernetes Engine (OKE) <p>Run the following commands:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm upgrade -i nginx-ingress ingress-nginx/ingress-nginx \\\n    --namespace nginx-ingress --create-namespace \\\n    --set controller.kind=DaemonSet \\\n    --set controller.service.externalIPs=\"{&lt;INTERNAL-IP&gt;,&lt;EXTERNAL-IP&gt;}\" # Replace &lt;INTERNAL-IP&gt; and &lt;EXTERNAL-IP&gt; with the internal and external IP addresses of one of the nodes\n</code></pre> <p>Run the following commands:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm install nginx-ingress ingress-nginx/ingress-nginx \\\n    --namespace nginx-ingress --create-namespace\n</code></pre> <p>Run the following commands:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm install nginx-ingress ingress-nginx/ingress-nginx \\\n    --namespace ingress-nginx --create-namespace \\\n    --set controller.service.annotations.oci.oraclecloud.com/load-balancer-type=nlb \\\n    --set controller.service.annotations.oci-network-load-balancer.oraclecloud.com/is-preserve-source=True \\\n    --set controller.service.annotations.oci-network-load-balancer.oraclecloud.com/security-list-management-mode=None \\\n    --set controller.service.externalTrafficPolicy=Local \\\n    --set controller.service.annotations.oci-network-load-balancer.oraclecloud.com/subnet=&lt;SUBNET-ID&gt; # Replace &lt;SUBNET-ID&gt; with the subnet ID of one of your cluster\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#nvidia-gpu-operator","title":"NVIDIA GPU Operator","text":"<p>Run:ai Cluster requires NVIDIA GPU Operator to be installed on the Kubernetes Cluster, supports version 22.9 to 24.6</p> <p>See the Installing the NVIDIA GPU Operator, followed by notes below:</p> <ul> <li>Use the default <code>gpu-operator</code> namespace . Otherwise, you must specify the target namespace using the flag <code>runai-operator.config.nvidiaDcgmExporter.namespace</code> as described in customized cluster installation.  </li> <li>NVIDIA drivers may already be installed on the nodes. In such cases, use the NVIDIA GPU Operator flags <code>--set driver.enabled=false</code>. DGX OS is one such example as it comes bundled with NVIDIA Drivers.  </li> <li>To use Dynamic MIG (deprecated), the GPU Operator must be installed with the flag <code>mig.strategy=mixed</code> as described in customized cluster installation. If the GPU Operator is already installed, edit the <code>clusterPolicy</code> by running</li> </ul> <pre><code>kubectl patch clusterPolicy cluster-policy -n gpu-operator --type=merge -p '{\"spec\":{\"mig\":{\"strategy\": \"mixed\"}}}\n</code></pre> <ul> <li>For distribution-specific additional instructions see below:</li> </ul> OpenShift Container Platform (OCP) <p>The Node Feature Discovery (NFD) Operator is a prerequisite for the NVIDIA GPU Operator in OpenShift. Install the NFD Operator using the Red Hat OperatorHub catalog in the OpenShift Container Platform web console. For more information see Installing the Node Feature Discovery (NFD) Operator</p> Elastic Kubernetes Service (EKS) <ul> <li>When setting-up the cluster, do not install the NVIDIA device plug-in (we want the NVIDIA GPU Operator to install it instead).  </li> <li>When using the eksctl tool to create a cluster, use the flag <code>--install-nvidia-plugin=false</code> to disable the installation.</li> </ul> <p>For GPU nodes, EKS uses an AMI which already contains the NVIDIA drivers. As such, you must use the GPU Operator flags: <code>--set driver.enabled=false</code></p> Google Kubernetes Engine (GKE) <p>Before installing the GPU Operator, create the <code>gpu-operator</code> namespace by running</p> <pre><code>kubectl create ns gpu-operator\n</code></pre> <p>create the following file:</p> resourcequota.yaml<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\nname: gcp-critical-pods\nnamespace: gpu-operator\nspec:\nscopeSelector:\n    matchExpressions:\n    - operator: In\n    scopeName: PriorityClass\n    values:\n    - system-node-critical\n    - system-cluster-critical\n</code></pre> <p>And then run:</p> <pre><code>kubectl apply -f resourcequota.yaml\n</code></pre> Rancher Kubernetes Engine 2 (RKE2) <p>Make sure to specify the <code>CONTAINERD_CONFIG</code> option exactly as outlined in the documentation and custom configuration guide, using the path <code>/var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl</code>. Do not create the file manually if it does not already exist. The GPU Operator will handle this configuration during deployment.</p> Oracle Kubernetes Engine (OKE) <ul> <li>During cluster setup, create a nodepool, and set <code>initial_node_labels</code> to include <code>oci.oraclecloud.com/disable-gpu-device-plugin=true</code> which disables the NVIDIA GPU device plugin.</li> <li>For GPU nodes, OKE defaults to Oracle Linux, which is incompatible with NVIDIA drivers. To resolve this, use a custom Ubuntu image instead.</li> </ul> <p>For troubleshooting information, see the NVIDIA GPU Operator Troubleshooting Guide.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#prometheus","title":"Prometheus","text":"<p>Run:ai Cluster requires Prometheus to be installed on the Kubernetes cluster.</p> <ul> <li>OpenShift comes pre-installed with prometheus  </li> <li>For RKE2 see Enable Monitoring instructions to install Prometheus</li> </ul> <p>There are many ways to install Prometheus. A simple example to install the community Kube-Prometheus Stack using helm, run the following commands:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n    -n monitoring --create-namespace --set grafana.enabled=false\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#optional-software-requirements","title":"Optional software requirements","text":"<p>Optional Run:ai capabilities, Distributed Training and Inference require additional Kubernetes applications (frameworks) to be installed on the cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#distributed-training","title":"Distributed training","text":"<p>Distributed training enables training of AI models over multiple nodes. This requires distributed training framework to be installed on the cluster. The following frameworks are supported:</p> <ul> <li>TensorFlow </li> <li>PyTorch </li> <li>XGBoost </li> <li>MPI v2</li> </ul> <p>There are many ways to install each framework. A simple example of installation is the Kubeflow Training Operator. Run:ai supports Training-Operator version 1.7.0 or higher - which includes TensorFlow, Pytorch, and XGBoost.</p> <p>To install run the following command:</p> <pre><code>kubectl apply -k \"github.com/kubeflow/training-operator.git/manifests/overlays/standalone?ref=v1.7.0\"\n</code></pre> <p>To install MPI v2, which is not included in the Kubeflow Training Operator, run the following command:</p> <pre><code>kubectl apply --server-side -f https://raw.githubusercontent.com/kubeflow/mpi-operator/master/deploy/v2beta1/mpi-operator.yaml\n</code></pre> <p>Note</p> <p>If you need both MPI v2 and Kubeflow Training Operator, follow the steps below:</p> <ul> <li>Install the Kubeflow Training operator as above.  </li> <li>Disable and delete MPI v1 in the Kubeflow Training Operator by running:</li> </ul> <pre><code>kubectl patch deployment training-operator -n kubeflow --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args\", \"value\": [\"--enable-scheme=tfjob\", \"--enable-scheme=pytorchjob\", \"--enable-scheme=xgboostjob\"]}]'\nkubectl delete crd mpijobs.kubeflow.org\n</code></pre> <ul> <li>Install MPI v2 as described above.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#inference","title":"Inference","text":"<p>Inference enables serving of AI models. This requires the Knative Serving framework to be installed on the cluster and supports Knative versions 1.10 to 1.15</p> <p>Follow the Installing Knative instructions. After installation, configure Knative to use the Run:ai scheduler and features, by running:</p> <pre><code>kubectl patch configmap/config-autoscaler \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"enable-scale-to-zero\":\"true\"}}' &amp;&amp; \\\nkubectl patch configmap/config-features \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"kubernetes.podspec-schedulername\":\"enabled\",\"kubernetes.podspec-affinity\":\"enabled\",\"kubernetes.podspec-tolerations\":\"enabled\",\"kubernetes.podspec-volumes-emptydir\":\"enabled\",\"kubernetes.podspec-securitycontext\":\"enabled\",\"kubernetes.podspec-persistent-volume-claim\":\"enabled\",\"kubernetes.podspec-persistent-volume-write\":\"enabled\",\"multi-container\":\"enabled\",\"kubernetes.podspec-init-containers\":\"enabled\"}}'\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#domain-name-requirement","title":"Domain Name Requirement","text":"<p>The following requirement must be followed for naming the domain.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#fully-qualified-domain-name-fqdn","title":"Fully Qualified Domain Name (FQDN)","text":"<p>You must have a Fully Qualified Domain Name (FQDN) to install Run:ai Cluster (ex: <code>runai.mycorp.local</code>). This cannot be an IP. The domain name must be accessible inside the organization only. You also need a TLS certificate (private and public) for HTTPS access.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/","title":"SaaS Cluster Setup Introduction","text":"<p>This section is a step-by-step guide for setting up a Run:ai cluster. </p> <ul> <li>A Run:ai cluster is a Kubernetes application installed on top of a Kubernetes cluster.</li> <li>A Run:ai cluster connects to the Run:ai control plane on the cloud. The control plane provides a control point as well as a monitoring and control user interface for Administrators and Researchers.</li> <li>A customer may have multiple Run:ai Clusters, all connecting to a single control plane.</li> </ul> <p>For additional details see the Run:ai system components</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#documents","title":"Documents","text":"<ul> <li>Review Run:ai cluster System Requirements and Network Requirements.</li> <li>Cluster Install step-by-step guid.</li> <li>Look for troubleshooting tips if required.</li> <li>Cluster Upgrade and Cluster Uninstall instructions. </li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#customization","title":"Customization","text":"<p>For a list of optional customizations see Customize Installation</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#additional-configuration","title":"Additional Configuration","text":"<p>For a list of advanced configuration scenarios such as configuring researcher authentication, Single sign-on limiting the installation to specific nodes, and more, see the Configuration Articles section.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#next-steps","title":"Next Steps","text":"<p>After setting up the cluster, you may want to start setting up Researchers. See: Researcher Setup.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/","title":"Cluster Upgrade","text":"<p>This article explains how to upgrade Run:ai cluster version.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#before-upgrade","title":"Before upgrade","text":"<p>There are a number of matters to consider prior to upgrading the Run:ai cluster version.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#system-and-network-requirements","title":"System and network requirements","text":"<p>Before upgrading the Run:ai cluster, validate that the latest system requirements and network requirements are met, as they can change from time to time.</p> <p>Important</p> <p>It is highly recommended to upgrade the Kubernetes version together with the Run:ai cluster version, to ensure compatibility with latest supported version of your Kubernetes distribution</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#helm","title":"Helm","text":"<p>The latest releases of the Run:ai cluster require Helm 3.14 or above.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade","title":"Upgrade","text":"<p>Follow the instructions to upgrade using Helm. The Helm commands to upgrade the Run:ai cluster version may differ between versions. The steps below describe how to get the instructions from the Run:ai UI.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#getting-the-installation-instructions","title":"Getting the installation instructions","text":"<p>Follow the setup and installation instructions below to get the installation instructions to upgrade the Run:ai cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#setup","title":"Setup","text":"<ol> <li>In the Run:ai UI, go to Clusters </li> <li>Select the cluster you want to upgrade  </li> <li>Click INSTALLATION INSTRUCTIONS </li> <li>Optional: Select the Run:ai cluster version (latest, by default)  </li> <li>Click CONTINUE</li> </ol>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#installation-instructions","title":"Installation instructions","text":"<ol> <li>Follow the installation instructions     run the Helm commands provided on your Kubernetes cluster (see the troubleshooting below if installation fails)  </li> <li>Click DONE </li> <li>Once installation is complete, validate the cluster is Connected and listed with the new cluster version (see the cluster troubleshooting scenarios). Once you have done this, the cluster is upgraded to the latest version.</li> </ol> <p>Note</p> <p>To upgrade to a specific version, modify the <code>--version</code> flag by specifying the desired <code>&lt;version-number&gt;</code>. You can find all available versions by using the <code>helm search repo</code> command.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter an issue with the cluster upgrade, use the troubleshooting scenario below.</p> Installation fails <p>If the Run:ai cluster upgrade fails, check the installation logs to identify the issue.</p> <p>Run the following script to print the installation logs:</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/run-ai/public/main/installation/get-installation-logs.sh\n</code></pre> Cluster status <p>If the Run:ai cluster upgrade completes, but the cluster status does not show as Connected, refer to the cluster troubleshooting scenarios</p> <p>. </p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/","title":"Customize Installation","text":"<p>This article explains the available configurations for customizing the Run:ai cluster installation.</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#helm-chart-values","title":"Helm chart values","text":"<p>The Run:ai cluster installation can be customized to support your environment via Helm values files or Helm install flags.</p> <p>These configurations are saved in the runaiconfig Kubernetes object and can be edited post-installation as needed. For more information, see Advanced Cluster Configurations.</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#values","title":"Values","text":"<p>The following table lists the available Helm chart values that can be configured to customize the Run:ai cluster installation.</p> Key Description Default global.image.registry (string) Global Docker image registry Default: <code>\"\"</code> global.additionalImagePullSecrets (list) List of image pull secrets references Default: <code>[]</code> spec.researcherService.ingress.tlsSecret (string) Existing secret key where cluster TLS Certificates are stored (non-OpenShift) Default: <code>runai-cluster-domain-tls-secret</code> spec.researcherService.route.tlsSecret (string) Existing secret key where cluster TLS Certificates are stored (OpenShift only) Default: <code>\"\"</code> spec.prometheus.spec.image (string) Due to a known issue In the Prometheus Helm chart, the imageRegistry setting is ignored. To pull the image from a different registry, you can manually specify the Prometheus image reference. Default: <code>quay.io/prometheus/prometheus</code> spec.prometheus.spec.imagePullSecrets (string) List of image pull secrets references in the runai namespace to use for pulling Prometheus images (relevant for air-gapped installations). Default: <code>[]</code>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/","title":"Install using Base Command Manager","text":"<p>This article explains the steps required to install the Run:ai cluster on a DGX Kubernetes Cluster using NVIDIA Base Command Manager (BCM).</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#runai-installer","title":"Run:ai Installer","text":"<p>The Run:ai Installer is an User Interface (UI) wizard that simplifies the deployment of Run:ai Cluster on DGX. The Run:ai installer can be installed via the BCM cluster wizard on cluster creation.</p> <p>Note</p> <p>For advanced configuration and custom deployment options, refer to the Install using Helm.</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#before-installation","title":"Before installation","text":"<p>There are a number of matters to consider prior to installing using the Run:ai Installer.</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#application-secret-key","title":"Application secret key","text":"<p>An Application secret key is required to connect the cluster to the Run:ai Platform, In order to get the Application secret key, a new cluster must be added.</p> <ol> <li>follow the Adding a new cluster setup instructions, Do not follow the Installation instructions.</li> <li>Once cluster instructions are displayed, find the <code>controlPlane.clientSecret</code> flag in the displayed Helm command, copy and save its value.</li> </ol> <p>Note</p> <p>For DGX Bundle customers, installing their first Run:ai cluster - The Application secret key will be provided by the Run:ai Support team.</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#system-and-network-requirements","title":"System and network requirements","text":"<p>Before installing the Run:ai cluster on a DGX system using BCM, ensure that your System requirements and Network requirements meets the necessary prerequisites.</p> <p>The BCM cluster wizard deploys essential Software Requirements, such as the Kubernetes Ingress Controller, NVIDIA GPU Operator, and Prometheus, as part of the Run:ai Installer deployment. Additional optional software requirements for Distributed training and Inference, requires manual setup.</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#tenant-name","title":"Tenant Name","text":"<p>Your tenant name is predefined and supplied by Run:ai. Each customer is provided with a unique, dedicated URL in the format <code>&lt;tenant-name&gt;.run.ai</code> which includes the required tenant name.</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#tls-certificate","title":"TLS certificate","text":"<p>A TLS private and public keys for the cluster\u2019s Fully Qualified Domain Name (FQDN) are required for HTTP access to the cluster</p> <p>Important</p> <p>TLS Certificate must be trusted, Self-signed certificates are not supported.</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#installation","title":"Installation","text":"<p>Follow these instructions to install using BCM.</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#installing-a-cluster","title":"Installing a cluster","text":"<p>The cluster installer is available via the locally installed BCM landing page,</p> <ol> <li>Go to the locally installed BCM landing page, Select the Run:ai tile or access directly to <code>http://&lt;BCM-CLUSTER-IP&gt;:30080/runai-installer</code> (HTTP only) </li> <li>Click VERIFY in order to check System Requirements are met. </li> <li>After verification completed successfully, click CONTINUE. </li> <li>Enter the cluster information and click CONTINUE. </li> <li>The Run:ai installation will start and should be complete within a few minutes </li> <li>Once a message of Run:ai was installed successfully! is displayed, Click on START USING RUN:AI to launch the login page of the tenant in a new browser tab. </li> </ol>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter an issue with the installation, try the troubleshooting scenario below.</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#runai-installer_1","title":"Run:ai Installer","text":"<p>The Run:ai Installer is a pod in Kubernetes. The pod is responsible for the installation preparation and prerequisite gathering phase. In case of an error during the Prerequisites verification, Run the following command to print the logs:</p> <pre><code>kubectl get pods -n runai | grep 'cluster-installer' # Find the cluster installer pod's name\nkubectl logs &lt;POD-NAME&gt; -n runai # Print the cluster installer pod logs\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#installation_1","title":"Installation","text":"<p>If the Run:ai cluster installation failed, check the installation logs to identify the issue. Run the following script to print the installation logs:</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/run-ai/public/main/installation/get-installation-logs.sh\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#cluster-status","title":"Cluster status","text":"<p>If the Run:ai cluster installation completed, but the cluster status did not change its status to Connected, check the cluster troubleshooting scenarios</p>"},{"location":"admin/runai-setup/cluster-setup/network-req/","title":"Network Requirements","text":"<p>The following network requirements are for the Run:ai cluster installation and usage.</p>"},{"location":"admin/runai-setup/cluster-setup/network-req/#external-access","title":"External access","text":"<p>Set out below are the domains to whitelist and ports to open for installation, upgrade, and usage of the application and its management.</p> <p>Ensure the inbound and outbound rules are correctly applied to your firewall.</p>"},{"location":"admin/runai-setup/cluster-setup/network-req/#inbound-rules","title":"Inbound rules","text":"<p>To allow your organization\u2019s Run:ai users to interact with the cluster using the Run:ai Command-line interface, or access specific UI features, certain inbound ports need to be open.</p> Name Description Source Destination Port Run:ai cluster Run:ai cluster HTTPS entrypoint 0.0.0.0 all k8s nodes 443"},{"location":"admin/runai-setup/cluster-setup/network-req/#outbound-rules","title":"Outbound rules","text":"<p>For the Run:ai cluster installation and usage, certain outbound ports must be open.</p> Name Description Source Destination Port Run:ai Platform Run:ai cloud instance Run:ai system nodes app.run.ai 443 Grafana Run:ai cloud metrics store Run:ai system nodes prometheus-us-central1.grafana.net and runailabs.com 443 Google Container Registry Run:ai image repository All K8S nodes gcr.io/run-ai-prod 443 JFrog Artifactory Run:ai Helm repository Helm client machine runai.jfrog.io 443 <p>The Run:ai installation has software requirements that require additional components to be installed on the cluster. This article includes simple installation examples which can be used optionally and require the following cluster outbound ports to be open:</p> Name Description Source Destination Port Kubernetes Registry Ingress Nginx image repository All K8S nodes registry.k8s.io 443 Google Container Registry GPU Operator, and Knative image repository All K8S nodes gcr.io 443 Red Hat Container Registry Prometheus Operator image repository All K8S nodes quay.io 443 Docker Hub Registry Training Operator image repository All K8S nodes docker.io 443 <p>Note</p> <p>If you are using an HTTP proxy, contact Run:ai support for further instructions.</p>"},{"location":"admin/runai-setup/cluster-setup/network-req/#internal-network","title":"Internal network","text":"<p>Ensure that all Kubernetes nodes can communicate with each other across all necessary ports. Kubernetes assumes full interconnectivity between nodes, so you must configure your network to allow this seamless communication. Specific port requirements may vary depending on your network setup.</p>"},{"location":"admin/runai-setup/cluster-setup/project-management/","title":"Manually Create Projects","text":""},{"location":"admin/runai-setup/cluster-setup/project-management/#manual-creation-of-namespaces-for-projects","title":"Manual Creation of Namespaces for Projects","text":""},{"location":"admin/runai-setup/cluster-setup/project-management/#introduction","title":"Introduction","text":"<p>The Administrator creates Run:ai Projects via the Run:ai user interface. When enabling Researcher Authentication you also assign users to Projects.</p> <p>Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically:</p> <ol> <li>Creates a namespace by the name of <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> <li>Labels the namespace as managed by Run:ai.</li> <li>Provides access to the namespace for Run:ai services.</li> <li>Associates users with the namespace.</li> </ol> <p>This process may need to be altered if,</p> <ul> <li>Researchers already have existing Kubernetes namespaces</li> <li>The organization's Kubernetes namespace naming convention does not allow the <code>runai-</code> prefix.</li> <li>The organization's policy does not allow the automatic creation of namespaces.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/project-management/#process","title":"Process","text":"<p>Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace:</p> <ul> <li>Disable namespace creation by setting the cluster flag <code>createNamespaces</code> to <code>false</code>. For more information see Advanced Cluster Configuration </li> <li>Using the Run:ai User Interface, create a new Project <code>&lt;PROJECT-NAME&gt;</code>. A namespace will not be created.</li> <li>Associate and existing namepace <code>&lt;NAMESPACE&gt;</code> with the Run:ai project by running:</li> </ul> <pre><code>kubectl label ns &lt;NAMESPACE&gt;  runai/queue=&lt;PROJECT_NAME&gt;\n</code></pre> <p>Caution</p> <p>Setting the <code>createNamespaces</code> flag to <code>false</code> moves the responsibility of creating namespaces to match Run:ai Projects to the administrator.</p>"},{"location":"admin/runai-setup/self-hosted/overview/","title":"Self Hosted Run:ai Installation","text":"<p>The self-hosted option is for organizations that cannot use a SaaS solution due to data leakage concerns.</p> <p>Run:ai self-hosting comes with two variants:</p> Self-hosting Type Description Connected The organization can freely download from the internet (though upload is not allowed) Air-gapped The organization has no connection to the internet  <p>The self-hosted installation is priced differently. For further information please talk to Run:ai sales. </p>"},{"location":"admin/runai-setup/self-hosted/overview/#self-hosting-with-kubernetes-vs-openshift","title":"Self-hosting with Kubernetes vs OpenShift","text":"<p>Run:ai has been certified with a specified set of Kubernetes distributions. The OpenShift installation is different from the rest. As such, the Run:ai self-hosted installation instructions are divided into two separate sections:</p> <ul> <li>OpenShift-based installation. See Run:ai OpenShift installation. The Run:ai operator for OpenShift is certified by Red Hat.</li> <li>Kubernetes-based installation. See Run:ai Kubernetes installation.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/additional-clusters/","title":"Installing additional Clusters","text":"<p>The first Run:ai cluster is typically installed on the same Kubernetes cluster as the Run:ai control plane. Run:ai supports multiple clusters per single control plane. This document is about installing additional clusters on different Kubernetes clusters.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/additional-clusters/#installation","title":"Installation","text":"<p>Follow the Run:ai SaaS installation network instructions as described in Domain name requirement.  Specifically:</p> <ol> <li>Install Run:ai prerequisites. Including ingress controller and Prometheus. </li> <li>The Cluster should have a dedicated URL with a trusted certificate.</li> <li>Create a secret in the Run:ai namespace containing the details of a trusted certificate. </li> <li>Run the <code>helm</code> command as instructed.  </li> </ol>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/","title":"Install the Run:ai Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/k8s/backend/#prerequisites-and-preparations","title":"Prerequisites and preparations","text":"<p>Make sure you have followed the Control Plane prerequisites and preparations.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#helm-install","title":"Helm install","text":"<p>Run the helm command below:</p> ConnectedAirgapped <pre><code>helm repo add runai-backend https://runai.jfrog.io/artifactory/cp-charts-prod\nhelm repo update\nhelm upgrade -i runai-backend -n runai-backend runai-backend/control-plane --version \"~2.19.0\" \\\n    --set global.domain=&lt;DOMAIN&gt;  # (1)\n</code></pre> <ol> <li>Domain name described here. </li> </ol> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-backend</code>.</p> <pre><code>helm upgrade -i runai-backend control-plane-&lt;VERSION&gt;.tgz  \\ # (1)\n    --set global.domain=&lt;DOMAIN&gt;  \\ # (2)\n    --set global.customCA.enabled=true \\  # (3)\n    -n runai-backend -f custom-env.yaml  # (4)\n</code></pre> <ol> <li>Replace <code>&lt;VERSION&gt;</code> with the Run:ai control plane version.</li> <li>Domain name described here. </li> <li>See the Local Certificate Authority instructions below</li> <li><code>custom-env.yaml</code> should have been created by the prepare installation script in the previous section. </li> </ol> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#additional-runai-configurations-optional","title":"Additional Run:ai configurations (optional)","text":"<p>There may be cases where you need to set additional properties, To apply the changes run <code>helm upgrade</code> and use <code>--set</code> to set specific configurations, and restart the relevant Run:ai pods so they can fetch the new configurations.</p> Key Change Description <code>global.ingress.ingressClass</code> Ingress class Run:ai default is using NGINX. If your cluster has a different ingress controller, you can configure the ingress class to be created by Run:ai <code>global.ingress.tlsSecretName</code> TLS secret name Run:ai requires the creation of a secret with domain certificate. If the <code>runai-backend</code> namespace already had such a secret, you can set the secret name here <code>&lt;component&gt;</code> <code>resources:</code> <code>limits:</code> <code>cpu: 500m</code> <code>memory: 512Mi</code> <code>requests:</code> <code>cpu: 250m</code> <code>memory: 256Mi</code> Pod request and limits Set Run:ai and 3rd party services' resources <code>disableIstioSidecarInjection.enabled</code> Disable Istio sidecar injection Disable the automatic injection of Istio sidecars across the entire Run:ai Control Plane services."},{"location":"admin/runai-setup/self-hosted/k8s/backend/#additional-3rd-party-configurations-optional","title":"Additional 3rd party configurations (optional)","text":"<p>The Run:ai Control Plane chart, includes multiple sub-charts of 3rd party components:</p> <ul> <li>PostgreSQL - Data store</li> <li>Thanos - Metrics Store</li> <li>Keycloakx - Identity &amp; Access Management</li> <li>Grafana - Analytics Dashboard</li> <li>Redis - Caching (Disabled, by default)</li> </ul> <p>Tip</p> <p>Click on any component, to view it's chart values and configurations</p> <p>If you have opted to connect to an external PostgreSQL database, refer to the additional configurations table below. Adjust the following parameters based on your connection details:</p> <ol> <li>Disable PostgreSQL deployment - <code>postgresql.enabled</code></li> <li>Run:ai connection details - <code>global.postgresql.auth</code></li> <li>Grafana connection details - <code>grafana.dbUser</code>, <code>grafana.dbPassword</code></li> </ol>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#postgresql","title":"PostgreSQL","text":"Key Change Description <code>postgresql.enabled</code> PostgreSQL installation If set to <code>false</code> the PostgreSQL will not be installed <code>global.postgresql.auth.host</code> PostgreSQL host Hostname or IP address of the PostgreSQL server <code>global.postgresql.auth.port</code> PostgreSQL port Port number on which PostgreSQL is running <code>global.postgresql.auth.username</code> PostgreSQL username Username for connecting to PostgreSQL <code>global.postgresql.auth.password</code> PostgreSQL password Password for the PostgreSQL user specified by <code>global.postgresql.auth.username</code> <code>global.postgresql.auth.postgresPassword</code> PostgreSQL default admin password Password for the built-in PostgreSQL superuser (<code>postgres</code>) <code>global.postgresql.auth.existingSecret</code> Postgres Credentials (secret) Existing secret name with authentication credentials <code>global.postgresql.auth.dbSslMode</code> Postgres connection SSL mode Set the SSL mode, see list in Protection Provided in Different Modes, <code>prefer</code> mode is not supported <code>postgresql.primary.initdb.password</code> PostgreSQL default admin password Set the same password as in <code>global.postgresql.auth.postgresPassword</code> (if changed) <code>postgresql.primary.persistence.storageClass</code> Storage class The installation to work with a specific storage class rather than the default one"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#thanos","title":"Thanos","text":"Key Change Description <code>thanos.receive.persistence.storageClass</code> Storage class The installation to work with a specific storage class rather than the default one"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#keycloakx","title":"Keycloakx","text":"Key Change Description <code>keycloakx.adminUser</code> User name of the internal identity provider administrator This user is the administrator of Keycloak <code>keycloakx.adminPassword</code> Password of the internal identity provider administrator This password is for the administrator of Keycloak <code>keycloakx.existingSecret</code> Keycloakx Credentials (secret) Existing secret name with authentication credentials <code>global.keycloakx.host</code> KeyCloak (Run:ai internal identity provider) host path Override the DNS for Keycloak. This can be used to access Keycloak from outside the Run:ai Control Plane cluster via ingress <p>The <code>keycloakx.adminUser</code> can only be set during the initial installation. The admin password, however, can also be changed later through the Keycloak UI, but you must also update the <code>keycloakx.adminPassword</code> value in the Helm chart using helm upgrade. Failing to update the Helm values after changing the password can lead to control plane services encountering errors.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#grafana","title":"Grafana","text":"Key Change Description <code>grafana.db.existingSecret</code> Grafana database connection credentials (secret) Existing secret name with authentication credentials <code>grafana.dbUser</code> Grafana database username Username for accessing the Grafana database <code>grafana.dbPassword</code> Grafana database password Password for the Grafana database user <code>grafana.admin.existingSecret</code> Grafana admin default credentials (secret) Existing secret name with authentication credentials <code>grafana.adminUser</code> Grafana username Override the Run:ai default user name for accessing Grafana <code>grafana.adminPassword</code> Grafana password Override the Run:ai default password for accessing Grafana"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#redis","title":"Redis","text":"Key Change Description <code>redisCache.auth.password</code> Redis (Runai internal cache mechanism) applicative password Override the default password <code>redisCache.auth.existingSecret</code> Redis credentials (secret) Existing secret name with authentication credentials"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#next-steps","title":"Next Steps","text":""},{"location":"admin/runai-setup/self-hosted/k8s/backend/#connect-to-runai-user-interface","title":"Connect to Run:ai User interface","text":"<p>Go to: <code>runai.&lt;domain&gt;</code>. Log in using the default credentials: User: <code>test@run.ai</code>, Password: <code>Abcd!234</code>. Go to the Users area and change the password.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#enable-forgot-password-optional","title":"Enable Forgot Password (optional)","text":"<p>To support the Forgot password functionality, follow the steps below.</p> <ul> <li>Go to <code>runai.&lt;domain&gt;/auth</code> and Log in.</li> <li>Under <code>Realm settings</code>, select the <code>Login</code> tab and enable the <code>Forgot password</code> feature.</li> <li>Under the <code>Email</code> tab, define an SMTP server, as explained here</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#install-runai-cluster","title":"Install Run:ai Cluster","text":"<p>Continue with installing a Run:ai Cluster.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/","title":"Self Hosted installation over Kubernetes - Cluster Setup","text":""},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#prerequisites","title":"Prerequisites","text":"<p>Install prerequisites as per System Requirements document.</p> <p>Note</p> <p>For self-hosted deployments, Kubernetes Ingress Controller and Cluster Fully Qualified Domain Name (FQDN) requirements are only necessary when the Run:ai Control Plane and Run:ai Cluster reside on seperate Kuebrnetes clusters.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#install-cluster","title":"Install Cluster","text":"ConnectedAirgapped <p>Perform the cluster installation instructions explained here.</p> <p>Perform the cluster installation instructions explained here.</p> <p>On the second tab of the cluster wizard, when copying the helm command for installation, you will need to use the pre-provided installation file instead of using helm repositories. As such:</p> <ul> <li>Do not add the helm repository and do not run <code>helm repo update</code>.</li> <li>Instead, edit the <code>helm upgrade</code> command. <ul> <li>Replace <code>runai/runai-cluster</code> with <code>runai-cluster-&lt;version&gt;.tgz</code>. </li> <li>Add  <code>--set global.image.registry=&lt;Docker Registry address&gt;</code> where the registry address is as entered in the preparation section</li> </ul> </li> </ul> <p>The command should look like the following:</p> <pre><code>helm upgrade -i runai-cluster runai-cluster-&lt;version&gt;.tgz \\\n    --set controlPlane.url=... \\\n    --set controlPlane.clientSecret=... \\\n    --set cluster.uid=... \\\n    --set cluster.url=... --create-namespace \\\n    --set global.image.registry=registry.mycompany.local \\\n</code></pre> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation. For more details see Understanding cluster access roles.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#optional-customize-installation","title":"(Optional) Customize Installation","text":"<p>To customize specific aspects of the cluster installation see customize cluster installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/next-steps/","title":"Next Steps","text":"<ul> <li>Create additional I Users.</li> <li>Set up Project-based Researcher Access Control.</li> <li>Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users.</li> <li>Review advanced setup and maintenace scenarios.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/","title":"Preparing for a Run:ai Kubernetes installation","text":"<p>The following section provides IT with the information needed to prepare for a Run:ai installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#prerequisites","title":"Prerequisites","text":"<p>Follow the prerequisites as explained in Self-Hosted installation over Kubernetes.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#software-artifacts","title":"Software artifacts","text":"ConnectedAirgapped <p>You should receive a file: <code>runai-reg-creds.yaml</code> from Run:ai Customer Support. The file provides access to the Run:ai Container registry.</p> <p>SSH into a node with <code>kubectl</code> access to the cluster and <code>Docker</code> installed. Run the following to enable image download from the Run:ai Container Registry on Google cloud:</p> <pre><code>kubectl create namespace runai-backend\nkubectl apply -f runai-reg-creds.yaml\n</code></pre> <p>You should receive a single file <code>runai-air-gapped-&lt;VERSION&gt;.tar.gz</code> from Run:ai customer support</p> <p>SSH into a node with <code>kubectl</code> access to the cluster and <code>Docker</code> installed.</p> <p>Run:ai assumes the existence of a Docker registry for images. Most likely installed within the organization. The installation requires the network address and port for the registry (referenced below as <code>&lt;REGISTRY_URL&gt;</code>). </p> <p>To extract Run:ai files, replace <code>&lt;VERSION&gt;</code> in the command below and run: </p> <pre><code>tar xvf runai-airgapped-package-&lt;VERSION&gt;.tar.gz\n\nkubectl create namespace runai-backend\n</code></pre> <p>Upload images</p> <p>Upload images to a local Docker Registry. Set the Docker Registry address in the form of <code>NAME:PORT</code> (do not add <code>https</code>):</p> <pre><code>export REGISTRY_URL=&lt;Docker Registry address&gt;\n</code></pre> <p>Run the following script (you must dockerd installed and at least 20GB of free disk space to run): </p> <pre><code>sudo -E ./setup.sh\n</code></pre> <p>If Docker is configured to run as non-root then <code>sudo</code> is not required.</p> <p>The script should create a file named <code>custom-env.yaml</code> which will be used by the control-plane installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#private-docker-registry-optional","title":"Private Docker Registry (optional)","text":"<p>To access the organization's docker registry it is required to set the registry's credentials (imagePullSecret)</p> <p>Create the secret named <code>runai-reg-creds</code> based on your existing credentials. For more information, see Pull an Image from a Private Registry.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#configure-your-environment","title":"Configure your environment","text":""},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#domain-certificate","title":"Domain Certificate","text":"<p>The Run:ai control plane requires a domain name (FQDN). You must supply a domain name as well as a trusted certificate for that domain.</p> <ul> <li>When installing the first Run:ai cluster on the same Kubernetes cluster as the control plane, the Run:ai cluster URL will be the same as the control-plane URL.</li> <li>When installing the Run:ai cluster on a separate Kubernetes cluster, follow the Run:ai Domain name requirement.</li> <li>If your network is air-gapped, you will need to provide the Run:ai control-plane and cluster with information about the local certificate authority.</li> </ul> <p>You must provide the domain's private key and crt as a Kubernetes secret in the <code>runai-backend</code> namespace. Run:</p> <pre><code>kubectl create secret tls runai-backend-tls -n runai-backend \\\n    --cert /path/to/fullchain.pem --key /path/to/private.pem\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#local-certificate-authority-air-gapped-only","title":"Local Certificate Authority (air-gapped only)","text":"<p>In air-gapped environments, you must prepare the public key of your local certificate authority as described here. It will need to be installed in Kubernetes for the installation to succeed.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#mark-runai-system-workers-optional","title":"Mark Run:ai system workers (optional)","text":"<p>You can optionally set the Run:ai control plane to run on specific nodes. Kubernetes will attempt to schedule Run:ai pods to these nodes. If lacking resources, the Run:ai nodes will move to another, non-labeled node.  </p> <p>To set system worker nodes run:</p> <pre><code>kubectl label node &lt;NODE-NAME&gt; node-role.kubernetes.io/runai-system=true\n</code></pre> <p>Warning</p> <p>Do not select the Kubernetes master as a <code>runai-system</code> node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#external-postgres-database-optional","title":"External Postgres database (optional)","text":"<p>If you have opted to use an external PostgreSQL database, you need to perform initial setup to ensure successful installation. Follow these steps:</p> <ol> <li> <p>Create a SQL script file, edit the parameters below, and save it locally:</p> <ul> <li>Replace <code>&lt;DATABASE_NAME&gt;</code> with a dedicate database name for RunAi in your PostgreSQL database.</li> <li>Replace <code>&lt;ROLE_NAME&gt;</code> with a dedicated role name (user) for RunAi database.</li> <li>Replace <code>&lt;ROLE_PASSWORD&gt;</code> with a password for the new PostgreSQL role.</li> <li>Replace <code>&lt;GRAFANA_PASSWORD&gt;</code> with the password to be set for Grafana integration.</li> </ul> <pre><code>-- Create a new database for runai\nCREATE DATABASE &lt;DATABASE_NAME&gt;; \n\n-- Create the role with login and password\nCREATE ROLE &lt;ROLE_NAME&gt;  WITH LOGIN PASSWORD '&lt;ROLE_PASSWORD&gt;'; \n\n-- Grant all privileges on the database to the role\nGRANT ALL PRIVILEGES ON DATABASE &lt;DATABASE_NAME&gt; TO &lt;ROLE_NAME&gt;; \n\n-- Connect to the newly created database\n\\c &lt;DATABASE_NAME&gt; \n\n-- grafana\nCREATE ROLE grafana WITH LOGIN PASSWORD '&lt;GRAFANA_PASSWORD&gt;'; \nCREATE SCHEMA grafana authorization grafana;\nALTER USER grafana set search_path='grafana';\n-- Exit psql\n\\q\n</code></pre> </li> <li> <p>Run the following command on a machine where PostgreSQL client (<code>pgsql</code>) is installed:</p> <pre><code>psql --host &lt;POSTGRESQL_HOST&gt; \\ # (1)\n--user &lt;POSTGRESQL_USER&gt; \\ # (2)\n--port &lt;POSTGRESQL_PORT&gt; \\ # (3)\n--dbname &lt;POSTGRESQL_DB&gt; \\ # (4)\n-a -f &lt;SQL_FILE&gt; \\ # (5)\n</code></pre> <ol> <li>Replace <code>&lt;POSTGRESQL_HOST&gt;</code> with the PostgreSQL ip address or hostname.</li> <li>Replace <code>&lt;POSTGRESQL_USER&gt;</code> with the PostgreSQL username.</li> <li>Replace <code>&lt;POSTGRESQL_PORT&gt;</code> with the port number where PostgreSQL is running.</li> <li>Replace <code>&lt;POSTGRESQL_DB&gt;</code> with the name of your PostgreSQL database.</li> <li>Replace <code>&lt;POSTGRESQL_DB&gt;</code> with the name of your PostgreSQL database.</li> <li>Replace <code>&lt;SQL_FILE&gt;</code> with the path to the SQL script created in the previous step.</li> </ol> </li> </ol>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#additional-permissions","title":"Additional permissions","text":"<p>As part of the installation, you will be required to install the Run:ai Control Plane and Cluster Helm Charts. The Helm Charts require Kubernetes administrator permissions. You can review the exact permissions provided by using the <code>--dry-run</code> on both helm charts.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#validate-prerequisites","title":"Validate Prerequisites","text":"<p>Once you believe that the Run:ai prerequisites and preperations are met, we highly recommend installing and running the Run:ai pre-install diagnostics script. The tool:</p> <ul> <li>Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking.</li> <li>Looks at additional components installed and analyze their relevance to a successful Run:ai installation.</li> </ul> <p>To use the script download the latest version of the script and run:</p> <pre><code>chmod +x preinstall-diagnostics-&lt;platform&gt;\n./preinstall-diagnostics-&lt;platform&gt; --domain &lt;dns-entry&gt;\n</code></pre> <p>If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file <code>runai-preinstall-diagnostics.txt</code> in the current directory and send it to Run:ai technical support.</p> <p>For more information on the script including additional command-line flags, see here.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#next-steps","title":"Next steps","text":"<p>Continue with installing the Run:ai Control Plane.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/","title":"Self-Hosted installation over Kubernetes - Prerequisites","text":"<p>Before proceeding with this document, please review the installation types documentation to understand the difference between air-gapped and connected installations. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#runai-components","title":"Run:ai Components","text":"<p>As part of the installation process you will install:</p> <ul> <li>A control-plane managing cluster</li> <li>One or more clusters</li> </ul> <p>Both the control plane and clusters require Kubernetes. Typically the control plane and first cluster are installed on the same Kubernetes cluster but this is not a must. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#installer-machine","title":"Installer machine","text":"<p>The machine running the installation script (typically the Kubernetes master) must have:</p> <ul> <li>At least 50GB of free space.</li> <li>Docker installed.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#helm","title":"Helm","text":"<p>Run:ai requires Helm 3.14 or later. To install Helm, see Installing Helm. If you are installing an air-gapped version of Run:ai, The Run:ai tar file contains the helm binary. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#cluster-hardware-requirements","title":"Cluster hardware requirements","text":"<p>The Run:ai control plane services require the following resources:</p> Component Required Capacity CPU 10 cores Memory 12GB Disk space 110GB <p>If Run:ai cluster is planned to be installed on the same cluster as the Run:ai control plane: Ensure the control plane requirements are in addition to the Run:ai cluster hardware requirements.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#runai-software-requirements","title":"Run:ai software requirements","text":""},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#cluster-nodes","title":"Cluster Nodes","text":"<p>See Run:ai Cluster prerequisites operating system requirements.</p> <p>Nodes are required to be synchronized by time using NTP (Network Time Protocol) for proper system functionality.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#kubernetes","title":"Kubernetes","text":"<p>See Run:ai Cluster prerequisites Kubernetes distribution requirements.</p> <p>The Run:ai control plane operating system prerequisites are identical.</p> <p>The Run:ai control-plane requires a default storage class to create persistent volume claims for Run:ai storage. The storage class, as per Kubernetes standards, controls the reclaim behavior: whether the Run:ai persistent data is saved or deleted when the Run:ai control plane is deleted. </p> <p>Note</p> <p>For a simple (nonproduction) storage class example see Kubernetes Local Storage Class. The storage class will set the directory <code>/opt/local-path-provisioner</code> to be used across all nodes as the path for provisioning persistent volumes.</p> <p>Then set the new storage class as default:</p> <pre><code>kubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#install-prerequisites","title":"Install prerequisites","text":""},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#ingress-controller","title":"Ingress Controller","text":"<p>The Run:ai control plane installation assumes an existing installation of NGINX as the ingress controller. You can follow the Run:ai Cluster prerequisites Kubernetes ingress controller installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#nvidia-gpu-operator","title":"NVIDIA GPU Operator","text":"<p>See Run:ai Cluster prerequisites NVIDIA GPU operator requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the NVIDIA prerequisites.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#prometheus","title":"Prometheus","text":"<p>See Run:ai Cluster prerequisites Prometheus requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the Prometheus prerequisites. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#inference-optional","title":"Inference (optional)","text":"<p>See Run:ai Cluster prerequisites Inference requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the Inference prerequisites. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#external-postgres-database-optional","title":"External Postgres database (optional)","text":"<p>The Run:ai control plane installation includes a default PostgreSQL database. However, you may opt to use an existing PostgreSQL database if you have specific requirements or preferences. Please ensure that your PostgreSQL database is version 16 or higher.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#next-steps","title":"Next steps","text":"<p>Continue to Preparing for a Run:ai Kubernetes Installation .</p>"},{"location":"admin/runai-setup/self-hosted/k8s/project-management/","title":"Self Hosted installation over Kubernetes - Create Projects","text":""},{"location":"admin/runai-setup/self-hosted/k8s/project-management/#introduction","title":"Introduction","text":"<p>The Administrator creates Run:ai Projects via the Run:ai user interface. When enabling Researcher Authentication you also assign users to Projects.</p> <p>Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically:</p> <ol> <li>Creates a namespace by the name of <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> <li>Labels the namespace as managed by Run:ai.</li> <li>Provides access to the namespace for Run:ai services.</li> <li>Associates users with the namespace.</li> </ol> <p>This process may need to be altered if,</p> <ul> <li>Researchers already have existing Kubernetes namespaces</li> <li>The organization's Kubernetes namespace naming convention does not allow the <code>runai-</code> prefix.</li> <li>The organization's policy does not allow the automatic creation of namespaces.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/project-management/#process","title":"Process","text":"<p>Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace:</p> <ul> <li>When setting up a Run:ai cluster, Disable namespace creation by setting the cluster flag <code>createNamespaces</code> to <code>false</code>.</li> <li>Using the Run:ai User Interface, create a new Project <code>&lt;PROJECT-NAME&gt;</code>. A namespace will not be created.</li> <li>Associate and existing namepace <code>&lt;NAMESPACE&gt;</code> with the Run:ai project by running:</li> </ul> <pre><code>kubectl label ns &lt;NAMESPACE&gt;  runai/queue=&lt;PROJECT_NAME&gt;\n</code></pre> <p>Caution</p> <p>Setting the <code>createNamespaces</code> flag to <code>false</code> moves the responsibility of creating namespaces to match Run:ai Projects to the administrator.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/","title":"Uninstall Run:ai","text":""},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/#uninstall-a-runai-cluster","title":"Uninstall a Run:ai Cluster","text":"<p>To uninstall the cluster see: cluster delete </p>"},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/#uninstall-the-runai-control-plane","title":"Uninstall the Run:ai Control Plane","text":"<p>To delete the control plane, run:</p> <pre><code>helm uninstall runai-backend -n runai-backend\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/","title":"Upgrade Run:ai","text":""},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#preparations","title":"Preparations","text":""},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#helm","title":"Helm","text":"<p>Run:ai requires Helm 3.14 or later. Before you continue, validate your installed helm client version. To install or upgrade Helm, see Installing Helm. If you are installing an air-gapped version of Run:ai, The Run:ai tar file contains the helm binary. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#software-files","title":"Software files","text":"ConnectedAirgapped <p>Run the helm command below:</p> <pre><code>helm repo add runai-backend https://runai.jfrog.io/artifactory/cp-charts-prod\nhelm repo update\n</code></pre> <ul> <li>Ask for a tar file <code>runai-air-gapped-&lt;NEW-VERSION&gt;.tar.gz</code> from Run:ai customer support. The file contains the new version you want to upgrade to. <code>&lt;NEW-VERSION&gt;</code> is the updated version of the Run:ai control plane.</li> <li>Upload the images as described here.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#before-upgrade","title":"Before upgrade","text":"<p>Before proceeding with the upgrade, it's crucial to apply the specific prerequisites associated with your current version of Run:ai and every version in between up to the version you are upgrading to.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-from-version-29","title":"Upgrade from version 2.9","text":"<p>Two significant changes to the control-plane installation have happened with version 2.12: PVC ownership, Ingress and installation customization. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#pvc-ownership","title":"PVC ownership","text":"<p>Run:ai will no longer directly create the PVCs that store Run:ai data (metrics and database). Instead, going forward,  * Run:ai requires a Kubernetes storage class to be installed. * The PVCs are created by the Kubernetes StatefulSets. </p> <p>The storage class, as per Kubernetes standards, controls the reclaim behavior: whether the data is saved or deleted when the Run:ai control plane is deleted.  </p> <p>To remove the ownership in an older installation, run:</p> <pre><code>kubectl patch pvc -n runai-backend pvc-thanos-receive  -p '{\"metadata\": {\"annotations\":{\"helm.sh/resource-policy\": \"keep\"}}}'\nkubectl patch pvc -n runai-backend pvc-postgresql  -p '{\"metadata\": {\"annotations\":{\"helm.sh/resource-policy\": \"keep\"}}}'\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#ingress","title":"Ingress","text":"<p>Delete the ingress object which will be recreated by the control plane upgrade</p> <pre><code>kubectl delete ing -n runai-backend runai-backend-ingress\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#installation-customization","title":"Installation customization","text":"<p>The Run:ai control-plane installation has been rewritten and is no longer using a backend values file. Instead, to customize the installation use standard <code>--set</code> flags. If you have previously customized the installation, you must now extract these customizations and add them as <code>--set</code> flag to the helm installation:</p> <ul> <li>Find previous customizations to the control plane if such exist. Run:ai provides a utility for that here <code>https://raw.githubusercontent.com/run-ai/docs/v2.13/install/backend/cp-helm-vals-diff.sh</code>. For information on how to use this utility please contact Run:ai customer support. </li> <li>Search for the customizations you found in the optional configurations table and add them in the new format. </li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-control-plane","title":"Upgrade Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-from-version-217-or-later","title":"Upgrade from version 2.17, or later","text":"ConnectedAirgapped <pre><code>helm get values runai-backend -n runai-backend &gt; runai_control_plane_values.yaml\nhelm upgrade runai-backend -n runai-backend runai-backend/control-plane --version \"~2.19.0\" -f runai_control_plane_values.yaml --reset-then-reuse-values\n</code></pre> <pre><code>helm get values runai-backend -n runai-backend &gt; runai_control_plane_values.yaml\nhelm upgrade runai-backend control-plane-&lt;NEW-VERSION&gt;.tgz -n runai-backend  -f runai_control_plane_values.yaml --reset-then-reuse-values\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-from-version-29_1","title":"Upgrade from version 2.9","text":"<ul> <li>Create a <code>tls secret</code> as described in the control plane installation. </li> <li>Upgrade the control plane as described in the control plane installation. During the upgrade, you must tell the installation not to create the two PVCs:</li> </ul> ConnectedAirgapped <pre><code>helm upgrade -i runai-backend -n runai-backend runai-backend/control-plane --version \"~2.19.0\" \\\n--set global.domain=&lt;DOMAIN&gt; \\\n--set postgresql.primary.persistence.existingClaim=pvc-postgresql \\ \n--set thanos.receive.persistence.existingClaim=pvc-thanos-receive \n</code></pre> <p>Note</p> <p>The helm repository name has changed from <code>runai-backend/runai-backend</code> to <code>runai-backend/control-plane</code>.</p> <pre><code>helm upgrade -i runai-backend control-plane-&lt;NEW-VERSION&gt;.tgz -n runai-backend \\\n--set global.domain=&lt;DOMAIN&gt; \\\n--set postgresql.primary.persistence.existingClaim=pvc-postgresql \\ \n--set thanos.receive.persistence.existingClaim=pvc-thanos-receive \n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-cluster","title":"Upgrade Cluster","text":"<p>To upgrade the cluster follow the instructions here.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/additional-clusters/","title":"Installing additional clusters","text":"<p>The first Run:ai cluster is typically installed on the same OpenShift cluster as the Run:ai control plane. Run:ai supports multiple clusters per single control plane. This document is about installing additional clusters on different OpenShift clusters.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/additional-clusters/#additional-cluster-installation","title":"Additional cluster installation","text":"<p>Create a new cluster, then:</p> <ul> <li>Select a target platform <code>OpenShift</code></li> <li>Select a Cluster location <code>Remote to Control Plane</code>.</li> <li>You must enter a specific cluster URL with the format <code>https://runai.apps.&lt;BASE_DOMAIN&gt;</code>. To get the base Domain run <code>oc get dns cluster -oyaml | grep baseDomain</code></li> <li>Ignore the instructions for creating a secret.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/","title":"Install the Run:ai Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/ocp/backend/#prerequisites-and-preparations","title":"Prerequisites and preparations","text":"<p>Make sure you have followed the Control Plane prerequisites and preparations.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#helm-install","title":"Helm Install","text":"<p>Run the helm command below:</p> ConnectedAirgapped <pre><code>helm repo add runai-backend https://runai.jfrog.io/artifactory/cp-charts-prod\nhelm repo update\nhelm upgrade -i runai-backend -n runai-backend runai-backend/control-plane --version \"~2.19.0\" \\\n    --set global.domain=runai.apps.&lt;OPENSHIFT-CLUSTER-DOMAIN&gt; \\ # (1)\n    --set global.config.kubernetesDistribution=openshift\n</code></pre> <ol> <li>The subdomain configured for the OpenShift cluster.</li> </ol> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-backend</code>.</p> <pre><code>helm upgrade -i runai-backend  ./control-plane-&lt;version&gt;.tgz -n runai-backend \\\n    --set global.domain=runai.apps.&lt;OPENSHIFT-CLUSTER-DOMAIN&gt; \\ # (1)\n    --set global.config.kubernetesDistribution=openshift \\\n    --set global.customCA.enabled=true \\ # (2)\n    -f custom-env.yaml  # (3)\n</code></pre> <ol> <li>The domain configured for the OpenShift cluster. To find out the OpenShift cluster domain, run <code>oc get routes -A</code></li> <li>See the Local Certificate Authority instructions below</li> <li><code>custom-env.yaml</code> should have been created by the prepare installation script in the previous section. </li> </ol> <p>(replace <code>&lt;version&gt;</code> with the control plane version)</p> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#additional-runai-configurations-optional","title":"Additional Run:ai configurations (optional)","text":"<p>There may be cases where you need to set additional properties, To apply the changes run <code>helm upgrade</code> and use <code>--set</code> to set specific configurations, and restart the relevant Run:ai pods so they can fetch the new configurations.</p> Key Change Description <code>&lt;component&gt;</code> <code>resources:</code> <code>limits:</code> <code>cpu: 500m</code> <code>memory: 512Mi</code> <code>requests:</code> <code>cpu: 250m</code> <code>memory: 256Mi</code> Pod request and limits Set Run:ai and 3rd party services' resources <code>disableIstioSidecarInjection.enabled</code> Disable Istio sidecar injection Disable the automatic injection of Istio sidecars across the entire Run:ai Control Plane services."},{"location":"admin/runai-setup/self-hosted/ocp/backend/#additional-3rd-party-configurations-optional","title":"Additional 3rd party configurations (optional)","text":"<p>The Run:ai Control Plane chart, includes multiple sub-charts of 3rd party components:</p> <ul> <li>PostgreSQL - Data store</li> <li>Keycloakx - Identity &amp; Access Management</li> <li>Grafana - Analytics Dashboard</li> <li>Redis - Caching (Disabled, by default)</li> </ul> <p>Tip</p> <p>Click on any component, to view it's chart values and configurations</p> <p>If you have opted to connect to an external PostgreSQL database, refer to the additional configurations table below. Adjust the following parameters based on your connection details:</p> <ol> <li>Disable PostgreSQL deployment - <code>postgresql.enabled</code></li> <li>Run:ai connection details - <code>global.postgresql.auth</code></li> <li>Grafana connection details - <code>grafana.dbUser</code>, <code>grafana.dbPassword</code></li> </ol>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#postgresql","title":"PostgreSQL","text":"Key Change Description <code>postgresql.enabled</code> PostgreSQL installation If set to <code>false</code> the PostgreSQL will not be installed <code>global.postgresql.auth.host</code> PostgreSQL host Hostname or IP address of the PostgreSQL server <code>global.postgresql.auth.port</code> PostgreSQL port Port number on which PostgreSQL is running <code>global.postgresql.auth.username</code> PostgreSQL username Username for connecting to PostgreSQL <code>global.postgresql.auth.password</code> PostgreSQL password Password for the PostgreSQL user specified by <code>global.postgresql.auth.username</code> <code>global.postgresql.auth.postgresPassword</code> PostgreSQL default admin password Password for the built-in PostgreSQL superuser (<code>postgres</code>) <code>global.postgresql.auth.existingSecret</code> Postgres Credentials (secret) Existing secret name with authentication credentials <code>global.postgresql.auth.dbSslMode</code> Postgres connection SSL mode Set the SSL mode, see list in Protection Provided in Different Modes, <code>prefer</code> mode is not supported <code>postgresql.primary.initdb.password</code> PostgreSQL default admin password Set the same password as in <code>global.postgresql.auth.postgresPassword</code> (if changed) <code>postgresql.primary.persistence.storageClass</code> Storage class The installation to work with a specific storage class rather than the default one"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#keycloakx","title":"Keycloakx","text":"Key Change Description <code>keycloakx.adminUser</code> User name of the internal identity provider administrator This user is the administrator of Keycloak <code>keycloakx.adminPassword</code> Password of the internal identity provider administrator This password is for the administrator of Keycloak <code>keycloakx.existingSecret</code> Keycloakx credentials (secret) Existing secret name with authentication credentials <code>global.keycloakx.host</code> KeyCloak (Run:ai internal identity provider) host path Override the DNS for Keycloak. This can be used to access Keycloak from outside the Run:ai Control Plane cluster via ingress <p>The <code>keycloakx.adminUser</code> can only be set during the initial installation. The admin password, however, can also be changed later through the Keycloak UI, but you must also update the <code>keycloakx.adminPassword</code> value in the Helm chart using helm upgrade. Failing to update the Helm values after changing the password can lead to control plane services encountering errors.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#grafana","title":"Grafana","text":"Key Change Description <code>grafana.db.existingSecret</code> Grafana database connection credentials (secret) Existing secret name with authentication credentials <code>grafana.dbUser</code> Grafana database username Username for accessing the Grafana database <code>grafana.dbPassword</code> Grafana database password Password for the Grafana database user <code>grafana.admin.existingSecret</code> Grafana admin default credentials (secret) Existing secret name with authentication credentials <code>grafana.adminUser</code> Grafana username Override the Run:ai default user name for accessing Grafana <code>grafana.adminPassword</code> Grafana password Override the Run:ai default password for accessing Grafana"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#redis","title":"Redis","text":"Key Change Description <code>redisCache.auth.password</code> Redis (Runai internal cache mechanism) applicative password Override the default password <code>redisCache.auth.existingSecret</code> Redis credentials (secret) Existing secret name with authentication credentials"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#next-steps","title":"Next steps","text":""},{"location":"admin/runai-setup/self-hosted/ocp/backend/#connect-to-runai-user-interface","title":"Connect to Run:ai user interface","text":"<ul> <li>Run: <code>oc get routes -n runai-backend</code> to find the Run:ai Administration User Interface URL.</li> <li>Log in using the default credentials: User: <code>test@run.ai</code>, Password: <code>Abcd!234</code>.</li> <li>Go to the Users area and change the password.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#enable-forgot-password-optional","title":"Enable Forgot Password (optional)","text":"<p>To support the Forgot password functionality, follow the steps below.</p> <ul> <li>Go to <code>runai.&lt;openshift-cluster-domain&gt;/auth</code> and Log in.</li> <li>Under <code>Realm settings</code>, select the <code>Login</code> tab and enable the <code>Forgot password</code> feature.</li> <li>Under the <code>Email</code> tab, define an SMTP server, as explained here</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#install-runai-cluster","title":"Install Run:ai Cluster","text":"<p>Continue with installing a Run:ai Cluster.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/","title":"Self-Hosted installation over OpenShift - Cluster Setup","text":""},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#prerequisites","title":"Prerequisites","text":"<p>Install prerequisites as per System Requirements document.  </p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#create-openshift-projects","title":"Create OpenShift Projects","text":"<p>Run:ai cluster installation uses several namespaces (or projects in OpenShift terminology). Run the following:</p> <pre><code>oc new-project runai\noc new-project runai-reservation\noc new-project runai-scale-adjust\n</code></pre> <p>The last namespace (<code>runai-scale-adjust</code>) is only required if the cluster is a cloud cluster and is configured for auto-scaling.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#cluster-installation","title":"Cluster Installation","text":"ConnectedAirgapped <p>Perform the cluster installation instructions explained in Cluster install. When creating a new cluster, select the OpenShift  target platform.</p> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-cluster</code>.</p> <p>Perform the cluster installation instructions explained in Cluster install. When creating a new cluster, select the OpenShift  target platform.</p> <p>On the second tab of the cluster wizard, when copying the helm command for installation, you will need to use the pre-provided installation file instead of using helm repositories. As such:</p> <ul> <li>Do not add the helm repository and do not run <code>helm repo update</code>.</li> <li>Instead, edit the <code>helm upgrade</code> command. <ul> <li>Replace <code>runai/runai-cluster</code> with <code>runai-cluster-&lt;version&gt;.tgz</code>. </li> <li>Add  <code>--set global.image.registry=&lt;Docker Registry address&gt;</code> where the registry address is as entered in the preparation section</li> <li>Add <code>--set global.customCA.enabled=true</code> and perform the instructions for local certificate authority.</li> </ul> </li> </ul> <p>The command should look like the following: <pre><code>helm upgrade -i runai-cluster runai-cluster-&lt;version&gt;.tgz \\\n    --set controlPlane.url=... \\\n    --set controlPlane.clientSecret=... \\\n    --set cluster.uid=... \\\n    --set cluster.url=... --create-namespace \\\n    --set global.image.registry=registry.mycompany.local \\\n    --set global.customCA.enabled=true\n</code></pre></p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#optional-customize-installation","title":"(Optional) Customize Installation","text":"<p>To customize specific aspects of the cluster installation see customize cluster installation.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#next-steps","title":"Next Steps","text":"<p>Continue to create Run:ai Projects.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/next-steps/","title":"Next Steps","text":"<ul> <li>Create additional Run:ai Users.</li> <li>Set up Project-based Researcher Access Control.</li> <li>Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users.</li> <li>Review advanced setup and maintenace scenarios.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/","title":"Preparing for a Run:ai OpenShift installation","text":"<p>The following section provides IT with the information needed to prepare for a Run:ai installation.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#prerequisites","title":"Prerequisites","text":"<p>See the Prerequisites section above.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#software-artifacts","title":"Software artifacts","text":"ConnectedAirgapped <p>You should receive a file: <code>runai-reg-creds.yaml</code> from Run:ai Customer Support. The file provides access to the Run:ai Container registry.</p> <p>SSH into a node with <code>oc</code> access (<code>oc</code> is the OpenShift command line) to the cluster and <code>Docker</code> installed.</p> <p>Run the following to enable image download from the Run:ai Container Registry on Google cloud:</p> <pre><code>oc apply -f runai-reg-creds.yaml -n runai-backend\n</code></pre> <p>You should receive a single file <code>runai-&lt;version&gt;.tar</code> from Run:ai customer support</p> <p>Run:ai assumes the existence of a Docker registry for images. Most likely installed within the organization. The installation requires the network address and port for the registry (referenced below as <code>&lt;REGISTRY_URL&gt;</code>). </p> <p>SSH into a node with <code>oc</code> access (<code>oc</code> is the OpenShift command line) to the cluster and <code>Docker</code> installed.</p> <p>To extract Run:ai files, replace <code>&lt;VERSION&gt;</code> in the command below and run: </p> <p><pre><code>tar xvf runai-airgapped-package-&lt;VERSION&gt;.tar.gz\n</code></pre> Upload images</p> <p>Upload images to a local Docker Registry. Set the Docker Registry address in the form of <code>NAME:PORT</code> (do not add <code>https</code>):</p> <pre><code>export REGISTRY_URL=&lt;Docker Registry address&gt;\n</code></pre> <p>Run the following script (you must have at least 20GB of free disk space to run): </p> <pre><code>./setup.sh\n</code></pre> <p>(If docker is configured to run as non-root then <code>sudo</code> is not required).</p> <p>The script should create a file named custom-env.yaml which will be used by the control-plane installation.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#private-docker-registry-optional","title":"Private Docker Registry (optional)","text":"<p>To access the organization's docker registry it is required to set the registry's credentials (imagePullSecret)</p> <p>Create the secret named <code>runai-reg-creds</code> in the <code>runai-backend</code> namespace based on your existing credentials. The configuration will be copied over to the <code>runai</code> namespace at cluster install. For more information, see Allowing pods to reference images from other secured registries.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#configure-your-environment","title":"Configure your environment","text":""},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#create-openshift-project","title":"Create OpenShift project","text":"<p>The Run:ai control plane uses a namespace (or project in OpenShift terminology) name <code>runai-backend</code>. You must create it before installing:</p> <pre><code>oc new-project runai-backend\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#local-certificate-authority-air-gapped-only","title":"Local Certificate Authority (air-gapped only)","text":"<p>In Air-gapped environments, you must prepare the public key of your local certificate authority as described here. It will need to be installed in Kubernetes for the installation to succeed.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#mark-runai-system-workers-optional","title":"Mark Run:ai system workers (optional)","text":"<p>You can optionally set the Run:ai control plane to run on specific nodes. Kubernetes will attempt to schedule Run:ai pods to these nodes. If lacking resources, the Run:ai nodes will move to another, non-labeled node.  </p> <p>To set system worker nodes run:</p> <pre><code>kubectl label node &lt;NODE-NAME&gt; node-role.kubernetes.io/runai-system=true\n</code></pre> <p>Warning</p> <p>Do not select the Kubernetes master as a <code>runai-system</code> node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#external-postgres-database-optional","title":"External Postgres database (optional)","text":"<p>If you have opted to use an external PostgreSQL database, you need to perform initial setup to ensure successful installation. Follow these steps:</p> <ol> <li> <p>Create a SQL script file, edit the parameters below, and save it locally:</p> <ul> <li>Replace <code>&lt;DATABASE_NAME&gt;</code> with a dedicate database name for RunAi in your PostgreSQL database.</li> <li>Replace <code>&lt;ROLE_NAME&gt;</code> with a dedicated role name (user) for RunAi database.</li> <li>Replace <code>&lt;ROLE_PASSWORD&gt;</code> with a password for the new PostgreSQL role.</li> <li>Replace <code>&lt;GRAFANA_PASSWORD&gt;</code>  with the password to be set for Grafana integration.</li> </ul> <pre><code>-- Create a new database for runai\nCREATE DATABASE &lt;DATABASE_NAME&gt;; \n\n-- Create the role with login and password\nCREATE ROLE &lt;ROLE_NAME&gt;  WITH LOGIN PASSWORD '&lt;ROLE_PASSWORD&gt;'; \n\n-- Grant all privileges on the database to the role\nGRANT ALL PRIVILEGES ON DATABASE &lt;DATABASE_NAME&gt; TO &lt;ROLE_NAME&gt;; \n\n-- Connect to the newly created database\n\\c &lt;DATABASE_NAME&gt; \n\n-- grafana\nCREATE ROLE grafana WITH LOGIN PASSWORD '&lt;GRAFANA_PASSWORD&gt;'; \nCREATE SCHEMA grafana authorization grafana;\nALTER USER grafana set search_path='grafana';\n-- Exit psql\n\\q\n</code></pre> </li> <li> <p>Run the following command on a machine where PostgreSQL client (<code>pgsql</code>) is installed:</p> <pre><code>psql --host &lt;POSTGRESQL_HOST&gt; \\ # (1)\n--user &lt;POSTGRESQL_USER&gt; \\ # (2)\n--port &lt;POSTGRESQL_PORT&gt; \\ # (3)\n--dbname &lt;POSTGRESQL_DB&gt; \\ # (4)\n-a -f &lt;SQL_FILE&gt; \\ # (5)\n</code></pre> <ol> <li>Replace <code>&lt;POSTGRESQL_HOST&gt;</code> with the PostgreSQL ip address or hostname.</li> <li>Replace <code>&lt;POSTGRESQL_USER&gt;</code> with the PostgreSQL username.</li> <li>Replace <code>&lt;POSTGRESQL_PORT&gt;</code> with the port number where PostgreSQL is running.</li> <li>Replace <code>&lt;POSTGRESQL_DB&gt;</code> with the name of your PostgreSQL database.</li> <li>Replace <code>&lt;POSTGRESQL_DB&gt;</code> with the name of your PostgreSQL database.</li> <li>Replace <code>&lt;SQL_FILE&gt;</code> with the path to the SQL script created in the previous step.</li> </ol> </li> </ol>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#additional-permissions","title":"Additional permissions","text":"<p>As part of the installation, you will be required to install the Control plane and Cluster Helm Charts. The Helm Charts require Kubernetes administrator permissions. You can review the exact permissions provided by using the <code>--dry-run</code> on both helm charts.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#validate-prerequisites","title":"Validate prerequisites","text":"<p>Once you believe that the Run:ai prerequisites and preperations are met, we highly recommend installing and running the Run:ai pre-install diagnostics script. The tool:</p> <ul> <li>Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking.</li> <li>Looks at additional components installed and analyzes their relevancy to a successful Run:ai installation.</li> </ul> <p>To use the script download the latest version of the script and run:</p> <pre><code>chmod +x preinstall-diagnostics-&lt;platform&gt;\n./preinstall-diagnostics-&lt;platform&gt; \n</code></pre> <p>If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file <code>runai-preinstall-diagnostics.txt</code> in the current directory and send it to Run:ai technical support.</p> <p>For more information on the script including additional command-line flags, see here.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#next-steps","title":"Next steps","text":"<p>Continue with installing the Run:ai Control Plane.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/","title":"Self Hosted installation over OpenShift - prerequisites","text":"<p>Before proceeding with this document, please review the installation types documentation to understand the difference between air-gapped and connected installations. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#runai-components","title":"Run:ai components","text":"<p>As part of the installation process you will install:</p> <ul> <li>A control-plane managing cluster</li> <li>One or more clusters</li> </ul> <p>Both the control plane and clusters require Kubernetes. Typically the control plane and first cluster are installed on the same Kubernetes cluster but this is not a must. </p> <p>Important</p> <p>In OpenShift environments, adding a cluster connecting to a remote control plane currently requires the assistance of customer support. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#installer-machine","title":"Installer machine","text":"<p>The machine running the installation script (typically the Kubernetes master) must have:</p> <ul> <li>At least 50GB of free space.</li> <li>Docker installed. </li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#helm","title":"Helm","text":"<p>Run:ai requires Helm 3.14 or later. To install Helm, see Installing Helm. If you are installing an air-gapped version of Run:ai, The Run:ai tar file contains the helm binary. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#cluster-hardware-requirements","title":"Cluster hardware requirements","text":"<p>The Run:ai control plane services require the following resources:</p> Component Required Capacity CPU 10 cores Memory 12GB Disk space 110GB <p>If Run:ai cluster is planned to be installed on the same cluster as the Run:ai control plane: Ensure the control plane requirements are in addition to the Run:ai cluster hardware requirements.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#runai-software-requirements","title":"Run:ai software requirements","text":""},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#cluster-nodes","title":"Cluster Nodes","text":"<p>Nodes are required to be synchronized by time using NTP (Network Time Protocol) for proper system functionality.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#openshift","title":"OpenShift","text":"<p>Run:ai supports OpenShift. OpenShift Versions supported are detailed in Kubernetes distribution.</p> <ul> <li>OpenShift must be configured with a trusted certificate. Run:ai installation relies on OpenShift to create certificates for subdomains. </li> <li>OpenShift must have a configured identity provider (Idp). </li> <li>If your network is air-gapped, you will need to provide the Run:ai control-plane and cluster with information about the local certificate authority.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#install-prerequisites","title":"Install prerequisites","text":""},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#nvidia-gpu-operator","title":"NVIDIA GPU Operator","text":"<p>See Run:ai Cluster prerequisites installing NVIDIA dependencies in OpenShift.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the NVIDIA prerequisites.</p> <p>Information on how to download the GPU Operator for air-gapped installation can be found in the NVIDIA GPU Operator pre-requisites. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#inference-optional","title":"Inference (optional)","text":"<p>See Run:ai Cluster prerequisites Inference requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the Inference prerequisites. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#external-postgresql-database-optional","title":"External PostgreSQL database (optional)","text":"<p>The Run:ai control plane installation includes a default PostgreSQL database. However, you may opt to use an existing PostgreSQL database if you have specific requirements or preferences. Please ensure that your PostgreSQL database is version 16 or higher.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#next-steps","title":"Next steps","text":"<p>Continue to Preparing for a Run:ai OpenShift Installation .</p>"},{"location":"admin/runai-setup/self-hosted/ocp/project-management/","title":"Self Hosted installation over OpenShift - Create Projects","text":""},{"location":"admin/runai-setup/self-hosted/ocp/project-management/#introduction","title":"Introduction","text":"<p>The Administrator creates Run:ai Projects via the Run:ai User Interface. When enabling Researcher Authentication you also assign users to Projects.</p> <p>Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically:</p> <ol> <li>Creates a namespace by the name of <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> <li>Labels the namespace as managed by Run:ai.</li> <li>Provides access to the namespace for Run:ai services.</li> <li>Associates users with the namespace.</li> </ol> <p>This process may need to be altered if,</p> <ul> <li>Researchers already have existing Kubernetes namespaces</li> <li>The organization's Kubernetes namespace naming convention does not allow the <code>runai-</code> prefix.</li> <li>The organization's policy does not allow the automatic creation of namespaces</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/project-management/#process","title":"Process","text":"<p>Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace:</p> <ul> <li>When setting up a Run:ai cluster, Disable namespace creation by setting the cluster flag <code>createNamespaces</code> to <code>false</code>.</li> <li>Using the Run:ai User Interface, create a new Project <code>&lt;PROJECT-NAME&gt;</code>. A namespace will not be created.</li> <li>Associate and existing namepace <code>&lt;NAMESPACE&gt;</code> with the Run:ai project by running:</li> </ul> <pre><code>oc label ns &lt;NAMESPACE&gt;  runai/queue=&lt;PROJECT_NAME&gt;\n</code></pre> <p>Caution</p> <p>Setting the <code>createNamespaces</code> flag to <code>false</code> moves the responsibility of creating namespaces to match Run:ai Projects to the administrator.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/uninstall/","title":"Uninstall Run:ai","text":"<p>See uninstall section here</p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/","title":"Upgrade Run:ai","text":""},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#preparations","title":"Preparations","text":""},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#helm","title":"Helm","text":"<p>Run:ai requires Helm 3.14 or later. Before you continue, validate your installed helm client version. To install or upgrade Helm, see Installing Helm. If you are installing an air-gapped version of Run:ai, The Run:ai tar file contains the helm binary. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#software-files","title":"Software files","text":"ConnectedAirgapped <p>Run the helm command below:</p> <pre><code>helm repo add runai-backend https://runai.jfrog.io/artifactory/cp-charts-prod\nhelm repo update\n</code></pre> <ul> <li>Ask for a tar file <code>runai-air-gapped-&lt;NEW-VERSION&gt;.tar.gz</code> from Run:ai customer support. The file contains the new version you want to upgrade to. <code>&lt;NEW-VERSION&gt;</code> is the updated version of the Run:ai control plane.</li> <li>Upload the images as described here.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#before-upgrade","title":"Before upgrade","text":"<p>Before proceeding with the upgrade, it's crucial to apply the specific prerequisites associated with your current version of Run:ai and every version in between up to the version you are upgrading to.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-from-version-29","title":"Upgrade from version 2.9","text":"<p>Two significant changes to the control-plane installation have happened with version 2.12: PVC ownership and installation customization. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#pvc-ownership","title":"PVC ownership","text":"<p>Run:ai no longer directly creates the PVCs that store Run:ai data (metrics and database). Instead, going forward, </p> <ul> <li>Run:ai requires a Kubernetes storage class to be installed.</li> <li>The PVCs are created by the Kubernetes StatefulSets. </li> </ul> <p>The storage class, as per Kubernetes standards, controls the reclaim behavior: whether the data is saved or deleted when the Run:ai control plane is deleted.  </p> <p>To remove the ownership in an older installation, run:</p> <pre><code>kubectl patch pvc -n runai-backend pvc-postgresql  -p '{\"metadata\": {\"annotations\":{\"helm.sh/resource-policy\": \"keep\"}}}'\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#installation-customization","title":"Installation customization","text":"<p>The Run:ai control-plane installation has been rewritten and is no longer using a backend values file. Instead, to customize the installation use standard <code>--set</code> flags. If you have previously customized the installation, you must now extract these customizations and add them as <code>--set</code> flag to the helm installation:</p> <ul> <li>Find previous customizations to the control plane if such exist. Run:ai provides a utility for that here <code>https://raw.githubusercontent.com/run-ai/docs/v2.13/install/backend/cp-helm-vals-diff.sh</code>. For information on how to use this utility please contact Run:ai customer support. </li> <li>Search for the customizations you found in the optional configurations table and add them in the new format.  </li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-control-plane","title":"Upgrade Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-from-version-217-or-later","title":"Upgrade from version 2.17, or later","text":"ConnectedAirgapped <pre><code>helm get values runai-backend -n runai-backend &gt; runai_control_plane_values.yaml\nhelm upgrade runai-backend -n runai-backend runai-backend/control-plane --version \"~2.19.0\" -f runai_control_plane_values.yaml --reset-then-reuse-values\n</code></pre> <pre><code>helm get values runai-backend -n runai-backend &gt; runai_control_plane_values.yaml\nhelm upgrade runai-backend control-plane-&lt;NEW-VERSION&gt;.tgz -n runai-backend  -f runai_control_plane_values.yaml --reset-then-reuse-values\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-from-version-29_1","title":"Upgrade from version 2.9","text":"ConnectedAirgapped <pre><code>helm upgrade -i runai-backend -n runai-backend runai-backend/control-plane --version \"~2.19.0\" \\\n--set global.domain=runai.apps.&lt;OPENSHIFT-CLUSTER-DOMAIN&gt; \\ #(1)\n--set global.config.kubernetesDistribution=openshift \\\n--set thanos.query.stores={thanos-grpc-port-forwarder:10901} \\\n--set postgresql.primary.persistence.existingClaim=pvc-postgresql\n</code></pre> <ol> <li>The subdomain configured for the OpenShift cluster.</li> </ol> <p>Note</p> <p>The helm repository name has changed from <code>runai-backend/runai-backend</code> to <code>runai-backend/control-plane</code>.</p> <pre><code>helm upgrade -i runai-backend  ./control-plane-&lt;NEW-VERSION&gt;.tgz -n runai-backend \\\n--set global.domain=runai.apps.&lt;OPENSHIFT-CLUSTER-DOMAIN&gt; \\ #(1)\n--set global.config.kubernetesDistribution=openshift \\\n--set thanos.query.stores={thanos-grpc-port-forwarder:10901} \\\n--set postgresql.primary.persistence.existingClaim=pvc-postgresql\n</code></pre> <ol> <li>The subdomain configured for the OpenShift cluster.</li> </ol>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-cluster","title":"Upgrade Cluster","text":"<p>To upgrade the cluster follow the instructions here.</p>"},{"location":"admin/troubleshooting/diagnostics/","title":"Diagnostic Tools","text":""},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-the-database-container","title":"Add Verbosity to the Database container","text":"<p>Run:ai Self-hosted installation contains an internal database. To diagnose database issues, you can run the database in debug mode.</p> <p>In the runai-backend-values, search for <code>postgresql</code>. Add: </p> <pre><code>postgresql:\n  image:\n    debug: true\n</code></pre> <p>Re-install the Run:ai control-plane and then review the database logs by running: </p> <pre><code>kubectl logs -n runai-backend runai-postgresql-0\n</code></pre>"},{"location":"admin/troubleshooting/diagnostics/#internal-networking-issues","title":"Internal Networking Issues","text":"<p>Run:ai is based on Kubernetes. Kubernetes runs its own internal subnet with a separate DNS service. If you see in the logs that services have trouble connecting, the problem may reside there.  You can find further information on how to debug Kubernetes DNS here. Specifically, it is useful to start a pod with networking utilities and use it for network resolution:</p> <pre><code>kubectl run -i --tty netutils --image=dersimn/netutils -- bash\n</code></pre>"},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-prometheus","title":"Add Verbosity to Prometheus","text":"<p>Add verbosity to Prometheus by editing RunaiConfig:</p> <pre><code>kubectl edit runaiconfig runai -n runai\n</code></pre> <p>Add a <code>debug</code> log level:</p> <pre><code>prometheus-operator:\n  prometheus:\n    prometheusSpec:\n      logLevel: debug\n</code></pre> <p>To view logs, run: <pre><code>kubectl logs prometheus-runai-prometheus-operator-prometheus-0 prometheus \\\n      -n monitoring -f --tail 100\n</code></pre></p>"},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-scheduler","title":"Add Verbosity to Scheduler","text":"<p>To view extended logs run:</p> <pre><code>kubectl edit ruaiconfig runai -n runai\n</code></pre> <p>Then under the <code>scheduler</code> section add:</p> <pre><code>runai-scheduler:\n   args:\n     verbosity: 6\n</code></pre> <p>Warning</p> <p>Verbose scheduler logs consume a significant amount of disk space.</p>"},{"location":"admin/troubleshooting/logs-collection/","title":"Logs Collection","text":"<p>This article provides instructions for IT administrators on collecting Run:ai logs for support, including prerequisites, CLI commands, and log file retrieval. It also covers enabling verbose logging for Prometheus and the Run:ai Scheduler.</p>"},{"location":"admin/troubleshooting/logs-collection/#collect-logs-to-send-to-support","title":"Collect logs to send to support","text":"<p>To collect Run:ai logs, follow these steps:</p>"},{"location":"admin/troubleshooting/logs-collection/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ensure that you have administrator-level access to the Kubernetes cluster where Run:ai is installed.  </li> <li>The Run:ai Administrator Command-Line Interface (CLI) must be installed.</li> </ul>"},{"location":"admin/troubleshooting/logs-collection/#step-by-step-instructions","title":"Step-by-Step Instructions","text":"<ol> <li>Run the Command from your local machine or a Bastion Host (secure server)    Open a terminal on your local machine (or any machine that has network access to the Kubernetes cluster) where the Run:ai Administrator CLI is installed.  </li> <li> <p>Collect the Logs     Execute the following command to collect the logs:  </p> <pre><code>runai-adm collect-logs\n</code></pre> <p>This command gathers all relevant Run:ai logs from the system and generate a compressed file.</p> </li> <li> <p>Locate the Generated File    After running the command, note the location of the generated compressed log file. You can retrieve and send this file to Run:ai Support for further troubleshooting.</p> </li> </ol> <p>Note</p> <p>The tar file packages the logs of Run:ai components only. It does not include logs of researcher containers that may contain private information</p>"},{"location":"admin/troubleshooting/logs-collection/#logs-verbosity","title":"Logs verbosity","text":"<p>Increase log verbosity to capture more detailed information, providing deeper insights into system behavior and make it easier to identify and resolve issues.</p>"},{"location":"admin/troubleshooting/logs-collection/#prerequisites_1","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>Access to the Kubernetes cluster where Run:ai is installed  </li> <li>Including necessary permissions to view and modify configurations.  </li> <li>kubectl installed and configured:  </li> <li>The Kubernetes command-line tool, <code>kubectl</code>, must be installed and configured to interact with the cluster.  </li> <li>Sufficient privileges to edit configurations and view logs.  </li> <li>Monitoring Disk Space  </li> <li>When enabling verbose logging, ensure adequate disk space to handle the increased log output, especially when enabling debug or high verbosity levels.</li> </ul>"},{"location":"admin/troubleshooting/logs-collection/#adding-verbosity","title":"Adding verbosity","text":"Adding verbosity to Prometheus <p>To increase the logging verbosity for Prometheus, follow these steps:</p> <ol> <li>Edit the <code>RunaiConfig</code> to adjust Prometheus log levels. Copy the following command to your terminal:  </li> </ol> <pre><code>kubectl edit runaiconfig runai -n runai\n</code></pre> <ol> <li>In the configuration file that opens, add or modify the following section to set the log level to <code>debug</code>:  </li> </ol> <pre><code>spec:\n    prometheus:\n        spec:\n            logLevel: debug\n</code></pre> <ol> <li>Save the changes. To view the Prometheus logs with the new verbosity level, run:  </li> </ol> <pre><code>kubectl logs -n runai prometheus-runai-0\n</code></pre> <p>This command streams the last 100 lines of logs from Prometheus, providing detailed information useful for debugging.</p> Adding verbosity to the scheduler <p>To enable extended logging for the Run:ai scheduler:</p> <ol> <li>Edit the <code>RunaiConfig</code> to adjust scheduler verbosity:  </li> </ol> <pre><code>kubectl edit runaiconfig runai -n runai\n</code></pre> <p>2  Add or modify the following section under the scheduler settings:  </p> <pre><code>runai-scheduler:\n    args:\n        verbosity: 6\n</code></pre> <p>This increases the verbosity level of the scheduler logs to provide more detailed output.</p> <p>Warning</p> <p>Enabling verbose logging can significantly increase disk space usage. Monitor your storage capacity and adjust the verbosity level as necessary.</p>"},{"location":"admin/troubleshooting/troubleshooting/","title":"Troubleshooting Run:ai","text":""},{"location":"admin/troubleshooting/troubleshooting/#installation","title":"Installation","text":"Upgrade fails with \"Ingress already exists\" <p>Symptom:  The installation fails with error: <code>Error: rendered manifests contain a resource that already exists. Unable to continue with install: IngressClass \"nginx\" in namespace \"\" exists</code></p> <p>Root cause: Run:ai installs <code>NGINX</code>, but there is an existing NGINX on the cluster. </p> <p>Resolution: In the Run:ai cluster YAML file, disable the installation of NGINX by setting:</p> <pre><code>ingress-nginx:\n    enabled: false\n</code></pre> How to get installation logs <p>Symptom: Installation fails and you need to troubleshoot the issue.</p> <p>Resolution: Run the following script to obtain any relevant installation logs in case of an error.</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/run-ai/public/main/installation/get-installation-logs.sh | bash\n</code></pre> Upgrade fails with \"rendered manifests contain a resource that already exists\" error <p>Symptom: The installation fails with error: <code>Error: rendered manifests contain a resource that already exists. Unable to continue with install:...</code></p> <p>Root cause: The Run:ai installation is trying to create a resource that already exists, which may be due to a previous installation that was not properly removed.</p> <p>Resolution: Run the following script to remove all Run:ai resources and reinstall:</p> <pre><code>helm template &lt;release-name&gt; &lt;chart-name&gt; --namespace &lt;namespace&gt; | kubectl delete -f -\n</code></pre> <p>Then reinstall Run:ai.</p> Pods are failing due to certificate issues <p>Symptom: Pods are failing with certificate issues.</p> <p>Root cause: The certificate provided during the Control Plane's installation is not valid.</p> <p>Resolution: Verify that the certificate is valid and trusted. If the certificate is valid, but is signed by a local CA, make sure you have followed the procedure for a local certificate authority.</p>"},{"location":"admin/troubleshooting/troubleshooting/#cluster-health","title":"Cluster Health","text":"<p>See Cluster Health Troubleshooting</p>"},{"location":"admin/troubleshooting/troubleshooting/#dashboard-issues","title":"Dashboard Issues","text":"No Metrics are showing on Dashboard <p>Symptom: No metrics are showing on dashboards at <code>https://&lt;company-name&gt;.run.ai/dashboards/now</code></p> <p>Typical root causes:</p> <ul> <li>Firewall-related issues.</li> <li>Internal clock is not synced.</li> <li>Prometheus pods are not running.</li> </ul> <p>Firewall issues</p> <p>Add verbosity to Prometheus as describe here.Verify that there are no errors. If there are connectivity-related errors you may need to:</p> <ul> <li>Check your firewall for outbound connections. See the required permitted URL list in Network requirements.</li> <li>If you need to set up an internet proxy or certificate, please contact Run:ai customer support. </li> </ul> <p>Machine Clocks are not synced</p> <p>Run: <code>date</code> on cluster nodes and verify that date/time is correct.  If not:</p> <ul> <li>Set the Linux time service (NTP).</li> <li>Restart Run:ai services. Depending on the previous time gap between servers, you may need to reinstall the Run:ai cluster</li> </ul> <p>Prometheus pods are not running</p> <p>Run: <code>kubectl get pods -n monitoring -o wide</code></p> <ul> <li>Verify that all pods are running.</li> <li>The default Prometheus installation is not built for high availability. If a node is down, the Prometheus pod may not recover by itself unless manually deleted. Delete the pod to see it start on a different node and consider adding a second replica to Prometheus.</li> </ul> GPU Related metrics not showing <p>Symptom: GPU-related metrics such as <code>GPU Nodes</code> and <code>Total GPUs</code> are showing zero but other metrics, such as <code>Cluster load</code> are shown.</p> <p>Root cause: An installation issue related to the NVIDIA stack.</p> <p>Resolution: </p> <p>Need to run through the NVIDIA stack and find the issue. The current NVIDIA stack looks as follows:</p> <ol> <li>NVIDIA Drivers (at the OS level, on every node)</li> <li>NVIDIA Docker (extension to Docker, on every node)</li> <li>Kubernetes Node feature discovery (mark node properties)</li> <li>NVIDIA GPU Feature discovery (mark nodes as \u201chaving GPUs\u201d)</li> <li>NVIDIA Device plug-in (Exposes GPUs to Kubernetes)</li> <li>NVIDIA DCGM Exporter (Exposes metrics from GPUs in Kubernetes)</li> </ol> <p>Run:ai requires the installation of the NVIDIA GPU Operator which installs the entire stack above. However, there are two alternative methods for using the operator:</p> <ul> <li>Use the default operator values to install 1 through 6.</li> <li>If  NVIDIA Drivers (#1 above) are already installed on all nodes, use the operator with a flag that disables drivers install. </li> </ul> <p>For more information see [System requirements](../runai-setup/cluster-setup/.</p> <p>NVIDIA GPU Operator</p> <p>Run: <code>kubectl get pods -n gpu-operator | grep nvidia</code> and verify that all pods are running.</p> <p>Node and GPU feature discovery</p> <p>Kubernetes Node feature discovery identifies and annotates nodes. NVIDIA GPU Feature Discovery identifies and annotates nodes with GPU properties. See that: </p> <ul> <li>All such pods are up.</li> <li>The GPU feature discovery pod is available for every node with a GPU.</li> <li>And finally, when describing nodes, they show an active <code>gpu/nvidia</code> resource.</li> </ul> <p>NVIDIA Drivers</p> <ul> <li>If NVIDIA drivers have been installed on the nodes themselves, ssh into each node and run <code>nvidia-smi</code>. Run <code>sudo systemctl status docker</code> and verify that docker is running. Run <code>nvidia-docker</code> and verify that it is installed and working.  Linux software upgrades may require a node restart.</li> <li>If NVIDIA drivers are installed by the Operator, verify that the NVIDIA driver daemonset has created a pod for each node and that all nodes are running. Review the logs of all such pods. A typical problem may be the driver version which is too advanced for the GPU hardware. You can set the driver version via operator flags. </li> </ul> <p>NVIDIA DCGM Exporter</p> <ul> <li>View the logs of the DCGM exporter pod and verify that no errors are prohibiting the sending of metrics. </li> <li>To validate that the dcgm-exporter exposes metrics, find one of the DCGM Exporter pods and run:</li> </ul> <pre><code>kubectl port-forward &lt;dcgm-exporter-pod-name&gt; 9400:9400\n</code></pre> <p>Then browse to http://localhost:9400/metrics and verify that the metrics have reached the DCGM exporter.</p> <ul> <li>The next step after the DCGM Exporter is <code>Prometheus</code>. To validate that metrics from the DCGM Exporter reach Prometheus, run:</li> </ul> <pre><code>kubectl port-forward svc/runai-cluster-kube-prometh-prometheus -n monitoring 9090:9090\n</code></pre> <p>Then browse to localhost:9090. In the UI, type <code>DCGM_FI_DEV_GPU_UTIL</code> as the metric name, and verify that the metric has reached Prometheus. </p> <p>If the DCGM Exporter is running correctly and exposing metrics, but this metric does not appear in Prometheus, there may be a connectivity issue between these components.</p> Allocation-related metrics not showing <p>Symptom: GPU Allocation-related metrics such as <code>Allocated GPUs</code> are showing zero but other metrics, such as <code>Cluster load</code> are shown.</p> <p>Root cause: The origin of such metrics is the scheduler. </p> <p>Resolution:</p> <ul> <li>Run: <code>kubectl get pods -n runai | grep scheduler</code>. Verify that the pod is running.</li> <li>Review the scheduler logs and look for errors. If such errors exist, contact Run:ai customer support. </li> </ul> All metrics are showing \"No Data\" <p>Symptom: All data on all dashboards is showing the text \"No Data\".</p> <p>Root cause: Internal issue with metrics infrastructure.</p> <p>Resolution: Please contact Run:ai customer support.</p>"},{"location":"admin/troubleshooting/troubleshooting/#authentication-issues","title":"Authentication Issues","text":"After a successful login, you are redirected to the same login page <p>For a self-hosted installation, check Linux clock synchronization as described above. Use the Run:ai preinstall diagnostics tool to validate System and network requirements and test this automatically. </p> Single-sign-on issues <p>For single-sign-on issues, see the troubleshooting section in the single-sign-on configuration documents. </p>"},{"location":"admin/troubleshooting/troubleshooting/#user-interface-submit-job-issues","title":"User Interface Submit Job Issues","text":"New Job button is grayed out <p>Symptom: The <code>New Job</code> button on the top right of the Job list is grayed out.</p> <p>Root Cause: This can happen due to multiple configuration issues: </p> <ul> <li>Open Chrome developer tools and refresh the screen.</li> <li>Under <code>Network</code> locate a network call error. Search for the HTTP error code.</li> </ul> <p>Resolution for 401 HTTP Error</p> <ul> <li>The Cluster certificate provided as part of the installation is valid and trusted (not self-signed).</li> <li>Researcher Authentication has not been properly configured. Try running <code>runai login</code> from the Command-line interface. Alternatively, run: <code>kubectl get pods -n kube-system</code>, identify the api-server pod and review its logs. </li> </ul> <p>Resolution for 403 HTTP Error</p> <p>Run: <code>kubectl get pods -n runai</code>, identify the <code>agent</code> pod, see that it's running, and review its logs.</p> New Job button is not showing <p>Symptom: The <code>New Job</code> button on the top right of the Job list does not show.</p> <p>Root Causes: (multiple)</p> <ul> <li>You do not have <code>Researcher</code> or <code>Research Manager</code> permissions.</li> <li>Under <code>Settings | General</code>, verify that <code>Unified UI</code> is on.</li> </ul> Submit form is distorted <p>Symptom: Submit form is showing vertical lines.</p> <p>Root Cause: The control plane does not know the cluster URL.</p> <p>Using the Run:ai user interface, go to the Clusters list. See that there is no cluster URL next to your cluster.</p> <p>Resolution: Cluster must be re-installed. </p> Submit form does not show the list of Projects <p>Symptom: When connected with Single-sign-on, in the Submit form, the list of Projects is empty.</p> <p>Root Cause:  SSO is on and researcher authentication is not properly configured as such.</p> <p>Resolution: Verify API Server settings as described in Researcher Authentication configuration.</p> Job form is not opening on OpenShift <p>Symptom: When clicking on \"New Job\" the Job forms does not load. Network shows 405</p> <p>Root Cause: An installation step has been missed. </p> <p>Resolution: Open the Cluster list and open the cluster installation wizard again. After selecting OpenShift, you will see a <code>patch</code> command at the end of the instruction set. Run it. </p>"},{"location":"admin/troubleshooting/troubleshooting/#networking-issues","title":"Networking Issues","text":"'admission controller' connectivity issue <p>Symptoms:</p> <ul> <li>Pods are failing with 'admission controller' connectivity errors.</li> <li>The command-line <code>runai submit</code> fails with an 'admission controller' connectivity error.</li> <li>Agent or cluster sync pods are crashing in self-hosted installation.</li> </ul> <p>Root cause: Connectivity issues between different nodes in the cluster.</p> <p>Resolution:</p> <ul> <li>Run the preinstall diagnostics tool to validate System and network requirements and test connectivity issues.</li> <li>Run: <code>kubectl get pods -n kube-system -o wide</code>. Verify that all networking pods are running. </li> <li>Run: <code>kubectl get nodes</code>. Check that all nodes are ready and connected.</li> <li>Run: <code>kubectl get pods -o wide -A</code> to see which pods are Pending or in Error and which nodes they belong to. </li> <li>See if pods from different nodes have trouble communicating with each other.</li> <li>Advanced, run: <code>kubectl exec &lt;pod-name&gt; -it /bin/sh</code> from a pod in one node and ping a pod from another. </li> </ul> Projects are not syncing <p>Symptom: Create a Project on the Run:ai user interface, then run: <code>runai list projects</code>. The new Project does not appear.</p> <p>Root cause: The Run:ai agent is not syncing properly. This may be due to firewall issues. </p> <p>Resolution</p> <ul> <li>Run: <code>runai pods -n runai | grep agent</code>. See that the agent is in Running state. Select the agent's full name and run: <code>kubectl logs -n runai runai-agent-&lt;id&gt;</code>.</li> <li>Verify that there are no errors. If there are connectivity-related errors you may need to check your firewall for outbound connections. See the required permitted URL list in Network requirements. </li> <li>If you need to set up an internet proxy or certificate, please contact Run:ai customer support. </li> </ul> Jobs are not syncing <p>Symptom: A Job on the cluster (<code>runai list jobs</code>) does not show in the Run:ai user interface Job list. </p> <p>Root cause: The Run:ai cluster-sync pod is not syncing properly.  </p> <p>Resolution: Search the cluster-sync pod for errors.</p>"},{"location":"admin/troubleshooting/troubleshooting/#job-related-issues","title":"Job-related Issues","text":"Jobs fail with ContainerCannotRun status  <p>Symptom: When running <code>runai list jobs</code>, your Job has a status of <code>ContainerCannotRun</code>.</p> <p>Root Cause: The issue may be caused due to an unattended upgrade of the NVIDIA driver.</p> <p>To verify, run: <code>runai describe job &lt;job-name&gt;</code>, and search for an error <code>driver/library version mismatch</code>.</p> <p>Resolution: Reboot the node on which the Job attempted to run.</p> <p>Going forward, we recommend blacklisting NVIDIA driver from unattended-upgrades. You can do that by editing <code>/etc/apt/apt.conf.d/50unattended-upgrades</code>, and adding <code>nvidia-driver-</code> to the <code>Unattended-Upgrade::Package-Blacklist</code> section. It should look something like that:</p> <pre><code>Unattended-Upgrade::Package-Blacklist {\n    // The following matches all packages starting with linux-\n    //  \"linux-\";\n    \"nvidia-driver-\";\n</code></pre>"},{"location":"admin/troubleshooting/troubleshooting/#inference-issues","title":"Inference Issues","text":"New Deployment button is grayed out <p>Symptoms: </p> <ul> <li>The <code>New workload type</code> -&gt; <code>Inference</code> button is grayed out.</li> <li>Cannot create a deployment via Inference API.</li> </ul> <p>Root Cause: Run:ai Inference prerequisites have not been met.</p> <p>Resolution: Review inference prerequisites and install accordingly.</p> Submitted workload type of inference remains in Pending state <p>Symptom: A submitted inference is not running.</p> <p>Root Cause: The patch statement to add the runai-scheduler has not been performed. </p> Workload of type inference status is \"Failed\" <p>Symptom: Inference status is always <code>Failed</code>.</p> <p>Root Cause: (multiple)</p> <ul> <li>Not enough resources in the cluster.</li> <li>Server model command is misconfigured (i.e sleep infinity).</li> <li>Server port is misconfigured. </li> </ul> Worload of type inference does not scale up from zero <p>Symptom: In the Inference form, when \"Auto-scaling\" is enabled, and \"Minimum Replicas\" is set to zero, the inference cannot scale up from zero.</p> <p>Root Cause: </p> <ul> <li>Clients are not sending requests.</li> <li>Clients are not using the same port/protocol as the server model.</li> <li>Server model command is misconfigured (i.e sleep infinity).</li> </ul>"},{"location":"admin/troubleshooting/troubleshooting/#command-line-interface-issues","title":"Command-line interface Issues","text":"Unable to install CLI due to certificate errors <p>Symptom: The curl command and download button to download the CLI is not working.</p> <p>Root Cause: The cluster is not accessible from the download location</p> <p>Resolution: </p> <p>Use an alternate method for downloading the CLI. Run:</p> <pre><code>kubectl port-forward -n runai svc/researcher-service 4180\n</code></pre> <p>In another shell, run: <pre><code>wget --content-disposition http://localhost:4180/cli/linux\n</code></pre></p> When running the CLI you get an error: open .../.kube/config.lock: permission denied <p>Symptom: When running any CLI command you get a permission denied error.</p> <p>Root Cause: The user running the CLI does not have read permissions to the <code>.kube</code> directory.</p> <p>Resolution: Change permissions for the directory.</p> When running 'runai logs', the logs are delayed <p>Symptom: Printout from the container is not immediately shown in the log. </p> <p>Root Cause: By default, Python buffers stdout, and stderr, which are not flushed in real-time. This may cause logs to appear sometimes minutes after being buffered.</p> <p>Resolution: Set the env var PYTHONUNBUFFERED to any non-empty string or pass -u to Python. e.g. <code>python -u main.py</code>.</p> CLI does not download properly on OpenShift <p>Symptom: When trying to download the CLI on OpenShift, the <code>wget</code> statement downloads a text file named <code>darwin</code> or <code>linux</code> rather than the binary <code>runai</code>.</p> <p>Root Cause: An installation step has been missed. </p> <p>Resolution: Open the Cluster list and open the cluster installation wizard again. After selecting OpenShift, you will see a <code>patch</code> command at the end of the instruction set. Run it. </p>"},{"location":"developer/overview-developer/","title":"Developer Documentation Overview","text":"<p>Developers can access Run:ai through various programmatic interfaces.</p>"},{"location":"developer/overview-developer/#api-architecture","title":"API Architecture","text":"<p>Run:ai is composed of a single, multi-tenant control plane. Each tenant can be connected to one or more GPU clusters. See Run:ai system components for detailed information.</p> <p>The following programming interfaces are available:</p> API Description Purpose Run:ai REST API Get and Modify any Run:ai business object This is the API mostly used by system developers. The API is also used by the Run:ai user interface as well as the new command-line interface Cluster API (Deprecated) Submit Workloads directly to the Cluster A YAML-based API allowing submittion of Workloads directly to the Cluster. With Run:ai 2.18, this API is replaced by the above Run:ai, which is now the recommended method Metrics API (deprecated) Get cluster metrics Get utilization metrics."},{"location":"developer/overview-developer/#runai-rest-api","title":"Run:ai REST API","text":"<p>Allows you to Add, delete, modify and list Run:ai meta-data objects such as Projects, Departments, Users. For Clusters of Run:ai 2.18 and above, allows the submitting of Workloasd. </p> <p>The API is provided as REST and is accessible via the control plane endpoint.  </p> <p>For more information see Run:ai REST API.</p> <p>Important</p> <p>The endpoints and fields specified in the API reference are the ones that are officially supported by Run:ai. Endpoints and fields that are not listed in the API reference are not supported.</p> <p>Run:ai does not recommend using API endpoints and fields marked as <code>deprecated</code> and will not add functionality to them. Once an API endpoint or field is marked as <code>deprecated</code>, Run:ai will stop supporting it after 2 major releases for self-hosted deployments, and after 6 months for SaaS deployments.</p> <p>For details, see the Deprecation notifications.</p>"},{"location":"developer/overview-developer/#cluster-api-deprecated","title":"Cluster API (Deprecated)","text":"<p>The Cluster API allows you to submit and delete Workloads directly to the cluster itself.</p> <p>The API is provided as Kubernetes API.</p> <p>Cluster API is accessible via the GPU cluster itself. As such, multiple clusters may have multiple endpoints.</p> <p>Important</p> <ul> <li>This API is replaced by a Run:ai REST API to submit jobs, which is now the recommended method for cluster versions of 2.18 and above. </li> <li>If you are looking to automate tasks with older versions of Run:ai, it's best to use the Run:ai Command-line interface which provides forward compatibility.  </li> </ul>"},{"location":"developer/overview-developer/#metrics-api","title":"Metrics API","text":"<p>Retrieve metrics from multiple GPU clusters.</p> <p>See the Metrics API document.</p>"},{"location":"developer/overview-developer/#api-authentication","title":"API Authentication","text":"<p>See API Authentication for information on how to gain authenticated access to Run:ai APIs.</p>"},{"location":"developer/rest-auth/","title":"API Authentication","text":"<p>The following document explains how to authenticate with Run:ai APIs.</p> <p>Run:ai APIs are accessed using bearer tokens. A token can be obtained in several ways:</p> <ul> <li>When logging into the Run:ai user interface, you enter an email and password (or authenticated via single sign-on) which are used to obtain a token.</li> <li>When using the Run:ai command-line, you use a Kubernetes profile and are authenticated by pre-running <code>runai login</code> (or oc login with OpenShift). The command attaches a token to the profile and allows you access to Run:ai functionality.</li> <li>When using Run:ai APIs, you need to create an Application through the Run:ai user interface. The Application is created with specific roles and contains a secret. Using the secret you can obtain a token and use it within subsequent API calls.</li> </ul>"},{"location":"developer/rest-auth/#create-a-client-application","title":"Create a Client Application","text":"<ul> <li>Open the Run:ai Run:ai User Interface.</li> <li>Go to <code>Settings &amp; Tools</code>, <code>Application</code> and create a new Application.</li> <li>Copy the <code>&lt;APPLICATION&gt;</code> and <code>&lt;SECRET KEY&gt;</code> to be used below</li> </ul>"},{"location":"developer/rest-auth/#access-rules-for-the-application","title":"Access rules for the Application","text":"<p>For you API requests to be accepted, you will need to set access rules for the application. To assign roles to an application, see Create or Delete rules.</p> <p>Use the Roles table to assign the correct roles to the application.</p>"},{"location":"developer/rest-auth/#request-an-api-token","title":"Request an API Token","text":"<p>Use the above parameters to get a temporary token to access Run:ai as follows.</p>"},{"location":"developer/rest-auth/#example-command-to-get-an-api-token","title":"Example command to get an API token","text":"<p>Replace <code>&lt;COMPANY-URL&gt;</code> below with:</p> <ul> <li> <p>For SaaS installations use <code>&lt;company&gt;.run.ai</code></p> </li> <li> <p>For self-hosted use the Run:ai user interface URL.</p> </li> </ul> cURLPython <pre><code>    curl  -X POST \\\n      'https://&lt;runai_url&gt;/api/v1/token' \\\n      --header 'Accept: */*' \\\n      --header 'Content-Type: application/json' \\\n      --data-raw '{\n      \"grantType\":\"app_token\",\n      \"AppId\":\"&lt;APPLICATION NAME&gt;\",\n      \"AppSecret\" : \"&lt;SECRET KEY&gt;\"\n    }'\n</code></pre> <pre><code>    import requests\n    import json\n    reqUrl = \"https://cp-590d-run-13764-kc-upgrade.runailabs.com/api/v1/token\"\n    headersList = {\n     \"Accept\": \"*/*\",\n     \"Content-Type\": \"application/json\"\n    }\n    payload = json.dumps({\n      \"grantType\":\"app_token\",\n      \"AppId\":\"&lt;APPLICATION NAME&gt;\",\n      \"AppSecret\" : \"&lt;SECRET KEY&gt;\"\n    })\n    response = requests.request(\"POST\", reqUrl, data=payload,  headers=headersList)\n    print(response.text)\n</code></pre>"},{"location":"developer/rest-auth/#response","title":"Response","text":"<p>The API response will look as follows:</p> API Response<pre><code>{\n  \"accessToken\": \"&lt;TOKEN&gt;\", \n}\n</code></pre> <p>To call Run:ai REST APIs, the application must pass the retrieved <code>accessToken</code> as a Bearer token in the Authorization header of your HTTP request.</p>"},{"location":"developer/admin-rest-api/overview/","title":"Run:ai REST API","text":"<p>The purpose of the Run:ai REST API is to provide an easy-to-use programming interface for administrative tasks.</p>"},{"location":"developer/admin-rest-api/overview/#endpoint-url-for-api","title":"Endpoint URL for API","text":"<p>The domain used for Run:ai REST APIs is the same domain used to browse for the Run:ai User Interface. Either <code>&lt;company&gt;.run.ai</code>, or <code>app.run.ai</code> for older tenants or a custom URL used for Self-hosted installations.</p>"},{"location":"developer/admin-rest-api/overview/#authentication","title":"Authentication","text":"<ul> <li>Create a Client Application to make API requests. Use the client application and secret, to obtain a time-bound bearer token (<code>&lt;ACCESS-TOKEN&gt;</code>). For details, see Calling REST APIs.</li> <li>Use the token for subsequent API calls.</li> </ul>"},{"location":"developer/admin-rest-api/overview/#example-usage","title":"Example Usage","text":"<p>For example, if you have an Administrator role, you can get a list of clusters by running:</p> cURLPython <pre><code>curl 'https://&lt;COMPANY-URL&gt;/v1/k8s/clusters' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;ACCESS-TOKEN&gt;'\n</code></pre> <pre><code>import http.client\n\nconn = http.client.HTTPSConnection(\"https://&lt;COMPANY-URL&gt;\")\nheaders = {\n    'content-type': \"application/json\",\n    'authorization': \"Bearer &lt;ACCESS-TOKEN&gt;\"\n    }\nconn.request(\"GET\", \"/v1/k8s/clusters\", headers=headers)\n\nres = conn.getresponse()\ndata = res.read()\n\nprint(data.decode(\"utf-8\"))\n</code></pre> <p>(replace <code>&lt;ACCESS-TOKEN&gt;</code> with the bearer token from above).</p> <p>For an additional example, see the following code. It is an example of how to use the Run:ai REST API to create a User and a Project and set the User to the Project.  </p>"},{"location":"developer/admin-rest-api/overview/#runai-rest-api-documentation","title":"Run:ai REST API Documentation","text":"<p>The Run:ai REST API offers developers a robust interface for interacting with and managing Run:ai metadata objects, including Projects, Departments, Clusters, and Users.</p> <p>Public API documentation is available at api-docs.run.ai. For self-hosted deployments, access the documentation at <code>https://&lt;control-plane-url&gt;/api/docs</code>.</p> <p>View Documentation</p>"},{"location":"developer/admin-rest-api/overview/#runai-api-policy","title":"Run:ai API Policy","text":"<p>At Run:ai, we are dedicated to delivering stable, reliable, and well-documented APIs. Our goal is to ensure that our APIs evolve in a predictable, transparent manner, offering users a seamless experience.</p> <p>Run:ai follows strict API design and operational standards to ensure a consistent and high-quality experience for users.</p>"},{"location":"developer/admin-rest-api/overview/#api-lifecycle-and-deprecation","title":"API Lifecycle and Deprecation","text":"<p>While our goal is to maintain stable and backward-compatible APIs, there may be times when breaking changes or deprecations are necessary.</p> <p>In case of breaking changes, the deprecated version of the API will be supported for two additional versions in self-hosted deployments and for six months in SaaS deployments. During this period, no new features or functionality will be added to the deprecated API.  When an API or API field is deprecated, the following process is followed: Documentation: The deprecated API or field is clearly labeled in the documentation, with a replacement provided where applicable. Release Notes: Information about deprecated APIs, including those scheduled for future removal, is included in the release notes. Customer Notification: Customers are notified of upcoming deprecations as part of the regular release communications.</p>"},{"location":"developer/admin-rest-api/overview/#api-removal","title":"API Removal","text":"<p>After the defined backward compatibility period has ended, deprecated APIs or fields are removed from both the codebase and the documentation.</p>"},{"location":"developer/cluster-api/other-resources/","title":"Support for other Kubernetes Applications","text":""},{"location":"developer/cluster-api/other-resources/#introduction","title":"Introduction","text":"<p>Kubernetes has several built-in resources that encapsulate running Pods. These are called Kubernetes Workloads and should not be confused with Run:ai Workloads.</p> <p>Examples of such resources are a Deployment that manages a stateless application, or a Job that runs tasks to completion.</p> <p>Run:ai natively runs Run:ai Workloads. A Run:ai workload encapsulates all the resources needed to run, creates them, and deletes them together. However, Run:ai, being an open platform allows the scheduling of any Kubernetes Workflow.</p>"},{"location":"developer/cluster-api/other-resources/#how-to","title":"How To","text":"<p>To run Kubernetes Workloads with Run:ai you must add the following to the YAML:</p> <ul> <li>A namespace that is associated with a Run:ai Project.</li> <li>A scheduler name: <code>runai-scheduler</code>.</li> <li>When using Fractions, use a specific syntax for the <code>nvidia/gpu</code> limit.</li> </ul>"},{"location":"developer/cluster-api/other-resources/#example-job","title":"Example: Job","text":"job1.yaml<pre><code>apiVersion: batch/v1\nkind: Job # (1)\nmetadata:\n  name: job1\n  namespace: runai-team-a # (2)\nspec:\n  template:\n    spec:\n      containers:\n      - name: job1-container\n        image: runai.jfrog.io/demo/quickstart\n        resources:\n          limits:\n            nvidia.com/gpu: 1 # (4)\n      restartPolicy: Never\n      schedulerName: runai-scheduler # (3)\n</code></pre> <ol> <li>This is a Kubernetes Job.</li> <li>Namespace: Replace <code>runai-team-a</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).</li> <li>The job to be scheduled with the Run:ai scheduler.</li> <li>To run with half a GPU replace 1 with \"0.5\" (with apostrophes).</li> </ol> <p>To submit the Job run:</p> <pre><code>kubectl apply -f job1.yaml\n</code></pre> <p>You will be able to see the Job in the Run:ai User interface, including all metrics and lists</p>"},{"location":"developer/cluster-api/other-resources/#example-deployment","title":"Example: Deployment","text":"deployment1.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment # (1)\nmetadata:\n  name: inference-1\n  namespace: runai-team-a # (2)\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: inference-1\n  template:\n    metadata:\n      labels:\n        app: inference-1\n    spec:\n      containers:\n        - resources:\n            limits:\n              nvidia.com/gpu: 1 # (4)\n          image: runai/example-marian-server\n          imagePullPolicy: Always\n          name: inference-1\n          ports:\n            - containerPort: 8888\n      schedulerName: runai-scheduler # (3)\n\n---\napiVersion: v1\nkind: Service # (5)\nmetadata:\n  labels:\n    app: inference-1\n  name: inference-1\nspec:\n  type: ClusterIP\n  ports:\n    - port: 8888\n      targetPort: 8888\n  selector:\n    app: inference-1\n</code></pre> <ol> <li>This is a Kubernetes Deployment.</li> <li>Namespace: Replace <code>runai-team-a</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).</li> <li>The job to be scheduled with the Run:ai scheduler.</li> <li>To run with half a GPU replace 1 with \"0.5\" (with apostrophes).</li> <li>This example also contains the creation of a service to connect to the deployment. It is not mandatory.</li> </ol> <p>To submit the Deployment run:</p> <pre><code>kubectl apply -f deployment1.yaml\n</code></pre>"},{"location":"developer/cluster-api/other-resources/#example-submit-a-cron-job-via-yaml","title":"Example: Submit a Cron job via YAML","text":"<p>The cron command-line utility is a job scheduler typically used to set up and maintain software environments at scheduled intervals. Run:ai now supports submitting jobs with cron using a YAML file. </p> <p>To submit a job using cron, run the following command:</p> <pre><code>kubectl apply -f &lt;file_name&gt;.yaml\n</code></pre> <p>The following is an example YAML file:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"* * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n          - (Mandatory) runai/queue: team-a\n        spec:\n          (Mandatory) schedulerName: runai-scheduler\n          containers:\n          - name: hello\n            image: busybox:1.28\n            imagePullPolicy: IfNotPresent\n            command:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n          (Optional) priorityClassName: build / train / inference / interactivePreemptible\n</code></pre>"},{"location":"developer/cluster-api/other-resources/#limitations","title":"Limitations","text":"<p>The Run:ai command line interface provides limited support for Kubernetes Workloads.</p>"},{"location":"developer/cluster-api/other-resources/#see-also","title":"See Also","text":"<p>Run:ai has specific integrations with additional third-party tools such as KubeFlow, MLFlow, and more. These integrations use the same instructions as described above.</p>"},{"location":"developer/cluster-api/reference/","title":"Reference","text":"<p>For a full reference for the YAML API parameters see the YAML Reference document.</p>"},{"location":"developer/cluster-api/submit-rest/","title":"Submitting Workloads via HTTP/REST","text":"<p>You can submit Workloads via HTTP calls, using the Kubernetes REST API.</p>"},{"location":"developer/cluster-api/submit-rest/#submit-workload-example","title":"Submit Workload Example","text":"<p>To submit a workload via HTTP, run the following:</p> <pre><code>curl -X POST \\ # (1) \n'https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/trainingworkloads' \\ \n    --header 'Content-Type: application/yaml' \\\n    --header 'Authorization: Bearer &lt;BEARER&gt;' \\  # (2) \n    --data-raw 'apiVersion: run.ai/v2alpha1\nkind: TrainingWorkload  # (3)\nmetadata:\n  name: job-1    \nspec:\n  gpu:\n    value: \"1\"\n  image:\n    value: runai.jfrog.io/demo/quickstart\n  name:\n    value: job-1  \n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code>, <code>distributedworkloads</code> or <code>inferenceworkloads</code> according to type.</li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> <li>See Submitting a Workload via YAML for an explanation of the YAML-based workload.</li> </ol> <p>Run: <code>runai list jobs</code> to see the new Workload.</p>"},{"location":"developer/cluster-api/submit-rest/#delete-workload-example","title":"Delete Workload Example","text":"<p>To delete a workload run:</p> <pre><code>curl -X DELETE \\ # (1) \n'https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/trainingworkloads/&lt;JOB-NAME&gt;' \\ \n    --header 'Content-Type: application/yaml' \\\n    --header 'Authorization: Bearer &lt;BEARER&gt;'   # (2)\n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code>, <code>distributedworkloads</code> or <code>inferenceworkloads</code> according to type.  Replace <code>&lt;JOB-NAME&gt;</code> with the name of the Job. </li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> </ol>"},{"location":"developer/cluster-api/submit-rest/#suspendstop-workload-example","title":"Suspend/Stop workload example","text":"<p>To suspend or stop a workload run:</p> <pre><code>curl -X PATCH \\ # (1) \n'https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/interactiveworkload/&lt;JOB-NAME&gt;' \\\n    --header 'Content-Type: application/json' \n    --header 'Authorization: Bearer &lt;TOKEN&gt;'# (2) \n    --data '{\"spec\":{\"active\": {\"value\": \"false\"}}}'\n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code>, <code>distributedworkloads</code> or <code>inferenceworkloads</code> according to type.  Replace <code>&lt;JOB-NAME&gt;</code> with the name of the Job. </li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> </ol>"},{"location":"developer/cluster-api/submit-rest/#using-other-programming-languages","title":"Using other Programming Languages","text":"<p>You can use any Kubernetes client library together with the YAML documentation above to submit workloads via other programming languages. For more information see Kubernetes client libraries.</p>"},{"location":"developer/cluster-api/submit-rest/#python-example","title":"Python example","text":"<p>Create the following file and run it via python:</p> create-train.py<pre><code>import json\nimport requests\n\n# (1)\nurl = \"https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/trainingworkloads\"\n\npayload = json.dumps({\n  \"apiVersion\": \"run.ai/v2alpha1\",\n  \"kind\": \"TrainingWorkload\",\n  \"metadata\": {\n    \"name\": \"train1\",\n    \"namespace\": \"runai-team-a\"\n  },\n  \"spec\": {\n    \"image\": {\n      \"value\": \"runai.jfrog.io/demo/quickstart\"\n    },\n    \"name\": {\n      \"value\": \"train1\"\n    },\n    \"gpu\": {\n      \"value\": \"1\"\n    }\n  }\n})\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;TOKEN&gt;' #(2)\n}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload) # (3)\n\nprint(json.dumps(json.loads(response.text), indent=4))\n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code>, <code>distributedworkloads</code>or <code>inferenceworkloads</code> according to type.</li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> <li>if you do not have a valid certificate, you can add the flag <code>verify=False</code>.</li> </ol>"},{"location":"developer/cluster-api/submit-yaml/","title":"Submitting Workloads via YAML","text":"<p>You can use YAML to submit Workloads directly to Run:ai. Below are examples of how to create training, interactive and inference workloads via YAML.</p> <p>For details on YAML parameters, see the YAML Reference.</p>"},{"location":"developer/cluster-api/submit-yaml/#submit-workload-example","title":"Submit Workload Example","text":"<p>Create a file named <code>training1.yaml</code> with the following text:</p> training1.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingWorkload # (1)\nmetadata:\n  name: job-1  # (2) \n  namespace: runai-team-a # (3)\nspec:\n  gpu:\n    value: \"1\"\n  image:\n    value: runai.jfrog.io/demo/quickstart\n  name:\n    value: job-1 # (4)\n</code></pre> <ol> <li>This is a Training workload.</li> <li>Kubernetes object name. Mandatory, but has no functional significance.</li> <li>Namespace: Replace <code>runai-team-a</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).</li> <li>Job name as appears in Run:ai. Can provide name, or create automatically if name prefix is configured. </li> </ol> <p>Change the namespace and run: <code>kubectl apply -f training1.yaml</code></p> <p>Run: <code>runai list jobs</code> to see the new Workload.</p>"},{"location":"developer/cluster-api/submit-yaml/#delete-workload-example","title":"Delete Workload Example","text":"<p>Run: <code>kubectl delete -f training1.yaml</code> to delete the Workload. </p>"},{"location":"developer/cluster-api/submit-yaml/#creating-a-yaml-syntax-from-a-cli-command","title":"Creating a YAML syntax from a CLI command","text":"<p>An easy way to create a YAML for a workload is to generate it from the <code>runai submit</code> command by using the <code>--dry-run</code> flag. For example, run:</p> <pre><code>runai submit build1 -i ubuntu -g 1 --interactive --dry-run \\\n     -- sleep infinity \n</code></pre> <p>The result will be the following Kubernetes object declaration:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: InteractiveWorkload  # (1)\nmetadata:\n  creationTimestamp: null\n  labels:\n    PreviousJob: \"true\"\n  name: job-0-2022-05-02t08-50-57\n  namespace: runai-team-a\nspec:\n  command:\n    value: sleep infinity\n  gpu:\n    value: \"1\"\n  image:\n    value: ubuntu\n  imagePullPolicy:\n    value: Always\n  name:\n    value: job-0\n\n... Additional internal and status properties...\n</code></pre> <ol> <li>This is an Interactive workload.</li> </ol>"},{"location":"developer/cluster-api/submit-yaml/#inference-workload-example","title":"Inference Workload Example","text":"<p>Creating an inference workload is similar to the above two examples.</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: InferenceWorkload\nmetadata:\n  name: inference1\n  namespace: runai-team-a\nspec:\n  name:\n    value: inference1\n  gpu:\n    value: \"0.5\"\n  image:\n    value: \"runai.jfrog.io/demo/example-triton-server\"\n  minScale:\n    value: 1\n  maxScale:\n    value: 2\n  metric:\n    value: concurrency # (1)\n  target:\n    value: 80  # (2)\n  ports:\n      items:\n        port1:\n          value:\n            container: 8000\n            protocol: http\n            serviceType: ServingPort\n</code></pre> <ol> <li>Possible metrics are throughput, concurrency and latency.</li> <li>Inference requires a port to receive requests.</li> </ol>"},{"location":"developer/cluster-api/submit-yaml/#suspendresume-interactivetraining-workload","title":"Suspend/Resume Interactive/Training Workload","text":"<p>To suspend training:</p> <p><pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingWorkload # \nmetadata:\n  name: job-1  #  \n  namespace: runai-team-a # \nspec:\n  gpu:\n    value: \"1\"\n  active:\n    value: false\n  image:\n    value: runai.jfrog.io/demo/quickstart\n  name:\n    value: job-1 # \n</code></pre> In order to suspend the workload, set <code>active</code> to <code>false</code>. To resume the workload, either set <code>active</code> to <code>true</code> or remove it entirely.</p>"},{"location":"developer/cluster-api/submit-yaml/#see-also","title":"See Also","text":"<ul> <li>To understand how to connect to the inference workload, see Inference Quickstart.</li> <li>To learn more about Inference and Run:ai see Inference overview.</li> </ul>"},{"location":"developer/cluster-api/workload-overview-dev/","title":"Cluster API (Deprecated)","text":"<p>The Run:ai Cluster API allows the submission of Workloads via YAML, directly to Kubernetes. </p> <p>Important</p> <p>With Run:ai 2.18 clusters, you can now submit Workloads via the Run:ai REST API. We recommend using this API if your cluster is of that version.  </p>"},{"location":"developer/cluster-api/workload-overview-dev/#workloads","title":"Workloads","text":"<p>Run:ai schedules Workloads. Run:ai workloads contain:</p> <ul> <li>The Kubernetes resource (Job, Deployment, etc) that is used to launch the container inside which the data science code runs.</li> <li>A set of additional resources that is required to run the Workload. Examples: a service entry point that allows access to the Job, a persistent volume claim to access data on the network and more.</li> </ul> <p>Run:ai supports the following Workloads types:</p> Workload Type Kubernetes Name Description Interactive <code>InteractiveWorkload</code> Submit an interactive workload Training <code>TrainingWorkload</code> Submit a training workload Distributed Training <code>DistributedWorkload</code> Submit a distributed training workload using TensorFlow, PyTorch or MPI Inference <code>InferenceWorkload</code> Submit an inference workload"},{"location":"developer/cluster-api/workload-overview-dev/#values","title":"Values","text":"<p>A Workload will typically have a list of values, such as name, image, and resources. A full list of values is available in the runai-submit Command-line reference.  </p> <p>You can also find the exact YAML syntax run:</p> <pre><code>kubectl explain TrainingWorkload.spec\n</code></pre> <p>(and similarly for other Workload types).</p> <p>To get information on a specific value (e.g. <code>node type</code>), you can also run:</p> <pre><code>kubectl explain TrainingWorkload.spec.nodeType\n</code></pre> <p>Result:</p> <pre><code>KIND:     TrainingWorkload\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: nodeType &lt;Object&gt;\n\nDESCRIPTION:\n     Specifies nodes (machines) or a group of nodes on which the workload will\n     run. To use this feature, your Administrator will need to label nodes as\n     explained in the Group Nodes guide at\n     https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag\n     can be used in conjunction with Project-based affinity. In this case, the\n     flag is used to refine the list of allowable node groups set in the\n     Project. For more information consult the Projects guide at\n     https://docs.run.ai/admin/admin-ui-setup/project-setup.\n\nFIELDS:\n   value    &lt;string&gt;\n</code></pre>"},{"location":"developer/cluster-api/workload-overview-dev/#how-to-submit","title":"How to Submit","text":"<p>A Workload can be submitted via various channels:</p> <ul> <li>The Run:ai user interface.</li> <li>The Run:ai command-line interface, via the runai submit command.</li> <li>The Run:ai Cluster API.</li> </ul>"},{"location":"developer/cluster-api/workload-overview-dev/#policies","title":"Policies","text":"<p>An Administrator can set Policies for Workload submission. Policies serve two purposes:</p> <ol> <li>To constrain the values a researcher can specify.</li> <li>To provide default values.</li> </ol> <p>For example, an administrator can,</p> <ul> <li>Set a maximum of 5 GPUs per Workload.</li> <li>Provide a default value of 1 GPU for each container.</li> </ul> <p>Each workload type has a matching kind of workload policy. For example, an <code>InteractiveWorkload</code> has a matching <code>InteractivePolicy</code></p> <p>A Policy of each type can be defined per-project. There is also a global policy that applies to any project that does not have a per-project policy.</p> <p>For further details on policies, see Policies.</p>"},{"location":"developer/metrics/metrics-api/","title":"Metrics and telemetry","text":""},{"location":"developer/metrics/metrics-api/#telemetry","title":"Telemetry","text":"<p>Telemetry is a numeric measurement recorded in real-time when emitted from the Run:ai cluster.</p>"},{"location":"developer/metrics/metrics-api/#metrics","title":"Metrics","text":"<p>Metrics are numeric measurements recorded over time that are emitted from the Run:ai cluster. Typical metrics involve utilization, allocation, time measurements and so on. Metrics are used in Run:ai dashboards as well as in the Run:ai administration user interface.</p> <p>The purpose of this document is to detail the structure and purpose of metrics emitted by Run:ai. This enables customers to create custom dashboards or integrate metric data into other monitoring systems.</p> <p>Run:ai provides metrics via the Run:ai Control-plane API. Previoulsy, Run:ai provided metrics information via direct access to an internal metrics store. This method is deprecated but is still documented here.</p>"},{"location":"developer/metrics/metrics-api/#metric-and-telemetry-scopes","title":"Metric and telemetry Scopes","text":"<p>Run:ai provides Control-plane API which supports and aggregates metrics at various levels.</p> Level Description Cluster A cluster is a set of Nodes Pools &amp; Nodes. With Cluster metrics, metrics are aggregated at the Cluster level Node Data is aggregated at the Node level. Node Pool Data is aggregated at the Node Pool level. Workload Data is aggregated at the Workload level. In some Workloads, e.g. with distributed workloads, these metrics aggregate data from all worker pods Pod The basic execution unit"},{"location":"developer/metrics/metrics-api/#supported-metrics","title":"Supported Metrics","text":"Metric Cluster Node Pool Node Workload Pod API Cluster API Node Pool API Workload API Pod API ALLOCATED_GPU TRUE TRUE TRUE AVG_WORKLOAD_WAIT_TIME TRUE TRUE CPU_LIMIT_CORES TRUE CPU_MEMORY_LIMIT_BYTES TRUE CPU_MEMORY_REQUEST_BYTES TRUE CPU_MEMORY_USAGE_BYTES TRUE TRUE TRUE CPU_MEMORY_UTILIZATION TRUE TRUE TRUE CPU_REQUEST_CORES TRUE CPU_USAGE_CORES TRUE TRUE TRUE CPU_UTILIZATION TRUE TRUE TRUE GPU_ALLOCATION TRUE GPU_MEMORY_REQUEST_BYTES TRUE GPU_MEMORY_USAGE_BYTES TRUE TRUE GPU_MEMORY_USAGE_BYTES_PER_GPU TRUE TRUE GPU_MEMORY_UTILIZATION TRUE TRUE GPU_MEMORY_UTILIZATION_PER_GPU TRU GPU_QUOTA TRUE TRUE GPU_UTILIZATION TRUE TRUE TRUE TRUE GPU_UTILIZATION_PER_GPU TRUE TRUE POD_COUNT TRUE RUNNING_POD_COUNT TRUE TOTAL_GPU TRUE TRUE TOTAL_GPU_NODES TRUE TRUE GPU_UTILIZATION_DISTRIBUTION TRUE TRUE UNALLOCATED_GPU TRUE TRUE"},{"location":"developer/metrics/metrics-api/#advanced-metrics","title":"Advanced Metrics","text":"<p>NVIDIA provides extended metrics at the Pod level. These are documented here. To enable these metrics please contact Run:ai customer support.</p> Metric Cluster Node Pool Workload Pod GPU_FP16_ENGINE_ACTIVITY_PER_GPU TRUE GPU_FP32_ENGINE_ACTIVITY_PER_GPU TRUE GPU_FP64_ENGINE_ACTIVITY_PER_GPU TRUE GPU_GRAPHICS_ENGINE_ACTIVITY_PER_GPU TRUE GPU_MEMORY_BANDWIDTH_UTILIZATION_PER_GPU TRUE GPU_NVLINK_RECEIVED_BANDWIDTH_PER_GPU TRUE GPU_NVLINK_TRANSMITTED_BANDWIDTH_PER_GPU TRUE GPU_PCIE_RECEIVED_BANDWIDTH_PER_GPU TRUE GPU_PCIE_TRANSMITTED_BANDWIDTH_PER_GPU TRUE GPU_SM_ACTIVITY_PER_GPU TRUE GPU_SM_OCCUPANCY_PER_GPU TRUE GPU_TENSOR_ACTIVITY_PER_GPU TRUE"},{"location":"developer/metrics/metrics-api/#_1","title":"Metrics via API","text":""},{"location":"developer/metrics/metrics-api/#supported-telemetry","title":"Supported telemetry","text":"telemetry Node Workload API Node API Workload API WORKLOADS_COUNT TRUE ALLOCATED_GPUS TRUE TRUE READY_GPU_NODES TRUE READY_GPUS TRUE TOTAL_GPU_NODES TRUE TOTAL_GPUS TRUE IDLE_ALLOCATED_GPUS TRUE FREE_GPUS TRUE TOTAL_CPU_CORES TRUE USED_CPU_CORES TRUE ALLOCATED_CPU_CORES TRUE TOTAL_GPU_MEMORY_BYTES TRUE USED_GPU_MEMORY_BYTES TRUE TOTAL_CPU_MEMORY_BYTES TRUE USED_CPU_MEMORY_BYTES TRUE ALLOCATED_CPU_MEMORY_BYTES TRUE"},{"location":"developer/metrics/metrics/","title":"Metrics API","text":""},{"location":"developer/metrics/metrics/#what-are-metrics","title":"What are Metrics","text":"<p>Metrics are numeric measurements recorded over time that are emitted from the Run:ai cluster. Typical metrics involve utilization, allocation, time measurements and so on. Metrics are used in Run:ai dashboards as well as in the Run:ai administration user interface.</p> <p>The purpose of this document is to detail the structure and purpose of metrics emitted by Run:ai to enable customers to create custom dashboards or integrate metric data into other monitoring systems.</p> <p>Run:ai uses Prometheus for collecting and querying metrics.</p> <p>Warning</p> <p>From cluster version 2.17 and onwards, Run:ai supports metrics via the Run:ai Control-plane API. Direct metrics queries (metrics that are queried directly from Prometheus) are deprecated.</p>"},{"location":"developer/metrics/metrics/#published-runai-metrics","title":"Published Run:ai Metrics","text":"<p>Following is the list of published Run:ai metrics, per cluster version (make sure to pick the right cluster version in the picker at the top of the page):</p> Metric name Labels Measurement Description runai_active_job_cpu_requested_cores {clusterId,  job_name, job_uuid} CPU Cores Workload's requested CPU cores runai_active_job_memory_requested_bytes {clusterId,  job_name, job_uuid} Bytes Workload's requested CPU memory runai_cluster_cpu_utilization {clusterId} 0 to 1 CPU utilization of the entire cluster runai_cluster_memory_used_bytes {clusterId} Bytes Used CPU memory of the entire cluster runai_cluster_memory_utilization {clusterId} 0 to 1 CPU memory utilization of the entire cluster runai_allocated_gpu_count_per_gpu {gpu, clusterId, node} 0/1 Is a GPU hosting a pod runai_last_gpu_utilization_time_per_gpu {gpu, clusterId, node} Unix time Last time GPU was not idle runai_requested_gpu_memory_mb_per_workload {clusterId, job_type, job_uuid, job_name, project, workload_id} MegaBytes Requested GPU memory per workload (0 if not specified by the user) runai_requested_gpus_per_workload {clusterId, workload_type, workload_id, workload_name, project} Double Number of requested GPUs per workload runai_run_time_seconds_per_workload {clusterId, workload_id, workload_name} Seconds Total run time per workload runai_wait_time_seconds_per_workload {clusterId, workload_id, workload_name} Seconds Total wait time per workload runai_node_cpu_requested_cores {clusterId, node} Double Sum of the requested CPU cores of all workloads running in a node runai_node_cpu_utilization {clusterId, node} 0 to 1 CPU utilization per node runai_node_memory_utilization {clusterId, node} 0 to 1 CPU memory utilization per node runai_node_requested_memory_bytes {clusterId, node} Bytes Sum of the requested CPU memory of all workloads running in a node runai_node_used_memory_bytes {clusterId, node} Bytes Used CPU memory per node runai_project_guaranteed_gpus {clusterId, project} Double Guaranteed GPU quota per project runai_project_info {memory_quota, cpu_quota, gpu_guaranteed_quota, clusterId, project, department} N/A Information on CPU, CPU memory, GPU quota per project runai_queue_info {memory_quota, cpu_quota, gpu_guaranteed_quota, clusterId, nodepool, queue_name, department} N/A Information on CPU, CPU memory, GPU quota per project/department per nodepool runai_cpu_limits_per_active_workload {clusterId, job_name , job_uuid} CPU Cores Workloads CPU limit (in number of cores). See link runai_job_cpu_usage {clusterId, workload_id, workload_name, project} Double Workloads CPU usage (in number of cores) runai_memory_limits_per_active_workload {clusterId, job_name, job_uuid} Bytes Workloads CPU memory limit. See link runai_active_job_memory_requested_bytes {clusterId, job_name, job_uuid} Bytes Workloads requested CPU memory. See link runai_job_memory_used_bytes {clusterId, workload_id, workload_name, project} Bytes Workloads used CPU memory runai_mig_mode_gpu_count {clusterId, node} Double Number of GPUs on MIG nodes (Deprecated) runai_gpu_utilization_per_gpu {clusterId, gpu, node} % GPU Utilization per GPU runai_gpu_utilization_per_node {clusterId, node} % GPU Utilization per Node runai_gpu_memory_used_mebibytes_per_gpu {clusterId, gpu, node} MiB Used GPU memory per GPU runai_gpu_memory_used_mebibytes_per_node {clusterId, node} MiB Used GPU memory per Node runai_gpu_memory_total_mebibytes_per_gpu {clusterId, gpu, node} MiB Total GPU memory per GPU runai_gpu_memory_total_mebibytes_per_node {clusterId, node} MiB Total GPU memory per Node runai_gpu_count_per_node {clusterId, node, modelName, ready, schedulable} Number Number of GPUs per Node runai_allocated_gpu_count_per_workload {clusterId, workload_id, workload_name, workload_type, user} Double Number of allocated GPUs per Workload runai_allocated_gpu_count_per_project {clusterId, project} Double Number of allocated GPUs per Project runai_gpu_memory_used_mebibytes_per_pod_per_gpu {clusterId, pod_name, pod_uuid, pod_namespace, node, gpu} MiB Used GPU Memory per Pod, per Gpu on which the workload is running runai_gpu_memory_used_mebibytes_per_workload {clusterId, workload_id, workload_name, workload_type, user} MiB Used GPU Memory per Workload runai_gpu_utilization_per_pod_per_gpu {clusterId, pod_name, pod_uuid, pod_namespace, node, gpu} % GPU Utilization per Pod per GPU runai_gpu_utilization_per_workload {clusterId, workload_id, workload_name, workload_type, user} % Average GPU Utilization per Workload runai_gpu_utilization_per_project {clusterId, project} % Average GPU Utilization per Project runai_last_gpu_utilization_time_per_workload {clusterId, workload_id, workload_name, workload_type, user} Seconds (Unix Timestamp) The Last Time (Unix Timestamp) That The Workload Utilized Any Of Its Allocated GPUs runai_gpu_idle_seconds_per_workload {clusterId, workload_id, workload_name, workload_type, user} Seconds Seconds Passed Since The Workload Utilized Any Of Its Allocated GPUs runai_allocated_gpu_count_per_pod {clusterId, pod_name, pod_uuid, pod_namespace, node} Double Number Of Allocated GPUs per Pod runai_allocated_gpu_count_per_node {clusterId, node} Double Number Of Allocated GPUs per Node runai_allocated_millicpus_per_pod {clusterId, pod_name, pod_uuid, pod_namespace, node} Integer Number Of Allocated Millicpus per Pod runai_allocated_memory_per_pod {clusterId, pod_name, pod_uuid, pod_namespace, node} Bytes Allocated Memory per Pod <p>Following is a list of labels appearing in Run:ai metrics:</p> Label Description clusterId Cluster Identifier department Name of Run:ai Department cpu_quota CPU limit per project gpu GPU index gpu_guaranteed_quota Guaranteed GPU quota per project image Name of Docker image namespace_name Namespace deployment_name Deployment name job_name Job name job_type Job type: training, interactive or inference job_uuid Job identifier workload_name Workload name workload_type Workload type: training, interactive or inference workload_uuid Workload identifier pod_name Pod name. A Workload can contain many pods. pod_namespace Pod namespace memory_quota CPU memory limit per project node Node name project Name of Run:ai Project status Workload status: Running, Pending, etc. For more information on Workload statuses see document user User identifier"},{"location":"developer/metrics/metrics/#other-metrics","title":"Other Metrics","text":"<p>Run:ai exports other metrics emitted by NVIDIA and Kubernetes packages, as follows:</p> Metric name Description runai_gpu_utilization_per_gpu GPU utilization kube_node_status_capacity The capacity for different resources of a node kube_node_status_condition The condition of a cluster node kube_pod_container_resource_requests_cpu_cores The number of CPU cores requested by container kube_pod_container_resource_requests_memory_bytes Bytes of memory requested by a container kube_pod_info Information about pod <p>For additional information, see Kubernetes kube-state-metrics and NVIDIA dcgm exporter.</p>"},{"location":"developer/metrics/metrics/#changed-metrics-and-api-mapping","title":"Changed metrics and API mapping","text":"<p>Starting in cluster version 2.17, some of the metrics names have been changed. In addition some Run:ai metrics are available as API endpoints. Using the API endpoints is more efficient and provides an easier way of retrieving metrics in any application. The following table lists the metrics that were changed.</p> Metric name in version 2.16 2.17 Change Description 2.17 API Endpoint runai_active_job_cpu_requested_cores available also via API https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_REQUEST_CORES\" metricType runai_active_job_memory_requested_bytes available also via API https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_MEMORY_REQUEST_BYTES\" metricType runai_cluster_cpu_utilization available also via API https://app.run.ai/api/v2/clusters/{clusterUuid}/metrics ; with \"CPU_UTILIZATION\" metricType runai_cluster_memory_utilization available also via API https://app.run.ai/api/v2/clusters/{clusterUuid}/metrics ; with \"CPU_MEMORY_UTILIZATION\" metricType runai_gpu_utilization_non_fractional_jobs no longer available runai_allocated_gpu_count_per_workload labels changed runai_gpu_utilization_per_pod_per_gpu available also via API https://app.run.ai/api/v1/workloads/{workloadId}/pods/{podId}/metrics ; with \"GPU_UTILIZATION_PER_GPU\" metricType runai_gpu_utilization_per_workload available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"GPU_UTILIZATION\" metricType runai_job_image no longer available runai_job_requested_gpu_memory available also via API and renamed to: \"runai_requested_gpu_memory_mb_per_workload\" with different labels https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"GPU_MEMORY_REQUEST_BYTES\" metricType runai_job_requested_gpus renamed to: \"runai_requested_gpus_per_workload\" with different labels runai_job_total_runtime renamed to: \"runai_run_time_seconds_per_workload\" with different labels runai_job_total_wait_time renamed to: \"runai_wait_time_seconds_per_workload\" with different labels runai_gpu_memory_used_mebibytes_per_workload available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"GPU_MEMORY_USAGE_BYTES\" metricType runai_gpu_memory_used_mebibytes_per_pod_per_gpu available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/pods/{podId}/metrics ; with \"GPU_MEMORY_USAGE_BYTES_PER_GPU\" metricType runai_node_gpu_used_memory_bytes renamed and changed units: \"runai_gpu_memory_used_mebibytes_per_node\" runai_node_total_memory_bytes renamed and changed units: \"runai_gpu_memory_total_mebibytes_per_node\" runai_project_info labels changed runai_active_job_cpu_limits available also via API and renamed to: \"runai_cpu_limits_per_active_workload\" https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_LIMIT_CORES\" metricType runai_job_cpu_usage available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_USAGE_CORES\" metricType runai_active_job_memory_limits available also via API and renamed to: \"runai_memory_limits_per_active_workload\" https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_MEMORY_LIMIT_BYTES\" metricType runai_running_job_memory_requested_bytes was a duplication of \"runai_active_job_memory_requested_bytes\", see above runai_job_memory_used_bytes available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_MEMORY_USAGE_BYTES\" metricType runai_job_swap_memory_used_bytes no longer available runai_gpu_count_per_node added labels runai_last_gpu_utilization_time_per_workload labels changed runai_gpu_idle_time_per_workload renamed to: \"runai_gpu_idle_seconds_per_workload\" with different labels"},{"location":"developer/metrics/metrics/#create-custom-dashboards","title":"Create custom dashboards","text":"<p>To create custom dashboards based on the above metrics, please contact Run:ai customer support.</p>"},{"location":"home/components/","title":"Run:ai System Components","text":""},{"location":"home/components/#components","title":"Components","text":"<p>Run:ai is made up of two components:</p> <ul> <li>The Run:ai cluster provides scheduling services and workload management.</li> <li>The Run:ai control plane provides resource management, Workload submission and cluster monitoring.</li> </ul> <p>Technology-wise, both are installed over a Kubernetes Cluster.</p> <p>Run:ai users:</p> <ul> <li>Researchers submit Machine Learning workloads via the Run:ai Console, the Run:ai Command-Line Interface (CLI), or directly by sending YAML files to Kubernetes.</li> <li>Administrators monitor and set priorities via the Run:ai User Interface</li> </ul> <p></p>"},{"location":"home/components/#runai-cluster","title":"Run:ai Cluster","text":"<ul> <li>Run:ai comes with its own Scheduler. The Run:ai scheduler extends the Kubernetes scheduler. It uses business rules to schedule workloads sent by Researchers.</li> <li>Run:ai schedules Workloads. Workloads include the actual researcher code running as a Kubernetes container, together with all the system resources required to run the code, such as user storage, network endpoints to access the container etc.</li> <li>The cluster uses an outbound-only, secure connection to synchronize with the Run:ai control plane. Information includes meta-data sync and various metrics on Workloads, Nodes etc.</li> <li>The Run:ai cluster is installed as a Kubernetes Operator</li> <li>Run:ai is installed in its own Kubernetes namespace named runai</li> <li>Workloads are run in the context of Run:ai Projects. Each Project is mapped to a Kubernetes namespace with its own settings and access control.</li> </ul>"},{"location":"home/components/#runai-control-plane-on-the-cloud","title":"Run:ai Control Plane on the cloud","text":"<p>The Run:ai control plane is used by multiple customers (tenants) to manage resources (such as Projects &amp; Departments), submit Workloads and monitor multiple clusters.</p> <p>A single Run:ai customer (tenant) defined in the control-plane, can manage multiple Run:ai clusters. So a single customer, can manage mutltiple GPU clusters in multiple locations/subnets from a single interface.</p>"},{"location":"home/components/#self-hosted-control-plane","title":"Self-hosted Control-Plane","text":"<p>The Run:ai control plane can also be locally installed. To understand the various installation options see the installation types document.</p>"},{"location":"home/data-privacy-details/","title":"Data Privacy","text":"<p>This article details the data privacy and compliance considerations for deploying Run:ai. It is intended to help administrators and compliance teams understand the data management practices involved with Run:ai. This ensures the permissions align with organizational policies and regulatory requirements before installation and during integration and onboarding of the various teams.</p> <p>When using the Run:ai SaaS cluster, the Control plane operates through the Run:ai cloud, requiring the transmission of certain data for control and analytics. Below is a detailed breakdown of the specific data sent to the Run:ai cloud in the SaaS offering.</p> <p>Note</p> <p>For organizations where data privacy policies do not align with this data transmission, Run:ai offers a self-hosted version. This version includes the control plane on premise and does not communicate with the cloud.</p>"},{"location":"home/data-privacy-details/#data-sent-to-the-runai-cloud","title":"Data sent to the Run:ai cloud","text":"Asset Details Workload Metrics Includes workload names, CPU, GPU, and memory metrics, as well as parameters provided during the <code>runai submit</code> command. Workload Assets Covers environments, compute resources, and data resources associated with workloads. Resource Credentials Credentials for cluster resources, encrypted with a SHA-512 algorithm specific to each tenant. Node Metrics Node-specific data including names, IPs, and performance metrics (CPU, GPU, memory). Cluster Metrics Cluster-wide metrics such as names, CPU, GPU, and memory usage. Projects &amp; Departments Includes names and quota information for projects and departments. Users User roles within Run:ai, email addresses, and passwords."},{"location":"home/data-privacy-details/#key-consideration","title":"Key consideration","text":"<p>Run:ai ensures that no deep-learning artefacts, such as code, images, container logs, training data, models, or checkpoints, are transmitted to the cloud. These assets remain securely within your organization's firewalls, safeguarding sensitive intellectual property and data.  </p>"},{"location":"home/data-privacy-details/#see-also","title":"See Also","text":"<p>The Run:ai privacy policy. </p>"},{"location":"home/overview/","title":"Run:ai Documentation Library","text":"<p>Welcome to the Run:ai documentation area. For an introduction about what is the Run:ai Platform see Run:ai platform on the run.ai website.</p> <p>The Run:ai documentation is targeting four personas:</p> <ul> <li> <p>Infrastructure Administrator - An IT person, responsible for the installation, setup and IT maintenance of the Run:ai product. Infrastructure Administrator documentation can be found here.</p> </li> <li> <p>Platform Administrator - Responsible for the day-to-day administration of the product. Platform Administrator documentation can be found here.</p> </li> <li> <p>Researcher \u2014 Using Run:ai to spin up notebooks, submit Workloads, prompt models, etc. Researcher documentation can be found here.</p> </li> <li> <p>Developer \u2014 Using various APIs to automate work with Run:ai. The Developer documentation can be found here.</p> </li> </ul>"},{"location":"home/overview/#how-to-get-support","title":"How to Get Support","text":"<p>To get support use the following channels:</p> <ul> <li> <p>On the Run:ai user interface at <code>&lt;company-name&gt;.run.ai</code>, use the 'Contact Support' link on the top right.</p> </li> <li> <p>Or submit a ticket by clicking the button below:</p> </li> </ul> <p>Submit a Ticket</p>"},{"location":"home/overview/#community","title":"Community","text":"<p>Run:ai provides its customers with access to the Run:ai Customer Community portal to submit tickets, track ticket progress and update support cases.</p> <p>Customer Community Portal</p> <p>Reach out to customer support for credentials.</p>"},{"location":"home/overview/#runai-cloud-status-page","title":"Run:ai Cloud Status Page","text":"<p>Run:ai cloud availability is monitored at status.run.ai.</p>"},{"location":"home/overview/#collect-logs-to-send-to-support","title":"Collect Logs to Send to Support","text":"<p>As an IT Administrator, you can collect Run:ai logs to send to support. For more information see logs collection.</p>"},{"location":"home/overview/#example-code","title":"Example Code","text":"<p>Code for the Docker images referred to on this site is available at https://github.com/run-ai/docs/tree/master/quickstart.</p> <p>The following images are used throughout the documentation:</p> Image Description Source runai.jfrog.io/demo/quickstart Basic training image. Multi-GPU support https://github.com/run-ai/docs/tree/master/quickstart/main runai.jfrog.io/demo/quickstart-distributed Distributed training using MPI and Horovod https://github.com/run-ai/docs/tree/master/quickstart/distributed zembutsu/docker-sample-nginx Build (interactive) with Connected Ports https://github.com/zembutsu/docker-sample-nginx runai.jfrog.io/demo/quickstart-x-forwarding Use X11 forwarding from Docker image https://github.com/run-ai/docs/tree/master/quickstart/x-forwarding runai.jfrog.io/demo/pycharm-demo Image used for tool integration (PyCharm and VSCode) https://github.com/run-ai/docs/tree/master/quickstart/python%2Bssh runai.jfrog.io/demo/example-triton-client and  runai.jfrog.io/demo/example-triton-server Basic Inference https://github.com/run-ai/models/tree/main/models/triton"},{"location":"home/overview/#contributing-to-the-documentation","title":"Contributing to the documentation","text":"<p>This documentation is made better by individuals from our customer and partner community. If you see something worth fixing, please comment at the bottom of the page or create a pull request via GitHub. The public GitHub repository can be found on the top-right of this page. </p>"},{"location":"home/product-support-policy/","title":"Product Support Policy","text":"<p>The product support levels for the Run:ai software are as follows:</p> Critical Bug Fixes Important Bug Fixes Full support V V Extended support V - End of support - - <ul> <li>Full support period: 12 months from the release date of a Major Version.</li> <li>Extended support period: 6 months after the end of the full support period.</li> <li>End of support: 18 months from the release date of a Major Version</li> </ul> <p>Notes:</p> <ol> <li>Run:ai may extend the support periods and/or otherwise amend this Product Support Levels Policy from time to time at its own discretion.</li> <li>Versioning: Run:ai versioning follows Semantic Version (SemVer) numbering scheme, \u201cMa.Mi.Pa\u201d, where:<ul> <li>Ma.Mi is a major version that contains new features, bug fixes and security updates (\u201cMajor Version\u201d).</li> <li>Pa is a patch level version that is focused on bug fixes and security updates. Run:ai version release dates are listed in theRun:ai product documentation.</li> </ul> </li> <li> <p>Critical Bug: a bug that represents a severity 1 support ticket, as listed in the Run:ai support agreement. Important bug:  a bug that represents a severity 1 or severity 2 support ticket, as listed in the Run:ai support agreement.</p> </li> <li> <p>Run:ai is built from 3 components: Run:ai Control Plane, Run:ai Cluster and Run:ai Command Line Inference (CLI). For full details about Run:ai system components see: https://docs.run.ai/latest/home/components.</p> </li> <li> <p>Run:ai Control Plane, Run:ai Cluster &amp; Run:ai CLI are always released together with the same version number.</p> </li> <li> <p>A supported Run:ai environment is built from:</p> <ul> <li> <p>Run:ai Control Plane of a version equal to or greater than the versions of each of the Run:ai Clusters.</p> </li> <li> <p>Run:ai CLI of a version equal to the version on the Run:ai Cluster.</p> </li> </ul> </li> <li> <p>From time to time, Run:ai may provide API deprecation notices under the product     documentation of the applicable Run:ai version. For full details about Run:ai API deprecation notice and support policy see: https://docs.run.ai/latest/developer/overview-developer/#api-support. </p> </li> </ol> <p>Last update: Aug 6 2024</p>"},{"location":"home/saas-updates/","title":"What's New for the Run:ai SaaS Platform","text":"<p>The release notes are aimed to provide transparency into the latest changes and improvements to Run:ai\u2019s SaaS platform. The updates include new features, optimizations, and fixes aimed at improving performance and user experience. </p> <p>Latest GA release notes (https://docs.run.ai/latest/home/whats-new-2-19/) </p>"},{"location":"home/saas-updates/#gradual-rollout","title":"Gradual Rollout","text":"<p>SaaS features are gradually rolled out to customers over the course of a week to ensure a smooth transition and minimize any potential disruption. </p>"},{"location":"home/saas-updates/#november-release","title":"November Release","text":""},{"location":"home/saas-updates/#product-enhancements","title":"Product Enhancements","text":"<ul> <li>The display of the default GPU quota for the default department has been updated. Previously, the GPU quota was shown as -1. It has now been changed to display as \"-\" for better clarity.  </li> <li>New permissions have been added for the Application Administrator role, enabling full CRUD (Create, Read, Update, Delete) capabilities for managing applications. </li> </ul>"},{"location":"home/saas-updates/#resolved-bugs","title":"Resolved Bugs","text":"ID Description RUN-23778 Resolved an issue where SAML mappers were displayed as null in the UI upon editing an Identity Provider (IdP). The mapper values now persist as expected, and associated attributes remain unchanged. RUN-23762 Fixed a bug that caused some customers to receive the incorrect version of the dashboard. This issue led to inconsistencies in the user interface and functionality, impacting affected users' ability to access the appropriate dashboard features. RUN-23735 Fixed an issue where the <code>limit</code> parameter on the Users page did not enforce the minimum value constraint. This allowed invalid values to be processed, potentially causing errors in pagination RUN-23669 Consumption report: The Inspect feature in Grafana, which allows users to export consumption data from the portal, has been re-enabled RUN-23664 An issue has been resolved where the GPU quota numbers displayed on the Department Overview page did not match the values shown on the Department Edit page. RUN-20116 An issue has been resolved where searching for certain pages in the UI only applied the search filter to the current page. Relevant tables are: Users, Applications, Workloads, Projects, departments, Node pools. RUN-23575 The dynamic refresh was not properly preserving the user\u2019s widget settings, causing them to reset to default values after each refresh cycle. RUN-23376 CLI v2: An issue was resolved where the runai logs command failed with a 401 Unauthorized error after a period of inactivity RUN-23373 An issue where AWS storage classes were not appearing when creating a new data source within a new workload has been resolved. Previously, AWS storage classes were only visible when creating a data source directly from the Data Sources tab."},{"location":"home/whats-new-2-13/","title":"Run:ai version 2.13","text":""},{"location":"home/whats-new-2-13/#version-2137","title":"Version 2.13.7","text":""},{"location":"home/whats-new-2-13/#release-date","title":"Release date","text":"<p>July 2023</p>"},{"location":"home/whats-new-2-13/#release-content","title":"Release content","text":"<ul> <li>Added filters to the historic quota ratio widget on the Quota management dashboard.</li> </ul>"},{"location":"home/whats-new-2-13/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-11080 Fixed an issue in OpenShift environments where log in via SSO with the <code>kubeadmin</code> user, gets blank pages for every page. RUN-11119 Fixed an issue where values that should be the Order of priority column are in the wrong column. RUN-11120 Fixed an issue where the Projects table does not show correct metrics when Run:ai version 2.13 is paired with a Run:ai 2.8 cluster. RUN-11121 Fixed an issue where the wrong over quota memory alert is shown in the Quota management pane in project edit form. RUN-11272 Fixed an issue in OpenShift environments where the selection in the cluster drop down in the main UI does not match the cluster selected on the login page."},{"location":"home/whats-new-2-13/#version-2134","title":"Version 2.13.4","text":""},{"location":"home/whats-new-2-13/#release-date_1","title":"Release date","text":"<p>July 2023</p>"},{"location":"home/whats-new-2-13/#fixed-issues_1","title":"Fixed issues","text":"Internal ID Description RUN-11089 Fixed an issue when creating an environment, commands in the Runtime settings pane and are not persistent and cannot be found in other assets (for example in a new Training)."},{"location":"home/whats-new-2-13/#version-2131","title":"Version 2.13.1","text":""},{"location":"home/whats-new-2-13/#release-date_2","title":"Release date","text":"<p>July 2023</p>"},{"location":"home/whats-new-2-13/#release-content_1","title":"Release content","text":"<ul> <li>Made an improvement so that occurrences of labels that are not in use anymore are deleted.</li> </ul>"},{"location":"home/whats-new-2-13/#fixed-issues_2","title":"Fixed issues","text":"<p>N/A</p>"},{"location":"home/whats-new-2-13/#version-2130","title":"Version 2.13.0","text":""},{"location":"home/whats-new-2-13/#release-content_2","title":"Release content","text":"<p>This version contains features and fixes from previous versions starting with 2.9. Refer to the prior versions for specific features and fixes. </p> <p>Projects</p> <ul> <li>Improved the Projects UI for ease of use. Projects follows UI upgrades and changes that are designed to make setting up of components and assets easier for administrators and researchers. To configure a project, see Projects.</li> </ul> <p>Dashboards</p> <ul> <li> <p>Added a new dashboard for Quota management, which provides an efficient means to monitor and manage resource utilization within the AI cluster. The dashboard filters the display of resource quotas based on Departments, Projects, and Node pools. For more information, see Quota management dashboard.</p> </li> <li> <p>Added to the Overview dashboard, the ability to filter the cluster by one or more node pools. For more information, see Node pools.</p> </li> </ul> <p>Nodes and Node pools</p> <ul> <li> <p>Run:ai scheduler supports 2 scheduling strategies: Bin Packing (default) and Spread. For more information, see Scheduling strategies. You can configure the scheduling strategy in the node pool level to improve the support of clusters with mixed types of resources and workloads. For configuration information, see Creating new node pools.</p> </li> <li> <p>GPU device level DCGM Metrics are collected per GPU and presented by Run:ai in the Nodes table. Each node contains a list of its embedded GPUs with their respective DCGM metrics. See DCGM Metrics for the list of metrics which are provided by NVidia DCGM and collected by Run:ai. Contact your Run:ai customer representative to enable this feature.</p> </li> </ul> <ul> <li>Added per node pool over-quota priority. Over-quota priority sets the relative amount of additional unused resources that an asset can get above its current quota. For more information, see Over-quota priority.</li> </ul> <ul> <li>Added support of associating workspaces to node pool. The association between workspaces and node pools is done using Compute resources section. In order to associate a compute resource to a node pool, in the Compute resource section, press More settings. Press Add new to add more node pools to the configuration. Drag and drop the node pools to set their priority.</li> </ul> <ul> <li>Added Node pool selection as part of the workload submission form. This allows researchers to quickly determine the list of node pools available and their priority. Priority is set by dragging and dropping them in the desired order of priority. In addition, when the node pool priority list is locked by a policy, the list isn't editable by the Researcher even if the workspace is created from a template or copied from another workspace.</li> </ul> <p>Time limit duration</p> <ul> <li> <p>Improved the behavior of any workload time limit (for example, Idle time limit) so that the time limit will affect existing workloads that were created before the time limit was configured. This is an optional feature which provides help in handling situations where researchers leave sessions open even when they do not need to access the resources. For more information, see Limit duration of interactive training jobs.</p> </li> <li> <p>Improved workspaces time limits. Workspaces that reach a time limit will now transition to a state of <code>stopped</code> so that they can be reactivated later.</p> </li> <li> <p>Added time limits for training jobs per project. Administrators (Department Admin, Editor) can limit the duration of Run:ai Training jobs per Project using a specified time limit value. This capability can assist administrators to limit the duration and resources consumed over time by training jobs in specific projects. Each training job that reaches this duration will be terminated.</p> </li> </ul> <p>Workload assets</p> <ul> <li>Extended the collaboration functionality for any workload asset such as Environment, Compute resource, and some Data source types. These assets are now shared with Departments in the organization in addition to being shared with specific projects, or the entire cluster.</li> </ul> <ul> <li>Added a search box for card galleries in any asset based workload creation form to provide an easy way to search for assets and resources. To filter use the asset name or one of the field values of the card.</li> </ul> <p>PVC data sources</p> <ul> <li>Added support for PVC block storage in the New data source form. In the New data source form for a new PVC data source, in the Volume mode field, select from Filesystem or Block. For more information, see Create a PVC data source.</li> </ul> <p>Credentials</p> <ul> <li>Added Docker registry to the Credentials menu. Users can create docker credentials for use in specific projects for image pulling. To configure credentials, see Configuring credentials.</li> </ul> <p>Policies</p> <ul> <li>Improved policy support by adding <code>DEFAULTS</code> in the <code>items</code> section in the policy. The <code>DEFAULTS</code> section sets the default behavior for items declared in this section. For example, this can be use to limit the submission of workloads only to existing PVCs. For more information and an example, see Policies, Complex values.</li> </ul> <ul> <li>Added support for making a PVC data source available to all projects. In the New data source form, when creating a new PVC data source, select All from the Project pane.</li> </ul> <p>Researcher API</p> <ul> <li>Extended researcher's API to allow stopping and starting of workloads using the API. For more information, see Submitting Workloads via HTTP/REST.</li> </ul> <p>Integrations</p> <ul> <li>Added support for Spark and Elastic jobs. For more information, see Running Spark jobs with Run:ai.</li> </ul> <ul> <li> <p>Added support for Ray jobs. Ray is an open-source unified framework for scaling AI and Python applications. For more information, see Integrate Run:ai with Ray.</p> </li> <li> <p>Added integration with Weights &amp; Biases Sweep to allow data scientists to submit hyperparameter optimization workloads directly from the Run:ai UI. To configure sweep, see Sweep configuration.</p> </li> </ul> <ul> <li>Added support for XGBoost. XGBoost, which stands for Extreme Gradient Boosting, is a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library. It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems. For more information, see runai submit-dist xgboost</li> </ul> <p>Compatability</p> <ul> <li>Added support for multiple OpenShift clusters. For configuration information, see Installing additional Clusters.</li> </ul>"},{"location":"home/whats-new-2-13/#installation","title":"Installation","text":"<ul> <li>The manual process of upgrading Kubernetes CRDs is no longer needed when upgrading to the most recent version (2.13) of Run:ai.</li> <li>From Run:ai 2.12 and above, the control-plane installation has been simplified and no longer requires the creation of a backend values file. Instead, install directly using <code>helm</code> as described in Install the Run:ai Control Plane.  </li> <li>From Run:ai 2.12 and above, the air-gapped, control-plane installation now generates a <code>custom-env.yaml</code> values file during the preparation stage. This is used when installing the control-plane.</li> </ul>"},{"location":"home/whats-new-2-13/#known-issues","title":"Known issues","text":"Internal ID Description RUN-11005 Incorrect error messages when trying to run <code>runai</code> CLI commands in an OpenShift environment. RUN-11009 Incorrect error message when a user without permissions to tries to delete another user."},{"location":"home/whats-new-2-13/#fixed-issues_3","title":"Fixed issues","text":"Internal ID Description RUN-9039 Fixed an issue where in the new job screen, after toggling off the preemptible flag, and a job is submitted, the job still shows as preemptible. RUN-9323 Fixed an issue with a non-scaleable error message when scheduling hundreds of nodes is not successful. RUN-9324 Fixed an issue where the scheduler did not take into consideration the amount of storage so there is no explanation that pvc is not ready. RUN-9902 Fixed an issue in OpenShift environments, where there are no metrics in the dashboard because Prometheus doesn\u2019t have permissions to monitor the <code>runai</code> namespace after an installation or upgrade to 2.9. RUN-9920 Fixed an issue where the <code>canEdit</code> key in a policy is not validated properly for itemized fields when configuring an interactive policy. RUN-10052 Fixed an issue when loading a new job from a template gives an error until there are changes made on the form. RUN-10053 Fixed an issue where the Node pool column is unsearchable in the job list. RUN-10422 Fixed an issue where node details show running workloads that were actually finished (successfully/failed/etc.). RUN-10500 Fixed an issue where jobs are shown as running even though they don't exist in the cluster. RUN-10813 Fixed an issue in adding a <code>data source</code> where the path is case sensitive and didn't allow uppercase."},{"location":"home/whats-new-2-15/","title":"What's New 2.15 - December 3, 2023","text":""},{"location":"home/whats-new-2-15/#release-content","title":"Release Content","text":""},{"location":"home/whats-new-2-15/#researcher","title":"Researcher","text":""},{"location":"home/whats-new-2-15/#jobs-workloads-trainings-and-workspaces","title":"Jobs, Workloads, Trainings, and Workspaces","text":"<ul> <li> <p>Added support to run distributed workloads via the training view in the UI. You can configure distributed training on the following:</p> <ul> <li>Trainings form</li> <li>Environments form</li> </ul> <p>You can select <code>single</code> or <code>multi-node (distributed)</code> training. When configuring distributed training, you will need to select a framework from the list. Supported frameworks now include:</p> <ul> <li>PyTorch</li> <li>Tensorflow</li> <li>XGBoost</li> <li>MPI</li> </ul> <p>For Trainings configuration, see Adding trainings. See your Run:ai representative to enable this feature. For Environments configuration, see Creating an Environment.</p> </li> <li> <p>Preview the new Workloads view. Workloads is a new view for jobs that are running in the AI cluster. The Workloads view provides a more advanced UI than the previous Jobs UI. The new table format provides:</p> <ul> <li>Improved views of the data</li> <li>Improved filters and search</li> <li>More information</li> </ul> <p>Use the toggle at the top of the Jobs page to switch to the Workloads view. For more information.</p> </li> <li> <p>Improved support for Kubeflow Notebooks. Run:ai now supports the scheduling of Kubeflow notebooks with fractional GPUs. Kubeflow notebooks are identified automatically and appear with a dedicated icon in the Jobs UI.</p> </li> <li>Improved the Trainings and Workspaces forms. Now the runtime field for Command and Arguments can be edited directly in the new Workspace or Training creation form.</li> <li>Added new functionality to the Run:ai CLI that allows submitting a workload with multiple service types at the same time in a CSV style format. Both the CLI and the UI now offer the same functionality. For more information, see runai submit.</li> <li>Improved functionality in the <code>runai submit</code> command so that the port for the container is specified using the <code>nodeport</code> flag. For more information, see <code>runai submit</code> --service-type <code>nodeport</code>.</li> </ul>"},{"location":"home/whats-new-2-15/#credentials","title":"Credentials","text":"<ul> <li>Improved Credentials creation. A Run:ai scope can now be added to credentials. For more information, see Credentials.</li> </ul>"},{"location":"home/whats-new-2-15/#environments","title":"Environments","text":"<ul> <li>Added support for workload types when creating a new or editing existing environments. Select from <code>single-node</code> or <code>multi-node (distributed)</code> workloads. The environment is available only on feature forms which are relevant to the workload type selected.</li> </ul>"},{"location":"home/whats-new-2-15/#volumes-and-storage","title":"Volumes and Storage","text":"<ul> <li>Added support for Ephemeral volumes in Workspaces. Ephemeral storage is temporary storage that gets wiped out and lost when the workspace is deleted. Adding Ephemeral storage to a workspace ties that storage to the lifecycle of the Workspace to which it was added. Ephemeral storage is added to the Workspace configuration form in the Volume pane. For configuration information, see Create a new workspace.</li> </ul>"},{"location":"home/whats-new-2-15/#templates","title":"Templates","text":"<ul> <li>Added support for Run:ai a Scope in the template form. For configuration information, see Creating templates.</li> </ul>"},{"location":"home/whats-new-2-15/#deployments","title":"Deployments","text":"<ul> <li>Improvements in the New Deployment form include:<ul> <li>Support for Tolerations. Tolerations guide the system to which node each pod can be scheduled to or evicted by matching between rules and taints defined for each Kubernetes node.</li> <li>Support for Multi-Process Service (MPS). MPS is a service which allows the running of parallel processes on the same GPU, which are all run by the same userid. To enable MPS support, use the toggle switch on the Deployments form.</li> </ul> <p>Note</p> <p>If you do not use the same userid, the processes will run in serial and could possibly degrade performance.</p> </li> </ul>"},{"location":"home/whats-new-2-15/#auto-delete-jobs","title":"Auto Delete Jobs","text":"<ul> <li>Added new functionality to the UI and CLI that provides configuration options to automatically delete jobs after a specified amount of time upon completion. Auto-deletion provides more efficient use of resources and makes it easier for researchers to manage their jobs. For more configuration options in the UI, see Auto deletion (Step 9) in Create a new workspace. For more information on the CLI flag, see --auto-deletion-time-after-completion.</li> </ul>"},{"location":"home/whats-new-2-15/#runai-administrator","title":"Run:ai Administrator","text":""},{"location":"home/whats-new-2-15/#authorization","title":"Authorization","text":"<ul> <li>Run:ai has now revised and updated the Role Based Access Control (RBAC) mechanism, expanding the scope of Kubernetes. Using the new RBAC mechanism makes it easier for administrators to manage access policies across multiple clusters and to define specific access rules over specific scopes for specific users and groups. Along with the revised RBAC mechanism, new user interface views are introduced to support the management of users, groups, and access rules. For more information, see Role based access control.</li> </ul>"},{"location":"home/whats-new-2-15/#policies","title":"Policies","text":"<ul> <li>During Workspaces and Training creation, assets that do not comply with policies cannot be selected. These assets are greyed out and have a button on the cards when the item does not comply with a configured policy. The button displays information about which policies are non-compliant.</li> <li>Added configuration options to Policies in order to prevent the submission of workloads that use data sources of type <code>host path</code>. This prevents data from being stored on the node, so that data is not lost when a node is deleted. For configuration information, see Prevent Data Storage on the Node.</li> <li>Improved flexibility when creating policies which provide the ability to allocate a <code>min</code> and a <code>max</code> value for CPU and GPU memory. For configuration information, see GPU and CPU memory limits in Configuring policies.</li> </ul>"},{"location":"home/whats-new-2-15/#nodes-and-node-pools","title":"Nodes and Node Pools","text":"<ul> <li>Node pools are now enabled by default. There is no need to enable the feature in the settings.</li> </ul>"},{"location":"home/whats-new-2-15/#quotas-and-over-quota","title":"Quotas and Over-Quota","text":"<ul> <li>Improved control over how over-quota is managed by adding the ability to block over-subscription of the quota in Projects or Departments. For more information, see Limit Over-Quota.</li> <li>Improved the scheduler fairness for departments using the <code>over quota priority</code> switch (in Settings). When the feature flag is disabled, over-quota weights are equal to the deserved quota and any excess resources are divided in the same proportion as the in-quota resources. For more information, see Over Quota Priority.</li> <li>Added new functionality to always guarantee in-quota workloads at the expense of inter-Department fairness. Large distributed workloads from one department may preempt in-quota smaller workloads from another department. This new setting in the <code>RunaiConfig</code> file preserves in-quota workloads, even if the department quota or over-quota-fairness is not preserved. For more information, see Scheduler Fairness.</li> </ul>"},{"location":"home/whats-new-2-15/#control-and-visibility","title":"Control and Visibility","text":""},{"location":"home/whats-new-2-15/#dashboards","title":"Dashboards","text":"<ul> <li>To ease the management of AI CPU and cluster resources, a new CPU focused dashboard was added for CPU based environments. The dashboards display specific information for CPU based nodes, node-pools, clusters, or tenants. These dashboards also include additional metrics that are specific to CPU based environments. This will help optimize visual information eliminating the views of empty GPU dashlets. For more information see CPU Dashboard.</li> <li>Improved the Consumption report interface by moving the Cost settings to the General settings menu.</li> <li>Added table to the Consumption dashboard that displays the consumption and cost per department. For more information, see Consumption dashboard.</li> </ul>"},{"location":"home/whats-new-2-15/#nodes","title":"Nodes","text":"<ul> <li>Improved the readability of the Nodes table to include more detailed statuses and descriptions. The added information in the table makes it easier to inspect issues that may impact resource availability in the cluster. For more information, see Node and Node Pool Status.</li> </ul>"},{"location":"home/whats-new-2-15/#ui-enhancements","title":"UI Enhancements","text":"<ul> <li>Added the ability to download a CSV file from any page that contains a table. Downloading a CSV provides a snapshot of the page's history over time, and helps with compliance tracking. All the columns that are selected (displayed) in the table are downloaded to the file.</li> </ul>"},{"location":"home/whats-new-2-15/#installation-and-configuration","title":"Installation and Configuration","text":""},{"location":"home/whats-new-2-15/#cluster-installation-and-configuration","title":"Cluster Installation and configuration","text":"<ul> <li>New cluster wizard for adding and installing new clusters to your system.</li> </ul>"},{"location":"home/whats-new-2-15/#openshift-support","title":"OpenShift Support","text":"<ul> <li>Added support for <code>restricted</code> policy for Pod Security Admission (PSA) on OpenShift only. For more information, see [Pod security admission](../admin/runai-setup/cluster-setup/</li> <li>Added the ability, in OpenShift environments, to configure cluster routes created by Run:ai instead of using the OpenShift certificate. For more information, see the table entry Dedicated certificate for the researcher service route.</li> </ul>"},{"location":"home/whats-new-2-16/","title":"Version 2.16","text":""},{"location":"home/whats-new-2-16/#release-content-january-25-2024","title":"Release Content - January 25, 2024","text":""},{"location":"home/whats-new-2-16/#researcher","title":"Researcher","text":"<ul> <li>Added enterprise level security for researcher tools such as Jupyter Notebooks, VSCode, or any other URL associated with the workload. Using this feature, anyone within the organization requesting access to a specific URL will be redirected to the login page to be authenticated and authorized. This results in protected URLs which cannot be reached from outside the organization. Researchers can enhance the URL privacy by using the Private toggle which means that only the researcher who created the workload can is authorized to access it. The Private toggle is available per tool that uses an external URL as a connection type and is located in the workload creation from in the UI in the environment section. This toggle sets a flag of <code>isPrivate</code> in the <code>connections</code> section of a policy for the connection type <code>ExternalUrl</code>. For more information, see Creating a new Workspace.</li> </ul>"},{"location":"home/whats-new-2-16/#jobs-workloads-and-workspaces","title":"Jobs, Workloads, and Workspaces","text":"<ul> <li>Added the capability view and edit policies directly in the project submission form. Pressing on Policy will open a window that displays the effective policy. For more information, see Viewing Project Policies.</li> </ul> <ul> <li> <p>Running machine learning workloads effectively on Kubernetes can be difficult, but Run:ai makes it easy. The new Workloads experience introduces a simpler and more efficient way to manage machine learning workloads, which will appeal to data scientists and engineers alike. The Workloads experience provides a fast, reliable, and easy to use unified interface.</p> <ul> <li>Fast-query of data from the new workloads service.</li> <li>Reliable data retrieval and presentation in the CLI, UI, and API.</li> <li>Easy to use single unified view with all workload types in one place.</li> </ul> <p>For more information, see Workloads Overview.</p> </li> <li> <p>Changed the workload default auto deletion time after completion value from <code>Never</code> to <code>90 days</code>. This ensures that environments will be cleaned from old data. This field is editable by default, allowing researchers the ability to change the value while submitting a workload. Using workload policies, administrators can increase, decrease, set the default value to <code>never</code>, or even lock access to this value so researchers can not edit it when they submit workloads.</p> </li> </ul>"},{"location":"home/whats-new-2-16/#assets","title":"Assets","text":"<ul> <li>When creating an asset such as data sources, credentials, or others, the scope is limited to the cluster selected at the top of the UI.</li> </ul>"},{"location":"home/whats-new-2-16/#runai-administrator","title":"Run:ai Administrator","text":"<ul> <li>Added the capability for administrators to configure messages to users when they log into the platform. Messages are configured using the Message Editor screen. For more information, see Administrator Messages.</li> </ul>"},{"location":"home/whats-new-2-16/#monitoring-and-analytics","title":"Monitoring and Analytics","text":"<ul> <li> <p>Added to the dashboard updated GPU and CPU resource availability.</p> <ul> <li>Added a chart displaying the number of free GPUs per node. Free GPU are GPUs that have not been allocated to a workload.</li> <li>Added a dashlet that displays the total vs. ready resources for GPUs and CPUs. The dashlet indicates how many total nodes are in the platform, and how many are available. </li> </ul> </li> <li> <p>Added additional columns to the consumption report for both Projects and Departments tables. The new columns are:</p> <ul> <li>GPU Idle allocated hours\u2014the portion of time the GPUs spend idle from the total allocation hours.</li> <li>CPU usage hours\u2014the actual usage time of CPU.</li> <li>Memory usage time\u2014the actual usage time of CPU memory.</li> </ul> <p>For more information, see Consumption Dashboard.</p> </li> </ul>"},{"location":"home/whats-new-2-16/#authentication-and-authorization","title":"Authentication and Authorization","text":"<ul> <li>SSO users who have logged into the system will now be visible in the Users table. In addition, added a column to the Users table for the type of user that was created (Local or SSO). For more information, see Adding, Updating, and Deleting Users.</li> </ul>"},{"location":"home/whats-new-2-16/#policies","title":"Policies","text":"<ul> <li> <p>Added new Policy Manager. The new Policy Manager provides administrators the ability to impose restrictions and default values on system resources. The new Policy Manager provides a YAML editor for the configuration of the policies. Administrators can easily add both Workspace or Training policies. The editor makes it easy to see the configuration that has been applied and provides a quick and easy method to edit the policies. The new Policy Editor* brings other important policy features such as the ability to see non-compliant resources in workloads. For more information, see Policies.</p> </li> <li> <p>Added a new policy manager. Enabling the New Policy Manager provides new tools to discover how resources are not compliant. Non-compliant resources and will appear greyed out and cannot be selected. To see how a resource is not compliant, press on the clipboard icon in the upper right hand corner of the resource. Policies can also be applied to specific scopes within the Run:ai platform. For more information, see Viewing Project Policies.</p> </li> </ul>"},{"location":"home/whats-new-2-16/#control-and-visibility","title":"Control and Visibility","text":"<ul> <li>Improved the clarity of the status column in the Clusters view. Now users have more insight about the actual status of Run:ai on the cluster. Users can now see extended details about the state of the Run:ai installation and services on the cluster, and its connectivity state. For more information, see Cluster status.</li> </ul>"},{"location":"home/whats-new-2-16/#deprecation-notifications","title":"Deprecation Notifications","text":"<p>Deprecation notifications allow you to plan for future changes in the Run:ai Platform. Deprecated features will be available for two versions ahead of the notification. For questions, see your Run:ai representative.</p>"},{"location":"home/whats-new-2-16/#project-migration","title":"Project migration","text":"<ul> <li> <p>Run:ai will be deprecating the migration of projects between departments. This affects:</p> <ul> <li>API\u2014the <code>departmentId</code> field will be marked as deprecated in the<code>put</code> endpoint in the <code>projects</code> category.</li> <li>User Interface\u2014there will no longer be an option to:<ul> <li>migrate projects to another department, when deleting departments.</li> <li>change departments, when editing a project.</li> </ul> </li> </ul> </li> </ul>"},{"location":"home/whats-new-2-16/#api-deprecations","title":"API deprecations","text":""},{"location":"home/whats-new-2-16/#removed-apis-and-api-fields-completed-deprecation","title":"Removed APIs and API fields (completed deprecation)","text":"<p>The following list of API endpoints and fields that have completed their deprecation process and therefore will be changed as follows:</p> Endpoint Change /v1/k8s/clusters The endpoint was removed and is replaced by /api/v1/clusters /v1/k8s/clusters/{uuid} The endpoint was removed and is replaced by /api/v1/clusters/{uuid} <p>Run:ai does not recommend using API endpoints and fields marked as deprecated and will not add functionality to them. Once an API endpoint or field is marked as deprecated, Run:ai will stop supporting it after 2 major releases for self-hosted deployments, and after 6 months for SaaS deployments.</p> <p>For a full explanation of the API Deprecation policy, see the Run:ai API Policy</p>"},{"location":"home/whats-new-2-17/","title":"Version 2.17","text":""},{"location":"home/whats-new-2-17/#release-content-april-14-2024","title":"Release Content - April 14, 2024","text":"<ul> <li>Deprecation notifications</li> <li>Breaking changes</li> </ul>"},{"location":"home/whats-new-2-17/#researcher","title":"Researcher","text":""},{"location":"home/whats-new-2-17/#scheduler","title":"Scheduler","text":"<ul> <li> <p>Added functionality to configure over provisioning ratios for node pools running any kind of workload. Over provisioning assumes that workloads are either under utilizing or intermittently using GPUs. This indicates that the real utilization is lower than the actual GPU allocation requested. Over provisioning allows the administrator to condense more workloads on a single GPU than what the workload required. For more information, see Optimize performance with Node Level Scheduler.</p> </li> <li> <p>Added the GPU Resource Optimization feature to the UI. Now you can enable and configure GPU Portion (Fraction) limit and GPU Memory Limit from the UI. For more information, see Compute resources UI with Dynamic Fractions. </p> </li> <li> <p>Added the ability to set Run:ai as the default scheduler for any project or namespace. This provides the administrator the ability to ensure that all workloads in a project or namespace are scheduled using the Run:ai scheduler. For more information, see Setting Run:ai as default scheduler.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#jobs-workloads-and-workspaces","title":"Jobs, Workloads, and Workspaces","text":"<ul> <li> <p>Added to the workload details view, the ability to filter by pod. You can now filter metrics and logs per pod or all the pods. Also, the Workloads table now has additional columns including connections and preemtability adding more at a glance information about the workload. In addition, using the Copy &amp; edit button, you can submit a new workload via CLI based on the selected workload. For more information, see Workloads.</p> </li> <li> <p>Added Inference to workload types. Inference workloads can now be created and managed from the unified Workloads table. The Deployments workload type has been deprecated, and replaced with Inference workloads which are submitted using the workload form. For more information, see Inference and for submitting an Inference workload, see Submitting workloads.</p> </li> <li> <p>Added functionality that supports a single workloads submission selection. Now you can submit workloads by pressing + New workloads in the Workloads table. You can submit the following workloads from this table:</p> <ul> <li>Workspace</li> <li>Training</li> <li>Inference</li> </ul> <p>This improvement phases out the previous version's Workspace and Jobs tables. The Jobs table and submission forms have been deprecated and can be reactivated. To reenable the Jobs table and forms, press Tools &amp; settings, then General, then Workloads, and then Toggle the Jobs view and the Jobs submission buttons. For more information, see Submitting workloads.</p> </li> <li> <p>Added the ability to configure a Kubernetes readiness probe. The readiness probe detects resources and workloads that are ready to receive traffic.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#assets","title":"Assets","text":"<ul> <li> <p>Added the capability to use a ConfigMap as a data source. The ability to use a ConfigMap as a data source can be configured in the Data sources UI, the CLI, and as part of a policy. For more information, see Setup a ConfigMap as a data source, Setup a ConfigMap as a volume using the CLI.</p> </li> <li> <p>Added a Status column to the Credentials table, and the Data sources table. The Status column displays the state of the resource and provides troubleshooting information about that asset. For more information, see the Credentials table and the Data sources table.</p> </li> <li> <p>Added functionality for asset creation that validates the asset based on version compatibility of the cluster or the control plane within a specific scope. At time of asset creation, invalid scopes will appear greyed out and will show a pop-up with the reason for the invalidation. This improvement is designed to increase the confidence that an asset is created properly and successfully.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#runai-administrator","title":"Run:ai Administrator","text":""},{"location":"home/whats-new-2-17/#configuration-and-administration","title":"Configuration and Administration","text":"<ul> <li> <p>Introducing a new Tools &amp; Settings menu. The new Tools &amp; Settings menu provides a streamlined UI for administrators to configure the Run:ai environment. The new UI is divided into categories that easily identify the areas where the administrator can change settings. The new categories include:</p> <ul> <li>Analytics\u2014features related to analytics and metrics.</li> <li>Resources\u2014features related to resource configuration and allocation.</li> <li>Workloads\u2014features related to configuration and submission of workloads.</li> <li>Security\u2014features related to configuration of SSO (Single Sign On).</li> <li>Notifications\u2014used for system notifications.</li> <li>Cluster authentication\u2014snippets related to Researcher authentication.</li> </ul> <p>Some features are now labeled either Experimental or Legacy. Experimental features are new features in the environment, that may have certain instabilities and may not perform as expected. Legacy features are features that are in the process of being deprecated, and may be removed in future versions.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#clusters","title":"Clusters","text":"<ul> <li> <p>Added new columns to the Clusters table to show Kubernetes distribution and version. This helps administrators view potential compatibility issues that may arise.</p> </li> <li> <p>Improved the location of the cluster filter. The cluster filter has been relocated to filter bar and the drop down cluster filter in the header of the page has been removed. This improvement creates the following:</p> <ul> <li> <p>Filter assets by cluster in the following tables:</p> <ul> <li>Data sources</li> <li>Environments</li> <li>Computer resources</li> <li>Templates</li> <li>Credentials</li> </ul> </li> <li> <p>Creating a new asset, will automatically display only the scope of the selected cluster.</p> </li> <li>Prevention of account (top most level in the Scope) from being selected when creating assets.</li> <li>Enforcement a cluster specific scope. This increases the confidence that an asset is created properly and successfully.</li> </ul> <p>Note</p> <p>This feature is only applicable if the all the clusters are version 2.17 and above.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#monitoring-and-analytics","title":"Monitoring and Analytics","text":"<ul> <li> <p>Improved GPU Overview dashboard. This improvement provides rich and extensive GPU allocation and performance data and now has interactive tiles that provide direct links to the Nodes, Workloads, and Departments tables. Hover over tiles with graphs to show rich data in the selected time frame filter. Tiles with graphs can be downloaded as CSV files. The new dashboard is enabled by default. Use the Go back to legacy view to return to the previous dashboard style. For more information, see Dashboard analysis.</p> </li> <li> <p>Updated the knative and autoscaler metrics. Run:ai currently supports the following metrics:</p> <ul> <li>Throughput</li> <li>Concurrency</li> </ul> <p>For more information, see Autoscaling metrics.</p> </li> <li> <p>Improved availability of metrics by using Run:ai APIs. Using the API endpoints is now the preferred method to retrieve metrics for use in any application. For more information, see Metrics.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#authentication-and-authorization","title":"Authentication and Authorization","text":"<ul> <li> <p>Added new functionality to SAML 2.0 identity provider configuration in the Security category of the General settings. The added functionality assists with troubleshooting SSO configuration and authentication issues that may arise. Now administrators now have the ability to:</p> <ul> <li>View and edit the identity provider settings for SAML 2.0</li> <li>Upload or download the SAML 2.0 identity provider metadata XML file.</li> </ul> </li> </ul> <p>For more information, see SSO UI configuration.</p>"},{"location":"home/whats-new-2-17/#deprecation-notifications","title":"Deprecation Notifications","text":"<p>Deprecation notifications allow you to plan for future changes in the Run:ai Platform.</p>"},{"location":"home/whats-new-2-17/#feature-deprecations","title":"Feature deprecations","text":"<p>Deprecated features will be available for two versions ahead of the notification. For questions, see your Run:ai representative. The following features have been marked for deprecation:</p> <ul> <li>Jobs\u2014the Jobs feature (submission form and view) has been moved to the category of Legacy. To enable them, go to Tools &amp; Settings, General, open the Workloads pane, and then toggle the Jobs view and Job submission switch to the enabled position.</li> <li>Deployments\u2014the Deployments feature has been removed. It has been replaced by Inference workloads. For more information, see Jobs, Workloads, and Workspaces above.</li> <li>Workspaces view\u2014the Workspaces menu has been removed. You can now submit a Workspace workload using the + New workload form from the Workloads table.</li> </ul>"},{"location":"home/whats-new-2-17/#api-support-and-endpoint-deprecations","title":"API support and endpoint deprecations","text":"<p>The endpoints and parameters specified in the API reference are the ones that are officially supported by Run:ai. For more information about Run:ai's API support policy and deprecation process, see Developer overview.</p>"},{"location":"home/whats-new-2-17/#deprecated-apis-and-api-fields","title":"Deprecated APIs and API fields","text":"<p>The following list of API endpoints and fields that have been marked for deprecation:</p>"},{"location":"home/whats-new-2-17/#jobs-and-pods-api","title":"Jobs and Pods API","text":"Deprecated Replacement /v1/k8s/clusters/{uuid}/jobs /api/v1/workloads /v1/k8s/clusters/{uuid}/jobs/count /api/v1/workloads/count /v1/k8s/clusters/{uuid}/jobs/{jobId}/pods /api/v1/workloads/{workloadId}/pods /v1/k8s/clusters/{uuid}/pods /api/v1/workloads/pods"},{"location":"home/whats-new-2-17/#clusters-api","title":"Clusters API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterUuid}/metrics /api/v1/clusters/{clusterUuid}/metrics"},{"location":"home/whats-new-2-17/#authorization-and-authentication-api","title":"Authorization and Authentication API","text":"Deprecated Replacement /v1/k8s/auth/token/exchange /api/v1/token /v1/k8s/auth/oauth/tokens/refresh /api/v1/token /v1/k8s/auth/oauth/apptoken /api/v1/token /v1/k8s/users/roles /api/v1/authorization/roles /v1/k8s/users /api/v1/users /v1/k8s/users/{userId} /api/v1/users/{userId} /v1/k8s/users/{userId}/roles /api/v1/authorization/access-rules /v1/k8s/apps /api/v1/apps /v1/k8s/apps/{clientId} /api/v1/apps/{appId} /v1/k8s/groups /api/v1/authorization/access-rules /v1/k8s/groups/{groupName} /api/v1/authorization/access-rules /v1/k8s/clusters/{clusterId}/departments/{department-id}/access-control /api/v1/authorization/access-rules /api/v1/authorization/access-rules - <code>subjectIdFilter</code> field Use <code>filterBy</code> / <code>sortBy</code> fields /api/v1/authorization/access-rules - <code>scopeType</code> field Use <code>filterBy</code> / <code>sortBy</code> fields /api/v1/authorization/access-rules - <code>roleId</code> field Use <code>filterBy</code> / <code>sortBy</code> fields"},{"location":"home/whats-new-2-17/#projects-api","title":"Projects API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterId}/projects - <code>permissions</code> field /api/v1/authorization/access-rules /v1/k8s/clusters/{clusterId}/projects - <code>resources</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/projects - <code>deservedGpus</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/projects - <code>maxAllowedGpus</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/projects - <code>gpuOverQuotaWeight</code> field Use <code>nodePoolResources</code> field"},{"location":"home/whats-new-2-17/#departments-api","title":"Departments API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterId}/departments - <code>resources</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/departments - <code>deservedGpus</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/departments - <code>allowOverQuota</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/departments - <code>maxAllowedGpus</code> field Use <code>nodePoolResources</code> field"},{"location":"home/whats-new-2-17/#policy-api","title":"Policy API","text":"Deprecated Replacement /api/v1/policy/workspace /api/v2/policy/workspaces /api/v1/policy/training /api/v2/policy/trainings"},{"location":"home/whats-new-2-17/#logo-api","title":"Logo API","text":"Deprecated Replacement /v1/k8s/tenant/{tenantId}/logo /api/v1/logo"},{"location":"home/whats-new-2-17/#removed-apis-and-api-fields-completed-deprecation","title":"Removed APIs and API fields (completed deprecation)","text":"<p>The following list of API endpoints and fields that have completed their deprecation process and therefore will be changed as follows:</p>"},{"location":"home/whats-new-2-17/#assets-api","title":"Assets API","text":"Endpoint Change /api/v1/asset/compute <code>gpuRequest</code> field was removed and is replaced by the following fields:  * <code>gpuDevicesRequest</code> (New and mandatory)  * <code>gpuRequestType</code> (New and mandatory if  <code>gpuDevicesRequest=1</code> otherwise optional for values 0 or greater than 1)  * <code>gpuPortion</code> was changed to <code>gpuPortionRequest</code> and accepts values between 0 and 1 (for example 0.75)  * <code>gpuPortionLimit</code> (New and optional)  * <code>gpuMemory</code> was changed to <code>gpuMemoryRequest</code>  * <code>gpuMemoryLimit</code> (New and optional)"},{"location":"home/whats-new-2-17/#metrics-deprecations","title":"Metrics deprecations","text":"<p>The following metrics are deprecated and replaced by API endpoints. For details about the replacement APIs, see Changed Metrics:</p> Metric runai_active_job_cpu_requested_cores runai_active_job_memory_requested_bytes runai_cluster_cpu_utilization runai_cluster_memory_utilization runai_gpu_utilization_per_pod_per_gpu runai_gpu_utilization_per_workload runai_job_requested_gpu_memory runai_gpu_memory_used_mebibytes_per_workload runai_gpu_memory_used_mebibytes_per_pod_per_gpu runai_active_job_cpu_limits runai_job_cpu_usage runai_active_job_memory_limits runai_job_memory_used_bytes <p>Run:ai does not recommend using API endpoints and fields marked as deprecated and will not add functionality to them. Once an API endpoint or field is marked as deprecated, Run:ai will stop supporting it after 2 major releases for self-hosted deployments, and after 6 months for SaaS deployments.</p> <p>For a full explanation of the API Deprecation policy, see the Run:ai API Policy</p>"},{"location":"home/whats-new-2-17/#breaking-changes","title":"Breaking changes","text":"<p>Breaking changes notifications allow you to plan around potential changes that may interfere your current workflow when interfacing with the Run:ai Platform.</p>"},{"location":"home/whats-new-2-17/#metrics","title":"Metrics","text":"<p>Be aware that some names of metrics have been changed. For more information, see Changed Metrics.</p>"},{"location":"home/whats-new-2-18/","title":"Version 2.18","text":""},{"location":"home/whats-new-2-18/#release-content-june-30-2024","title":"Release Content - June 30, 2024","text":"<ul> <li>Deprecation notifications</li> <li>Breaking changes</li> </ul>"},{"location":"home/whats-new-2-18/#researcher","title":"Researcher","text":""},{"location":"home/whats-new-2-18/#jobs-workloads-and-workspaces","title":"Jobs, Workloads, and Workspaces","text":"<ul> <li> <p>Added to UI backoff limit functionality to Training and Workspace workloads. The backoff limit is the maximum number of retry attempts for failed workloads. After reaching the limit, the workload's status will change to <code>Failed</code>. The UI will display the default number of retries based on 6 attempts for each pod in the workload. (For example, 6 pods = 36 attempts).</p> </li> <li> <p>Updated Auto-deletion time default value from never to 30 days. The Auto-deletion time count starts when any Run:ai workload reaches a a completed, or failed status will be automatically deleted (including logs). This change only affects new or cloned workloads.</p> </li> <li> <p>Added new Data sources of type Secret to workload form. Data sources of type Secret are used to hide 3rd party access credentials when submitting workloads. For more information, see Submitting Workloads.</p> </li> <li> <p>Added new graphs for Inference workloads. The new graphs provide more information for Inference workloads to help analyze performance of the workloads. New graphs include Latency, Throughput, and number of replicas. For more information, see Workloads View. (Requires minimum cluster version v2.18).</p> </li> <li> <p>Added latency metric for autoscaling. This feature allows automatic scale-up/down the number of replicas of a Run:ai inference workload based on the threshold set by the ML Engineer. This ensures that response time is kept under the target SLA. (Requires minimum cluster version v2.18).</p> </li> <li> <p>Improved autoscaling for inference models by taking out ChatBot UI from models images. By moving ChatBot UI to predefined Environments, autoscaling is more accurate by taking into account all types of requests (API, and ChatBot UI). Adding a ChatBot UI environment preset by Run:ai allows AI practitioners to easily connect them to workloads.</p> </li> <li> <p>Added more precision to trigger auto-scaling to zero. Now users can configure a precise consecutive idle threshold custom setting to trigger Run:ai inference workloads to scale-to-zero. (Requires minimum cluster version v2.18).</p> </li> <li> <p>Added Hugging Face catalog integration of community models. Run:ai has added Hugging Face integration directly to the inference workload form, providing the ability to select models (vLLM models) from Hugging Face. This allows organizations to quickly experiment with the latest open source community language models. For more information on how Hugging Face is integrated, see Hugging Face.</p> </li> <li> <p>Improved access permissions to external tools. This improvement now allows more granular control over which personas can access external tools (external URLs) such as Jupyter Notebooks, Chatbot UI, and others. For configuration information, see Submitting workloads. (Requires minimum cluster version v2.18).</p> </li> <li> <p>Added a new API for submitting Run:ai inference workloads. This API allows users to easily submit inference workloads. This new API provides a consistent user experience for workload submission which maintains data integrity across all the user interfaces in the Run:ai platform. (Requires minimum cluster version v2.18).</p> </li> </ul>"},{"location":"home/whats-new-2-18/#command-line-interface-v2","title":"Command Line Interface V2","text":"<ul> <li> <p>Added an improved, researcher-focused Command Line Interface (CLI). The improved CLI brings usability enhancements for the Researcher which include:</p> <ul> <li>Support multiple clusters</li> <li>Self-upgrade</li> <li>Interactive mode</li> <li>Align CLI to be data consistent with UI and API</li> <li>Improved usability and performance</li> </ul> <p>This is an early access feature available for customers to use; however, be aware that there may be functional gaps versus the older, V1 CLI. For more information about installing and using the V2 CLI, see CLI V2. (Requires minimum cluster version v2.18).</p> </li> </ul>"},{"location":"home/whats-new-2-18/#gpu-memory-swap","title":"GPU memory swap","text":"<ul> <li>Added new GPU to CPU memory swap. To ensure efficient usage of an organization\u2019s resources, Run:ai provides multiple features on multiple layers to help administrators and practitioners maximize their existing GPUs resource utilization.  Run:ai\u2019s GPU memory swap feature helps administrators and AI practitioners to further increase the utilization of existing GPU HW by improving GPU sharing between AI initiatives and stakeholders. This is done by expending the GPU physical memory to the CPU memory which is typically an order of magnitude larger than that of the GPU. For more information see, GPU Memory Swap. (Requires minimum cluster version v2.18).</li> </ul>"},{"location":"home/whats-new-2-18/#yaml-workload-reference-table","title":"YAML Workload Reference table","text":"<ul> <li>Added a new YAML reference document that contains the value types and workload YAML references. Each table contains the field name, its description and the supported Run:ai workload types. The YAML field details contains information on the value type and currently available example workload snippets. For more information see, YAML Reference PDF.</li> </ul>"},{"location":"home/whats-new-2-18/#email-notifications-workload-status-and-timeouts","title":"Email Notifications - Workload Status and timeouts","text":"<ul> <li>Added new Email notification system. AI Practitioners can setup the types of workload notifications they want to receive. In order to receive email notifications, you must ensure that the admin has enabled and configured notifications for the tenant. For more information, see Email notifications.</li> </ul>"},{"location":"home/whats-new-2-18/#assets","title":"Assets","text":"<ul> <li>Improved UI asset creation form by adding a Description field. Now asset creators can add a free text description(max 250 characters) to any asset created. The description field is intended to help explain the nature and goal of the asset, this way AI practitioners will be able to make better decisions when choosing their assets in workload creation.</li> </ul>"},{"location":"home/whats-new-2-18/#runai-administrator","title":"Run:ai Administrator","text":""},{"location":"home/whats-new-2-18/#data-sources","title":"Data Sources","text":"<ul> <li> <p>Added Data Volumes new feature. Data Volumes are snapshots of datasets stored in Kubernetes Persistent Volume Claims (PVCs). They act as a central repository for training data, and offer several key benefits. </p> <ul> <li>Managed with dedicated permissions\u2014Data Admins, a new role within Run.ai, have exclusive control over data volume creation, data population, and sharing.</li> <li>Shared between multiple scopes\u2014unlike other Run:ai data sources, data volumes can be shared across projects, departments, or clusters. This promotes data reuse and collaboration within your organization.</li> <li>Coupled to workloads in the submission process\u2014similar to other Run:ai data sources, Data volumes can be easily attached to AI workloads during submission, specifying the data path within the workload environment.</li> </ul> <p>For more information, see Data Volumes. (Requires minimum cluster version v2.18).</p> </li> <li> <p>Added new data source of type Secret. Run:ai now allows you to configure a Credential as a data source. A Data source of type Secret is best used in workloads so that access to 3rd party interfaces and storage used in containers, keep access credentials hidden. For more information, see Secrets as a data source.</p> </li> <li> <p>Updated the logic of data source initializing state which keeps the workload in \u201cinitializing\u201d status until S3 data is fully mapped. For more information see Sidecar containers documentation.</p> </li> <li> <p>Additional storage unit sizes MiB, GiB &amp; TiB (Megabyte, Gigabyte, and Terabyte respectively) added to the UI and API when creating a new data source of type PVC.</p> </li> </ul>"},{"location":"home/whats-new-2-18/#credentials","title":"Credentials","text":"<ul> <li>Added new Generic secret to Credentials. Credentials had been used only for access to data sources (S3, Git, etc.). However, AI practitioners need to use secrets to access sensitive data (interacting with 3rd party APIs, or other services) without having to put their credentials in their source code. Generic secrets leverage multiple key value pairs which helps reduce the number of Kubernetes resources and simplifies resource management by reducing the overhead associated with maintaining multiple Secrets. Generic secrets are best used as a data source of type Secret so that they can be used in containers to keep access credentials hidden. (Requires minimum cluster version v2.18).</li> </ul>"},{"location":"home/whats-new-2-18/#single-sign-on","title":"Single Sign On","text":"<ul> <li> <p>Added support for Single Sign On using OpenShift v4 (OIDC based). When using OpenShift, you must first define OAuthClient which interacts with OpenShift's OAuth server to authenticate users and request access tokens. For more information, see Single Sign-On.</p> </li> <li> <p>Added OIDC scopes to authentication requests. OIDC Scopes are used to specify what access privileges are being requested for access tokens. The scopes associated with the access tokens determine what resource are available when they are used to access OAuth 2.0 protected endpoints. Protected endpoints may perform different actions and return different information based on the scope values and other parameters used when requesting the presented access token. For more information, see UI configuration.</p> </li> </ul>"},{"location":"home/whats-new-2-18/#ownership-protection","title":"Ownership protection","text":"<ul> <li>Added new ownership protection feature. Run:ai Ownership Protection ensures that only authorized users can delete or modify workloads. This feature is designed to safeguard important jobs and configurations from accidental or unauthorized modifications by users who did not originally create the workload. For configuration information, see your Run:ai representative.</li> </ul>"},{"location":"home/whats-new-2-18/#email-notifications","title":"Email notifications","text":"<ul> <li> <p>Added new email notifications feature. Email Notifications sends alerts for critical workload life cycle changes empowering data scientists to take necessary actions and prevent delays.</p> <ul> <li>System administrators will need to configure the email notifications. For more information, see System notifications.</li> </ul> </li> </ul>"},{"location":"home/whats-new-2-18/#policy-for-distributed-and-inference-workloads-in-the-api","title":"Policy for distributed and inference workloads in the API","text":"<ul> <li>Added a new API for creating distributed training workload policies and inference workload policies. These new policies in the API allow to set defaults, enforce rules and impose setup on distributed training and inference workloads. For distributed policies, worker and master may require different rules due to their different specifications. The new capability is currently available via API only. Documentation on submitting policies to follow shortly.</li> </ul>"},{"location":"home/whats-new-2-18/#policy-for-distributed-and-inference-workloads-in-the-api_1","title":"Policy for distributed and inference workloads in the API","text":"<ul> <li>Added a new API for creating distributed training workload policies and inference workload policies. These new policies in the API allow to set defaults, enforce rules and impose setup on distributed training and inference workloads. For distributed policies, worker and master may require different rules due to their different specifications. The new capability is currently available via API only. Documentation on submitting policies to follow shortly.</li> </ul>"},{"location":"home/whats-new-2-18/#deprecation-notifications","title":"Deprecation Notifications","text":"<p>Existing notifications feature requires cluster configuration, is being deprecated in favor of an improved Notification System. If you have been using the existing notifications feature in the cluster, you can continue to use it for the next two versions. It is recommend that you change to the new notifications system in the Control Plane for better control and improved message granularity.</p>"},{"location":"home/whats-new-2-18/#feature-deprecations","title":"Feature deprecations","text":"<p>Deprecated features will be available for two versions ahead of the notification. For questions, see your Run:ai representative.</p>"},{"location":"home/whats-new-2-18/#api-support-and-endpoint-deprecations","title":"API support and endpoint deprecations","text":"<p>The endpoints and parameters specified in the API reference are the ones that are officially supported by Run:ai. For more information about Run:ai's API support policy and deprecation process, see note under Developer overview.</p>"},{"location":"home/whats-new-2-18/#deprecated-apis-and-api-fields","title":"Deprecated APIs and API fields","text":""},{"location":"home/whats-new-2-18/#cluster-api-deprecation","title":"Cluster API Deprecation","text":"<p>Run:ai REST API now supports job submission. The older, Cluster API is now deprecated. </p>"},{"location":"home/whats-new-2-18/#departments-api","title":"Departments API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterId}/departments /api/v1/org-unit/departments /v1/k8s/clusters/{clusterId}/departments/{department-id} /api/v1/org-unit/departments/{departmentId} /v1/k8s/clusters/{clusterId}/departments/{department-id} /api/v1/org-unit/departments/{departmentId}+PUT/PATCH /api/v1/org-unit/departments/{departmentId}/resources"},{"location":"home/whats-new-2-18/#projects-api","title":"Projects API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterId}/projects /api/v1/org-unit/projects /v1/k8s/clusters/{clusterId}/projects/{id} /api/v1/org-unit/projects/{projectId} /v1/k8s/clusters/{clusterId}/projects/{id} /api/v1/org-unit/projects/{projectId} +\u00a0/api/v1/org-unit/projects/{projectId}/resources <p>Run:ai does not recommend using API endpoints and fields marked as deprecated and will not add functionality to them. Once an API endpoint or field is marked as deprecated, Run:ai will stop supporting it after 2 major releases for self-hosted deployments, and after 6 months for SaaS deployments.</p> <p>For a full explanation of the API Deprecation policy, see the Run:ai API Policy</p>"},{"location":"home/whats-new-2-18/#breaking-changes","title":"Breaking changes","text":"<p>Breaking changes notifications allow you to plan around potential changes that may interfere your current workflow when interfacing with the Run:ai Platform.</p>"},{"location":"home/whats-new-2-19/","title":"What\u2019s New in Version 2.19","text":""},{"location":"home/whats-new-2-19/#release-content","title":"Release Content  <ul> <li>Deprecation notifications </li> </ul>","text":""},{"location":"home/whats-new-2-19/#researchers","title":"Researchers","text":""},{"location":"home/whats-new-2-19/#improved-visibility-into-pending-workloads","title":"Improved visibility into pending workloads","text":"<p>For workloads with the status of \"Pending,\" the user can click the \u201ci\u201d icon next to the status to view details of why the workload hasn\u2019t been scheduled. (Requires a minimum cluster version of v2.19)    </p>"},{"location":"home/whats-new-2-19/#new-workload-events","title":"New workload events","text":"<p>There are now new GPU resource optimization-related messages that are viewable as workload events. These events help users understand the decisions made by the Run:ai GPU toolkit while handling Run:ai\u2019s GPU resource optimization features.   Run:ai\u2019s GPU resource optimization offers unique capabilities that take GPU utilization to a new level and helps customers increase their productivity while maximizing their return on GPU investment. (Requires a minimum cluster version of v2.19)    </p>"},{"location":"home/whats-new-2-19/#improved-command-line-interface-autocompletion","title":"Improved command line interface autocompletion","text":"<p>CLI V2 now autocompletes nouns such as project names and workload names for better data consistency with the UI, auto-upgrades, and interactive mode.</p>"},{"location":"home/whats-new-2-19/#details-pane-in-the-workloads-view","title":"Details pane in the Workloads view","text":"<p>A new DETAILS tab for workloads has been added and presents additional workload information, including Container command, Environment variables, and CLI command syntax (if the workload was submitted via CLI). </p>"},{"location":"home/whats-new-2-19/#container-path-outside-the-data-source-asset","title":"Container path outside the data source asset","text":"<p>AI practitioners can now override the predefined container path for each data source when submitting a workload via the Run:ai UI. While the container path must still be specified as part of the data source asset, researchers can now override the default container path when submitting workloads. (Requires a minimum cluster version of v2.16)     </p>"},{"location":"home/whats-new-2-19/#node-toleration-for-workloads","title":"Node toleration for workloads","text":"<p>Researchers can now optionally set tolerations for workloads, letting them bypass node taints during workload submission via the Run:ai UI.   To use this feature, make sure it is activated under General Settings.   For more information, refer to the Kubernetes Taints and Tolerations Guide. (Requires a minimum cluster version of v2.19)    </p>"},{"location":"home/whats-new-2-19/#topology-aware-scheduling","title":"Topology-aware scheduling","text":"<p>When submitting a distributed training workload through the Run:ai UI, researchers can enable topology-aware scheduling. This feature allows an optimized placement within specific placement groups, such as regions, availability zones, or other topologies. To use this, make sure it is activated under General Settings. (Requires a minimum cluster version of v2.19)    </p>"},{"location":"home/whats-new-2-19/#bulk-deletion-of-workloads","title":"Bulk deletion of workloads","text":"<p>Users can now delete workloads in bulk via the Run:ai UI. They\u2019ll be notified if they try to delete workloads for which they don\u2019t have permissions (and those workloads will not be deleted in this process). Multi-selection can also be done using standard keyboard functions. (Requires a minimum cluster version of v2.19)     </p>"},{"location":"home/whats-new-2-19/#enhanced-policy-representation-in-the-runai-ui","title":"Enhanced policy representation in the Run:ai UI","text":"<p>To improve AI practitioners' understanding of administrators\u2019 policy rules and defaults, the UI now includes more clarity to the enforcement and the default values representation for workload fields that are not encapsulated in the asset selection. This update aims to make policy enforcement more intuitive and transparent for practitioners. (Requires a minimum cluster version of v2.18)    </p>"},{"location":"home/whats-new-2-19/#configuration-of-credentials-as-environment-variables","title":"Configuration of credentials as environment variables","text":"<p>Researchers can now easily define pre-configured credentials as environment variables to access private resources. This is available through the Run:ai UI during the workload submission process, specifically under the runtime settings section. (Requires a minimum cluster version pf v2.18)    </p>"},{"location":"home/whats-new-2-19/#expanded-scope-of-configmap-as-data-source","title":"Expanded scope of ConfigMap as data source","text":"<p>When creating a data source of type ConfigMap, researchers can now not only select a project but also a cluster or department. (Requires a minimum cluster version of v2.19)    </p>"},{"location":"home/whats-new-2-19/#improved-workload-scheduling-algorithm","title":"Improved workload scheduling algorithm","text":"<p>The Run:ai scheduler algorithm for handling large distributed workloads has been improved and is now more efficient, resulting in better handling of large distributed workloads, and better performance. (Requires a minimum cluster version of v2.19)    </p>"},{"location":"home/whats-new-2-19/#ml-engineer-inference","title":"ML Engineer (Inference)","text":""},{"location":"home/whats-new-2-19/#additional-data-sources-for-inference-workloads","title":"Additional data sources for inference workloads","text":"<p>When submitting an inference workload via the UI and API, users can now use NFS and hostPath data sources. (Requires a minimum cluster version of v2.19)    </p>"},{"location":"home/whats-new-2-19/#hugging-face-integration-improvements","title":"Hugging Face integration improvements","text":"<p>To reduce errors when submitting inference workloads, additional validations are done for the Hugging Face integration, ensuring that only valid models are submitted, thus enhancing overall reliability. (Requires a minimum cluster version of v2.19)    </p>"},{"location":"home/whats-new-2-19/#rolling-inference-updates","title":"Rolling inference updates","text":"<p>ML engineers can now roll updates onto existing inference workloads. Once the revised workload (the update) is up and running, request traffic is redirected to the new version of the workload and the previous version is terminated, ensuring that services are not impacted during the update.</p> <p>See Inference overview for more information. (Requires a minimum cluster version of v2.19)    </p>"},{"location":"home/whats-new-2-19/#inference-endpoint-authorization","title":"Inference endpoint authorization","text":"<p>When sharing inference endpoints securely using Run:ai, ML engineers can limit access to the endpoint by specifying the authorized users or groups allowed to use the service (i.e., send requests to the endpoint) after being authenticated. This restriction is especially important when handling sensitive information or when you want to manage costs by sharing the service with a controlled group of consumers. (Requires a minimum cluster version of v2.19)    </p>"},{"location":"home/whats-new-2-19/#runai-developer","title":"Run:ai Developer","text":""},{"location":"home/whats-new-2-19/#metrics-and-telemetry","title":"Metrics and telemetry","text":"<p>Additional metrics and telemetry are available via the API. For more information, see the details below and in Metrics API: </p> <ul> <li>Metrics (over time)  <ul> <li>Cluster  <ul> <li>TOTAL_GPU_NODES  </li> <li>GPU_UTILIZATION_DISTRIBUTION  </li> <li>UNALLOCATED_GPU  </li> </ul> </li> <li>Nodepool  <ul> <li>TOTAL_GPU_NODES   </li> <li>GPU_UTILIZATION_DISTRIBUTION   </li> <li>UNALLOCATED_GPU  </li> </ul> </li> <li>Workload  <ul> <li>GPU_ALLOCATION  </li> </ul> </li> <li>Node  <ul> <li>GPU_UTILIZATION_PER_GPU  </li> <li>GPU_MEMORY_UTILIZATION_PER_GPU  </li> <li>GPU_MEMORY_USAGE_BYTES_PER_GPU  </li> <li>CPU_USAGE_CORES  </li> <li>CPU_UTILIZATION  </li> <li>CPU_MEMORY_USAGE_BYTES  </li> <li>CPU_MEMORY_UTILIZATION  </li> </ul> </li> </ul> </li> <li>Telemetry (current time)  <ul> <li>Node  <ul> <li>ALLOCATED_GPUS  </li> <li>TOTAL_CPU_CORES  </li> <li>USED_CPU_CORES  </li> <li>ALLOCATED_CPU_CORES  </li> <li>TOTAL_GPU_MEMORY_BYTES  </li> <li>USED_GPU_MEMORY_BYTES  </li> <li>TOTAL_CPU_MEMORY_BYTES  </li> <li>USED_CPU_MEMORY_BYTES  </li> <li>ALLOCATED_CPU_MEMORY_BYTES  </li> <li>IDLE_ALLOCATED_GPUS</li> </ul> </li> </ul> </li> </ul>"},{"location":"home/whats-new-2-19/#administrator","title":"Administrator","text":""},{"location":"home/whats-new-2-19/#pagination-in-user-api","title":"Pagination in user API","text":"<p>Pagination has been added, removing the limitation to the number of users listed in the Run:ai UI.</p>"},{"location":"home/whats-new-2-19/#audit-log","title":"Audit log","text":"<p>The audit log has been updated, so system admins can view audit logs directly in the Run:ai UI and download them in CSV or JSON formats, providing flexible options for data analysis and compliance reporting. Version 2.19 reintroduces a fully functional audit log (event history), ensuring comprehensive tracking across projects, departments, access rules, and more. In the new version, all entities are logged except logins and workloads.    For more information, see Audit logs.</p>"},{"location":"home/whats-new-2-19/#platform-administrator","title":"Platform Administrator","text":""},{"location":"home/whats-new-2-19/#department-scheduling-rules","title":"Department scheduling rules","text":"<p>Scheduling rules have been added at the department level. For more information, see scheduling rules.  </p>"},{"location":"home/whats-new-2-19/#department-node-pool-priority","title":"Department node pool priority","text":"<p>Node pool priority has been added at the department level. For more information, see node pools </p>"},{"location":"home/whats-new-2-19/#department-and-project-grids","title":"Department and project grids","text":"<p>There is now improved filtering and sorting in the Projects and Departments views, including a multi-cluster view and new filters.   </p>"},{"location":"home/whats-new-2-19/#overview-dashboard","title":"Overview dashboard","text":"<p>\u201cIdle allocated GPU devices\u201d has been added to the Overview dashboard.</p>"},{"location":"home/whats-new-2-19/#workload-policy-for-distributed-training-workloads-in-the-runai-ui","title":"Workload policy for distributed training workloads in the Run:ai UI","text":"<p>Distributed workload policies can now be created via the Run:ai UI. Admins can set defaults, enforce rules, and impose setup on distributed training through the UI YAML, as well as view the distributed policies (both in the policy grid and while submitting workloads). For distributed policies, workers and leaders may require different rules due to their different specifications. (Requires a minimum cluster version of v2.18)    </p>"},{"location":"home/whats-new-2-19/#reconciliation-of-policy-rules","title":"Reconciliation of policy rules","text":"<p>A reconciliation mechanism for policy rules has been added to enhance flexibility in the policy submission process. Previously, if a specific field was governed by a policy for a certain hierarchy, other organizational units couldn\u2019t submit a policy with rules that regarded this specific field. Now, new policies for hierarchies that mention an existing policy field will no longer be blocked.   The effective rules are selected based on the following logic:  1. For the compute and security sections in the workload spec of the Run:ai API, the highest hierarchy is chosen for the effective policy (tenant &gt; cluster &gt; department &gt; project).  2. For any other fields in the policy, the lowest hierarchy closest to the actual workload becomes the effective for the policy (similar to policy defaults).    Additionally, while viewing the effective policy, each rule displays its source of the origin policy, allowing users to clearly understand the selected hierarchy of the effective policy.  |  (Requires a minimum cluster version of v2.18)  </p>"},{"location":"home/whats-new-2-19/#infrastructure-administrator","title":"Infrastructure Administrator","text":""},{"location":"home/whats-new-2-19/#support-for-cos-over-gke","title":"Support for COS over GKE","text":"<p>With Run:ai version 2.19, the Run:ai cluster on Google Kubernetes Engine (GKE) supports Container-Optimized OS (COS) when NVIDIA GPU Operator 24.6 or newer is installed. This is in addition to the already supported Ubuntu on GKE.  </p>"},{"location":"home/whats-new-2-19/#runai-and-karpenter","title":"Run:ai and Karpenter","text":"<p>Run:ai now supports working with Karpenter. Karpenter is an open-source Kubernetes cluster auto-scaler built for cloud deployments. Karpenter optimizes the cloud cost of a customer\u2019s cluster by moving workloads between different node types, bin-packing nodes, using lower-cost nodes where possible, scaling up new nodes on demand, and shutting down unused nodes with the goal of optimizing and reducing costs.  (Requires a minimum cluster version of v2.19)  </p> <p>Please read the documentation for more information on Run:ai and Karpenter integration considerations.</p>"},{"location":"home/whats-new-2-19/#control-and-visibility-ui-changes","title":"Control and Visibility (UI changes)","text":""},{"location":"home/whats-new-2-19/#new-runai-ui-navigation","title":"New Run:ai UI navigation","text":"<p>The platform navigation has been updated to offer a more modern design, easier  navigation, and address all personas interacting with the UI.  </p> <p>The left-side menu now has seven categories, each with its own reorganized sub-options that appear in the pane next to the menu options.</p> <p>If you close the sub-options pane, you can hover over the categories, and the sub-options float and can be used in the same way.</p> <p>The options presented in the menu and categories continue to match each user\u2019s permissions, as in the legacy navigation.</p> <p>Below is the full list of menu and sub-options and changes:</p> <p>Analytics   Displays the Run:ai dashboards allowing the different users to analyze, plan, and improve system performance AI workload execution.    This category contains the following options:</p> <ul> <li>Overview  </li> <li>Quota management  </li> <li>Analytics  </li> <li>Consumption  </li> <li>Multi-cluster overview  </li> </ul> <p>Workload manager Enables AI practitioners to develop modes, train them, and deploy them into production.  All supported tools and capabilities can be found here. This category contains the following options: </p> <ul> <li>Workloads  </li> <li>Deleted workloads (now separated from current workloads. If not visible, it can be activated from Settings -&gt; Workloads -&gt; Deleted workloads)  </li> <li>Templates  </li> <li>Assets (these options are visible via a collapsible menu)  <ul> <li>Models  </li> <li>Environments  </li> <li>Compute resources  </li> <li>Data sources  </li> <li>Credentials</li> </ul> </li> </ul> <p>Resources Enables viewing and managing all cluster resources. In the new navigation, nodes and node pools have been split into different grids. This category contains the following options:</p> <ul> <li>Clusters  </li> <li>Node pools (separated from the Nodes page to its own page)  </li> <li>Nodes  </li> </ul> <p>Organization  Maps system organizations to ensure that resource allocation and policies align with the organizational structure, business projects, and priorities. This category contains the following options:  </p> <ul> <li>Departments  </li> <li>Projects  </li> </ul> <p>Access Makes it possible to provide authorization of the different system users to perform actions and alignment with their role and scope of projects within the organization.  This was moved from the legacy menu where it appeared in the header of the screen under Tools and Settings. This category contains the following options:  </p> <ul> <li>Users  </li> <li>Applications  </li> <li>Roles (separated from the Access rules and roles page to its own page)  </li> <li>Access rules (separated from the Access rules and roles page to its own page)  </li> </ul> <p>Policies Presents the tools to enforce controls over the AI infrastructure enabling different users to be effective while working in alignment with organizational policies.  This category contains the following options:  </p> <ul> <li>Workload policies  </li> </ul> <p>Admin  Presents all administrator functions of the Run:ai platform.  This was moved from the legacy menu where it appeared in the header of the screen under Tools and Settings. This category contains the following options:  </p> <ul> <li>General settings (previously General)  </li> <li>Event history  </li> </ul> <p>For users with more than one cluster, in the legacy version the cluster selection appeared in the header of the page. In the new navigation, the cluster selection is part of the grid and changes only affect the items on that page.  </p> <p>If a user prefers not to use the new UI navigation, there is an option to switch back to the legacy navigation by clicking the Back to legacy navigation option.  </p> <p>Installation and configuration</p> <ul> <li>Tenant logos can now be uploaded to the Run:ai UI via API. The logo should be in base64 format and should not be white to avoid blending into the background. The logo should be no more than 20px tall. See Upload logo for tenant API.  </li> <li>Run:ai now supports NVIDIA GPU Operator version 24.6  </li> <li>Run:ai now supports Kubernetes version 1.31</li> </ul>"},{"location":"home/whats-new-2-19/#deprecation-notifications","title":"Deprecation notifications","text":""},{"location":"home/whats-new-2-19/#feature-deprecations","title":"Feature deprecations","text":""},{"location":"home/whats-new-2-19/#legacy-jobs-view","title":"Legacy Jobs view","text":"<p>The legacy Jobs view will be fully deprecated in the Q1/25 release. We recommend that all users adopt the Workloads view, which offers all the capabilities of the legacy Jobs view with additional enhancements.      SaaS customers will gradually be transitioned to the Workloads view during Q4/24.  </p> <p>Note</p> <p>Users can still submit workloads via the legacy Jobs submission form.</p>"},{"location":"home/whats-new-2-19/#dynamic-mig-deprecation","title":"Dynamic MIG deprecation","text":"<p>Dynamic MIG deprecation process starts with Run:ai v2.19 (Q4/24 release)</p> <ul> <li>The feature is still available and MIG Profile APIs still function but are marked as Deprecated. See the table below for more details.</li> <li>In Q1/25 release, \u2018Dynamic MIG\u2019 will not be usable anymore but the APIs will still be accessible.</li> <li>In Q2/25 all \u2018Dynamic MIG\u2019 APIs will be fully deprecated.</li> </ul>"},{"location":"home/whats-new-2-19/#legacy-navigation-runai-ui","title":"Legacy navigation - Run:ai UI","text":"<p>The legacy navigation will be fully deprecated in the Q1/25 release, and during Q1/25 for SaaS customers.</p>"},{"location":"home/whats-new-2-19/#api-support-and-endpoint-deprecations","title":"API support and endpoint deprecations","text":"Deprecated Replacement /v1/k8s/audit /api/v1/audit/log /api/v1/asset/compute/spec/migProfile /api/v1/workloads/spec/compute/migProfile /api/v1/workloads/workspaces/spec/compute/migProfile /api/v1/workloads/Trainings/spec/compute/migProfile /api/v1/workloads/Inferences/spec/compute/migProfile /api/v1/workloads/distributed/spec/compute/migProfile /api/v1/workloads/distributed/masterSpec/compute/migProfile <p>Run:ai does not recommend using API endpoints and fields marked as deprecated and will not add functionality to them. Once an API endpoint or field is marked as deprecated, Run:ai will stop supporting it after 2 major releases for self-hosted deployments, and after 6 months for SaaS deployments.</p> <p>For a full explanation of the API Deprecation policy, see the Run:ai API Policy</p>"},{"location":"home/whats-new-2-19/#documentation-enhancements","title":"Documentation enhancements","text":""},{"location":"home/whats-new-2-19/#workload-policy-documentation","title":"Workload policy documentation","text":"<p>A comprehensive set of articles detailing the usage and the process of submitting new workload policies has been introduced. It covers the structure, syntax, best practices, and examples for configuring policy YAML files. The new documentation includes step-by-step explanations of how to create a new rule in a policy, together with information of the different value types, rule types, and policy spec sections. For more information, refer to the Policies section.  </p>"},{"location":"home/changelog/hotfixes-2-13/","title":"Changelog Version 2.13","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.13.</p>"},{"location":"home/changelog/hotfixes-2-13/#version-21348-march-14-2024","title":"Version 2.13.48 - March 14, 2024","text":"Internal ID Description RUN-16787 Fixed an issue after an upgrade to 2.13 where distributed PyTorch jobs were not able to run due to PVCs being assigned to only worker pods. RUN-16626 Fixed an issue in SSO environments, where Workspaces created using a template were assigned the template creator's UID/GID and not the Workspace creator's UID/GID. RUN-16357 Fixed an issue where pressing the Project link in Jobs screen redirects the view to the Projects of a different cluster in multi-cluster environments."},{"location":"home/changelog/hotfixes-2-13/#version-21343-february-15-2024","title":"Version 2.13.43 - February 15, 2024","text":"Internal ID Description RUN-14946 Fixed an issue where Dashboards are displaying the hidden Grafana path."},{"location":"home/changelog/hotfixes-2-13/#version-21337","title":"Version 2.13.37","text":"Internal ID Description RUN-13300 Fixed an issue where projects will appear with a status of empty while waiting for the project controller to update its status. This was caused because the cluster-sync works faster than the project controller."},{"location":"home/changelog/hotfixes-2-13/#version-21335-december-19-2023","title":"Version 2.13.35 - December 19, 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content","title":"Release content","text":"<ul> <li>Added the ability to set node affinity for Prometheus.</li> </ul>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-14472 Fixed an issue where template updates were not being applied to the workload. RUN-14434 Fixed an issue where <code>runai_allocated_gpu_count_per_gpu</code> was multiplied by seven. RUN-13956 Fixed an issue where editing templates failed. RUN-13825 Fixed an issue when deleting a job that is allocated a fraction of a GPU, an associated configmap is not deleted. RUN-13343 Fixed an issue in pod status calculation."},{"location":"home/changelog/hotfixes-2-13/#version-21331","title":"Version 2.13.31","text":"Internal ID Description RUN-11367 Fixed an issue where a double click on SSO Users redirects to a blank screen. RUN-10560 Fixed an issue where the <code>RunaiDaemonSetRolloutStuck</code> alert did not work."},{"location":"home/changelog/hotfixes-2-13/#version-21325","title":"Version 2.13.25","text":"Internal ID Description RUN-13171 Fixed an issue when a cluster is not connected the actions in the Workspace and Training pages are still enabled. After the corrections, the actions will be disabled."},{"location":"home/changelog/hotfixes-2-13/#version-21321","title":"Version 2.13.21","text":"Internal ID Description RUN-12563 Fixed an issue where users are unable to login after upgrading the control plane from 2.9.16 to 2.13.16. To correct the issue, secrets need to be upgraded manually in keycloak."},{"location":"home/changelog/hotfixes-2-13/#version-21320-september-28-2023","title":"Version 2.13.20 - September 28, 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content_1","title":"Release content","text":"<ul> <li>Added the prevention of selecting tenant or department scopes for credentials, and the prevention of selecting s3, PVC, and Git data sources if the cluster version does not support these.</li> <li>Quota management is now enabled by default.</li> </ul> Internal ID Description RUN-12923 Fixed an issue in upgrading due to a misconfigured Docker image for airgapped systems in 2.13.19. The helm chart contained an error, and the image is not used even though it is packaged as part of the tar. RUN-12928, RUN-12968 Fixed an issue in upgrading Prometheus due to a misconfigured image for airgapped systems in 2.13.19. The helm chart contained an error, and the image is not used even though it is packaged as part of the tar. RUN-12751 Fixed an issue when upgrading from 2.9 to 2.13 results with a missing engine-config file. RUN-12717 Fixed an issue where the user that is logged in as researcher manager can't see the clusters. RUN-12642 Fixed an issue where assets-sync could not restart due to failing to get token from control plane. RUN-12191 Fixed an issue where there was a timeout while waiting for the <code>runai_allocated_gpu_count_per_project</code> metric to return values. RUN-10474 Fixed an issue where the <code>runai-conatiner-toolkit-exporter</code> DaemonSet fails to start."},{"location":"home/changelog/hotfixes-2-13/#version-21319-september-27-2023","title":"Version 2.13.19 - September 27, 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content_2","title":"Release content","text":"<ul> <li>Added the ability to identify Kubeflow notebooks and display them in the Jobs table.</li> <li>Added the ability to schedule Kubelow workloads.</li> <li>Added functionality that displays Jobs that only belong to the user that is logged in.</li> <li> Added and refined alerts to the state of Run:ai components, schedule latency, and warnings for out of memory on Jobs.</li> <li>Added the ability to work with restricted PSA policy.</li> </ul>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues_1","title":"Fixed issues","text":"Internal ID Description RUN-12650 Fixed an issue that used an incorrect metric in analytics GPU ALLOCATION PER NODE panel. Now the correct allocation is in percentage. RUN-12602 Fixed an issue in <code>runaiconfig</code> where the <code>WorkloadServices</code> spec has memory requests/limits and cpu requests/limits and gets overwritten with the system default. RUN-12585 Fixed an issue where the workload-controller creates a delay in running jobs. RUN-12031 Fixed an issue when upgrading from 2.9 to 2.13 where the Scheduler pod fails to upgrade due to the change of owner. RUN-11091 Fixed an issue where the Departments feature is disabled, you are not able to schedule non-preemable jobs."},{"location":"home/changelog/hotfixes-2-13/#version-21313","title":"Version 2.13.13","text":"Internal ID Description RUN-11321 Fixed an issue where metrics always showed CPU Memory Utilization and CPU Compute Utilization as 0. RUN-11307 Fixed an issue where node affinity might change mid way through a job. Node affinity in now calculated only once at job submission. RUN-11129 Fixed an issue where CRDs are not automatically upgraded when upgrading from 2.9 to 2.13."},{"location":"home/changelog/hotfixes-2-13/#version-21312-august-7-2023","title":"Version 2.13.12 - August 7, 2023","text":"Internal ID Description RUN-11476 Fixed an issue with analytics node pool filter in Allocated GPUs per Project panel."},{"location":"home/changelog/hotfixes-2-13/#version-21311","title":"Version 2.13.11","text":"Internal ID Description RUN-11408 Added to the Run:ai job-controller 2 configurable parameters <code>QPS</code> and <code>Burst</code> which are applied as environment variables in the job-controller Deployment object."},{"location":"home/changelog/hotfixes-2-13/#version-2137-july-2023","title":"Version 2.13.7 - July 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content_3","title":"Release content","text":"<ul> <li>Added filters to the historic quota ratio widget on the Quota management dashboard.</li> </ul>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues_2","title":"Fixed issues","text":"Internal ID Description RUN-11080 Fixed an issue in OpenShift environments where log in via SSO with the <code>kubeadmin</code> user, gets blank pages for every page. RUN-11119 Fixed an issue where values that should be the Order of priority column are in the wrong column. RUN-11120 Fixed an issue where the Projects table does not show correct metrics when Run:ai version 2.13 is paired with a Run:ai 2.8 cluster. RUN-11121 Fixed an issue where the wrong over quota memory alert is shown in the Quota management pane in project edit form. RUN-11272 Fixed an issue in OpenShift environments where the selection in the cluster drop down in the main UI does not match the cluster selected on the login page."},{"location":"home/changelog/hotfixes-2-13/#version-2134","title":"Version 2.13.4","text":""},{"location":"home/changelog/hotfixes-2-13/#release-date","title":"Release date","text":"<p>July 2023</p>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues_3","title":"Fixed issues","text":"Internal ID Description RUN-11089 Fixed an issue when creating an environment, commands in the Runtime settings pane and are not persistent and cannot be found in other assets (for example in a new Training)."},{"location":"home/changelog/hotfixes-2-13/#version-2131-july-2023","title":"Version 2.13.1 - July 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content_4","title":"Release content","text":"<ul> <li>Made an improvement so that occurrences of labels that are not in use anymore are deleted.</li> </ul>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues_4","title":"Fixed issues","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-15/","title":"Changelog Version 2.15","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.15.</p>"},{"location":"home/changelog/hotfixes-2-15/#version-2159-february-5-2024","title":"Version 2.15.9 - February 5, 2024","text":""},{"location":"home/changelog/hotfixes-2-15/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-15296 Fixed an issue where the <code>resources</code> parameter was deprecated in the Projects and Departments API."},{"location":"home/changelog/hotfixes-2-15/#version-2154-january-5-2024","title":"Version 2.15.4 - January 5, 2024","text":""},{"location":"home/changelog/hotfixes-2-15/#fixed-issues_1","title":"Fixed issues","text":"Internal ID Description RUN-15026 Fixed an issue in workloads that were built on a cluster that does not support the NFS field. RUN-14907 Fixed an issue after an upgrade where the Analytics dashboard was missing the time ranges from before the upgrade. RUN-14903 Fixed an issue where internal operations were exposed to the customer audit log. RUN-14062 Fixed an issue in the Overview dashboard where the content for the Running Workload per Type panel did not fit."},{"location":"home/changelog/hotfixes-2-15/#version-2152-february-5-2024","title":"Version 2.15.2 - February 5, 2024","text":""},{"location":"home/changelog/hotfixes-2-15/#fixed-issues_2","title":"Fixed issues","text":"Internal ID Description RUN-14434 Fixed an issue where the Allocated GPUs metric was multiplied by seven."},{"location":"home/changelog/hotfixes-2-15/#version-2151-december-17-2023","title":"Version 2.15.1 - December 17, 2023","text":""},{"location":"home/changelog/hotfixes-2-15/#release-content","title":"Release content","text":"<ul> <li> <p>Added environment variables for customizable QPS and burst support.</p> </li> <li> <p>Added the ability to support running multiple Prometheus replicas.</p> </li> </ul>"},{"location":"home/changelog/hotfixes-2-15/#fixed-issues_3","title":"Fixed issues","text":"Internal ID Description RUN-14292 Fixed an issue where BCM installations were failing due to missing <code>create cluster</code> permissions. RUN-14289 Fixed an issue where metrics were not working due to an incorrect parameter in the cluster-config file. RUN-14198 Fixed an issue in services where multi nodepool jobs were not scheduled due to an unassigned nodepool status. RUN-14191 Fixed an issue where a consolidation failure would cause unnecessary evictions. RUN-14154 Fixed an issue in the New cluster form, whefre the dropdown listed versions that were incompatible with the installed control plane. RUN-13956 Fixed an issue in the Jobs table where templates were not edited successfully. RUN-13891 Fixed an issue where Ray job statuses were shown as empty. RUN-13825 Fixed an issue where GPU sharing configmaps were not deleted. RUN-13628 Fixed an issue where the <code>pre-install</code> pod failed to run <code>pre-install</code> tasks due to the request being denied (Unauthorized). RUN-13550 Fixed an issue where environments were not recovering from a node restart due to a missing GPU runtime class for containerized nodes. RUN-11895 Fixed an issue where the wrong amount of GPU memory usage was shown (is now MB). RUN-11681 Fixed an issue in OpenShift environments where some metrics were not shown on dashboards when the GPU Operator from the RedHat marketplace was installed."},{"location":"home/changelog/hotfixes-2-15/#version-2150","title":"Version 2.15.0","text":""},{"location":"home/changelog/hotfixes-2-15/#fixed-issues_4","title":"Fixed issues","text":"Internal ID Description RUN-13456 Fixed an issue where the Researcher L1 role did not have permissions to create and manage credentials. RUN-13282 Fixed an issue where Workspace logs crashed unexpectedly after restarting. RUN-13121 Fixed an issue in not being able to launch jobs using the API after an upgrade overrode a change in keycloak for applications which have a custom mapping to an email. RUN-13103 Fixed an issue in the Workspaces and Trainings table where the action buttons were not greyed out for users with only the view role. RUN-12993 Fixed an issue where Prometheus was reporting metrics even though the cluster was disconnected. RUN-12978 Fixed an issue after an upgrade, where permissions fail to sync to a project due to a missing application name in the CRD. RUN-12900 Fixed an issue in the Projects table, when sorting by Allocated GPUs, the projects were displayed alphabetically and not numerically. RUN-12846 Fixed an issue after a control-plane upgrade, where GPU, CPU, and Memory Cost fields (in the Consumption Reports) were missing when not using Grafana. RUN-12824 Fixed an issue where airgapped environments tried to pull an image from gcr.io (Internet). RUN-12769 Fixed an issue where SSO users were unable to see projects in Job Form unless the group they belong to was added directly to the project. RUN-12602 Fixed an issue in the documentation where the <code>WorkloadServices</code> configuration in the <code>runaiconfig</code> file was incorrect. RUN-12528 Fixed an issue where the Workspace duration scheduling rule was suspending workspaces regardless of the configured duration. RUN-12298 Fixed an issue where projects were not shown in the Projects table due to the API not sanitizing the project name at time of creation. RUN-12157 Fixed an issue where querying pods completion time returned a negative number. RUN-10560 Fixed an issue where no Prometheus alerts were sent due to a misconfiguration of the parameter <code>RunaiDaemonSetRolloutStuck</code>."},{"location":"home/changelog/hotfixes-2-16/","title":"Changelog Version 2.16","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.16.</p>"},{"location":"home/changelog/hotfixes-2-16/#version-21665","title":"Version 2.16.65","text":"Internal ID Description RUN-21448 Fixed an issue with degraded workload so the condition would reflect the actual state. RUN-20680 Fixed an issue where the workload page did not present the requested GPU."},{"location":"home/changelog/hotfixes-2-16/#version-21657","title":"Version 2.16.57","text":"Internal ID Description RUN-20388 Fixed an issue where cluster-sync caused a memory leak."},{"location":"home/changelog/hotfixes-2-16/#version-21625","title":"Version 2.16.25","text":"Internal ID Description RUN-17241 Fixed an issue where the nodes page showed nodes as not ready due to \"tookit not installed\"."},{"location":"home/changelog/hotfixes-2-16/#version-21621","title":"Version 2.16.21","text":"Internal ID Description RUN-16463 Fixed an issue after a cluster upgrade to v2.16, where some metrics of pre-existing workloads were displayed incorrectly in the Overview Dashboard."},{"location":"home/changelog/hotfixes-2-16/#version-21618","title":"Version 2.16.18","text":"Internal ID Description RUN-16486 Fixed an issue in the Workloads creation form where the GPU fields of the compute resource tiles were showing no data."},{"location":"home/changelog/hotfixes-2-16/#version-21616","title":"Version 2.16.16","text":"Internal ID Description RUN-16340 Fixed an issue in the Workloads table where filters were not saved correctly."},{"location":"home/changelog/hotfixes-2-16/#version-21615","title":"Version 2.16.15","text":""},{"location":"home/changelog/hotfixes-2-16/#release-content","title":"Release content","text":"<ul> <li>Implemented a new Workloads API to support the Workloads feature.</li> </ul>"},{"location":"home/changelog/hotfixes-2-16/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-16070 Fixed an issue where missing metrics caused the Nodepools table to appear empty."},{"location":"home/changelog/hotfixes-2-16/#version-21614","title":"Version 2.16.14","text":""},{"location":"home/changelog/hotfixes-2-16/#release-content_1","title":"Release content","text":"<p>*Improved overall performance by slowing down metrics updates from 10 seconds to 30 seconds.</p>"},{"location":"home/changelog/hotfixes-2-16/#fixed-issues_1","title":"Fixed issues","text":"Internal ID Description RUN-16255 Fixed an issue in the Analytics dashboard where the GPU Allocation per Node and GPU Memory Allocation per Node panels were displaying incorrect data. RUN-16035 Fixed an issue in the Workloads table where completed pods continue to be counted in the requested resources column."},{"location":"home/changelog/hotfixes-2-16/#version-21612","title":"Version 2.16.12","text":""},{"location":"home/changelog/hotfixes-2-16/#fixed-issues_2","title":"Fixed issues","text":"Internal ID Description RUN-16110 Fixed an issue where creating a training workload (single or multi-node) with a new PVC or Volume, resulted in the Workloads table showing the workload in the Unknown/Pending status. RUN-16086 Fixed an issue in airgapped environments where incorrect installation commands were shown when upgrading to V2.15."},{"location":"home/changelog/hotfixes-2-16/#version-21611","title":"Version 2.16.11","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-16/#version-2169","title":"Version 2.16.9","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-16/#version-2168","title":"Version 2.16.8","text":""},{"location":"home/changelog/hotfixes-2-16/#release-content_2","title":"Release content","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-16/#version-2167","title":"Version 2.16.7","text":""},{"location":"home/changelog/hotfixes-2-16/#release-content_3","title":"Release content","text":"<ul> <li>Added an API endpoint that retrieves data from a workloads's pod.</li> </ul>"},{"location":"home/changelog/hotfixes-2-16/#fixed-issues_3","title":"Fixed issues","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-16/#version-2166","title":"Version 2.16.6","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-17/","title":"Changelog Version 2.17","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.17.</p>"},{"location":"home/changelog/hotfixes-2-17/#version-21763","title":"Version 2.17.63","text":"Internal ID Description RUN-21448 Fixed an issue where a degraded workload was stuck and could not be released."},{"location":"home/changelog/hotfixes-2-17/#version-21746","title":"Version 2.17.46","text":"Internal ID Description RUN-20136 Updated postgres version."},{"location":"home/changelog/hotfixes-2-17/#version-21743","title":"Version 2.17.43","text":"Internal ID Description RUN-19949 Fixed an issue where runai submit arguments were not parsed correctly to the command."},{"location":"home/changelog/hotfixes-2-17/#version-21741","title":"Version 2.17.41","text":"Internal ID Description RUN-19870 Added debug logs to cluster-sync"},{"location":"home/changelog/hotfixes-2-17/#version-21726","title":"Version 2.17.26","text":"Internal ID Description RUN-19189 Fixed an issue in cluster-sync that sometimes caused unnecessary sync process to the control-plane."},{"location":"home/changelog/hotfixes-2-17/#version-21725","title":"Version 2.17.25","text":"Internal ID Description RUN-16357 Fixed an issue where the Project button in the Jobs screen redirects to the Projects page but on the wrong cluster."},{"location":"home/changelog/hotfixes-2-17/#version-21710","title":"Version 2.17.10","text":"Internal ID Description RUN-18065 Fixed an issue where the legacy job sumbission configuration was not available in the Settings page"},{"location":"home/changelog/hotfixes-2-17/#version-2170","title":"Version 2.17.0","text":"Internal ID Description RUN-20010 Fixed an issue of reduced permissions that run:ai grants users"},{"location":"home/changelog/hotfixes-2-18/","title":"Changelog Version 2.18","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.18.</p>"},{"location":"home/changelog/hotfixes-2-18/#hotfixes","title":"Hotfixes","text":"Internal ID Hotfix # Description RUN-25558 2.18.88 Fixed a memory issue when handling external workloads (deployments, ray etc.) which when they were scaled caused ETCD memory to increase. RUN-25466 2.18.88 Fixed an issue where an environment variable with the value SECRET was not valid as only SECRET:xxx was accepted. RUN-24700 2.18.88 CLI v2: Workload describe command no longer requires type or framework flags. RUN-25499 2.18.87 Fixed an issue where policy update request would fail to sync to the cluster\" perhaps mention the cluster version. RUN-25303 2.18.85 Fixed an issue where submitting with the --attach flag was supported only in a workspace workload. RUN-25061 2.18.84 Fixed a security vulnerability in github.com.go-git.go-git.v5 with CVE CVE-2025-21613 with severity HIGH. RUN-24857 2.18.84 Fixed a security vulnerability in golang.org.x.net with CVE CVE-2024-45338 with severity HIGH. RUN-17284 2.18.84 Fixed an issue where workloads were suspended when set with the termination after preemption option. RUN-24521 2.18.83 Fixed a security vulnerability in golang.org.x.crypto with CVE CVE-2024-45337 with severity HIGH. RUN-24733 2.18.83 Fixed an issue where department admins were unable to load the quota management page. RUN-25094 2.18.82 Fixed an issue where OpenShift could not be upgraded due to a broken 3rd binary. RUN-24921 2.18.80 Fixed a security vulnerability in golang.org.x.net and golang.org.x.crypto. RUN-24632 2.18.80 Fixed an issue where an existing monitoring Prometheus setup deployed in an unexpected namespace was reported as missing, causing Run:ai installation to fail on the cluster. The installation mechanism now searches for the monitoring prerequisite in additional relevant namespaces. RUN-24693 2.18.80 Fixed an issue where users were unable to provide metric store authentication details using secret references. RUN-24752 2.18.79 Fixed an issue where a workload would move to a failed state when created with a custom NodePort that was already allocated. RUN-24649 2.18.79 Fixed an issue where submitting a workload with <code>existingPvc=false</code> and not providing a <code>claimName</code> resulted in auto-generating a <code>claimName</code> that included both upper and lower case letters. Since Kubernetes rejects uppercase letters, the workload would fail. The behavior has been updated to generate names using only lowercase letters. RUN-24595 2.18.78 Fixed an issue where the new CLI did not parse master and worker commands/args simultaneously for distributed workloads. RUN-23914 2.18.78 Fixed an issue where unexpected behavior could occur if an application was capturing a graph while memory was being swapped in as part of the GPU memory swap feature. RUN-24020 2.18.77 Fixed a security vulnerability in k8s.io.kubernetes with CVE CVE-2024-0793. RUN-24021 2.18.77 Fixed a security vulnerability in pam with CVE CVE-2024-10963. RUN-23798 2.18.75 Fixed an issue in distributed PyTorch workloads where the worker pods are deleted immediately after completion, not allowing logs to be viewed. RUN-23838 2.18.74 Fixed an issue where the command-line interface could not access resources when configured as single-sign on in a self-hosted environment. RUN-23561 2.18.74 Fixed an issue where the frontend in airgapped environment attempted to download font resources from the internet. RUN-23789 2.18.73 Fixed an issue where in some cases, it was not possible to download the latest version of the command line interface. RUN-23790 2.18.73 Fixed an issue where in some cases it was not possible to download the Windows version of the command line interface. RUN-23855 2.18.73 Fixed an issue where the pods list in the UI showed past pods. RUN-23909 2.18.73 Fixed an issue where users based on group permissions cannot see dashboards. RUN-23857 2.18.72 Dashboard to transition from Grafana v9 to v10. RUN-24010 2.18.72 Fixed an infinite loop issue in the cluster-sync service. RUN-23040 2.18.72 Fixed an edge case where the Run:ai container toolkit hangs when user is spawning hundreds of sub-processes. RUN-23802 2.18.70 Fixed an issue where new scheduling rules were not applied to existing workloads, if those new rules were set on existing projects which had no scheduling rules before. RUN-23211 2.18.70 Fixed an issue where workloads were stuck at \"Pending\" when the command-line interface flag --gpu-memory was set to zero. RUN-23778 2.18.68 Fixed an issue where in single-sign-on configuration, the mapping of UID and other properties would sometimes disappear. RUN-23762 2.18.68 Fixed an issue where the wrong version of a Grafana dashboard was displayed in the UI. RUN-21198 2.18.66 Fixed an issue where creating a training workload via yaml (kubectl apply -f) and specifying spec.namePrefix, created infinite jobs. RUN-23541 2.18.65 Fixed an issue where in some cases workload authorization did not work properly due to wrong oidc configuration. RUN-23283 2.18.64 Fixed a permissions issue with the Analytics dashboard post upgrade for SSO Users RUN-23420 2.18.63 Replaced Redis with Keydb RUN-23140 2.18.63 Fixed an issue where distributed workloads were created with the wrong types RUN-23130 2.18.63 Fixed an issue where inference-workload-controller crashed when WorkloadOwnershipProtection was enabled RUN-23334 2.18.62 Updated core Dockerfiles to ubi9 RUN-23296 2.18.62 Fixed an issue in the CLI where runai attach did not work with auto-complete RUN-23215 2.18.62 Fixed an issue where metrics requests from backend to mimir failed for certain tenants. RUN-22138 2.18.62 Fixed an issue where private URL user(s) input was an email and not a string. RUN-23282 2.18.61 CLI documentation fixes RUN-23055 2.18.60 Fixed unified Distributed and Training CLI commands RUN-23243 2.18.59 Fixed an issue where the scope tree wasn't calculating permissions correctly RUN-22463 2.18.59 Fixed an error in CLI bash command RUN-22314 2.18.59 Fixed distributed framework filtering in API commands RUN-23142 2.18.58 Fixed an issue where advanced GPU metrics per-gpu don't have gpu label RUN-23001 2.18.58 Fixed an issue of false overcommit on out-of-memory killed in the \u201cswap\u201d feature. RUN-22851 2.18.58 Fixed an issue where client may get stuck on device lock acquired during \u201cswap\u201d out-migration RUN-22758 2.18.58 Fixed an issue where inference workload showed wrong status when submission failed. RUN-22544 2.18.58 Updated Grafana version for security vulnerabilities. RUN-23055 2.18.57 Fixed the unified Distributed and Training CLI commands RUN-23014 2.18.56 Fixed an issue where node-scale-adjuster might not create a scaling pod if it is in cool-down and the pod was not updated after that. RUN-22660 2.18.56 Fixed an issue where workload charts have an unclear state RUN-22457 2.18.55 Fixed an issue where in rare edge cases the cluster-sync pod was out of memory. RUN-21825 2.18.55 Fixed all CVEs in Run:ai's Goofys-based image used for S3 integration. RUN-22871 2.18.55 Fixed an issue in runai-container-toolkit where in certain cases when a process is preempted, OOMKill metrics were not published correctly. RUN-22250 2.18.55 Fixed an issue where workloads trying to use an ingress URL which is already in use were behaving inconsistentyly instead of failing immediately. RUN-22880 2.18.55 Fixed an issue where the minAvailable field for training-operator CRDs did not consider all possible replica specs. RUN-22073 2.18.55 Fixed an issue where runai-operator failed to parse cluster URLs ending with '/'. RUN-22453 2.18.55 Fixed an issue where in rare edge cases the workload-overseer pod experienced a crash. RUN-22763 2.18.55 Fixed an issue where in rare edge cases an 'attach' command from CLI-V2 caused a crash in the cluster-api service. RUN-21948 2.18.49 Fixed an issue where in rare edge cases workload child resources could have duplicate names, causing inconsistent behavior. RUN-22623 2.18.49 Fixed an issue in Openshift where workloads were not suspended when reaching their idle GPU time limit. RUN-22600 2.18.49 Fixed an issue in AWS EKS clusters where the V1-CLI returned an empty table when listing all projects as an administrator. RUN-21878 2.18.49 Added a label to disable container toolkit from running on certain nodes <code>run.ai/container-toolkit-enabled</code>. RUN-22452 2.18.47 Fixed an issue where the scheduler has signature errors if TopologySpreadConstraints was partially defined. RUN-22570 2.18.47 Updated git-sync image to version v4.3.0. RUN-22054 2.18.46 Fixed an issue where users could not attach to jobs. RUN-22377 2.18.46 Removed uncached client from accessrule-controller. RUN-21697 2.18.46 Fixed an issue where client may deadlock on suspension during allocation request. RUN-20073 2.18.45 Fixed an issue where it wasn't possible to authenticate with user credentials in the CLI. RUN-21957 2.18.45 Fixed an issue where there was a missing username-loader container in inference workloads. RUN-22276 2.18.39 Fixed an issue where Knative external URL was missing from the Connections modal. RUN-22280 2.18.39 Fixed an issue when setting scale to zero - there was no pod counter in the Workload grid. RUN-19811 2.18.39 Added an option to set k8s tolerations to run:ai daemonsets (container-toolkit, runai-device-plugin, mig-parted, node-exporter, etc..) . RUN-22128 2.18.39 Added GID, UID, Supplemental groups to the V1 CLI. RUN-21800 2.18.37 Fixed an issue with old workloads residing in the cluster. RUN-21907 2.18.34 Fixed an issue where the SSO user credentials contain supplementary groups as string instead of int. RUN-21272 2.18.31 Fixed an issue with multi-cluster credinatils creation, specifically with the same name in different clusters. RUN-20680 2.18.29 Fixed an issue where workloads page do not present requested GPU. RUN-21200 2.18.29 Fixed issues with upgrades and connections from v2.13. RUN-20970 2.18.27 Fixed an issue with PUT APIs. RUN-20927 2.18.26 Fixed an issue where node affinity was not updated correctly in projects edit. RUN-20084 2.18.26 Fixed an issue where default department were deleted instead of a message being displayed. RUN-21062 2.18.26 Fixed issues with the API documentation. RUN-20434 2.18.25 Fixed an issue when creating a Project/Department with memory resources requires 'units'. RUN-20923 2.18.25 Fixed an issue with projects/departments page loading slowly. RUN-19872 2.18.23 Fixed an issue where the Toolkit crashes and fails to create and replace the publishing binaries. RUN-20861 2.18.22 Fixed an issue where a pod is stuck on pending due to a missing resource reservation pod. RUN-20842 2.18.22 Fixed an issue of illegal model name with \".\" in hugging face integration. RUN-20791 2.18.22 Fix an issue where notifications froze after startup. RUN-20865 2.18.22 Fixed an issue where default departments are not deleted when a cluster is deleted. RUN-20698 2.18.21 Fixed an issue where 2 processes requests a device at the same time received the same GPU, causing failures. RUN-20760 2.18.18 Fixed an issue where workload protection UI shows wrong status. RUN-20612 2.18.15 Fixed an issue where it was impossible with the use-table-data to hide node pool columns when there is only one default node pool. RUN-20735 2.18.15 Fixed an issue where nodePool.name is undefined RUN-20721 2.18.12 Added error handling to nodes pages. RUN-20578 2.18.10 Fixed an issue regarding policy enforcement. RUN-20188 2.18.10 Fixed issue with defining SSO in OpenShift identity provider. RUN-20673 2.18.9 Fixed an issue where a researcher uses a distributed elastic job, it is possible that in a specific flow it is scheduled on more than one node-pools. RUN-20360 2.18.7 Fixed an issue where the workload network status was misleading. RUN-22107 2.18.7 Fixed an issue where passwords containing $ were removed from the configuration. RUN-20510 2.18.5 Fixed an issue with external workloads - argocd workflow failed to be updated. RUN-20516 2.18.4 Fixed an issue when after deploying to prod, the cluster-service and authorization-service got multiple OOMKilled every ~1 hour. RUN-20485 2.18.2 Changed policy flags to Beta. RUN-20005 2.18.1 Fixed an issue where a sidecar container failure failed the workload. RUN-20169 2.18.1 Fixed an issue allowing the addition of annotations and labels to workload resources. RUN-20108 2.18.1 Fixed an issue exposing service node ports to workload status. RUN-20160 2.18.1 Fixed an issue with version display when installing a new cluster in an airgapped environment. RUN-19874 2.18.1 Fixed an issue when copying and editing a workload with group access to a tool and the group wasn't removed when selecting users option. RUN-19893 2.18.1 Fixed an issue when using a float number in the scale to zero inactivity value - custom which sometimes caused the submission to fail. RUN-20087 2.18.1 Fixed an issue where inference graphs should be displayed only for minimum cluster versions. RUN-10733 2.18.1 Fixed an issue where we needed to minify and obfuscate our code in production. RUN-19962 2.18.1 Fixed an issue to fix sentry domains regex and map them to relevant projects. RUN-20104 2.18.1 Fixed an issue where frontend Infinite loop on keycloak causes an error. RUN-19906 2.18.1 Fixed an issue where inference workload name validation fails with 2.16 cluster. RUN-19605 2.18.1 Fixed an issue where authorized users should support multiple users (workload-controller) . RUN-19903 2.18.1 Fixed an issue where inference chatbot creation fails with 2.16 cluster. RUN-20409 2.18.1 Fixed an issue where clicking on create new compute during the runai model flow did nothing. RUN-11224 2.18.1 Fixed an issue where ruani-adm collect all logs was not collecting all logs. RUN-20478 2.18.1 Improved workloads error status in overview panel. RUN-19850 2.18.1 Fixed an issue where an application administrator could not submit a job with CLI. RUN-19863 2.18.1 Fixed an issue where department admin received 403 on get tenants and cannot login to UI. RUN-19904 2.18.1 Fixed an issue when filtering by allocatedGPU in get workloads with operator returns incorrect result. RUN-19925 2.18.1 Fixed an issue when upgrade from v2.16 to v2.18 failed on worklaods migrations. RUN-19887 2.18.1 Fixed an issue in the UI when there is a scheduling rule of timeout, the form opened with the rules collapsed and written \"none\". RUN-19941 2.18.1 Fixed an issue where completed and failed jobs were shown in view pods in nodes screen. RUN-19940 2.18.1 Fixed an issue where setting gpu quota failed because the department quota was taken from wrong department. RUN-19890 2.18.1 Fixed an issue where editing a project by removing its node-affinity stuck updating. RUN-20120 2.18.1 Fixed an issue where project update fails when there is no cluster version. RUN-20113 2.18.1 Fixed an issue in the Workloads table where a researcher does not see other workloads once they clear their filters. RUN-19915 2.18.1 Fixed an issue when turning departments toggles on on cluster v2.11+ the gpu limit is -1 and there is ui error. RUN-20178 2.18.1 Fixed an issue where dashboard CPU tabs appeared in new overview. RUN-20247 2.18.1 Fixed an issue where you couldn't create a workload with namespace of a deleted project. RUN-20138 2.18.1 Fixed an issue where the system failed to create node-type on override-backend env. RUN-18994 2.18.1 Fixed an issue where some limitations for department administrator are not working as expected. RUN-19830 2.18.1 Fixed an issue where resources (GPU, CPU, Memory) units were added to k8s events that are published by run:ai scheduler making our messages more readable."},{"location":"home/changelog/hotfixes-2-18/#version-2180-fixes","title":"Version 2.18.0 Fixes","text":"Internal ID Description RUN-20734 Fixed an issue where the enable/disable toggle for the feature was presenting wrong info. RUN-19895 Fixed an issue of empty state for deleted workloads which is incorrect. RUN-19507 Fixed an issue in V1 where get APIs are missing required field in swagger leading to omit empty. RUN-20246 Fixed an issue in Departments v1 org unit where if unrecognizable params are sent, an error is returned. RUN-19947 Fixed an issue where pending multi-nodepool podgroups got stuck after cluster upgrade. RUN-20047 Fixed an issue where Workload status shows as \"deleting\" rather than \"deleted\" in side panel. RUN-20163 Fixed an issue when a DV is shared with a department and a new project is added to this dep - no pvc/pv is created. RUN-20484 Fixed an issue where Create Projects Requests Returned 500 - services is not a valid ResourceType. RUN-20354 Fixed an issue when deleting a department with projects resulted in projects remaining in environment with the status NotReady."},{"location":"home/changelog/hotfixes-2-19/","title":"Changelog Version 2.19","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.19.</p>"},{"location":"home/changelog/hotfixes-2-19/#hotfixes","title":"Hotfixes","text":"Internal ID Hotfix # Description RUN-25558 2.19.59 Fixed a memory issue when handling external workloads (deployments, ray etc.) which when they were scaled caused ETCD memory to increase. RUN-24700 2.19.57 CLI v2: Workload describe command no longer requires type or framework flags. RUN-25511 2.19.57 Fixed an issue where deleting a workload in the CLI v2 caused an error due to a missing response body. The CLI now correctly receives and handles the expected response body. RUN-24858 2.19.56 Fixed High vulnerability CVE-2024-56344 for third party open source 'systeminformation'. RUN-25466 2.19.56 Fixed an issue where an environment variable with the value SECRET was not valid as only SECRET:xxx was accepted. RUN-17284 2.19.49 Fixed an issue where workloads were suspended when set with the termination after preemption option. RUN-25290 2.19.49 Fixed a security vulnerability in golang.org/x/net v0.33.0 with CVE-2024-45338 with severity HIGH. RUN-25234 2.19.49 Fixed security vulnerabilities by updating oauth2 proxy image to the latest. RUN-25234 2.19.48 Fixed an authentication issue in CLI V1. RUN-25062 2.19.45 Fixed a security vulnerability in github.com.go-git.go-git.v5 with CVE CVE-2025-21614 with severity HIGH. RUN-25061 2.19.45 Fixed a security vulnerability in github.com.go-git.go-git.v5 with CVE CVE-2025-21613 with severity HIGH. RUN-24857 2.19.45 Fixed a security vulnerability in golang.org.x.net with CVE CVE-2024-45338 with severity HIGH. RUN-24733 2.19.45 Fixed an issue where users were unable to load the quota management dashboard. RUN-25094 2.19.44 Fixed an issue where OpenShift could not be upgraded due to a broken 3rd binary. RUN-24026 2.19.40 Fixed a security vulnerability in krb5-libs with CVE CVE-2024-3596. RUN-24649 2.19.40 Fixed an issue where submitting a workload with <code>existingPvc=false</code> and not providing a <code>claimName</code> resulted in auto-generating a <code>claimName</code> that included both upper and lower case letters. Since Kubernetes rejects uppercase letters, the workload would fail. The behavior has been updated to generate names using only lowercase letters. RUN-24632 2.19.40 Fixed an issue where an existing Prometheus monitoring setup deployed in an unexpected namespace was reported as missing, causing Run:ai installation to fail on the cluster. The installation mechanism now searches for the monitoring prerequisite in additional relevant namespaces. RUN-24693 2.19.40 Fixed an issue where users were unable to provide metric store authentication details using secret references. RUN-23744 2.19.40 Fixed an issue where refreshing some pages (such as the settings, policy, and access rules) removed the side navigation. RUN-24715 2.19.40 Fixed an issue in the templates form where selecting Secret as a data source got stuck in an infinite loading page. RUN-24831 2.19.40 Fixed an issue where some edge cases triggered consolidation without it actually being necessary. RUN-24873 2.19.40 Fixed an issue where users were unable to configure email notifications regarding workload statuses. RUN-24921 2.19.40 Fixed a security vulnerability in golang.org.x.net and golang.org.x.crypto. RUN-23914 2.19.38 Fixed an issue where unexpected behavior could occur if an application was capturing a graph while memory was being swapped in as part of the GPU memory swap feature. RUN-24521 2.19.36 Fixed a security vulnerability in golang.org.x.crypto with CVE CVE-2024-45337 with severity HIGH. RUN-24595 2.19.36 Fixed an issue where the new command-line interface did not parse master and worker commands/args simultaneously for distributed workloads. RUN-24565 2.19.34 Fixed an issue where the UI was hanging at times during Hugging Face model memory calculation. RUN-24021 2.19.33 Fixed a security vulnerability in pam with CVE-2024-10963. RUN-24506 2.19.33 Fixed a security vulnerability in krb5-libs with CVE-2024-3596. RUN-24259 2.19.31 Fixed an issue where the option to reset a local user password is sometimes not available. RUN-23798 2.19.30 Fixed an issue in distributed PyTorch workloads where the worker pods are deleted immediately after completion, not allowing logs to be viewed. RUN-24184 2.19.28 Fixed an issue in database migration when upgrading from 2.16 to 2.19. RUN-23752 2.19.27 Fixed an issue in the distributed training submission form when a policy on the master pod was applied. RUN-23040 2.19.27 Fixed an edge case where the Run:ai container toolkit hangs when user is spawning hundreds of sub-processes. RUN-23211 2.19.27 Fixed an issue where workloads were stuck at \"Pending\" when the command-line interface flag --gpu-memory was set to zero. RUN-23561 2.19.27 Fixed an issue where the frontend in airgapped environment attempted to download font resources from the internet. RUN-23789 2.19.27 Fixed an issue where in some cases, it was not possible to download the latest version of the command-line interface. RUN-23790 2.19.27 Fixed an issue where in some cases it was not possible to download the Windows version of the command-line interface. RUN-23802 2.19.27 Fixed an issue where new scheduling rules were not applied to existing workloads, if those new rules were set on existing projects which had no scheduling rules before. RUN-23838 2.19.27 Fixed an issue where the command-line interface could not access resources when configured as single-sign on in a self-hosted environment. RUN-23855 2.19.27 Fixed an issue where the pods list in the UI showed past pods. RUN-23857 2.19.27 Dashboard to transition from Grafana v9 to v10. RUN-24010 2.19.27 Fixed an infinite loop issue in the cluster-sync service. RUN-23669 2.19.25 Fixed an issue where export function of consumption Grafana dashboard was not showing. RUN-23778 2.19.24 Fixed an issue where mapping of UID and other properties disappears. RUN-23770 2.19.24 Fixed an issue where older overview dashboard does not filter on cluster, even though a cluster is selected. RUN-23762 2.19.24 Fixed an issue where the wrong version of a Grafana dashboard was displayed in the UI. RUN-23752 2.19.24 Fixed an issue in the distributed training submission form when a policy on the master pod was applied. RUN-23664 2.19.24 Fixed an issue where the GPU quota numbers on the department overview page did not mach the department edit page. RUN-21198 2.19.22 Fixed an issue where creating a training workload via yaml (kubectl apply -f) and specifying spec.namePrefix, created infinite jobs. RUN-23583 2.19.21 Fixed an issue where the new UI navigation bar sometimes showed multiple selections. RUN-23541 2.19.21 Fixed an issue where authorization was not working properly in SaaS due to wrong oidc URL being used. RUN-23376 2.19.21 Fixed an issue where the new command-line interface required re-login after 10 minutes. RUN-23162 2.19.21 Fixed an issue where older audit logs did not show on the new audit log UI. RUN-23385 2.19.20 Fixed an issue where calls to api/v1/notifications/config/notifications would return 502. RUN-23382 2.19.20 Fixed an issue where all nodepools were deleted on cluster upgrade. RUN-23374 2.19.20 Fixed an issue where \"ghost\" nodepool in project settings prevents workload creation via UI/API. RUN-23291 2.19.20 CLI - change text to be user friendly. RUN-23283 2.19.20 Fixed a permissions issue with the Analytics dashboard post upgrade for SSO Users. RUN-23208 2.19.20 Upload the source map to sentry only. RUN-22642 2.19.20 infw-controller service tests for the reconcile. RUN-23373 2.19.19 Fixed an issue where a new data source couldn't be created from the \"New Workload\" form. RUN-23368 2.19.19 Fixed an issue where the getProjects v1 API returned a list of users which was not always in the same order. RUN-23333 2.19.19 Fixed an issue where node pool with overProvisioningRatio greater than 1 cannot be created. RUN-23215 2.19.18 Fixed an issue where metrics requests from backend to mimir failed for certain tenants. RUN-23334 2.19.17 Updated some dockerfiles to the latest ubi9 image for security vulnerabilities. RUN-23318 2.19.16 Fixed an issue where some projects held faulty data which caused the getProjectById API to fail. RUN-23140 2.19.16 Fixed an issue where distributed workloads were created with the wrong types RUN-22069 2.19.16 Fixed an issue where JWT parse with claims failed to parse token without Keyfunc. RUN-23321 2.19.15 Fixed and issue where the GetProjectById wrapper API of the org-unit client in the runai-common-packages ignored errors. RUN-23296 2.19.15 Fixed an issue in the CLI where runai attach did not work with auto-complete. RUN-23282 2.19.15 CLI documentation fixes. RUN-23245 2.19.15 Fixed an issue where the binder service didn't update the pod status. RUN-23057 2.19.15 OCP 2.19 upgrade troubleshooting. RUN-22138 2.19.15 Fixed an issue where private URL user(s) input was an email and not a string. RUN-23243 2.19.14 Fixed an issue where the scope tree wasn't calculating permissions correctly. RUN-23208 2.19.14 Upload the source map to sentry only. RUN-23198 2.19.14 Fixed an issue where external-workload-integrator sometimes crashed for RayJob. RUN-23191 2.19.13 Fixed an issue where creating workloads in the UI returned only the first 50 projects. RUN-23142 2.19.12 Fixed an issue where advanced GPU metrics per-gpu did not have gpu label. RUN-23139 2.19.12 Fixed an issue where inference workload showed wrong status. RUN-23027 2.19.12 Deprecated migProfiles API fields. RUN-23001 2.19.12 Fixed an issue of false overcommit on out-of-memory kills in the Swap feature. RUN-22851 2.19.12 Fixed an issue where client may get stuck on device lock acquired during \u201cswap\u201d out-migration. RUN-22771 2.19.12 Fixed an issue where get cluster by id with metadata verbosity returned zero values. RUN-22742 2.19.12 Fixed user experience issue in inference autoscaling. RUN-22725 2.19.12 Fixed an issue where the cloud operator failed to get pods in nodes UI. RUN-22720 2.19.12 Fixed an issue where the cloud operator failed to get projects in node pools UI. RUN-22700 2.19.12 Added auto refresh to the overview dashboard, Pods modal in the Workloads page, and Event history page. RUN-22544 2.19.12 Updated Grafana version for security vulnerabilities. RUN-23083 2.19.11 Fixed an issue where workload actions were blocked in the UI when the cluster had any issues. RUN-22771 2.19.11 Fixed an issue where the getClusterById API with metadata verbosity returned zero values."},{"location":"home/changelog/hotfixes-2-19/#version-2190-fixes","title":"Version 2.19.0 Fixes","text":"Internal ID Description RUN-21756 Fixed an issue where the NFS mount path doesn\u2019t accept \u201c{}\u201d characters. RUN-21475 Fixed an issue where users failed to select the compute resource from UI if the compute resource is last in the list and has a long name."},{"location":"platform-admin/overview/","title":"Overview: Platform Administrator","text":"<p>The Platform Administrator is responsible for the day-to-day administration of the product. </p> <p>As part of the Platform Administrator documentation you will find:</p> <ul> <li>Provide the right access level to users.</li> <li>Configure Run:ai meta-data such as Projects, Departments, Node pools etc.  </li> <li>Understand Researcher Workloads and set up Workload Policies and Assets.</li> <li>Review possible integrations with third-party products. </li> <li>Analyze system performance and perform suggested actions. </li> </ul>"},{"location":"platform-admin/aiinitiatives/overview/","title":"AI Initiatives","text":"<p>AI initiatives refer to advancing research, development, and implementation of AI technologies. These initiatives represent your business needs and involve collaboration between individuals, teams, and other stakeholders. AI initiatives require compute resources and a methodology to effectively and efficiently use those compute resources and split them among the different AI initiatives stakeholders. The building blocks of AI compute resources are GPUs, CPUs, and CPU memory, which are built into nodes (servers) and can be further grouped into node pools. Nodes and node pools are part of a Kubernetes Cluster.</p> <p>To manage AI initiatives in Run:ai you should:</p> <ul> <li>Map your organization and initiatives to projects and optionally departments  </li> <li>Map compute resources (node pools and quotas) to projects and optionally departments  </li> <li>Assign users (e.g. AI practitioners, ML engineers, Admins) to projects and departments</li> </ul>"},{"location":"platform-admin/aiinitiatives/overview/#mapping-your-organization","title":"Mapping your organization","text":"<p>The way you map your AI initiatives and organization into Run:ai projects and departments should reflect your organization\u2019s structure and Project management practices. There are multiple options, and we provide you here with 3 examples of typical forms in which to map your organization, initiatives, and users into Run:ai, but of course, other ways that suit your requirements are also acceptable.</p>"},{"location":"platform-admin/aiinitiatives/overview/#based-on-individuals","title":"Based on individuals","text":"<p>A typical use case would be students (individual practitioners) within a faculty (business unit) - an individual practitioner may be involved in one or more initiatives. In this example, the resources are accounted for by the student (project) and aggregated per faculty (department). Department = business unit / Project = individual practitioner</p> <p></p>"},{"location":"platform-admin/aiinitiatives/overview/#based-on-business-units","title":"Based on business units","text":"<p>A typical use case would be an AI service (business unit) split into AI capabilities (initiatives) - an individual practitioner may be involved in several initiatives. In this example, the resources are accounted for by Initiative (project) and aggregated per AI service (department).</p> <p>Department = business unit / Project = initiative</p> <p></p>"},{"location":"platform-admin/aiinitiatives/overview/#based-on-the-organizational-structure","title":"Based on the organizational structure","text":"<p>A typical use case would be a business unit split into teams - an individual practitioner is involved in a single team (project) but the team may be involved in several AI initiatives. In this example, the resources are accounted for by team (project) and aggregated per business unit (department).</p> <p>Department = business unit  /  Project = team</p> <p></p>"},{"location":"platform-admin/aiinitiatives/overview/#mapping-your-resources","title":"Mapping your resources","text":"<p>AI initiatives require compute resources such as GPUs and CPUs to run. Compute resources in any organization are limited, either due to the number of servers (nodes) owned by the organization is limited, the budget it can spend to lease resources in the cloud or spending for in-house servers is also limited. Every organization strives to optimize the usage of its resources by maximizing their utilization and providing all users with their needs. Therefore, the organization needs to split resources according to the organization's internal priorities and budget constraints. But even after splitting the resources, the orchestration layer should still provide fairness between the resourced consumers, and allow access to unused resources to minimize scenarios of idle resources.</p> <p>Another aspect of resource management is how to group your resources effectively, especially in large environments, or environments that are made of heterogeneous types of hardware, where some users need to use specific hardware types, or where other users should avoid occupying critical hardware of some users or initiatives.</p> <p>Run:ai assists you with all of these complex issues by allowing you to map your cluster resources to node pools, then map each Project and Department a quota allocation per node pool, and set access rights to unused resources (Over quota) per node pool.</p>"},{"location":"platform-admin/aiinitiatives/overview/#grouping-your-resources","title":"Grouping your resources","text":"<p>There are several reasons why you would group resources (nodes) into node pools:</p> <ul> <li>Control the GPU type to use in heterogeneous hardware environment - in many cases, AI models can be optimized per hardware type they will use, e.g. a training workload that is optimized for H100 does not necessarily run optimally on an A100, and vice versa. Therefore segmenting into node pools, each with a different hardware type gives the AI researcher and ML engineer better control of where to run.  </li> <li>Quota control - splitting to node pools allows the admin to set specific quota per hardware type, e.g. give high priority project guaranteed access to advanced GPU hardware, while keeping lower priority project with a lower quota or even with no quota at all for that high-end GPU, but give it a \u201cbest-effort\u201d access only (i.e. if the high priority guaranteed project is not using those resources).  </li> <li>Multi-region or multi-availability-zone cloud environments - if some or all of your clusters run on the cloud (or even on-premise) but any of your clusters uses different physical locations or different topologies (e.g. racks), you probably want to segment your resources per region/zone/topology to be able to control where to run your workloads, how much quota to assign to specific environments (per project, per department), even if all those locations are all using the same hardware type. This methodology can help in optimizing the performance of your workloads because of the superior performance of local computing such as the locality of distributed workloads, local storage etc.  </li> <li>Explainability and predictability - large environments are complex to understand, this becomes even more complex when an environment is loaded. To maintain users\u2019 satisfaction and their understanding of the resources state, as well as to keep predictability of your workload chances to get scheduled, segmenting your cluster into smaller pools may significantly help.  </li> <li>Scale - Run:ai implementation of node pools has many benefits, one of the main of them is scale. Each node pool has its own scheduler instance, therefore allowing the cluster to handle more nodes and schedule workloads faster when segmented into node pools vs. one large cluster. To allow your workloads to use any resource within a cluster that is split to node pools, a second-level Scheduler is in charge of scheduling workloads between node pools according to your preferences and resource availability.  </li> <li>Prevent mutual exclusion - Some AI workloads consume CPU-only resources, to prevent those workloads from consuming the CPU resources of GPU nodes and thus block GPU workloads from using those nodes, it is recommended to group CPU-only nodes into a dedicated node pool(s) and assign a quota for CPU projects to CPU node-pools only while keeping GPU node-pools with zero quota and optionally \u201cbest-effort\u201d over-quota access for CPU-only projects.</li> </ul>"},{"location":"platform-admin/aiinitiatives/overview/#grouping-examples","title":"Grouping Examples","text":"<p>Set out below are illustrations of different grouping options.                              </p> <p>Example: grouping nodes by topology</p> <p></p> <p>Example: grouping nodes by hardware type</p> <p></p>"},{"location":"platform-admin/aiinitiatives/overview/#assigning-your-resources","title":"Assigning your resources","text":"<p>After the initial grouping of resources, it is time to associate resources to AI initiatives, this is performed by assigning quotas to projects and optionally to departments. Assigning GPU quota to a project, on a node pool basis, means that the workloads submitted by that project are entitled to use those GPUs as guaranteed resources and can use them for all workload types.</p> <p>However, what happens if the project requires more resources than its quota? This depends on the type of workloads that the user wants to submit. If the user requires more resources for non-preemptible workloads, then the quota must be increased, because non-preemptible workloads require guaranteed resources. On the other hand, if the type of workload is, for example, a model Training workload that is preemptible - in this case the project can exploit unused resources of other projects, as long as the other projects don\u2019t need them. Over-quota is set per project on a node-pool basis and per department.</p> <p>Administrators can use quota allocations to prioritize resources between users, teams, and AI initiatives. The administrator can completely prevent the use of certain node pools by a project or department by setting the node pool quota to 0 and disabling over quota for that node pool, or it can keep the quota to 0 and enable over-quota to that node pool and allow access based on resource availability only (e.g. unused GPUs). However, when a project with a non-zero quota needs to use those resources, the Scheduler reclaims those resources back and preempts the preemptible workloads of over-quota projects. As an administrator, you can also have an impact on the amount of over-quota resources a project or department uses.</p> <p>It is essential to make sure that the sum of all projects' quota does NOT surpass that of the Department, and that the sum of all departments does not surpass the number of physical resources, per node pool and for the entire cluster (we call such behavior - \u2018over-subscription\u2019). The reason over-subscription is not recommended is that it may produce unexpected scheduling decisions, especially those that might preempt \u2018non-preemptive\u2019 workloads or fail to schedule workloads within quota, either non-preemptible or preemptible, thus quota cannot be considered anymore as \u2018guaranteed\u2019. Admins can opt-in a system flag that helps to prevent over-subscription scenarios.</p> <p>Example: assigning resources to projects</p> <p></p>"},{"location":"platform-admin/aiinitiatives/overview/#assigning-users-to-projects-and-departments","title":"Assigning users to projects and departments","text":"<p>Run:ai system is using \u2018Role Based Access Control\u2019 (RBAC) to manage users\u2019 access rights to the different objects of the system, its resources, and the set of allowed actions. To allow AI researchers, ML engineers, Project Admins, or any other stakeholder of your AI initiatives to access projects and use AI compute resources with their AI initiatives, the administrator needs to assign users to projects. After a user is assigned to a project with the proper role, e.g. \u2018L1 Researcher\u2019, the user can submit and monitor its workloads under that project. Assigning users to departments is usually done to assign \u2018Department Admin\u2019 to manage a specific department. Other roles, such as \u2018L1 Researcher\u2019, can also be assigned to departments, this allows the researcher access to all projects within that department.</p>"},{"location":"platform-admin/aiinitiatives/overview/#submitting-workloads","title":"Submitting workloads","text":"<p>Now that resources are grouped into node pools, organizational units or business initiatives are mapped into projects and departments, projects\u2019 quota parameters are set per node pool, and users are assigned to projects, you can finally submit workloads from a project and use compute resources to run your AI initiatives.</p> <p>When a workload is submitted, it goes to the chosen Kubernetes cluster, and the Run:ai Scheduler handles it.</p> <p>The Scheduler\u2019s main role is to find the best-suited node or nodes for each submitted workload, so that those nodes match the resources and other characteristics requested by the workload while adhering to the quota and fairness principles of the Run:ai system. A workload can be a single pod running on a single node, or a distributed workload using multiple pods, each running on a node (or part of a node). It is not rare to find large training workloads using 128 nodes and even more, or inference workloads using multiple pods and nodes. There are numerous types of workloads, some are Kubernetes native and some are 3rd party extensions on top of Kubernetes native pods.  The Run:ai Scheduler schedules any Kubernetes native workloads, Run:ai workloads, or any type of 3rd party workload.</p>"},{"location":"platform-admin/aiinitiatives/overview/#scopes-in-the-organization","title":"Scopes in the organization","text":"<p>This is an example of an organization, as represented in the Run:ai platform:</p> <p></p> <p>The organizational tree is structured from top down under a single node headed by the account. The account is comprised of clusters, departments and projects.</p> <p>Note</p> <p>Different roles and permissions can be granted to specific clusters, departments and projects within an organization.</p> <p>The organizational tree is structured from top down under a single node headed by the account. The account is comprised of clusters, departments and projects.</p> <p>After mapping and building your hierarchal structured organization as shown above, you can assign or associate various Run:ai components (e.g. workloads, roles, assets, policies, and more) to different parts of the organization - these organizational parts are the Scopes. The following organizational example consists of 5 optional scopes:</p> <p></p> <p>Note</p> <p>When a scope is selected, the very same unit, including all of its subordinates (both existing and any future subordinates, if added), are selected as well.</p>"},{"location":"platform-admin/aiinitiatives/org/departments/","title":"Departments","text":"<p>This article explains the procedure for managing departments</p> <p>Departments are a grouping of projects. By grouping projects into a department, you can set quota limitations to a set of projects, create policies that are applied to the department, and create assets that can be scoped to the whole department or a partial group of descendent projects</p> <p>For example, in an academic environment, a department can be the Physics Department grouping various projects (AI Initiatives) within the department, or grouping projects where each project represents a single student.</p>"},{"location":"platform-admin/aiinitiatives/org/departments/#departments","title":"Departments","text":"<p>The Departments table can be found under Departments in the Run:ai platform.</p> <p>Note</p> <p>Departments are disabled, by default. If you cannot see Departments in the menu, then it must be enabled by your Administrator, under General Settings \u2192 Resources \u2192 Departments</p> <p>The Departments table lists all departments defined for a specific cluster and allows you to manage them. You can switch between clusters by selecting your cluster using the filter at the top.</p> <p></p> <p>The Departments table consists of the following columns:</p> Column Description Department The name of the department Node pool(s) with quota The node pools associated with this department. By default, all node pools within a cluster are associated with each department. Administrators can change the node pools\u2019 quota parameters for a department. Click the values under this column to view the list of node pools with their parameters (as described below) GPU quota GPU quota associated with the department Total GPUs for projects The sum of all projects\u2019 GPU quotas associated with this department Project(s) List of projects associated with this department Subject(s) The users, SSO groups, or applications with access to the project. Click the values under this column to view the list of subjects with their parameters (as described below). This column is only viewable if your role in Run:ai platform allows you those permissions. Allocated GPUs The total number of GPUs allocated by successfully scheduled workloads in projects associated with this department GPU allocation ratio The ratio of Allocated GPUs to GPU quota. This number reflects how well the department\u2019s GPU quota is utilized by its descendant projects. A number higher than 100% means the department is using over-quota GPUs. A number lower than 100% means not all projects are utilizing their quotas. A quota becomes allocated once a workload is successfully scheduled. Creation time The timestamp for when the department was created Workload(s) The list of workloads under projects associated with this department. Click the values under this column to view the list of workloads with their resource parameters (as described below) Cluster The cluster that the department is associated with"},{"location":"platform-admin/aiinitiatives/org/departments/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/aiinitiatives/org/departments/#node-pools-with-quota-associated-with-the-department","title":"Node pools with quota associated with the department","text":"<p>Click one of the values of Node pool(s) with quota column, to view the list of node pools and their parameters</p> Column Description Node pool The name of the node pool is given by the administrator during node pool creation. All clusters have a default node pool created automatically by the system and named \u2018default\u2019. GPU quota The amount of GPU quota the administrator dedicated to the department for this node pool (floating number, e.g. 2.3 means 230% of a GPU capacity) CPU (Cores) The amount of CPU (cores) quota the administrator has dedicated to the department for this node pool (floating number, e.g. 1.3 Cores = 1300 mili-cores). The \u2018unlimited\u2019 value means the CPU (Cores) quota is not bound and workloads using this node pool can use as many CPU (Cores) resources as they need (if available) CPU memory The amount of CPU memory quota the administrator has dedicated to the department for this node pool (floating number, in MB or GB). The \u2018unlimited\u2019 value means the CPU memory quota is not bounded and workloads using this node pool can use as much CPU memory resource as they need (if available). Allocated GPUs The total amount of GPUs allocated by workloads using this node pool under projects associated with this department. The number of allocated GPUs may temporarily surpass the GPU quota of the department if over-quota is used. Allocated CPU (Cores) The total amount of CPUs (cores) allocated by workloads using this node pool under all projects associated with this department. The number of allocated CPUs (cores) may temporarily surpass the CPUs (Cores) quota of the department if over-quota is used. Allocated CPU memory The actual amount of CPU memory allocated by workloads using this node pool under all projects associated with this department. The number of Allocated CPU memory may temporarily surpass the CPU memory quota if over-quota is used."},{"location":"platform-admin/aiinitiatives/org/departments/#subjects-authorized-for-the-project","title":"Subjects authorized for the project","text":"<p>Click one of the values of the Subject(s) column, to view the list of subjects and their parameters. This column is only viewable if your role in the Run:ai system affords you those permissions.</p> Column Description Subject A user, SSO group, or application assigned with a role in the scope of this department Type The type of subject assigned to the access rule (user, SSO group, or application). Scope The scope of this department within the organizational tree. Click the name of the scope to view the organizational tree diagram, you can only view the parts of the organizational tree for which you have permission to view. Role The role assigned to the subject, in this department\u2019s scope Authorized by The user who granted the access rule Last updated The last time the access rule was updated <p>Note</p> <p>A role given in a certain scope, means the role applies to this scope and any descendant scopes in the organizational tree.</p>"},{"location":"platform-admin/aiinitiatives/org/departments/#adding-a-new-department","title":"Adding a new department","text":"<p>To create a new Department:</p> <ol> <li>Click +NEW DEPARTMENT  </li> <li>Select a scope.     By default, the field contains the scope of the current UI context cluster, viewable at the top left side of your screen. You can change the current UI context cluster by clicking the \u2018Cluster: cluster-name\u2019 field and applying another cluster as the UI context. Alternatively, you can choose another cluster within the \u2018+ New Department\u2019 form by clicking the organizational tree icon on the right side of the scope field, opening the organizational tree and selecting one of the available clusters.  </li> <li>Enter a name for the department. Department names must start with a letter and can only contain lower case latin letters, numbers or a hyphen ('-\u2019).  </li> <li>Under Quota Management, select a quota for the department. The Quota management section may contain different fields depending on pre-created system configuration. Possible system configurations are:  <ul> <li>Existence of Node Pools  </li> <li>CPU Quota - Allow setting a quota for CPU resources.</li> </ul> </li> </ol> <p>When no node pools are configured, you can set the following quota parameters:</p> <ul> <li>GPU Devices   The number of GPUs you want to allocate for this department (decimal number). This quota is consumed by the department\u2019s subordinated project.  </li> <li>CPUs (cores) (when CPU quota is set)    The number of CPU cores you want to allocate for this department (decimal number). This quota is consumed by the department\u2019s subordinated projects  </li> <li>CPUs memory (when CPU quota is set)    The amount of CPU memory you want to allocate for this department (in Megabytes or Gigabytes). This quota is consumed by the department\u2019s subordinated projects</li> </ul> <p>When node pools are enabled, it is possible to set the above quota parameters for each node-pool separately.</p> <ul> <li>Order of priority This column is displayed only if more than one node pool exists. The default order in which the Scheduler uses node pools to schedule a workload.  This means, the Scheduler first tries to allocate resources using the highest priority node pool, followed by the next in priority, until it reaches the lowest priority node pool list, then the Scheduler starts from the highest priority again. The Scheduler uses the department list of prioritized node pools, only if the order of priority of node pools is not set in project or the workload during submission (either by an admin policy or by the user). An empty value indicates that the node pool is not part of the department\u2019s default node pool priority list, but a node pool can still be chosen by the admin policy or a user during workload submission. Department nodepool priority sets defaults to the subordinate projects but does not enforce it, meaning projects are free to change their priority.</li> <li>In addition, you can decide whether to allow a department to go over-quota. Allowing over-quota at the department level means that one department can receive more resources than its quota when not required by other departments. If the over-quota is disabled, workloads running under subordinated projects are not able to use more resources than the department\u2019s quota, but each project can still go over-quota (if enabled at the project level) up to the department\u2019s quota.</li> </ul> <p>Unlimited CPU(Cores) and CPU memory quotas are an exception - in this case, workloads of subordinated projects can consume available resources up to the physical limitation of the cluster or any of the node pools.</p> <p>Example of Quota management:</p> <p></p> <ol> <li>Click CREATE DEPARTMENT</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/departments/#adding-an-access-rule-to-a-department","title":"Adding an access rule to a department","text":"<p>To create a new access rule for a department:</p> <ol> <li>Select the department you want to add an access rule for  </li> <li>Click ACCESS RULES  </li> <li>Click +ACCESS RULE  </li> <li>Select a subject  </li> <li>Select or enter the subject identifier:  <ul> <li>User Email for a local user created in Run:ai or for SSO user as recognized by the IDP  </li> <li>Group name as recognized by the IDP  </li> <li>Application name as created in Run:ai  </li> </ul> </li> <li>Select a role  </li> <li>Click SAVE RULE  </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/departments/#deleting-an-access-rule-from-a-department","title":"Deleting an access rule from a department","text":"<p>To delete an access rule from a department:</p> <ol> <li>Select the department you want to remove an access rule from  </li> <li>Click ACCESS RULES  </li> <li>Find the access rule you would like to delete  </li> <li>Click on the trash icon  </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/departments/#editing-a-department","title":"Editing a department","text":"<ol> <li>Select the Department you want to edit  </li> <li>Click EDIT  </li> <li>Update the Department and click SAVE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/departments/#viewing-a-departments-policy","title":"Viewing a department\u2019s policy","text":"<p>To view the policy of a department:</p> <ol> <li>Select the department for which you want to view its policies.     This option is only active if the department has defined policies in place.  </li> <li>Click VIEW POLICY and select the workload type for which you want to view the policies:     a. Workspace workload type policy with its set of rules     b. Training workload type policies with its set of rules  </li> <li>In the Policy form, view the workload rules that are enforcing your department for the selected workload type as well as the defaults:  <ul> <li>Parameter - The workload submission parameter that Rule and Default is applied on  </li> <li>Type (applicable for data sources only) - The data source type (Git, S3, nfs, pvc etc.)  </li> <li>Default - The default value of the Parameter  </li> <li>Rule - Set up constraints on workload policy fields  </li> <li>Source - The origin of the applied policy (cluster, department or project)  </li> </ul> </li> </ol> <p>Notes</p> <ul> <li>The policy affecting the department consists of rules and defaults. Some of these rules and defaults may be derived from the policies of a parent cluster (source). You can see the source of each rule in the policy form.  </li> <li>A policy set for a department affects all subordinated projects and their workloads, according to the policy workload type</li> </ul>"},{"location":"platform-admin/aiinitiatives/org/departments/#deleting-a-department","title":"Deleting a department","text":"<ol> <li>Select the department you want to delete  </li> <li>Click DELETE  </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>Deleting a department permanently deletes its subordinated projects, any assets created in the scope of this department, and any of its subordinated projects such as compute resources, environments, data sources, templates, and credentials. However, workloads running within the department\u2019s subordinated projects, or the policies defined for this department or its subordinated projects - remain intact and running.</p>"},{"location":"platform-admin/aiinitiatives/org/departments/#reviewing-a-department","title":"Reviewing a department","text":"<ol> <li>Select the department you want to review  </li> <li>Click REVIEW  </li> <li>Review and click CLOSE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/departments/#using-api","title":"Using API","text":"<p>Go to the Departments API reference to view the available actions</p>"},{"location":"platform-admin/aiinitiatives/org/projects/","title":"Projects","text":"<p>This article explains the procedure to manage Projects.</p> <p>Researchers submit AI workloads. To streamline resource allocation and prioritize work, Run:ai introduces the concept of Projects. Projects are the tool to implement resource allocation policies as well as the segregation between different initiatives. A project may represent a team, an individual, or an initiative that shares resources or has a specific resource quota. Projects may be aggregated in Run:ai departments.</p> <p>For example, you may have several people involved in a specific face-recognition initiative collaborating under one project named \u201cface-recognition-2024\u201d. Alternatively, you can have a project per person in your team, where each member receives their own quota.</p>"},{"location":"platform-admin/aiinitiatives/org/projects/#projects-table","title":"Projects table","text":"<p>The Projects table can be found under Projects in the Run:ai platform.</p> <p>The Projects table provides a list of all projects defined for a specific cluster, and allows you to manage them. You can switch between clusters by selecting your cluster using the filter at the top.</p> <p></p> <p>The Projects table consists of the following columns:</p> Column Description Project The name of the project Department The name of the parent department. Several projects may be grouped under a department. Status The Project creation status. Projects are manifested as Kubernetes namespaces. The project status represents the Namespace creation status. Node pool(s) with quota The node pools associated with the project. By default, a new project is associated with all node pools within its associated cluster. Administrators can change the node pools\u2019 quota parameters for a project. Click the values under this column to view the list of node pools with their parameters (as described below) Subject(s) The users, SSO groups, or applications with access to the project. Click the values under this column to view the list of subjects with their parameters (as described below). This column is only viewable if your role in the Run:ai platform allows you those permissions. Allocated GPUs The total number of GPUs allocated by successfully scheduled workloads under this project GPU allocation ratio The ratio of Allocated GPUs to GPU quota. This number reflects how well the project\u2019s GPU quota is utilized by its descendent workloads. A number higher than 100% indicates the project is using over-quota GPUs. GPU quota The GPU quota allocated to the project. This number represents the sum of all node pools\u2019 GPU quota allocated to this project. Allocated CPUs (Core) The total number of CPU cores allocated by workloads submitted within this project. (This column is only available if the CPU Quota setting is enabled, as described below). Allocated CPU Memory The total number of CPUs allocated by successfully scheduled workloads under this project. (This column is only available if the CPU Quota setting is enabled, as described below). CPU quota (Cores) CPU quota allocated to this project. (This column is only available if the CPU Quota setting is enabled, as described below). This number represents the sum of all node pools\u2019 CPU quota allocated to this project. The \u2018unlimited\u2019 value means the CPU (cores) quota is not bounded and workloads using this project can use as many CPU (cores) resources as they need (if available). CPU memory quota CPU memory quota allocated to this project. (This column is only available if the CPU Quota setting is enabled, as described below). This number represents the sum of all node pools\u2019 CPU memory quota allocated to this project. The \u2018unlimited\u2019 value means the CPU memory quota is not bounded and workloads using this Project can use as much CPU memory resources as they need (if available). CPU allocation ratio The ratio of Allocated CPUs (cores) to CPU quota (cores). This number reflects how much the project\u2019s \u2018CPU quota\u2019 is utilized by its descendent workloads. A number higher than 100% indicates the project is using over-quota CPU cores. CPU memory allocation ratio The ratio of Allocated CPU memory to CPU memory quota. This number reflects how well the project\u2019s \u2018CPU memory quota\u2019 is utilized by its descendent workloads. A number higher than 100% indicates the project is using over-quota CPU memory. Node affinity of training workloads The list of Run:ai node-affinities. Any training workload submitted within this project must specify one of those Run:ai node affinities, otherwise it is not submitted. Node affinity of interactive workloads The list of Run:ai node-affinities. Any interactive (workspace) workload submitted within this project must specify one of those Run:ai node affinities, otherwise it is not submitted. Idle time limit of training workloads The time in days:hours:minutes after which the project stops a training workload not using its allocated GPU resources. Idle time limit of preemptible workloads The time in days:hours:minutes after which the project stops a preemptible interactive (workspace) workload not using its allocated GPU resources. Idle time limit of non preemptible workloads The time in days:hours:minutes after which the project stops a non-preemptible interactive (workspace) workload not using its allocated GPU resources.. Interactive workloads time limit The duration in days:hours:minutes after which the project stops an interactive (workspace) workload Training workloads time limit The duration in days:hours:minutes after which the project stops a training workload Creation time The timestamp for when the project was created Workload(s) The list of workloads associated with the project. Click the values under this column to view the list of workloads with their resource parameters (as described below). Cluster The cluster that the project is associated with"},{"location":"platform-admin/aiinitiatives/org/projects/#node-pools-with-quota-associated-with-the-project","title":"Node pools with quota associated with the project","text":"<p>Click one of the values of Node pool(s) with quota column, to view the list of node pools and their parameters</p> Column Description Node pool The name of the node pool is given by the administrator during node pool creation. All clusters have a default node pool created automatically by the system and named \u2018default\u2019. GPU quota The amount of GPU quota the administrator dedicated to the project for this node pool (floating number, e.g. 2.3 means 230% of GPU capacity). CPU (Cores) The amount of CPUs (cores) quota the administrator has dedicated to the project for this node pool (floating number, e.g. 1.3 Cores = 1300 mili-cores). The \u2018unlimited\u2019 value means the CPU (Cores) quota is not bounded and workloads using this node pool can use as many CPU (Cores) resources as they require, (if available). CPU memory The amount of CPU memory quota the administrator has dedicated to the project for this node pool (floating number, in MB or GB). The \u2018unlimited\u2019 value means the CPU memory quota is not bounded and workloads using this node pool can use as much CPU memory resource as they need (if available). Allocated GPUs The actual amount of GPUs allocated by workloads using this node pool under this project. The number of allocated GPUs may temporarily surpass the GPU quota if over-quota is used. Allocated CPU (Cores) The actual amount of CPUs (cores) allocated by workloads using this node pool under this project. The number of allocated CPUs (cores) may temporarily surpass the CPUs (Cores) quota if over-quota is used. Allocated CPU memory The actual amount of CPU memory allocated by workloads using this node pool under this Project. The number of Allocated CPU memory may temporarily surpass the CPU memory quota if over-quota is used. Order of priority The default order in which the Scheduler uses node-pools to schedule a workload. This is used only if the order of priority of node pools is not set in the workload during submission, either by an admin policy or the user. An empty value means the node pool is not part of the project\u2019s default list, but can still be chosen by an admin policy or the user during workload submission"},{"location":"platform-admin/aiinitiatives/org/projects/#subjects-authorized-for-the-project","title":"Subjects authorized for the project","text":"<p>Click one of the values in the Subject(s) column, to view the list of subjects and their parameters. This column is only viewable, if your role in the Run:ai system affords you those permissions.</p> Column Description Subject A user, SSO group, or application assigned with a role in the scope of this Project Type The type of subject assigned to the access rule (user, SSO group, or application) Scope The scope of this project in the organizational tree. Click the name of the scope to view the organizational tree diagram, you can only view the parts of the organizational tree for which you have permission to view. Role The role assigned to the subject, in this project\u2019s scope Authorized by The user who granted the access rule Last updated The last time the access rule was updated"},{"location":"platform-admin/aiinitiatives/org/projects/#workloads-associated-with-the-project","title":"Workloads associated with the project","text":"<p>Click one of the values of Workload(s) column, to view the list of workloads and their parameters</p> Column Description Workload The name of the workload, given during its submission. Optionally, an icon describing the type of workload is also visible Type The type of the workload, e.g. Workspace, Training, Inference Status The state of the workload and time elapsed since the last status change Created by The subject that created this workload Running/ requested pods The number of running pods out of the number of requested pods for this workload. e.g. a distributed workload requesting 4 pods but may be in a state where only 2 are running and 2 are pending Creation time The date and time the workload was created GPU compute request The amount of GPU compute requested (floating number, represents either a portion of the GPU compute, or the number of whole GPUs requested) GPU memory request The amount of GPU memory requested (floating number, can either be presented as a portion of the GPU memory, an absolute memory size in MB or GB, or a MIG profile) CPU memory request The amount of CPU memory requested (floating number, presented as an absolute memory size in MB or GB) CPU compute request The amount of CPU compute requested (floating number, represents the number of requested Cores)"},{"location":"platform-admin/aiinitiatives/org/projects/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/aiinitiatives/org/projects/#adding-a-new-project","title":"Adding a new project","text":"<p>To create a new Project:</p> <ol> <li>Click +NEW PROJECT  </li> <li>Select a scope, you can only view clusters if you have permission to do so - within the scope of the roles assigned to you  </li> <li>Enter a name for the project    Project names must start with a letter and can only contain lower case Latin letters, numbers or a hyphen ('-\u2019)  </li> <li>Namespace associated with Project    Each project has an associated (Kubernetes) namespace in the cluster.     All workloads under this project use this namespace.    a. By default, Run:ai creates a namespace based on the Project name (in the form of <code>runai-&lt;name&gt;</code>)    b. Alternatively, you can choose an existing namespace created for you by the cluster administrator  </li> <li>In the Quota management section, you can set the quota parameters and prioritize resources  <ul> <li>Order of priority This column is displayed only if more than one node pool exists. The default order in which the Scheduler uses node pools to schedule a workload. This means the Scheduler first tries to allocate resources using the highest priority node pool, then the next in priority, until it reaches the lowest priority node pool list, then the Scheduler starts from the highest again. The Scheduler uses the Project list of prioritized node pools, only if the order of priority of node pools is not set in the workload during submission, either by an admin policy or by the user. Empty value means the node pool is not part of the Project\u2019s default node pool priority list, but a node pool can still be chosen by the admin policy or a user during workload submission  </li> <li>Node pool This column is displayed only if more than one node pool exists. It represents the name of the node pool.  </li> <li>GPU devices The number of GPUs you want to allocate for this project in this node pool (decimal number).  </li> <li>CPUs (Cores) This column is displayed only if CPU quota is enabled via the General settings. Represents the number of CPU cores you want to allocate for this project in this node pool (decimal number).  </li> <li>CPU memory This column is displayed only if CPU quota is enabled via the General settings. The amount of CPU memory you want to allocate for this project in this node pool (in Megabytes or Gigabytes).  </li> <li>Over quota / Over quota priority If over-quota priority is enabled via the General settings then over-quota priority is presented, otherwise over-quota  is presented  <ul> <li>Over quota When enabled, the project can use non-guaranteed overage resources above its quota in this node pool. The amount of the non-guaranteed overage resources for this project is calculated proportionally to the project quota in this node pool. When disabled, the project cannot use more resources than the guaranteed quota in this node pool.  </li> <li>Over quota priority Represents a weight used to calculate the amount of non-guaranteed overage resources a project can get on top of its quota in this node pool. All unused resources are split between projects that require the use of overage resources:  <ul> <li>Medium The default value. The Admin can change the default to any of the following values: High, Low, Lowest, or None.  </li> <li>None When set, the project cannot use more resources than the guaranteed quota in this node pool.  </li> <li>Lowest Over-quota priority \u2018lowest\u2019 has a unique behavior, because its weight is 0, it can only use over-quota (unused overage) resources if no other project needs them, and any project with a higher over-quota priority can snap the average resources at any time.</li> </ul> </li> </ul> </li> </ul> </li> </ol> <p>Note</p> <p>Setting the quota to 0 (either GPU, CPU, or CPU memory) and the over-quota to \u2018disabled\u2019 or over-quota priority to \u2018none\u2019 means the project is blocked from using those resources on this node pool.</p> <p>When no node pools are configured, you can set the same parameters but it is for the whole project, instead of per node pool.</p> <p>After node pools are created, you can set the above parameters for each node-pool separately.</p> <p></p> <ol> <li> <p>Set Scheduling rules as required. You can have a scheduling rule for:  </p> <ul> <li>Idle GPU timeout Preempt a workload that does not use GPUs for more than a specified duration. You can apply a single rule per workload type - Preemptive Workspaces, Non-preemptive Workspaces, and Training.  </li> </ul> <p>Note</p> <p>To make \u2018Idle GPU timeout\u2019 effective, it must be set to a shorter duration than that workload duration of the same workload type.  </p> <ul> <li>Workspace duration Preempt workspaces after a specified duration. This applies to both preemptive and non-preemptive Workspaces.  </li> <li>Training duration Preempt a training workload after a specified duration.  </li> <li>Node type (Affinity) Node type is used to select a group of nodes, usually with specific characteristics such as a hardware feature, storage type, fast networking interconnection, etc. The scheduler uses node type as an indication of which nodes should be used for your workloads, within this project. Node type is a label in the form of run.ai/type and a value (e.g. run.ai/type = dgx200) that the administrator uses to tag a set of nodes. Adding the node type to the project\u2019s scheduling rules enables the user to submit workloads with any node type label/value pairs in this list, according to the workload type - Workspace or Training. The Scheduler then schedules workloads using a node selector, targeting nodes tagged with the Run:ai node type label/value pair. Node pools and a node type can be used in conjunction with each other. For example, specifying a node pool and a smaller group of nodes from that node pool that includes a fast SSD memory or other unique characteristics.  </li> </ul> </li> <li> <p>Click CREATE PROJECT</p> </li> </ol>"},{"location":"platform-admin/aiinitiatives/org/projects/#adding-an-access-rule-to-a-project","title":"Adding an access rule to a project","text":"<p>To create a new access rule for a project:</p> <ol> <li>Select the project you want to add an access rule for  </li> <li>Click ACCESS RULES  </li> <li>Click +ACCESS RULE  </li> <li>Select a subject  </li> <li>Select or enter the subject identifier:  <ol> <li>User Email for a local user created in Run:ai or for SSO user as recognized by the IDP  </li> <li>Group name as recognized by the IDP  </li> <li>Application name as created in Run:ai  </li> </ol> </li> <li>Select a role  </li> <li>Click SAVE RULE  </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/projects/#deleting-an-access-rule-from-a-project","title":"Deleting an access rule from a project","text":"<p>To delete an access rule from a project:</p> <ol> <li>Select the project you want to remove an access rule from  </li> <li>Click ACCESS RULES  </li> <li>Find the access rule you want to delete  </li> <li>Click on the trash icon  </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/projects/#editing-a-project","title":"Editing a project","text":"<p>To edit a project:</p> <ol> <li>Select the project you want to edit  </li> <li>Click EDIT  </li> <li>Update the Project and click SAVE</li> </ol>"},{"location":"platform-admin/aiinitiatives/org/projects/#viewing-a-projects-policy","title":"Viewing a project\u2019s policy","text":"<p>To view the policy of a project:</p> <ol> <li>Select the project for which you want to view its policies. This option is only active for projects with defined policies in place.  </li> <li>Click VIEW POLICY and select the workload type for which you want to view the policies:     a. Workspace workload type policy with its set of rules     b. Training workload type policies with its set of rules  </li> <li>In the Policy form, view the workload rules that are enforcing your project for the selected workload type as well as the defaults:  <ul> <li>Parameter - The workload submission parameter that Rules and Defaults are applied to  </li> <li>Type (applicable for data sources only) - The data source type (Git, S3, nfs, pvc etc.)  </li> <li>Default - The default value of the Parameter  </li> <li>Rule - Set up constraints on workload policy fields  </li> <li>Source - The origin of the applied policy (cluster, department or project)  </li> </ul> </li> </ol> <p>Note</p> <p>The policy affecting the project consists of rules and defaults. Some of these rules and defaults may be derived from policies of a parent cluster and/or department (source). You can see the source of each rule in the policy form.</p>"},{"location":"platform-admin/aiinitiatives/org/projects/#deleting-a-project","title":"Deleting a project","text":"<p>To delete a project:</p> <ol> <li>Select the project you want to delete  </li> <li>Click DELETE  </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>Deleting a project does not delete its associated namespace, any of the workloads running using this namespace, or the policies defined for this project. However, any assets created in the scope of this project such as compute resources, environments, data sources, templates and credentials, are permanently deleted from the system.</p>"},{"location":"platform-admin/aiinitiatives/org/projects/#using-api","title":"Using API","text":"<p>Go to the Projects API reference to view the available actions</p>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/","title":"Scheduling Rules","text":"<p>This article explains the procedure of configuring and managing Scheduling rules.  Scheduling rules refer to restrictions applied over workloads. These restrictions apply to either the resources (nodes) on which workloads can run or to the duration of the workload run time.  Scheduling rules are set for projects or departments and apply to a specific workload type. Once scheduling rules are set, all matching workloads associated with the project or (subordinate projects in case of department) have the restrictions as defined when the workload was submitted. New scheduling rules added, are not applied over already created workloads associated with that project/department.</p>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#scheduling-rules","title":"Scheduling Rules","text":"<p>There are 3 types of scheduling rules:</p>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#workload-duration-time-limit","title":"Workload duration (time limit)","text":"<p>This rule limits the duration of a workload run time. Workload run time is calculated as the total time in which the workload was in status Running. You can apply a single rule per workload type - Preemptive Workspaces, Non-preemptive Workspaces, and Training.  </p>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#idle-gpu-time-limit","title":"Idle GPU time limit","text":"<p>This rule limits the total GPU time of a workload. Workload idle time is counted from the first time the workload is in status Running and the GPU was idle. We calculate idleness by employing the <code>runai_gpu_idle_seconds_per_workload</code> metric. This metric determines the total duration of zero GPU utilization within each 30-second interval. If the GPU remains idle throughout the 30-second window, 30 seconds are added to the idleness sum; otherwise, the idleness count is reset. You can apply a single rule per workload type - Preemptible Workspaces, Non-preemptible Workspaces, and Training.  </p> <p>Note</p> <p>To make <code>Idle GPU timeout</code> effective, it must be set to a shorter duration than that workload duration of the same workload type. </p>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#node-type-affinity","title":"Node type (Affinity)","text":"<p>Node type is used to select a group of nodes, typically with specific characteristics such as a hardware feature, storage type, fast networking interconnection, etc. The scheduler uses node type as an indication of which nodes should be used for your workloads, within this project.  </p> <p>Node type is a label in the form of <code>run.ai/type</code> and a value (e.g. <code>run.ai/type = dgx200</code>) that the administrator uses to tag a set of nodes. Adding the node type to the project\u2019s scheduling rules mandates the user to submit workloads with a node type label/value pairs from this list, according to the workload type - Workspace or Training. The Scheduler then schedules workloads using a node selector, targeting nodes tagged with the Run:ai node type label/value pair. Node pools and a node type can be used in conjunction. For example, specifying a node pool and a smaller group of nodes from that node pool that includes a fast SSD memory or other unique characteristics.</p>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#labelling-nodes-for-node-types-grouping","title":"Labelling nodes for node types grouping","text":"<p>The administrator should use a node label with the key of run.ai/type and any coupled value</p> <p>To assign a label to nodes you want to group, set the \u2018node type (affinity)\u2019 on each relevant node:</p> <ol> <li> <p>Obtain the list of nodes and their current labels by coping the following to your terminal:   <pre><code>kubectl get nodes --show-labels\n</code></pre></p> </li> <li> <p>Annotate a specific node with a new label by coping the following to your terminal:   <pre><code>kubectl label node &lt;node-name&gt; run.ai/type=&lt;value&gt;\n</code></pre></p> </li> </ol>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#adding-a-scheduling-rule-to-a-projectdepartment","title":"Adding a scheduling rule to a project/department","text":"<p>To add a scheduling rule:</p> <ol> <li>Select the project/department for which you want to add a scheduling rule  </li> <li>Click EDIT </li> <li>In the Scheduling rules section click +RULE </li> <li>Select the rule type </li> <li>Select the workload type and time limitation period </li> <li>For Node type, choose one or more labels for the desired nodes.  </li> <li>Click SAVE</li> </ol> <p>Note</p> <p>You can review the defined rules in the Projects table in the relevant column.</p>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#editing-the-projectdepartment-scheduling-rule","title":"Editing the project/department scheduling rule","text":"<p>To edit a scheduling rule:</p> <ol> <li>Select the project/department for which you want to edit its scheduling rule  </li> <li>Click EDIT </li> <li>Find the scheduling rule you would like to edit  </li> <li>Edit the rule  </li> <li>Click SAVE</li> </ol> <p>Note</p> <p>When a editing an inherited rule on a project/department (a rule defined by the department), you can only restrict the rule limitation</p>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#deleting-the-projectdepartment-scheduling-rule","title":"Deleting the project/department scheduling rule","text":"<p>To delete a scheduling rule:</p> <ol> <li>Select the project/department from which you want to delete a scheduling rule  </li> <li>Click EDIT </li> <li>Find the scheduling rule you would like to delete  </li> <li>Click on the x icon  </li> <li>Click SAVE</li> </ol> <p>!!! You cannot delete rules inherited from the department from the project's set of rules</p>"},{"location":"platform-admin/aiinitiatives/org/scheduling-rules/#using-api","title":"Using API","text":"<p>Go to the Projects API reference to view the available actions</p>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/","title":"Node Pools","text":"<p>This article explains the procedure for managing Node pools.</p> <p>Node pools assist in managing heterogeneous resources effectively. A node pool is a Run:ai construct representing a set of nodes grouped into a bucket of resources using a predefined node label (e.g. NVidia GPU type) or an administrator-defined node label (any key/value pair).</p> <p>Typically, the grouped nodes share a common feature or property, such as GPU type or other HW capability (such as Infiniband connectivity), or represent a proximity group (i.e. nodes interconnected via a local ultra-fast switch). Researchers and ML Engineers would typically use node pools to run specific workloads on specific resource types.</p> <p>Platform administrators can create, view, edit, and delete node pools. Creating a new node pool creates a new instance of the Run:ai scheduler, workloads submitted to a node pool will be scheduled using the node pool\u2019s designated scheduler instance.</p> <p>Once a new node pool is created, it is automatically assigned to all Projects and Departments with a quota of zero GPU resources, unlimited CPU resources, and over-quota enabled (Medium priority if over-quota priority is enabled). This allows any Project and Department to use any node pool when over-quota is enabled, even if the administrator has not assigned a quota for a specific node pool in a Project or Department.</p> <p>Workloads can be submitted using a prioritized list of node pools, the node pool selector picks one node pool at a time (according to the prioritized list) and the designated node pool scheduler instance handles the submission request and tries to match the requested resources within that node pool. If the scheduler cannot find resources to satisfy the submitted workload, the node pool selector will move the request to the next node pool in the prioritized list, if no node pool satisfies the request, the node pool selector will start from the first node pool again until one of the node pools satisfies the request.</p>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#node-pools-table","title":"Node pools table","text":"<p>The Node pools table can be found under Nodes in the Run:ai platform.</p> <p>The Node pools table lists all the node pools defined in the Run:ai platform and allows you to manage them.</p> <p>Note</p> <p>By default, the Run:ai platform includes a single node pool named \u2018default\u2019. When no other node pool is defined, all existing and new nodes are associated with the \u2018default\u2019 node pool. When deleting a node pool, if no other node pool matches any of the nodes\u2019 labels, the node will be included in the default node pool.</p> <p></p> <p>The Node pools table consists of the following columns:</p> Column Description Node pool The node pool name, set by the administrator during its creation (the node pool name cannot be changed after its creation). Status Node pool status. A \u2018Ready\u2019 status means the scheduler can use this node pool to schedule workloads. \u2018Empty\u2019 status means no nodes are currently included in that node pool. Label key  Label value The node pool controller will use this node-label key-value pair to match nodes into this node pool. Node(s) List of nodes included in this node pool. Click the field to view details (the details are in the Nodes article). GPU devices The total number of GPU devices installed into nodes included in this node pool. For example, a node pool that includes 12 nodes each with 8 GPU devices would show a total number of 96 GPU devices. GPU memory The total amount of GPU memory included in this node pool. The total amount of GPU memory installed in nodes included in this node pool. For example, a node pool that includes 12 nodes, each with 8 GPU devices, and each device with 80 GB of memory would show a total memory amount of 7.68 TB. Projects\u2019 GPU quota The sum of all Projects\u2019 assigned GPU quota in this node pool. Allocated GPUs The total allocation of GPU devices in units of GPUs (decimal number). For example, if 3 GPUs are 50% allocated, the field prints out the value 1.50. This value represents the portion of GPU memory consumed by all running pods using this node pool. \u2018Allocated GPUs\u2019 can be larger than \u2018Projects\u2019 GPU quota\u2019 if over-quota is used by workloads, but not larger than GPU devices. Used GPU memory The actual amount of memory (in GB or MB) used by pods running on nodes that are included in this node pool. GPU compute utilization The average compute utilization of all GPU devices included in this node pool (decimal percentage) GPU memory utilization The average memory utilization of all GPU devices included this node pool (decimal percentage) CPUs (Cores) The number of CPU cores installed on nodes included in this node CPU memory The total amount of CPU memory installed on nodes using this node pool Projects\u2019 CPU quota (Cores) The sum of all Projects\u2019 assigned CPU quota in this node pool. Projects\u2019 CPU memory quota The sum of all Projects\u2019 assigned CPU memory quota in this node pool. Allocated CPUs (Cores) The total allocation of CPU compute in units of Cores (decimal number). This value represents the amount of CPU cores consumed by all running pods using this node pool. \u2018Allocated CPUs\u2019 can be larger than \u2018Projects\u2019 GPU quota\u2019 if over-quota is used by workloads, but not larger than CPUs (Cores). Allocated CPU memory The total allocation of CPU memory in units of TB/GB/MB (decimal number). This value represents the amount of CPU memory consumed by all running pods using this node pool. \u2018Allocated CPUs\u2019 can be larger than \u2018Projects\u2019 CPU memory quota\u2019 if over-quota is used by workloads, but not larger than CPU memory. Used CPU memory The total amount of actually used CPU memory by pods running on nodes included in this node pool. Pods may allocate memory but not use all of it, or go beyond their CPU memory allocation if using Limit &gt; Request for CPU memory (burstable workloads). CPU compute utilization The average utilization of all CPU compute resources on nodes included in this node pool (percentage) CPU memory utilization The average utilization of all CPU memory resources on nodes included in this node pool (percentage) GPU placement strategy Sets the Scheduler strategy for the assignment of pods requesting both GPU and CPU resources to nodes, which can be either Bin-pack or Spread. By default, Bin-Pack is used, but can be changed to Spread by editing the node pool. When set to Bin-pack the scheduler will try to fill nodes as much as possible before using empty or sparse nodes, when set to spread the scheduler will try to keep nodes as sparse as possible by spreading workloads across as many nodes as it succeeds. CPU placement strategy Sets the Scheduler strategy for the assignment of pods requesting only CPU resources to nodes, which can be either Bin-pack or Spread. By default, Bin-Pack is used, but can be changed to Spread by editing the node pool. When set to Bin-pack the scheduler will try to fill nodes as much as possible before using empty or sparse nodes, when set to spread the scheduler will try to keep nodes as sparse as possible by spreading workloads across as many nodes as it succeeds. Last update The date and time when the node pool was last updated Creation time The date and time when the node pool was created Workload(s) List of workloads running on nodes included in this node pool, click the field to view details (described below in this article)"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#workloads-associated-with-the-node-pool","title":"Workloads associated with the node pool","text":"<p>Click one of the values in the Workload(s) column, to view the list of workloads and their parameters.</p> <p>Note</p> <p>This column is only viewable if your role in the Run:ai platform gives you read access to workloads, even if you are allowed to view workloads, you can only view the workloads within your allowed scope. This means, there might be more pods running on this node than appear in the list your are viewing.</p> Column Description Workload The name of the workload. If the workloads\u2019 type is one of the recognized types (for example: Pytorch, MPI, Jupyter, Ray, Spark, Kubeflow, and many more), an appropriate icon is printed. Type The Run:ai platform type of the workload - Workspace, Training, or Inference Status The state of the workload. The Workloads state is described in the \u2018Run:ai Workloads\u2019 article. Created by The User or Application created this workload Running/requested pods The number of running pods out of the number of requested pods within this workload. Creation time The workload\u2019s creation date and time Allocated GPU compute The total amount of GPU compute allocated by this workload. A workload with 3 Pods, each allocating 0.5 GPU, will show a value of 1.5 GPUs for the workload. Allocated GPU memory The total amount of GPU memory allocated by this workload. A workload with 3 Pods, each allocating 20GB, will show a value of 60 GB for the workload. Allocated CPU compute (cores) The total amount of CPU compute allocated by this workload. A workload with 3 Pods, each allocating 0.5 Core, will show a value of 1.5 Cores for the workload. Allocated CPU memory The total amount of CPU memory allocated by this workload. A workload with 3 Pods, each allocating 5 GB of CPU memory, will show a value of 15 GB of CPU memory for the workload."},{"location":"platform-admin/aiinitiatives/resources/node-pools/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV  </li> <li>Show/Hide details - Click to view additional information on the selected row</li> </ul>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#adding-a-new-node-pool","title":"Adding a new node pool","text":"<p>To create a new node pool:</p> <ol> <li>Click +NEW NODE POOL </li> <li>Enter a name for the node pool.     Node pools names must start with a letter and can only contain lowercase Latin letters, numbers or a hyphen ('-\u2019)  </li> <li> <p>Enter the node pool label:    The node pool controller will use this node-label key-value pair to match nodes into this node pool.  </p> <ul> <li> <p>Key is the unique identifier of a node label.  </p> <ul> <li>The key must fit the following regular expression:  <code>^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?/?([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]$</code> </li> <li>The administrator can put an automatically preset label such as the nvidia.com/gpu.product that labels the GPU type or any other key from a node label.  </li> </ul> </li> <li> <p>Value is the value of that label identifier (key). The same key may have different values, in this case, they are    considered as different labels.  </p> <ul> <li>Value must fit the following regular expression: <code>^(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?$</code> </li> </ul> </li> <li>A node pool is defined by a single key-value pair. You must not use different labels that are set on the same node by    different node pools, this situation may lead to unexpected results.  </li> </ul> </li> <li> <p>Set the GPU placement strategy:  </p> <ul> <li>Bin-pack - Place as many workloads as possible in each GPU and node to use fewer resources and maximize GPU and node vacancy.  </li> <li>Spread Spread workloads across as many GPUs and nodes as possible to minimize the load and maximize the available resources per workload.  </li> <li>GPU workloads are workloads that request both GPU and CPU resources</li> </ul> </li> <li> <p>Set the CPU placement strategy:  </p> <ul> <li>Bin-pack - Place as many workloads as possible in each CPU and node to use fewer resources and maximize CPU and node vacancy.  </li> <li>Spread - Spread workloads across as many CPUs and nodes as possible to minimize the load and maximize the available resources per workload.  </li> <li>CPU workloads are workloads that request purely CPU resources  </li> </ul> </li> <li> <p>Click CREATE NODE POOL</p> </li> </ol>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#labeling-nodes-for-node-pool-grouping","title":"Labeling nodes for node-pool grouping:","text":"<p>The Infrastructure Administrator can use a preset node label such as the <code>nvidia.com/gpu.product</code> that labels the GPU type, or configure any other node label (e.g. <code>faculty=physics</code>).</p> <p>To assign a label to nodes you want to group into a node pool, set a node label on each node:</p> <ol> <li> <p>Get the list of nodes and their current labels using the following command:    <pre><code>kubectl get nodes --show-labels\n</code></pre></p> </li> <li> <p>Annotate a specific node with a new label using the following command:    <pre><code>kubectl label node &lt;node-name&gt; &lt;key&gt;=&lt;value&gt;\n</code></pre></p> </li> </ol>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#editing-a-node-pool","title":"Editing a node pool","text":"<ol> <li>Select the node pool you want to edit  </li> <li>Click EDIT </li> <li>Update the node pool and click SAVE</li> </ol>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#deleting-a-node-pool","title":"Deleting a node pool","text":"<ol> <li>Select the node pool you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>The <code>default</code> node pool cannot be deleted. When deleting a node pool, if no other node pool matches any of the nodes\u2019 labels, the node will be included in the default node pool.</p>"},{"location":"platform-admin/aiinitiatives/resources/node-pools/#using-api","title":"Using API","text":"<p>Go to the Node pools API reference to view the available actions</p>"},{"location":"platform-admin/aiinitiatives/resources/nodes/","title":"Nodes","text":"<p>This article explains the procedure for managing Nodes.</p> <p>Nodes are Kubernetes elements automatically discovered by the Run:ai platform. Once a node is discovered by the Run:ai platform, an associated instance is created in the Nodes table, administrators can view the Node\u2019s relevant information, and Run:ai scheduler can use the node for Scheduling.</p>"},{"location":"platform-admin/aiinitiatives/resources/nodes/#nodes-table","title":"Nodes table","text":"<p>The Nodes table can be found under Nodes in the Run:ai platform.</p> <p>The Nodes table displays a list of predefined nodes available to users in the Run:ai platform.</p> <p>Note</p> <ul> <li>It is not possible to create additional nodes, or edit, or delete existing nodes.  </li> <li>Only users with relevant permissions can view the table.</li> </ul> <p></p> <p>The Nodes table consists of the following columns:</p> Column Description Node The Kubernetes name of the node Status The state of the node. Nodes in the Ready state are eligible for scheduling. If the state is Not ready then the main reason appears in parenthesis on the right side of the state field. Hovering the state lists the reasons why a node is Not ready. Node pool The name of the associated node pool. By default, every node in the Run:ai platform is associated with the default node pool, if no other node pool is associated GPU type The GPU model, for example, H100, or V100 GPU devices The number of GPU devices installed on the node. Clicking this field pops up a dialog with details per GPU (described below in this article) Free GPU devices The current number of fully vacant GPU devices GPU memory The total amount of GPU memory installed on this node. For example, if the number is 640GB and the number of GPU devices is 8, then each GPU is installed with 80GB of memory (assuming the node is assembled of homogenous GPU devices) Allocated GPUs The total allocation of GPU devices in units of GPUs (decimal number). For example, if 3 GPUs are 50% allocated, the field prints out the value 1.50. This value represents the portion of GPU memory consumed by all running pods using this node Used GPU memory The actual amount of memory (in GB or MB) used by pods running on this node. GPU compute utilization The average compute utilization of all GPU devices in this node GPU memory utilization The average memory utilization of all GPU devices in this node CPU (Cores) The number of CPU cores installed on this node CPU memory The total amount of CPU memory installed on this node Allocated CPU (Cores) The number of CPU cores allocated by pods running on this node (decimal number, e.g. a pod allocating 350 mili-cores shows an allocation of 0.35 cores). Allocated CPU memory The total amount of CPU memory allocated by pods running on this node (in GB or MB) Used CPU memory The total amount of actually used CPU memory by pods running on this node. Pods may allocate memory but not use all of it, or go beyond their CPU memory allocation if using Limit &gt; Request for CPU memory (burstable workload) CPU compute utilization The utilization of all CPU compute resources on this node (percentage) CPU memory utilization The utilization of all CPU memory resources on this node (percentage) Used swap CPU memory The amount of CPU memory (in GB or MB) used for GPU swap memory (* future) Pod(s) List of pods running on this node, click the field to view details (described below in this article)"},{"location":"platform-admin/aiinitiatives/resources/nodes/#gpu-devices-for-node","title":"GPU devices for node","text":"<p>Click one of the values in the GPU devices column, to view the list of GPU devices and their parameters.</p> Column Description Index The GPU index, read from the GPU hardware. The same index is used when accessing the GPU directly Used memory The amount of memory used by pods and drivers using the GPU (in GB or MB) Compute utilization The portion of time the GPU is being used by applications (percentage) Memory utilization The portion of the GPU memory that is being used by applications (percentage) Idle time The elapsed time since the GPU was used (i.e. the GPU is being idle for \u2018Idle time\u2019)"},{"location":"platform-admin/aiinitiatives/resources/nodes/#pods-associated-with-node","title":"Pods associated with node","text":"<p>Click one of the values in the Pod(s) column, to view the list of pods and their parameters.</p> <p>Note</p> <p>This column is only viewable if your role in the Run:ai platform gives you read access to workloads, even if you are allowed to view workloads, you can only view the workloads within your allowed scope. This means, there might be more pods running on this node than appear in the list your are viewing.</p> Column Description Pod The Kubernetes name of the pod. Usually name of the pod is made of the name of the parent workload if there is one, and an index for unique for that pod instance within the workload Status The state of the pod. In steady state this should be Running and the amount of time the pod is running Project The Run:ai project name the pod belongs to. Clicking this field takes you to the Projects table filtered by this project name Workload The workload name the pod belongs to. Clicking this field takes you to the Workloads table filtered by this workload name Image The full path of the image used by the main container of this pod Creation time The pod\u2019s creation date and time"},{"location":"platform-admin/aiinitiatives/resources/nodes/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV  </li> <li>Show/Hide details - Click to view additional information on the selected row</li> </ul>"},{"location":"platform-admin/aiinitiatives/resources/nodes/#showhide-details","title":"Show/Hide details","text":"<p>Click a row in the Nodes table and then click the Show details button at the upper right side of the action bar. The details screen appears, presenting the following metrics graphs:</p> <ul> <li>GPU utilization   Per GPU graph and an average of all GPUs graph, all on the same chart, along an adjustable period allows you to see the trends of all GPUs compute utilization (percentage of GPU compute) in this node.  </li> <li>GPU memory utilization   Per GPU graph and an average of all GPUs graph, all on the same chart, along an adjustable period allows you to see the trends of all GPUs memory usage (percentage of the GPU memory) in this node.  </li> <li>CPU compute utilization   The average of all CPUs\u2019 cores compute utilization graph, along an adjustable period allows you to see the trends of CPU compute utilization (percentage of CPU compute) in this node.  </li> <li>CPU memory utilization   The utilization of all CPUs memory in a single graph, along an adjustable period allows you to see the trends of CPU memory utilization (percentage of CPU memory) in this node.  </li> <li> <p>CPU memory usage   The usage of all CPUs memory in a single graph, along an adjustable period allows you to see the trends of CPU memory usage (in GB or MB of CPU memory) in this node.</p> </li> <li> <p>For GPUs charts - Click the GPU legend on the right-hand side of the chart, to activate or deactivate any of the GPU lines.  </p> </li> <li>You can click the date picker to change the presented period  </li> <li>You can use your mouse to mark a sub-period in the graph for zooming in, and use the \u2018Reset zoom\u2019 button to go back to the preset period  </li> <li>Changes in the period affect all graphs on this screen.</li> </ul>"},{"location":"platform-admin/aiinitiatives/resources/nodes/#using-api","title":"Using API","text":"<p>Go to the Nodes API reference to view the available actions</p>"},{"location":"platform-admin/authentication/accessrules/","title":"Access Rules","text":"<p>This article explains the procedure to manage Access rules.</p> <p>Access rules provide users, groups, or applications privileges to system entities.</p> <p>An access rule is the assignment of a role to a subject in a scope: <code>&lt;Subject&gt;</code> is a <code>&lt;Role&gt;</code> in a <code>&lt;Scope&gt;</code>.</p> <p>For example, user user@domain.com is a department admin in department A.</p>"},{"location":"platform-admin/authentication/accessrules/#access-rules-table","title":"Access rules table","text":"<p>The Access rules table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The Access rules table provides a list of all the access rules defined in the platform and allows you to manage them.</p> <p>Note</p> <p>Flexible management</p> <p>It is also possible to manage access rules directly for a specific user, application, project, or department.</p> <p></p> <p>The Access rules table consists of the following columns:</p> Column Description Type The type of subject assigned to the access rule (user, SSO group, or application). Subject The user, SSO group, or application assigned with the role Role The role assigned to the subject Scope The scope to which the subject has access. Click the name of the scope to see the scope and its subordinates Authorized by The user who granted the access rule Creation time The timestamp for when the rule was created Last updated The last time the access rule was updated"},{"location":"platform-admin/authentication/accessrules/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/authentication/accessrules/#adding-new-access-rules","title":"Adding new access rules","text":"<p>To add a new access rule:</p> <ol> <li>Click +NEW ACCESS RULE </li> <li>Select a subject User, SSO Group, or Application </li> <li>Select or enter the subject identifier:  <ul> <li>User Email for a local user created in Run:ai or for SSO user as recognized by the IDP  </li> <li>Group name as recognized by the IDP  </li> <li>Application name as created in Run:ai  </li> </ul> </li> <li>Select a role  </li> <li>Select a scope  </li> <li>Click SAVE RULE</li> </ol> <p>Note</p> <p>An access rule consists of a single subject with a single role in a single scope. To assign multiple roles or multiple scopes to the same subject, multiple access rules must be added.</p>"},{"location":"platform-admin/authentication/accessrules/#editing-an-access-rule","title":"Editing an access rule","text":"<p>Access rules cannot be edited. To change an access rule, you must delete the rule, and then create a new rule to replace it.</p>"},{"location":"platform-admin/authentication/accessrules/#deleting-an-access-rule","title":"Deleting an access rule","text":"<ol> <li>Select the access rule you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol>"},{"location":"platform-admin/authentication/accessrules/#using-api","title":"Using API","text":"<p>Go to the Access rules API reference to view the available actions</p>"},{"location":"platform-admin/authentication/applications/","title":"Applications","text":"<p>This article explains the procedure to manage applications and it\u2019s permissions.</p> <p>Applications are used for API integrations with Run:ai. An application contains a secret key. Using the secret key you can obtain a token and use it within subsequent API calls.</p> <p>Applications are managed locally and assigned with Access Rules to manage its permissions.</p> <p>For example, application ci-pipeline-prod assigned with a Researcher role in Cluster: A.</p>"},{"location":"platform-admin/authentication/applications/#applications-table","title":"Applications table","text":"<p>The Applications table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The Applications table provides a list of all the applications defined in the platform, and allows you to manage them.</p> <p></p> <p>The Applications table consists of the following columns:</p> Column Description Application The name of the application Status The status of the application Access rule(s) The access rules assigned to the application Last login The timestamp for the last time the user signed in Created by The user who created the application Creation time The timestamp for when the application was created Last updated The last time the application was updated"},{"location":"platform-admin/authentication/applications/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/authentication/applications/#creating-an-application","title":"Creating an application","text":"<p>To create an application:</p> <ol> <li>Click +NEW APPLICATION </li> <li>Enter the application\u2019s Name </li> <li>Click CREATE </li> <li>Copy the credentials and store it securely:  <ul> <li>Application name </li> <li>Secret key </li> </ul> </li> <li>Click DONE</li> </ol> <p>Note</p> <p>The secret key is visible only at the time of creation, it cannot be recovered but can be regenerated.</p>"},{"location":"platform-admin/authentication/applications/#adding-an-access-rule-to-an-application","title":"Adding an access rule to an application","text":"<p>To create an access rule:</p> <ol> <li>Select the application you want to add an access rule for  </li> <li>Click ACCESS RULES </li> <li>Click +ACCESS RULE </li> <li>Select a role  </li> <li>Select a scope  </li> <li>Click SAVE RULE </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/authentication/applications/#deleting-an-access-rule-from-an-application","title":"Deleting an access rule from an application","text":"<p>To delete an access rule:</p> <ol> <li>Select the application you want to remove an access rule from  </li> <li>Click ACCESS RULES </li> <li>Find the access rule assigned to the user you would like to delete  </li> <li>Click on the trash icon  </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/authentication/applications/#regenerating-key","title":"Regenerating key","text":"<p>To regenerate an application\u2019s key:</p> <ol> <li>Select the application you want to regenerate it\u2019s secret key  </li> <li>Click REGENERATE KEY </li> <li>Click REGENERATE </li> <li>Review the user\u2019s credentials and store it securely:  <ul> <li>Application name  </li> <li>Secret key </li> </ul> </li> <li>Click DONE</li> </ol> <p>Warning</p> <p>Regenerating an application key revokes its previous key.</p>"},{"location":"platform-admin/authentication/applications/#deleting-an-application","title":"Deleting an application","text":"<ol> <li>Select the application you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol>"},{"location":"platform-admin/authentication/applications/#using-api","title":"Using API","text":"<p>Go to the Applications, Access rules API reference to view the available actions</p>"},{"location":"platform-admin/authentication/roles/","title":"Roles","text":"<p>This article explains the available roles in the Run:ai platform.</p> <p>A role is a set of permissions that can be assigned to a subject in a scope.</p> <p>A permission is a set of actions (View, Edit, Create and Delete) over a Run:ai entity (e.g. projects, workloads, users).Roles table</p> <p>The Roles table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The Roles table displays a list of predefined roles available to users in the Run:ai platform. It is not possible to create additional rules or edit or delete existing rules.</p> <p></p> <p>The Roles table consists of the following columns:</p> Column Description Role The name of the role Created by The name of the role creator Creation time The timestamp when the role was created"},{"location":"platform-admin/authentication/roles/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/authentication/roles/#reviewing-a-role","title":"Reviewing a role","text":"<ol> <li>To review a role click the role name on the table  </li> <li>In the role form review the following:  <ul> <li>Role name   The name of the role  </li> <li>Entity  A system-managed object that can be viewed, edited, created or deleted by a user based on their assigned role and scope  </li> <li>Actions  The actions that the role assignee is authorized to perform for each entity  <ul> <li>View If checked, an assigned user with this role can view instances of this type of entity within their defined scope  </li> <li>Edit If checked, an assigned user with this role can change the settings of an instance of this type of entity within their defined scope  </li> <li>Create If checked, an assigned user with this role can create new instances of this type of entity within their defined scope  </li> <li>Delete If checked, an assigned user with this role can delete instances of this type of entity within their defined scope</li> </ul> </li> </ul> </li> </ol>"},{"location":"platform-admin/authentication/roles/#roles-in-runai","title":"Roles in Run:ai","text":"<p>Run:ai supports the following roles and their permissions:  Under each role is a detailed list of the actions that the role assignee is authorized to perform for each entity.</p> Compute resource administrator <p></p> Data source administrator <p></p> Data volume administrator <p></p> Department administrator <p></p> Department viewer <p></p> Editor <p></p> Environment administrator <p></p> L1 researcher <p></p> L2 researcher <p></p> ML engineer <p></p> Research manager <p></p> System administrator <p></p> Template administrator <p></p> Viewer <p></p>"},{"location":"platform-admin/authentication/roles/#permitted-workloads","title":"Permitted workloads","text":"<p>When assigning a role with either one, all or any combination of the View, Edit, Create and Delete permissions for workloads, the subject has permissions to manage not only Run:ai native workloads (Workspace, Training, Inference), but also a list of 3rd party workloads:</p> <ul> <li>k8s: StatefulSet</li> <li>k8s: ReplicaSet</li> <li>k8s: Pod</li> <li>k8s: Deployment</li> <li>batch: Job</li> <li>batch: CronJob</li> <li>machinelearning.seldon.io: SeldonDeployment</li> <li>kubevirt.io: VirtualMachineInstance</li> <li>kubeflow.org: TFJob</li> <li>kubeflow.org: PyTorchJob</li> <li>kubeflow.org: XGBoostJob</li> <li>kubeflow.org: MPIJob</li> <li>kubeflow.org: MPIJob</li> <li>kubeflow.org: Notebook</li> <li>kubeflow.org: ScheduledWorkflow</li> <li>amlarc.azureml.com: AmlJob</li> <li>serving.knative.dev: Service</li> <li>workspace.devfile.io: DevWorkspace</li> <li>ray.io: RayCluster</li> <li>ray.io: RayJob</li> <li>ray.io: RayService</li> <li>ray.io: RayCluster</li> <li>ray.io: RayJob</li> <li>ray.io: RayService</li> <li>tekton.dev: TaskRun</li> <li>tekton.dev: PipelineRun</li> <li>argoproj.io: Workflow</li> </ul>"},{"location":"platform-admin/authentication/roles/#using-api","title":"Using API","text":"<p>Go to the Roles API reference to view the available actions.</p>"},{"location":"platform-admin/authentication/users/","title":"Users","text":"<p>This article explains the procedure to manage users and their permissions.</p> <p>Users can be managed locally, or via the Identity provider, while assigned with Access Rules to manage its permissions.</p> <p>For example, user user@domain.com is a department admin in department A.</p>"},{"location":"platform-admin/authentication/users/#users-table","title":"Users table","text":"<p>The Users table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>The users table provides a list of all the users in the platform. You can manage local users and manage user permissions (access rules) for both local and SSO users.</p> <p>Note</p> <p>Single Sign-On users</p> <p>SSO users are managed by the identity provider and appear once they have signed in to Run:ai</p> <p></p> <p>The Users table consists of the following columns:</p> Column Description User The unique identity of the user (email address) Type The type of the user - SSO / local Last login The timestamp for the last time the user signed in Access rule(s) The access rules assigned to the user Created By The user who created the user Creation time The timestamp for when the user was created Last updated The last time the user was updated"},{"location":"platform-admin/authentication/users/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/authentication/users/#creating-a-local-user","title":"Creating a local user","text":"<p>To create a local user:</p> <ol> <li>Click +NEW LOCAL USER </li> <li>Enter the user\u2019s Email address </li> <li>Click CREATE </li> <li>Review and copy the user\u2019s credentials:  <ul> <li>User Email </li> <li>Temporary password to be used on first sign-in  </li> </ul> </li> <li>Click DONE</li> </ol> <p>Note</p> <p>The temporary password is visible only at the time of user\u2019s creation, and must be changed after the first sign-in</p>"},{"location":"platform-admin/authentication/users/#adding-an-access-rule-to-a-user","title":"Adding an access rule to a user","text":"<p>To create an access rule:</p> <ol> <li>Select the user you want to add an access rule for  </li> <li>Click ACCESS RULES </li> <li>Click +ACCESS RULE </li> <li>Select a role  </li> <li>Select a scope  </li> <li>Click SAVE RULE </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/authentication/users/#deleting-users-access-rule","title":"Deleting user\u2019s access rule","text":"<p>To delete an access rule:</p> <ol> <li>Select the user you want to remove an access rule from  </li> <li>Click ACCESS RULES </li> <li>Find the access rule assigned to the user you would like to delete  </li> <li>Click on the trash icon  </li> <li>Click CLOSE</li> </ol>"},{"location":"platform-admin/authentication/users/#resetting-a-user-password","title":"Resetting a user password","text":"<p>To reset a user\u2019s password:</p> <ol> <li>Select the user you want to reset it\u2019s password  </li> <li>Click RESET PASSWORD </li> <li>Click RESET </li> <li>Review and copy the user\u2019s credentials:  <ul> <li>User Email </li> <li>Temporary password to be used on next sign-in  </li> </ul> </li> <li>Click DONE</li> </ol>"},{"location":"platform-admin/authentication/users/#deleting-a-user","title":"Deleting a user","text":"<ol> <li>Select the user you want to delete  </li> <li>Click DELETE </li> <li>In the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>To ensure administrative operations are always available, at least one local user with System Administrator role should exist.</p>"},{"location":"platform-admin/authentication/users/#using-api","title":"Using API","text":"<p>Go to the Users, Access rules API reference to view the available actions</p>"},{"location":"platform-admin/integrations/integration-overview/","title":"Integrations with Run:ai","text":"<p>The table below summarizes the integration capabilities with various third-party products. </p>"},{"location":"platform-admin/integrations/integration-overview/#integration-support","title":"Integration support","text":"<p>Support for integrations varies. Where mentioned below, the integration is supported out of the box with Run:ai. With other integrations,  our customer success team has previous experience with integrating with the third party software and many times the community portal will contain additional reference documentation provided on an as-is basis.</p> <p>The Run:ai community portal is password protected and access is provided to customers and partners.</p>"},{"location":"platform-admin/integrations/integration-overview/#integrations","title":"Integrations","text":"Tool Category Run:ai support details Additional Information Triton Orchestration Supported Usage via docker base image. Quickstart inference example Spark Orchestration Community Support  It is possible to schedule Spark workflows with the Run:ai scheduler. For details, please contact Run:ai customer support.  Sample code can be found in the Run:ai customer success community portal: https://runai.my.site.com/community/s/article/How-to-Run-Spark-jobs-with-Run-AI Kubeflow Pipelines Orchestration Community Support It is possible to schedule kubeflow pipelines with the Run:ai scheduler. For details please contact Run:ai customer support. Sample code can be found in the Run:ai customer success community portalhttps://runai.my.site.com/community/s/article/How-to-integrate-Run-ai-with-Kubeflow Apache Airflow Orchestration Community Support It is possible to schedule Airflow workflows with the Run:ai scheduler. For details, please contact Run:ai customer support.                              Sample code can be found in the Run:ai customer success community portal: https://runai.my.site.com/community/s/article/How-to-integrate-Run-ai-with-Apache-Airflow Argo workflows Orchestration Community Support It is possible to schedule Argo workflows with the Run:ai scheduler. For details, please contact Run:ai customer support.  Sample code can be found in the Run:ai customer success community portal https://runai.my.site.com/community/s/article/How-to-integrate-Run-ai-with-Argo-Workflows SeldonX Orchestration Community Support It is possible to schedule Seldon Core workloads with the Run:ai scheduler. For details, please contact Run:ai customer success.                          Sample code can be found in the Run:ai customer success community portal: https://runai.my.site.com/community/s/article/How-to-integrate-Run-ai-with-Seldon-Core Jupyter Notebook Development Supported Run:ai provides integrated support with Jupyter Notebooks. Quickstart example: https://docs.run.ai/latest/Researcher/Walkthroughs/quickstart-jupyter/ Jupyter Hub Development Community Support It is possible to submit Run:ai workloads via JupyterHub. For more information please contact Run:ai customer support PyCharm Development Supported Containers created by Run:ai can be accessed via PyCharm. PyCharm example VScode Development Supported - Containers created by Run:ai can be accessed via Visual Studio Code. example - You can automatically launch Visual Studio code web from the Run:ai console. example. Kubeflow notebooks Development Community Support It is possible to launch a kubeflow notebook with the Run:ai scheduler. For details please contact Run:ai customer support Sample code can be found in the Run:ai customer success community portal:https://runai.my.site.com/community/s/article/How-to-integrate-Run-ai-with-Kubeflow Ray training, inference, data processing. Community Support It is possible to schedule Ray jobs with the Run:ai scheduler.   Sample code can be found in the Run:ai customer success community portal https://runai.my.site.com/community/s/article/How-to-Integrate-Run-ai-with-Ray TensorBoard Experiment tracking Supported Run:ai comes with a preset Tensorboard Environment asset. TensorBoard example.  Additional sample Weights &amp; Biases Experiment tracking Community Support It is possible to schedule W&amp;B workloads with the Run:ai scheduler. For details, please contact Run:ai customer success. ClearML Experiment tracking Community Support It is possible to schedule ClearML workloads with the Run:ai scheduler. For details, please contact Run:ai customer success. MLFlow Model Serving Community Support It is possible to use ML Flow together with the Run:ai scheduler. For details, please contact Run:ai customer support.     Sample code can be found in the Run:ai customer success community portal: https://runai.my.site.com/community/s/article/How-to-integrate-Run-ai-with-MLflow  Additional MLFlow sample Hugging Face Repositories Supported Run:ai provides an out of the box integration with Hugging Face Docker Registry Repositories Supported Run:ai allows using a docker registry as a Credentials asset. S3 Storage Supported Run:ai communicates with S3 by defining a data source asset. Github Storage Supported Run:ai communicates with GitHub by defining it as a data source  asset Tensorflow Training Supported Run:ai provides out of the box support for submitting TensorFlow workloads via API or by submitting workloads via user interface. Pytorch Training Supported Run:ai provides out of the box support for submitting PyTorch workloads via API or by submitting workloads via user interface. Kubeflow MPI Training Supported Run:ai provides out of the box support for submitting MPI workloads via API or by submitting workloads via user interface XGBoost Training Supported Run:ai provides out of the box support for submitting XGBoost workloads via API or by submitting workloads via user interface Karpenter Cost Optimization Supported Run:ai provides out of the box support for Karpenter to save cloud costs. Integration notes with Karpenter can be found here"},{"location":"platform-admin/integrations/integration-overview/#kubernetes-workloads-integration","title":"Kubernetes Workloads Integration","text":"<p>Kubernetes has several built-in resources that encapsulate running Pods. These are called Kubernetes Workloads and should not be confused with Run:ai Workloads.</p> <p>Examples of such resources are a Deployment that manages a stateless application, or a Job that runs tasks to completion.</p> <p>Run:ai natively runs Run:ai Workloads. A Run:ai workload encapsulates all the resources needed to run, creates them, and deletes them together. However, Run:ai, being an open platform allows the scheduling of any Kubernetes Workflow.</p> <p>For more information see Kubernetes Workloads Integration.</p>"},{"location":"platform-admin/integrations/karpenter/","title":"Working with Karpenter","text":"<p>Karpenter is an open-source, Kubernetes cluster autoscaler built for cloud deployments. Karpenter optimizes the cloud cost of a customer\u2019s cluster by moving workloads between different node types, consolidating workloads into fewer nodes, using lower-cost nodes where possible, scaling up new nodes when needed, and shutting down unused nodes.</p> <p>Karpenter\u2019s main goal is cost optimization. Unlike Karpenter, Run:ai\u2019s scheduler optimizes for fairness and resource utilization. Therefore, there are a few potential friction points when using both on the same cluster.</p>"},{"location":"platform-admin/integrations/karpenter/#friction-points-using-karpenter-with-runai","title":"Friction points using Karpenter with Run:ai","text":"<ol> <li>Karpenter looks for \u201cunschedulable\u201d pending workloads and may try to scale up new nodes to make those workloads schedulable. However, in some scenarios, these workloads may exceed their quota parameters, and the Run:ai scheduler will put them into a pending state.</li> <li>Karpenter is not aware of the Run:ai fractions mechanism and may try to interfere incorrectly.</li> <li>Karpenter preempts any type of workload (i.e., high-priority, non-preemptible workloads will potentially be interrupted and moved to save cost).</li> <li>Karpenter has no pod-group (i.e., workload) notion or gang scheduling awareness, meaning that Karpenter is unaware that a set of \u201carbitrary\u201d pods is a single workload. This may cause Karpenter to schedule those pods into different node pools (in the case of multi-node-pool workloads) or scale up or down a mix of wrong nodes.</li> </ol>"},{"location":"platform-admin/integrations/karpenter/#mitigating-the-friction-points","title":"Mitigating the friction points","text":"<p>Run:ai scheduler mitigates the friction points using the following techniques (each numbered bullet below corresponds to the related friction point listed above):</p> <ol> <li>Karpenter uses a \u201cnominated node\u201d to recommend a node for the scheduler. The Run:ai scheduler treats this as a \u201cpreferred\u201d recommendation, meaning it will try to use this node, but it\u2019s not required and it may choose another node.</li> <li>Fractions - Karpenter won\u2019t consolidate nodes with one or more pods that cannot be moved. The Run:ai reservation pod is marked as \u2018do not evict\u2019 to allow the Run:ai scheduler to control the scheduling of fractions.</li> <li>Non-preemptible workloads - Run:ai marks non-preemptible workloads as \u2018do not evict\u2019 and Karpenter respects this annotation.</li> <li>Run:ai node pools (single-node-pool workloads) - Karpenter respects the \u2018node affinity\u2019 that Run:ai sets on a pod, so Karpenter uses the node affinity for its recommended node. For the gang-scheduling/pod-group (workload) notion, Run:ai scheduler considers Karpenter directives as preferred recommendations rather than mandatory instructions and overrides Karpenter instructions where appropriate.</li> </ol>"},{"location":"platform-admin/integrations/karpenter/#deployment-considerations","title":"Deployment Considerations","text":"<ul> <li>Using multi-node-pool workloads<ul> <li>Workloads may include a list of optional nodepools. Karpenter is not aware that only a single node pool should be selected out of that list for the workload. It may therefore recommend putting pods of the same workload into different node pools and may scaleup nodes from different node pools to serve a \u201cmulti-node-pool\u201d workload instead of nodes on the selected single node pool.</li> <li>If this becomes an issue (i.e., if Karpenter scales up the wrong node types), users can set an inter-pod affinity using the node pool label or another common label as a \u2018topology\u2019 identifier. This will force Karpenter to choose nodes from a single-node pool per workload, selecting from any of the node pools listed as allowed by the workload.</li> <li>An alternative approach is to use a single-node pool for each workload instead of multi-node pools.</li> </ul> </li> <li>Consolidation<ul> <li>To make Karpenter more effective when using its consolidation function, users should consider separating preemptible and non-preemptible workloads, either by using node pools, node affinities, taint/tollerations, or inter-pod anti-affinity.</li> <li>If users don\u2019t separate preemptible and non-preemptible workloads (i.e., make them run on different nodes), Karpenter\u2019s ability to consolidate (binpack) and shut down nodes will be reduced, but it is still effective.</li> </ul> </li> <li>Conflicts between binpacking and spread policies<ul> <li>If Run:ai is used with a scheduling spread policy, it will clash with Karpenter\u2019s default binpacks/consolidation policy, and the outcome may be a deployment that is not optimized for any of these policies.</li> <li>Usually spread is used for Inference, which is non-preemptible and therefore not controlled by Karpenter (Run:ai scheduler will mark those workloads as \u2018do not evict\u2019 for Karpenter), so this should not present a real deployment issue for customers.</li> </ul> </li> </ul>"},{"location":"platform-admin/performance/dashboard-analysis/","title":"Introduction","text":"<p>The Run:ai Administration User Interface provides a set of dashboards that help you monitor Clusters, Cluster Nodes, Projects, and Workloads. This document provides the key metrics to monitor, how to assess them as well as suggested actions.</p> <p>Dashboards are used by system administrators to analyze and diagnose issues that relate to:</p> <ul> <li>Physical Resources.</li> <li>Organization resource allocation and utilization.</li> <li>Usage characteristics.</li> </ul> <p>System administrators need to know important information about the physical resources that are currently being used. Important information such as:</p> <ul> <li>Resource health.</li> <li>Available resources and their distribution.</li> <li>Is there a lack of resources.</li> <li>Are resources being utilized correctly.</li> </ul> <p>With this information, system administrators can hone in on:</p> <ul> <li>How resources are allocated across the organization.</li> <li>How the different organizational units utilized quotas and resources within those quotas.</li> <li>The actual performance of the organizational units.</li> </ul> <p>These dashboards give system administrators the ability to drill down to see details of the different types of workloads that each of the organizational units is running. These usage and performance metrics ensure that system administrators can then take actions to correct issues that affect performance.</p> <p>There are 5 dashboards:</p> <ul> <li>GPU/CPU Overview dashboard\u2014Provides information about what is happening right now in the cluster.</li> <li>Quota Management dashboard\u2014Provides information about quota utilization.</li> <li>Analytics dashboard\u2014Provides long term analysis of cluster behavior.</li> <li>Multi-Cluster Overview dashboard\u2014Provides a more holistic, multi-cluster view of what is happening right now. The dashboard is intended for organizations that have more than one connected cluster.</li> <li>Consumption dashboard\u2014Provides information about resource consumption.</li> </ul>"},{"location":"platform-admin/performance/dashboard-analysis/#gpucpu-overview-dashboard-new-and-legacy","title":"GPU/CPU Overview Dashboard (New and legacy)","text":"<p>The Overview dashboard provides information about what is happening right now in the cluster.  Administrators can view high-level information on the state of the cluster. The dashboard has two tabs that change the display to provide a focused view for GPU Dashboards (default view) and CPU Dashboards.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#gpu-dashboard","title":"GPU Dashboard","text":"<p>The GPU dashboard displays specific information for GPU based nodes, node-pools, clusters, or tenants. These dashboards also include additional metrics that specific to GPU based environments. The dashboard contains tiles that show information about specific resource allocation and performance metrics. The tiles are interactive allowing you to link directly to the assets or drill down to specific scopes. Use the time frame selector to choose a time frame for all the tiles in the dashboard.</p> <p>The dashboard has the following tiles:</p> <ul> <li>Ready nodes\u2014displays GPU nodes that are in the ready state.</li> <li>Ready GPU devices\u2014displays the number of GPUs in nodes that are in the ready state.</li> <li>Allocated GPU compute\u2014displays the total number of GPUs allocated from all the nodes.</li> <li>Running workloads\u2014displays the number of running workloads.</li> <li>Pending workloads\u2014displays the number of workloads in the pending status.</li> <li>Allocation ration by node pool\u2014displays the percentage of GPUs allocated per node pool. Hover over the bar for detailed information. Use the scope selected at the bottom of the graph to drill down for more details.</li> <li>Free resources by node pool\u2014the graph displays the amount of free resources per node pool. Press a entry in the graph for more details. Hover over the resource bubbles for specific details for the workers in the node. Use the ellipsis to download the graph as a CSV file.</li> <li>Resource allocation by workload type\u2014displays the resource allocation by workload type. Hover over the bar for detailed information. Use the scope selected at the bottom of the graph to drill down for more details. Use the ellipsis to download the graph as a CSV file.</li> <li>Workload by status\u2014displays the number of workloads for each status in the workloads table. Hover over the bar for detailed information. Use the scope selected at the bottom of the graph to drill down for more details. Use the ellipsis to download the graph as a CSV file.</li> <li>Resources utilization\u2014displays the resource utilization over time. The right pane of the graph shows the average utilization of the selected time frame of the dashboard. Hover over the graph to see details of a specific time in the graph. Use the ellipsis to download the graph as a CSV file.</li> <li>Resource allocation\u2014displays the resource allocation over time. The right pane of the graph shows the average allocation of the selected time frame of the dashboard. Hover over the graph to see details of a specific time in the graph. Use the ellipsis to download the graph as a CSV file.</li> </ul>"},{"location":"platform-admin/performance/dashboard-analysis/#cpu-dashboard","title":"CPU Dashboard","text":"<p>The CPU dashboards display specific information for CPU based nodes, node-pools, clusters, or tenants. These dashboards also include additional metrics that specific to CPU based environments.</p> <p>To enable CPU Dashboards:</p> <ol> <li>Press the <code>Tools &amp; Settings</code> icon, then press <code>General</code>.</li> <li>Open the <code>Analytics</code> pane and toggle the Show CPU dashboard switch to enable the feature.</li> </ol> <p>Toggle the switch to <code>disable</code> to disable CPU Dashboards option.</p> <p>The dashboard contains the following tiles:</p> <ul> <li>Total CPU Nodes\u2014displays the total amount of CPU nodes.</li> <li>Ready CPU nodes\u2014displays the total amount of CPU nodes in the ready state.</li> <li>Total CPUs\u2014displays the total amount of CPUs.</li> <li>Ready CPUs\u2014displays the total amount of CPUs in the ready state.</li> <li>Allocated CPUs\u2014displays the amount of allocated CPUs.</li> <li>Running workloads\u2014displays the amount of workloads in the running state.</li> <li>Pending workloads\u2014displays the amount of workloads in the pending state.</li> <li>Allocated CPUs per project\u2014displays the amount of CPUs allocated per project.</li> <li>Active projects\u2014displays the active projects  with the CPU allocation and amount of running and pending workloads.</li> <li>Utilization per resource type\u2014displays the CPU compute and CPU memory utilization over time.</li> <li>CPU compute utilization\u2014displays the current CPU compute utilization.</li> <li>CPU memory utilization\u2014displays the current CPU memory utilization.</li> <li>Pending workloads\u2014displays the requested resources and wait time for workloads in the pending status.</li> <li>Workloads with error\u2014displays the amount of workloads that are currently not running due to an error.</li> <li>Workload Count per CPU Compute Utilization\u2014</li> <li>5 longest running workloads\u2014displays up to 5 of workloads that have the longest running time.</li> </ul> <p>Analysis and Suggested actions:</p> Review Analysis  &amp; Actions Interactive Workloads are too frequently idle Consider setting time limits for interactive Workloads through the Projects tab.\u00a0  Consider also reducing GPU/CPU quotas for specific Projects to encourage users to run more training Workloads as opposed to interactive Workloads (note that interactive Workloads can not use more than the GPU/CPU quota assigned to their Project). Training Workloads are too frequently idle Identify and notify the right users and work with them to improve the utilization of their training scripts"},{"location":"platform-admin/performance/dashboard-analysis/#workloads-with-an-error","title":"Workloads with an Error","text":"<p>Search for Workloads with an error status. These Workloads may be holding GPUs/CPUs without actually using them.</p> <p>Analysis and Suggested actions:</p> <p>Search for workloads with an Error status on the Workloads view and discuss with the Job owner. Consider deleting these Workloads to free up the resources for other users.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#workloads-with-a-long-duration","title":"Workloads with a Long Duration","text":"<p>View list of 5 longest Workloads.</p> <p>Analysis and Suggested actions:</p> Review Analysis &amp; Actions Training Workloads run for too long Ask users to view their Workloads and analyze whether useful work is being done. If needed, stop their Workloads. Interactive Workloads run for too long Consider setting time limits for interactive Workloads via the Project editor."},{"location":"platform-admin/performance/dashboard-analysis/#job-queue","title":"Job Queue","text":"<p>Identify queueing bottlenecks.</p> <p>Analysis and Suggested actions:</p> Review Analysis &amp; Actions Cluster is fully loaded Go over the table of active Projects and check that fairness between Projects was enforced, by reviewing the number of allocated GPUs/CPUs for each Project, ensuring each Project was allocated with its fair-share portion of the cluster. Cluster is not fully loaded Go to the Workloads view to review the resources requested for that Job (CPU, CPU memory, GPU, GPU memory). Go to the Nodes view to verify that there is no Node with enough free resources that can host that Job. <p>Also, check the command that the user used to submit the job. The Researcher may have requested a specific Node for that Job.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#analytics-dashboard","title":"Analytics Dashboard","text":"<p>The Analytics dashboard provides means for viewing historical data on cluster information such as:</p> <ul> <li>Utilization across the cluster</li> <li>GPU usage by different Projects, including allocation and utilization, broken down into interactive and training Workloads</li> <li>Breakdown of running Workloads into interactive, training, and GPU versus CPU-only Workloads, including information on queueing (number of pending Workloads and requested GPUs),</li> <li>Status of Nodes in terms of availability and allocated and utilized resources.</li> </ul> <p>The dashboard has a dropdown filter for node pools and Departments. From the dropdown, select one or more node pools. The default setting is <code>all</code>.</p> <p>The information presented in Analytics can be used in different ways for identifying problems and fixing them. Below are a few examples.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#node-downtime","title":"Node Downtime","text":"<p>View the overall available resources per Node and identify cases where a Node is down and there was a reduction in the number of available resources.</p> <p>How to: view the following panel.</p> <p></p> <p>Analysis and Suggested actions:</p> <p>Filter according to time range to understand for how long the Node is down.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#gpu-allocation","title":"GPU Allocation","text":"<p>Track GPU allocation across time.</p> <p>How to: view the following panels.</p> <p></p> <p>The panel on the right-hand side shows the cluster-wide GPU allocation and utilization versus time, whereas the panels on the left-hand side show the cluster-wide GPU allocation and utilization averaged across the filtered time range.</p> <p>Analysis and Suggested actions:</p> <p>If the allocation is too low for a long period, work with users to run more workloads and to better utilize the Cluster.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#track-gpu-utilization","title":"Track GPU utilization","text":"<p>Track whether Researchers efficiently use the GPU resources they have allocated for themselves.</p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>If utilization is too low for a long period, you will want to identify the source of the problem:</p> <ul> <li>Go to \u201cAverage GPU Allocation &amp; Utilization\u201d</li> <li>Look for Projects with large GPU allocations for interactive Workloads or Projects that poorly utilize their training Workloads. Users tend to poorly utilize their GPUs in interactive sessions because of the dev &amp; debug nature of their work which typically is an iterative process with long idle GPU time. On many occasions users also don\u2019t shut down their interactive Workloads, holding their GPUs idle and preventing others from using them.</li> </ul> Review Analysis &amp; Actions Low GPU utilization is due to interactive Workloads being used too frequently Consider setting time limits for interactive Workloads through the Projects tab or reducing GPU quotas to encourage users to run more training Workloads as opposed to interactive Workloads (note that interactive Workloads can not use more than the GPU quota assigned to their Project). Low GPU utilization is due to users poorly utilizing their GPUs in training sessions Identify Projects with bad GPU utilization in training Workloads, notify the users and work with them to improve their code and the way they utilize their GPUs."},{"location":"platform-admin/performance/dashboard-analysis/#training-vs-interactive-researcher-maturity","title":"Training vs. Interactive -- Researcher maturity","text":"<p>Track the number of running Workloads and the breakdown into interactive, training, and CPU-only Workloads.</p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>We would want to encourage users to run more training Workloads than interactive Workloads, as it is the key to achieving high GPU utilization across the Cluster:</p> <ul> <li>Training Workloads run to completion and free up their resources automatically when training ends</li> <li>Training Workloads can be preempted, queued, and resumed automatically by the Run:ai system according to predefined policies which increases fairness and Cluster utilization.</li> </ul>"},{"location":"platform-admin/performance/dashboard-analysis/#pending-queue-size","title":"Pending Queue Size","text":"<p>Track how long is the queue for pending Workloads</p> <p>How to: view the following panels:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>Consider buying more GPUs:</p> <ul> <li>When there are too many Workloads are waiting in queue for too long.</li> <li>With a large number of requested GPUs.</li> <li>While the Cluster is fully loaded and well utilized.</li> </ul>"},{"location":"platform-admin/performance/dashboard-analysis/#cpu-memory-utilization","title":"CPU &amp; Memory Utilization","text":"<p>Track CPU and memory Node utilization and identify times where the load on specific Nodes is high.</p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>If the load on specific Nodes is too high, it may cause problems with the proper operation of the Cluster and the way workloads are running.</p> <p>Consider adding more CPUs, or adding additional CPU-only nodes for Workloads that do only CPU processing.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#multi-cluster-overview-dashboard","title":"Multi-Cluster overview dashboard","text":"<p>Provides a holistic, aggregated view across Clusters, including information about Cluster and Node utilization, available resources, and allocated resources. With this dashboard, you can identify Clusters that are down or underutilized and go to the Overview of that Cluster to explore further.</p> <p></p>"},{"location":"platform-admin/performance/dashboard-analysis/#consumption-dashboard","title":"Consumption dashboard","text":"<p>This dashboard enables users and admins to view consumption usage using run:AI services. The dashboard provides views based on configurable filters and timelines. The dashboard also provides costing analysis for GPU, CPU, and memory costs for the system.</p> <p></p> <p>The dashboard has 4 tiles for:</p> <ul> <li>Cumulative GPU allocation per Project or Department</li> <li>Cumulative CPU allocation per Project or Department</li> <li>Cumulative memory allocation per Project or Department</li> <li>Consumption types</li> </ul> <p>Use the drop down menus at the top of the dashboard to apply filters for:</p> <ul> <li>Project or department</li> <li>Per project (single, multiple, or all)</li> <li>Per department (single, multiple or all)</li> <li>Per cluster (single, multiple, all)</li> </ul> <p>To enable the Consumption Dashboard:</p> <ol> <li>Press the <code>Tools &amp; Settings</code> icon, then press <code>General</code>.</li> <li>Open the <code>Analytics</code> pane and toggle the Consumption switch to enable the feature.</li> <li>Enter the cost of:</li> <li>GPU compute / Hour</li> <li>CPU compute / Hour</li> <li>CPU memory / Hour</li> </ol> <p>Use the time picker dropdown to select relative time range options and set custom absolute time ranges. You can change the Timezone and fiscal year settings from the time range controls by clicking the Change time settings button.</p> <p>Note</p> <p>Dashboard data updates once an hour.</p> <p></p> <p>You can change the refresh interval using the refresh interval drop down.</p> <p>The dashboard has a 2 consumption tables that display the total consumption of resources. Hover over an entry in the table to filter it in or out of the table.</p> <p>The Total consumption table includes consumption details based on the filters selected. Fields include:</p> <ul> <li>Project</li> <li>Department</li> <li>GPU hours</li> <li>CPU hours</li> <li>Memory hours</li> <li>GPU Idle allocated hours\u2014the portion of time the GPUs spend idle from the total allocation hours.</li> <li>CPU usage hours\u2014the actual usage time of CPU.</li> <li>Memory usage time\u2014the actual usage time of CPU memory.</li> <li>GPU cost (only when configured)</li> <li>CPU cost (only when configured)</li> <li>CPU memory (only when configured)</li> </ul> <p>The Total department consumption table includes consumption details for each department, or details for departments selected in the filters. Fields include:</p> <ul> <li>Department</li> <li>GPU hours</li> <li>CPU hours</li> <li>Memory hours</li> <li>GPU Idle allocated hours\u2014the portion of time the GPUs spend idle from the total allocation hours.</li> <li>CPU usage hours\u2014the actual usage time of CPU.</li> <li>Memory usage time\u2014the actual usage time of CPU memory.</li> <li>GPU cost (only when configured)</li> <li>CPU cost (only when configured)</li> <li>CPU memory (only when configured)</li> </ul> <p>The dashboard has a graph of the GPU allocation over time.</p> <p>!</p> <p>The dashboard has a graph of the Project over-quota GPU consumption.</p> <p>!</p>"},{"location":"platform-admin/performance/dashboard-analysis/#quota-management-dashboard","title":"Quota management dashboard","text":"<p>The Quota management dashboard provides an efficient means to monitor and manage resource utilization within the AI cluster. The dashboard is divided into sections with essential metrics and data visualizations to identify resource usage patterns, potential bottlenecks, and areas for optimization. The sections of the dashboard include:</p> <ul> <li>Add Filter</li> <li>Quota / Total</li> <li>Allocated / Quota</li> <li>Pending workloads</li> <li>Quota by node pool</li> <li>Allocation by node pool</li> <li>Pending workloads by node pool</li> <li>Departments with lowest allocation by node pool</li> <li>Projects with lowest allocation ratio by node pool</li> <li>Over time allocation / quota</li> </ul>"},{"location":"platform-admin/performance/dashboard-analysis/#add-filter","title":"Add Filter","text":"<p>Use the Add Filter dropdown to select filters for the dashboard. The filters will change the data shown on the dashboard. Available filters are:</p> <ul> <li>Departments</li> <li>Projects</li> <li>Nodes</li> </ul> <p>Select a filter from the dropdown, then select a item from the list, and press apply.</p> <p>Note</p> <p>You can create a filter with multiple categories, but you can use each category and item only once.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#quota-total","title":"Quota / Total","text":"<p>This section shows the number of GPUs that are in the quota based on the filter selection. The quota of GPUs is the number of GPUs that are reserved for use.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#allocated-quota","title":"Allocated / Quota","text":"<p>This section shows the number of GPUs that are allocated based on the filter selection. Allocated GPUs are the number of GPUs that are being used.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#pending-workloads","title":"Pending workloads","text":"<p>This section shows the number workloads that are pending based on the filter selection. Pending workloads are workloads that have not started.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#quota-by-node-pool","title":"Quota by node pool","text":"<p>This section shows the quota of GPUs by node pool based on the filter. The quota is the number of GPUs that are reserved for use. You can drill down into the data in this section by pressing on the graph or the link at the bottom of the section.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#allocation-by-node-pool","title":"Allocation by node pool","text":"<p>This section shows the allocation of GPUs by node pool based on the filter. The allocation is the number of GPUs that are being used. You can drill down into the data in this section by pressing on the graph or the link at the bottom of the section.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#pending-workloads-by-node-pool","title":"Pending workloads by node pool","text":"<p>This section shows the number of pending workloads by node pool. You can drill down into the data in this section by pressing on the graph or the link at the bottom of the section.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#departments-with-lowest-allocation-by-node-pool","title":"Departments with lowest allocation by node pool","text":"<p>This section shows the departments with the lowest allocation of GPUs by percentage relative to the total number of GPUs.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#projects-with-lowest-allocation-ratio-by-node-pool","title":"Projects with lowest allocation ratio by node pool","text":"<p>This section shows the projects with the lowest allocation of GPUS by percentage relative to the total number of GPUs.</p>"},{"location":"platform-admin/performance/dashboard-analysis/#over-time-allocation-quota","title":"Over time allocation / quota","text":"<p>This section shows the allocation of GPUs from the quota over a period of time.</p>"},{"location":"platform-admin/workloads/workload-overview/","title":"Workloads Overview","text":"<p>Runai is an open platform and supports three types of workloads each with a different set of features:</p> <ul> <li>Run:ai native workloads.</li> <li>Third party integrations.</li> <li>Typical Kubernetes workloads.</li> </ul>"},{"location":"platform-admin/workloads/workload-overview/#runai-native-workloads","title":"Run:ai native workloads","text":"<p>Run:ai native workloads are workloads (trainings, workspaces, deployments) that are fully controlled by Run:ai. Run: workloads are the most comprehensive and include Third party integrations and Typical Kubernetes workload types. Specific characteristics of Run: ai native workloads include:</p> <ol> <li>Submitting of workloads via UI/CLI.</li> <li>Workload control (delete/stop/connect).</li> <li>Workload policies (default rules for all policies, specific workload policies, and enforcing of those rules).</li> <li>Scheduling rules.</li> <li>Role based access control.</li> </ol>"},{"location":"platform-admin/workloads/workload-overview/#third-party-integrations","title":"Third party integrations","text":"<p>Third party integrations are tools that Run:ai supports and manages. These are tools that are typically used to build workloads for specific purposes. Third party integrations also include Typical Kubernetes workloads. Specific characteristics of third party tool support include:</p> <ol> <li>Smart gang scheduling (workload aware).</li> <li>Specific workload aware visibility so that different kinds of pods are identified as a single workload (for example, GPU Utilization, workload view, dashboards).</li> </ol> <p>For more information, see Supported integrations.</p>"},{"location":"platform-admin/workloads/workload-overview/#typical-kubernetes-workloads","title":"Typical Kubernetes workloads","text":"<p>Typical Kubernetes workloads are any kind of workload built for Kubernetes. The Run:ai platform allows you to submit standard Kubernetes CRDs. Specific characteristics of Typical Kubernetes workloads that Run:ai can manage include:</p> <ol> <li>Fairness</li> <li>Nodepools</li> <li>Bin packing/spread</li> <li>Fractions</li> <li>Overprovisioning</li> </ol>"},{"location":"platform-admin/workloads/workload-overview/#workloads-view","title":"Workloads View","text":"<p>Run:ai makes it easy to run machine learning workloads effectively on Kubernetes. Run:ai provides both a UI and API interface that introduces a simple and more efficient way to manage machine learning workloads, which will appeal to data scientists and engineers alike.</p> <p>The Workloads table provides:</p> <ul> <li>Changing of the layout of the Workloads table by pressing Columns to add or remove columns from the table.</li> <li>Download the table to a CSV file by pressing More, then pressing Download as CSV.</li> <li>Search for a workload by pressing Search and entering the name of the workload.</li> <li>Advanced workload management.</li> <li>Added workload statuses for better tracking of workload flow.</li> </ul> <p>To create new workloads, press New Workload.</p>"},{"location":"platform-admin/workloads/workload-overview/#api-documentation","title":"API Documentation","text":"<p>Access the platform API documentation for more information on using the API to manage workloads.</p>"},{"location":"platform-admin/workloads/workload-overview/#managing-workloads","title":"Managing Workloads","text":"<p>You can manage a workload by selecting one from the view. Once selected, you can:</p> <ul> <li>Run a workload.</li> <li>Stop a workload.</li> <li>Connect to a workload\u2014provides a connection to the selected workload's designated tool. Press the item in the column to show the connection URL.</li> <li>Delete a workload.</li> <li> <p>Copy and edit a workload\u2014use this function to run another workload based on the selected workload.</p> <ul> <li>If the workload was submitted using the UI, then a copy of the original workload form will open allowing you to make changes to the workload properties.</li> <li>If the workload was submitted using the CLI, then a window shows with the original CLI command. Copy the command and make changes to the submission.</li> </ul> </li> <li> <p>Show details\u2014provides in-depth information about the selected workload including:</p> <ul> <li>Event history\u2014workload status over time. Use the filter to search through the history for specific events.</li> <li> <p>Metrics\u2014use the drop down to filter metrics per pod and over time. Select a category from the list below:</p> <ul> <li>Throughput\u2014total of requests per second across all* replica at any given time</li> <li>Latency\u2014average of time it took to answer any request across all replicas at any given time</li> <li>Number of replicas\u2014Total number of all replicas at any given time</li> <li>GPU compute utilization\u2014hover over for individual GPU details</li> <li>GPU memory usage\u2014hover over for individual GPU details</li> <li>CPU usage\u2014hover over for usage details</li> <li>CPU memory usage\u2014hover over for usage details</li> </ul> </li> <li> <p>Logs\u2014logs of the selected workload. Use the drop down to filter metrics per pod. Use the Download button to download the logs.</p> </li> </ul> </li> </ul>"},{"location":"platform-admin/workloads/workload-overview/#workloads-status","title":"Workloads Status","text":"<p>The Status column shows the current status of the workload. The following table describes the statuses presented:</p> Phase Name Description Entry Condition Exit Condition Creating Workload setup is initiated in the cluster. Resources and pods are now provisioning. A workload is submitted. A multi-pod group is created. Pending Workload is queued and awaiting resource allocation. A pod group exists. All pods are scheduled. Initializing Workload is retrieving images, starting containers, and preparing pods. All pods are scheduled\u2014handling of multi-pod groups TBD. All pods are initialized or a failure to initialize is detected. Running Workload is currently in progress with all pods operational. All pods initialized (all containers in pods are ready). Job completion or failure. Degraded Pods may not align with specifications, network services might be incomplete, or persistent volumes may be detached. Check your logs for specific details. Pending\u2014All pods are running but with issues.  Running\u2014All pods are running with no issues. Running\u2014All resources are OK. Completed\u2014 Job finished with fewer resources.Failed\u2014Job failure or user-defined rules. Deleting Workload and its associated resources are being decommissioned from the cluster. Deleting of the workload. Resources are fully deleted. Stopped Workload is on hold and resources are intact but inactive. Stopping the workload without deleting resources. Transitioning back to the initializing phase or proceeded to deleting the workload. Failed Image retrieval failed or containers experienced a crash. Check your logs for specific details. An error occurs preventing the successful completion of the job. Terminal state. Completed Workload has successfully finished its execution. The job has finished processing without errors. Terminal state."},{"location":"platform-admin/workloads/workload-overview/#successful-flow","title":"Successful flow","text":"<p>A successful flow will follow the following flow chart:</p> <pre><code>flowchart LR\n A(Creating) --&gt; B(Pending)\n B--&gt;C(Initializing)\n C--&gt;D(Running)\n D--&gt;E(Completed)</code></pre> <p>To get the full experience of Run:ai\u2019s environment and platform use the following types of workloads.</p> <ul> <li>Workspaces</li> <li>Trainings (Only available when using the Jobs view)</li> <li>Distributed training</li> <li>Deployments.</li> </ul>"},{"location":"platform-admin/workloads/workload-overview/#workload-related-integrations","title":"Workload-related Integrations","text":"<p>See Integrations.</p>"},{"location":"platform-admin/workloads/assets/compute/","title":"Compute Resources","text":"<p>This article explains what compute resources are and how to create and use them.</p> <p>Compute resources are one type of workload asset. A compute resource is a template that simplifies how workloads are submitted and can be used by AI practitioners when they submit their workloads.</p> <p>A compute resource asset is a preconfigured building block that encapsulates all the specifications of compute requirements for the workload including:</p> <ul> <li>GPU devices and GPU memory  </li> <li>CPU memory and CPU compute</li> </ul>"},{"location":"platform-admin/workloads/assets/compute/#compute-resource-table","title":"Compute resource table","text":"<p>The Compute resource table can be found under Compute resources in the Run:ai UI.</p> <p>The Compute resource table provides a list of all the compute resources defined in the platform and allows you to manage them.</p> <p></p> <p>The Compute resource table consists of the following columns:</p> Column Description Compute resource The name of the compute resource Description A description of the essence of the compute resource GPU devices request per pod The number of requested physical devices per pod of the workload that uses this compute resource GPU memory request per device The amount of GPU memory per requested device that is granted to each pod of the workload that uses this compute resource CPU memory request The minimum amount of CPU memory per pod of the workload that uses this compute resource CPU memory limit The maximum amount of CPU memory per pod of the workload that uses this compute resource CPU compute request The minimum number of CPU cores per pod of the workload that uses this compute resource CPU compute limit The maximum number of CPU cores per pod of the workload that uses this compute resource Scope The scope of this compute resource within the organizational tree. Click the name of the scope to view the organizational tree diagram Workload(s) The list of workloads associated with the compute resource Template(s) The list of workload templates that use this compute resource Created by The name of the user who created the compute resource Creation time The timestamp for when the rule was created Cluster The cluster that the compute resource is associated with"},{"location":"platform-admin/workloads/assets/compute/#workloads-associated-with-the-compute-resource","title":"Workloads associated with the compute resource","text":"<p>Click one of the values in the Workload(s) column to view the list of workloads and their parameters.</p> Column Description Workload The workload that uses the compute resource Type (Workspace/Training/Inference) Status Represents the workload lifecycle. see the full list of workload status"},{"location":"platform-admin/workloads/assets/compute/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table</li> </ul>"},{"location":"platform-admin/workloads/assets/compute/#adding-new-compute-resource","title":"Adding new compute resource","text":"<p>To add a new compute resource:</p> <ol> <li>Go to the Compute resource table  </li> <li>Click +NEW COMPUTE RESOURCE </li> <li>Select under which cluster to create the compute resource  </li> <li>Select a scope </li> <li>Enter a name for the compute resource. The name must be unique.  </li> <li>Optional: Provide a description of the essence of the compute resource  </li> <li> <p>Set the resource types needed within a single node    (The Run:ai scheduler tries to match a single node that complies with the compute resource for each of the workload\u2019s pods)  </p> <ul> <li> <p>GPU </p> <ul> <li>GPU devices per pod The number of devices (physical GPUs) per pod    (for example, if you requested 3 devices per pod and the running workload using this compute resource consists of 3 pods, there are 9 physical GPU devices used in total)  </li> </ul> <p>Note</p> <ul> <li>You can insert a whole number of devices (0; 1; 2; 3; \u2026)  </li> <li>When setting it to zero, the workload using this computer resource neither requests or uses GPU resources while running  </li> <li>Only when setting it to 1, a fraction of a GPU memory can be requested  </li> <li>When setting a number higher than 1, the entire GPU memory of the devices is used by the running workloads  </li> </ul> <ul> <li>GPU memory per device <ul> <li>Select the memory request format  <ul> <li>% (of device) - Fraction of a GPU device\u2019s memory  </li> <li>MB (memory size) - An explicit GPU memory unit  </li> <li>GB (memory size) - An explicit GPU memory unit  </li> <li>Multi-instance GPU (MIG) - MIG profile (Deprecated)  </li> </ul> </li> <li>Set the memory Request - The minimum amount of GPU memory that is provisioned per device. This means that any pod of a running workload that uses this compute resource, receives this amount of GPU memory for each device(s) the pod utilizes  </li> <li>Optional: Set the memory Limit - The maximum amount of GPU memory that is provisioned per device. This means that any pod of a running workload that uses this compute resource, receives at most this amount of GPU memory for each device(s) the pod utilizes. To set a Limit, first enable the limit toggle. The limit value must be equal to or higher than the request.  </li> </ul> </li> </ul> <p>Note</p> <ul> <li>GPU memory limit is disabled by default. If you cannot see the Limit toggle in the compute resource form, then it must be enabled by your Administrator, under General Settings \u2192 Resources \u2192 GPU resource optimization  </li> <li>When a Limit is set and is bigger than the Request, the scheduler allows each pod to reach the maximum amount of GPU memory in an opportunistic manner (only upon availability).  </li> <li>If the GPU Memory Limit is bigger that the Request the pod is prone to be killed by the Run:ai toolkit (out of memory signal). The greater the difference between the GPU memory used and the request, the higher the risk of being killed  </li> <li>If GPU resource optimization is turned off, the minimum and maximum are in fact equal  </li> </ul> </li> <li> <p>CPU </p> <ul> <li>CPU compute per pod <ul> <li>Select the units for the CPU compute (Cores / Millicores)  </li> <li>Set the CPU compute Request - the minimum amount of CPU compute that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives this amount of CPU compute for each pod.  </li> <li>Optional: Set the CPU compute Limit - The maximum amount of CPU compute that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives at most this amount of CPU compute. To set a Limit, first enable the limit toggle. The limit value must be equal to or higher than the request. By default, the limit is set to \u201cUnlimited\u201d - which means that the pod may consume all the node's free CPU compute resources.  </li> </ul> </li> <li>CPU memory per pod <ul> <li>Select the units for the CPU memory (MB / GB)  </li> <li>Set the CPU memory Request - The minimum amount of CPU memory that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives this amount of CPU memory for each pod.  </li> <li>Optional: Set the CPU memory Limit - The maximum amount of CPU memory that is provisioned per pod. This means that any pod of a running workload that uses this compute resource, receives at most this amount of CPU memory. To set a Limit, first enable the limit toggle. The limit value must be equal to or higher than the request. By default, the limit is set to \u201cUnlimited\u201d - Meaning that the pod may consume all the node's free CPU memory resources.  </li> </ul> </li> </ul> <p>Note</p> <p>If the CPU Memory Limit is bigger that the Request the pod is prone to be killed by the operating system (out of memory signal). The greater the difference between the CPU memory used and the request, the higher the risk of being killed.  </p> </li> </ul> </li> <li> <p>Optional: More settings  </p> <ul> <li>Increase shared memory size When enabled, the shared memory size available to the pod is increased from the default 64MB to the node's total available memory or the CPU memory limit, if set above.  </li> <li>Set extended resource(s) Click +EXTENDED RESOURCES to add resource/quantity pairs. For more information on how to set extended resources, see the Extended resources and Quantity guides  </li> </ul> </li> <li> <p>Click CREATE COMPUTE RESOURCE</p> <p>Note</p> <p>It is also possible to add compute resources directly when creating a specific Workspace, training or inference workload.</p> </li> </ol>"},{"location":"platform-admin/workloads/assets/compute/#editing-a-compute-resource","title":"Editing a compute resource","text":"<p>To edit a compute resource:</p> <ol> <li>Select the compute resource from the table  </li> <li>Click RENAME to edit its name and description</li> </ol> <p>Note</p> <p>Additional fields can be edited using the API.</p>"},{"location":"platform-admin/workloads/assets/compute/#copying-editing-a-compute-resource","title":"Copying &amp; editing a compute resource","text":"<p>To copy &amp; edit a compute resource:</p> <ol> <li>Select the compute resource you want to duplicate  </li> <li>Click COPY &amp; EDIT </li> <li>Update the compute resource and click CREATE COMPUTE RESOURCE</li> </ol>"},{"location":"platform-admin/workloads/assets/compute/#deleting-a-compute-resource","title":"Deleting a compute resource","text":"<ol> <li>Select the compute resource you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion  </li> </ol> <p>Note</p> <p>It is not possible to delete a compute resource being used by an existing workload and template.</p>"},{"location":"platform-admin/workloads/assets/compute/#using-api","title":"Using API","text":"<p>Go to the Compute resources API reference to view the available actions</p>"},{"location":"platform-admin/workloads/assets/credentials/","title":"Credentials","text":"<p>This article explains what credentials are and how to create and use them.</p> <p>Credentials are a workload asset that simplify the complexities of Kubernetes secrets. They consist of and mask sensitive access information, such as passwords, tokens, and access keys, which are necessary for gaining access to various resources.</p> <p>Credentials are crucial for the security of AI workloads and the resources they require, as they restrict access to authorized users, verify identities, and ensure secure interactions. By enforcing the protection of sensitive data, credentials help organizations comply with industry regulations, fostering a secure environment overall.</p> <p>Essentially, credentials enable AI practitioners to access relevant protected resources, such as private data sources and Docker images, thereby streamlining the workload submission process.</p>"},{"location":"platform-admin/workloads/assets/credentials/#credentials-table","title":"Credentials table","text":"<p>The Credentials table can be found under Credentials in the Run:ai User interface.</p> <p>The Credentials table provides a list of all the credentials defined in the platform and allows you to manage them.</p> <p></p> <p>The Credentials table comprises the following columns:</p> Column Description Credentials The name of the credentials Description A description of the credentials Type The type of credentials, e.g., Docker registry Status The different lifecycle phases and representation of the credentials\u2019 condition Data source(s) The private data source(s) that are accessed using the credentials Created by The user who created the credentials Scope The scope of this compute resource within the organizational treeClick the name of the scope to view the organizational tree diagram Creation time The timestamp of when the credentials were created Cluster The cluster with which the credentials are associated"},{"location":"platform-admin/workloads/assets/credentials/#credentials-status","title":"Credentials status","text":"<p>The following table describes the credentials\u2019 condition and whether they were created successfully for the selected scope.</p> Status Description No issues found No issues were found while creating the credentials (this status may change while propagating the credentials to the selected scope) Issues found Issues found while propagating the credentials Issues found Failed to access the cluster Creating\u2026 Credentials are being created Deleting\u2026 Credentials are being deleted No status When the credentials\u2019 scope is an account, or the current version of the cluster is not up to date, the status cannot be displayed"},{"location":"platform-admin/workloads/assets/credentials/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values</li> <li>Search - Click SEARCH and type the value to search by</li> <li>Sort - Click each column header to sort by</li> <li>Column selection - Click COLUMNS and select the columns to display in the table</li> <li>Refresh - Click REFRESH to update the table with the latest data</li> </ul>"},{"location":"platform-admin/workloads/assets/credentials/#adding-new-credentials","title":"Adding new credentials","text":"<p>Creating credentials is limited to specific roles.</p> <p>To add a new credential:</p> <ol> <li>Go to the Credentials table:</li> <li>Click +NEW CREDENTIALS</li> <li>Select the credential type from the list     Follow the step-by-step guide for each credential type:</li> </ol>"},{"location":"platform-admin/workloads/assets/credentials/#docker-registry","title":"Docker registry","text":"<p>These credentials allow users to authenticate and pull images from a Docker registry, enabling access to containerized applications and services.</p> <p>After creating the credentials, it is used automatically when pulling images.</p> <ol> <li>Select a scope.</li> <li>Enter a name for the credential. The name must be unique.</li> <li>Optional: Provide a description of the credentials</li> <li>Set how the credential is created<ul> <li>Existing secret (in the cluster)     This option applies when the purpose is to create credentials based on an existing secret<ul> <li>Select a secret from the list (The list is empty if no secrets were created in advance)</li> </ul> </li> <li>New secret (recommended)     A new secret is created together with the credentials. New secrets are not added to the list of existing secrets.<ul> <li>Enter the username, password, and Docker registry URL</li> </ul> </li> </ul> </li> <li>Click CREATE CREDENTIALS</li> </ol> <p>After the credentials are created, check their status to monitor their proper creation across the selected scope.</p>"},{"location":"platform-admin/workloads/assets/credentials/#access-key","title":"Access key","text":"<p>These credentials are unique identifiers used to authenticate and authorize access to cloud services or APIs, ensuring secure communication between applications. They typically consist of two parts:</p> <ul> <li>An access key ID</li> <li>A secret access key</li> </ul> <p>The purpose of this credential type is to allow access to restricted data.</p> <ol> <li>Select a scope.</li> <li>Enter a name for the credential. The name must be unique.</li> <li>Optional: Provide a description of the credential</li> <li>Set how the credential is created<ul> <li>Existing secret (in the cluster)     This option applies when the purpose is to create credentials based on an existing secret<ul> <li>Select a secret from the list (The list is empty if no secrets were created in advance)</li> </ul> </li> <li>New secret (recommended)     A new secret is created together with the credentials. New secrets are not added to the list of existing secrets.<ul> <li>Enter the Access key and Access secret</li> </ul> </li> </ul> </li> <li>Click CREATE CREDENTIALS</li> </ol> <p>After the credentials are created, check their status to monitor their proper creation across the selected scope.</p>"},{"location":"platform-admin/workloads/assets/credentials/#username-password","title":"Username &amp; password","text":"<p>These credentials require a username and corresponding password to access various resources, ensuring that only authorized users can log in.</p> <p>The purpose of this credential type is to allow access to restricted data.</p> <ol> <li>Select a scope</li> <li>Enter a name for the credential. The name must be unique.</li> <li>Optional: Provide a description of the credentials</li> <li>Set how the credential is created<ul> <li>Existing secret (in the cluster)     This option applies when the purpose is to create credentials based on an existing secret<ul> <li>Select a secret from the list (The list is empty if no secrets were created in advance)</li> </ul> </li> <li>New secret (recommended)     A new secret is created together with the credentials. New secrets are not added to the list of existing secrets.<ul> <li>Enter the username and password</li> </ul> </li> </ul> </li> <li>Click CREATE CREDENTIALS</li> </ol> <p>After the credentials are created, check their status to monitor their proper creation across the selected scope.</p>"},{"location":"platform-admin/workloads/assets/credentials/#generic-secret","title":"Generic secret","text":"<p>These credentials are a flexible option that consists of multiple keys &amp; values and can store various sensitive information, such as API keys or configuration data, to be used securely within applications.</p> <p>The purpose of this credential type is to allow access to restricted data.</p> <ol> <li>Select a scope</li> <li>Enter a name for the credential. The name must be unique.</li> <li>Optional: Provide a description of the credentials</li> <li>Set how the credential is created<ul> <li>Existing secret (in the cluster)     This option applies when the purpose is to create credentials based on an existing secret<ul> <li>Select a secret from the list (The list is empty if no secrets were created in advance)</li> </ul> </li> <li>New secret (recommended)     A new secret is created together with the credentials. New secrets are not added to the list of existing secrets.<ul> <li>Click +KEY &amp; VALUE - to add key/value pairs to store in the new secret</li> </ul> </li> </ul> </li> <li>Click CREATE CREDENTIALS</li> </ol>"},{"location":"platform-admin/workloads/assets/credentials/#editing-credentials","title":"Editing credentials","text":"<p>To rename a credential:</p> <ol> <li>Select the credential from the table</li> <li>Click Rename to edit its name and description</li> </ol>"},{"location":"platform-admin/workloads/assets/credentials/#deleting-credentials","title":"Deleting credentials","text":"<p>To delete a credential:</p> <ol> <li>Select the credential you want to delete</li> <li>Click DELETE</li> <li>In the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>Credentials cannot be deleted if they are being used by a workload and template.</p>"},{"location":"platform-admin/workloads/assets/credentials/#using-credentials","title":"Using credentials","text":"<p>You can use credentials (secrets) in various ways within the system</p>"},{"location":"platform-admin/workloads/assets/credentials/#access-private-data-sources","title":"Access private data sources","text":"<p>To access private data sources, attach credentials to data sources of the following types: Git, S3 Bucket</p>"},{"location":"platform-admin/workloads/assets/credentials/#use-directly-within-the-container","title":"Use directly within the container","text":"<p>To use the secret directly from within the container, you can choose between the following options</p> <ol> <li>Get the secret mounted to the file system by using the Generic secret data source</li> <li>Get the secret as an environment variable injected into the container. There are two equivalent ways to inject the environment variable.       a. By adding it to the Environment asset.        b. By adding it ad-hoc as part of the workload.</li> </ol>"},{"location":"platform-admin/workloads/assets/credentials/#creating-secrets-in-advance","title":"Creating secrets in advance","text":"<p>Add secrets in advance to be used when creating credentials via the Run:ai UI.</p> <p>Follow the steps below for each required scope:</p> Cluster scopeDepartment scopeProject scope <ol> <li>Create the secret in the Run:ai namespace (runai)</li> <li>To authorize Run:ai to use the secret, label it: <code>run.ai/cluster-wide: \"true\"</code></li> <li>Label the secret with the correct credential type:<ol> <li>Docker registry - <code>run.ai/resource: \"docker-registry\"</code></li> <li>Access key - <code>run.ai/resource: \"access-key\"</code></li> <li>Username and password - <code>run.ai/resource: \"password\"</code></li> <li>Generic secret - <code>run.ai/resource: \"generic\"</code> \u05bf</li> </ol> </li> </ol> <ol> <li>Create the secret in the Run:ai namespace (runai)</li> <li>To authorize Run:ai to use the secret, label it: <code>run.ai/department: \"&lt;department id&gt;\"</code></li> <li>Label the secret with the correct credential type:<ol> <li>Docker registry - <code>run.ai/resource: \"docker-registry\"</code></li> <li>Access key - <code>run.ai/resource: \"access-key\"</code></li> <li>Username and password - <code>run.ai/resource: \"password\"</code></li> <li>Generic secret - <code>run.ai/resource: \"generic\"</code></li> </ol> </li> </ol> <ol> <li>Create the secret in the project\u2019s namespace</li> <li>Label the secret with the correct credential type:<ol> <li>Docker registry - <code>run.ai/resource: \"docker-registry\"</code></li> <li>Access key - <code>run.ai/resource: \"access-key\"</code></li> <li>Username and password - <code>run.ai/resource: \"password\"</code></li> <li>Generic secret - <code>run.ai/resource: \"generic\"</code></li> </ol> </li> </ol> <p>The secret is now displayed for that scope in the list of existing secrets.</p>"},{"location":"platform-admin/workloads/assets/credentials/#using-api","title":"Using API","text":"<p>To view the available actions, go to the Credentials API reference</p>"},{"location":"platform-admin/workloads/assets/data-volumes/","title":"Data Volumes","text":"<p>Data volumes offer a powerful solution for storing, managing, and sharing AI training data within the Run:ai platform. They promote collaboration, simplify data access control, and streamline the AI development lifecycle.</p> <p>Data volumes are snapshots of datasets stored in Kubernetes Persistent Volume Claims (PVCs). They act as a central repository for training data.</p>"},{"location":"platform-admin/workloads/assets/data-volumes/#why-use-a-data-volume","title":"Why use a data volume?","text":"<ol> <li>Sharing with multiple scopes     Unlike other Run:ai data sources, data volumes can be shared across projects, departments, or clusters, encouraging data reuse and collaboration within the organization.</li> <li>Storage saving     A single copy of the data can be used across multiple scopes</li> </ol>"},{"location":"platform-admin/workloads/assets/data-volumes/#typical-use-cases","title":"Typical use cases","text":"<ol> <li>Sharing large data sets     In large organizations, the data is often stored in a remote location, which can be a barrier for large model training. Even if the data is transferred into the cluster, sharing it easily with multiple users is still challenging. Data volumes can help share the data seamlessly, with maximum security and control.</li> <li>Sharing data with colleagues     When sharing training results, generated data sets, or other artifacts with team members is needed, data volumes can help make the data available easily.</li> </ol>"},{"location":"platform-admin/workloads/assets/data-volumes/#prerequisites","title":"Prerequisites","text":"<p>To create a data volume, there must be a project with a PVC in its namespace.</p> <p>Working with data volumes is currently available using the API. To view the available actions, go to the Data volumes API reference.</p>"},{"location":"platform-admin/workloads/assets/data-volumes/#adding-a-new-data-volume","title":"Adding a new data volume","text":"<p>Data volume creation is limited to specific roles</p>"},{"location":"platform-admin/workloads/assets/data-volumes/#adding-scopes-for-a-data-volume","title":"Adding scopes for a data volume","text":"<p>Data volume sharing (adding scopes) is limited to specific roles</p> <p>Once created, the data volume is available to its originating project (see the prerequisites above).</p> <p>Data volumes can be shared with additional scopes in the organization.</p>"},{"location":"platform-admin/workloads/assets/data-volumes/#who-can-use-a-data-volume","title":"Who can use a data volume?","text":"<p>Data volumes are used when submitting workloads. Any user, application or SSO group with a role that has permissions to create workloads can also use data volumes.</p> <p>Researchers can list available data volumes within their permitted scopes for easy selection.</p>"},{"location":"platform-admin/workloads/assets/datasources/","title":"Data Sources","text":"<p>This article explains what data sources are and how to create and use them.</p> <p>Data sources are a type of workload asset and represent a location where data is actually stored. They may represent a remote data location, such as NFS, Git, or S3, or a Kubernetes local resource, such as PVC, ConfigMap, HostPath, or Secret.</p> <p>This configuration simplifies the mapping of the data into the workload\u2019s file system and handles the mounting process during workload creation for reading and writing. These data sources are reusable and can be easily integrated and used by AI practitioners while submitting workloads across various scopes.</p>"},{"location":"platform-admin/workloads/assets/datasources/#data-sources-table","title":"Data sources table","text":"<p>The data sources table can be found under Data sources in the Run:ai platform.</p> <p>The data sources table provides a list of all the data sources defined in the platform and allows you to manage them.</p> <p></p> <p>The data sources table comprises the following columns:</p> Column Description Data source The name of the data source Description A description of the data source Type The type of data source connected \u2013 e.g., S3 bucket, PVC, or others Status The different lifecycle phases and representation of the data source condition Scope The scope of the data source within the organizational tree. Click the scope name to view the organizational tree diagram Workload(s) The list of existing workloads that use the data source Template(s) The list of workload templates that use the data source Created by The user who created the data source Creation time The timestamp for when the data source was created Cluster The cluster that the data source is associated with"},{"location":"platform-admin/workloads/assets/datasources/#data-sources-status","title":"Data sources status","text":"<p>The following table describes the data sources' condition and whether they were created successfully for the selected scope.</p> Status Description No issues found No issues were found while creating the data source Issues found Issues were found while propagating the data source credentials Issues found The cluster could not be accessed Creating\u2026 The data source is being created No status / \u201c-\u201d When the data source\u2019s scope is an account, the current version of the cluster is not up to date, or the asset is not a cluster-syncing entity, the status can\u2019t be displayed"},{"location":"platform-admin/workloads/assets/datasources/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values</li> <li>Search - Click SEARCH and type the value to search by</li> <li>Sort - Click each column header to sort by</li> <li>Column selection - Click COLUMNS and select the columns to display in the table</li> <li>Download table - Click MORE and then click \u2018Download as CSV\u2019</li> <li>Refresh - Click REFRESH to update the table with the latest data</li> </ul>"},{"location":"platform-admin/workloads/assets/datasources/#adding-a-new-data-source","title":"Adding a new data source","text":"<p>To create a new data source:</p> <ol> <li>Click +NEW DATA SOURCE</li> <li>Select the data source type from the list. Follow the step-by-step guide for each data source type:</li> </ol>"},{"location":"platform-admin/workloads/assets/datasources/#nfs","title":"NFS","text":"<p>A Network File System (NFS) is a Kubernetes concept used for sharing storage in the cluster among different pods. Like a PVC, the NFS volume\u2019s content remains preserved, even outside the lifecycle of a single pod. However, unlike PVCs, which abstract storage management, NFS provides a method for network-based file sharing. The NFS volume can be pre-populated with data and can be mounted by multiple pod writers simultaneously. At Run:ai, an NFS-type data source is an abstraction that is mapped directly to a Kubernetes NFS volume. This integration allows multiple workloads under various scopes to mount and present the NFS data source.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Set the data origin<ul> <li>Enter the NFS server (host name or host IP)</li> <li>Enter the NFS path</li> </ul> </li> <li>Set the data target location<ul> <li>Container path</li> </ul> </li> <li>Optional: Restrictions<ul> <li>Prevent data modification - When enabled, the data will be mounted with read-only permissions</li> </ul> </li> <li>Click CREATE DATA SOURCE</li> </ol>"},{"location":"platform-admin/workloads/assets/datasources/#pvc","title":"PVC","text":"<p>A Persistent Volume Claim (PVC) is a Kubernetes concept used for managing storage in the cluster, which can be provisioned by an administrator or dynamically by Kubernetes using a StorageClass. PVCs allow users to request specific sizes and access modes (read/write once, read-only many). At Run:ai, a PVC-type data source is an abstraction that is mapped directly to a Kubernetes PVC. By leveraging PVCs as data sources, Run:ai enables access to persistent storage for workloads, ensuring that data remains consistent and accessible across various scopes and workloads, beyond the lifecycle of individual pods. This ensures that data generated by AI workloads is not lost when pods are rescheduled or updated, providing a seamless and efficient storage solution that can handle the large datasets typically associated with AI projects.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Select PVC:<ul> <li>Existing PVC     This option is relevant when the purpose is to create a PVC-type data source based on an existing PVC in the cluster<ul> <li>Select a PVC from the list - (The list is empty if no existing PVCs were created in advance)</li> </ul> </li> <li>New PVC - creates a new PVC in the cluster. New PVCs are not added to the Existing PVCs list.     When creating a PVC-type data source and selecting the \u2018New PVC\u2019 option, the PVC is immediately created in the cluster (even if no workload has requested this PVC).</li> </ul> </li> <li>Select the storage class<ul> <li>None - Proceed without defining a storage class</li> <li>Custom storage class - This option applies when selecting a storage class based on existing storage classes.     To add new storage classes to the storage class list, and for additional information, check Kubernetes storage classes</li> </ul> </li> <li>Select the access mode(s) (multiple modes can be selected)<ul> <li>Read-write by one node - The volume can be mounted as read-write by a single node.</li> <li>Read-only by many nodes - The volume can be mounted as read-only by many nodes.</li> <li>Read-write by many nodes - The volume can be mounted as read-write by many nodes.</li> </ul> </li> <li>Set the claim size and its units</li> <li>Select the volume mode</li> <li>Set the data target location<ul> <li>container path</li> </ul> </li> <li>Optional: Prevent data modification - When enabled, the data will be mounted with read-only permission.</li> <li>Click CREATE DATA SOURCE</li> </ol> <p>After the data source is created, check its status to monitor its proper creation across the selected scope.</p>"},{"location":"platform-admin/workloads/assets/datasources/#s3-bucket","title":"S3 Bucket","text":"<p>The S3 bucket data source enables the mapping of a remote S3 bucket into the workload\u2019s file system. Similar to a PVC, this mapping remains accessible across different workload executions, extending beyond the lifecycle of individual pods. However, unlike PVCs, data stored in an S3 bucket resides remotely, which may lead to decreased performance during the execution of heavy machine learning workloads. As part of the Run:ai connection to the S3 bucket, you can create credentials in order to access and map private buckets.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Set the data origin<ul> <li>Set the S3 service URL</li> <li>Select the credentials<ul> <li>None - for public buckets</li> <li>Credential names - This option is relevant for private buckets based on existing credentials that were created for the scope.     To add new credentials to the credentials list, and for additional information, check the Credentials article.</li> </ul> </li> <li>Enter the bucket name</li> </ul> </li> <li>Set the data target location<ul> <li>container path</li> </ul> </li> <li>Click CREATE DATA SOURCE</li> </ol> <p>After a private data source is created, check its status to monitor its proper creation across the selected scope.</p>"},{"location":"platform-admin/workloads/assets/datasources/#git","title":"Git","text":"<p>A Git-type data source is a Run:ai integration, that enables code to be copied from a Git branch into a dedicated folder in the container. It is mainly used to provide the workload with the latest code repository. As part of the integration with Git, in order to access private repositories, you can add predefined credentials to the data source mapping.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Set the data origin<ul> <li>Set the Repository URL</li> <li>Set the Revision (branch, tag, or hash)- If left empty, it will use the 'HEAD' (latest)</li> <li>Select the credentials<ul> <li>None - for public repositories</li> <li>Credential names - This option applies to private repositories based on existing credentials that were created for the scope.     To add new credentials to the credentials list, and for additional information, check the Credentials article.</li> </ul> </li> </ul> </li> <li>Set the data target location<ul> <li>container path</li> </ul> </li> <li>Click CREATE DATA SOURCE</li> </ol> <p>After a private data source is created, check its status to monitor its proper creation across the selected scope.</p>"},{"location":"platform-admin/workloads/assets/datasources/#host-path","title":"Host path","text":"<p>A Host path volume is a Kubernetes concept that enables mounting a host path file or a directory on the workload\u2019s file system. Like a PVC, the host path volume\u2019s data persists across workloads under various scopes. It also enables data serving from the hosting node.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Set the data origin<ul> <li>host path</li> </ul> </li> <li>Set the data target location<ul> <li>container path</li> </ul> </li> <li>Optional: Prevent data modification - When enabled, the data will be mounted with read-only permissions.</li> <li>Click CREATE DATA SOURCE</li> </ol>"},{"location":"platform-admin/workloads/assets/datasources/#configmap","title":"ConfigMap","text":"<p>A ConfigMap data source is a Run:ai abstraction for the Kubernetes ConfigMap concept. The ConfigMap is used mainly for storage that can be mounted on the workload container for non-confidential data. It is usually represented in key-value pairs (e.g., environment variables, command-line arguments etc.). It allows you to decouple environment-specific system configurations from your container images, so that your applications are easily portable. ConfigMaps must be created on the cluster prior to being used within the Run:ai system.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Set the data origin<ul> <li>Select the ConfigMap name (The list is empty if no existing ConfigMaps were created in advance).</li> </ul> </li> <li>Set the data target location<ul> <li>container path</li> </ul> </li> <li>Click CREATE DATA SOURCE</li> </ol>"},{"location":"platform-admin/workloads/assets/datasources/#secret","title":"Secret","text":"<p>A secret-type data source enables the mapping of a credential into the workload\u2019s file system. Credentials are a workload asset that simplify the complexities of Kubernetes Secrets. The credentials mask sensitive access information, such as passwords, tokens, and access keys, which are necessary for gaining access to various resources.</p> <ol> <li>Select the cluster under which to create this data source</li> <li>Select a scope</li> <li>Enter a name for the data source. The name must be unique.</li> <li>Optional: Provide a description of the data source</li> <li>Set the data origin<ul> <li>Select the credentials     To add new credentials, and for additional information, check the Credentials article.</li> </ul> </li> <li>Set the data target location<ul> <li>container path</li> </ul> </li> <li>Click CREATE DATA SOURCE</li> </ol> <p>After the data source is created, check its status to monitor its proper creation across the selected scope.</p> <p>Note</p> <p>It is also possible to add data sources directly when creating a specific workspace, training or inference workload</p>"},{"location":"platform-admin/workloads/assets/datasources/#editing-a-data-source","title":"Editing a data source","text":"<p>To edit a data source:</p> <ol> <li>Select the data source from the table</li> <li>Click Rename to provide it with a new name</li> <li>Click Copy &amp; Edit to make any changes to the data source</li> </ol>"},{"location":"platform-admin/workloads/assets/datasources/#deleting-a-data-source","title":"Deleting a data source","text":"<p>To delete a data source:</p> <ol> <li>Select the data source you want to delete</li> <li>Click DELETE</li> <li>Confirm you want to delete the data source</li> </ol> <p>Note</p> <p>It is not possible to delete an environment being used by an existing workload or template.</p>"},{"location":"platform-admin/workloads/assets/datasources/#using-api","title":"Using API","text":"<p>To view the available actions, go to the Data sources API reference.</p>"},{"location":"platform-admin/workloads/assets/environments/","title":"Environments","text":"<p>This article explains what environments are and how to create and use them.</p> <p>Environments are one type of workload asset. An environment consists of a configuration that simplifies how workloads are submitted and can be used by AI practitioners when they submit their workloads.</p> <p>An environment asset is a preconfigured building block that encapsulates aspects for the workload such as:</p> <ul> <li>Container image and container configuration  </li> <li>Tools and connections  </li> <li>The type of workload it serves</li> </ul>"},{"location":"platform-admin/workloads/assets/environments/#environments-table","title":"Environments table","text":"<p>The Environments table can be found under Environments in the Run:ai platform.</p> <p>The Environment table provides a list of all the environment defined in the platform and allows you to manage them.</p> <p></p> <p>The Environments table consists of the following columns:</p> Column Description Environment The name of the environment Description A description of the essence of the environment Scope The scope of this environment within the organizational tree. Click the name of the scope to view the organizational tree diagram Image The application or service to be run by the workload Workload Architecture This can be either standard for running workloads on a single node or distributed for running distributed workloads on a multiple nodes Tool(s) The tools and connection types the environment exposes Workload(s) The list of existing workloads that use the environment Workload types The workload types that can use the environment Template(s) The list of workload templates that use this environment Created by The user who created the environment. By default Run:ai UI comes with preinstalled environments created by Run:ai Creation time The timestamp for when the environment was created Cluster The cluster that the environment is associated with"},{"location":"platform-admin/workloads/assets/environments/#tools-associated-with-the-environment","title":"Tools associated with the environment","text":"<p>Click one of the values in the tools column to view the list of tools and their connection type.</p> Column Description Tool name The name of the tool or application AI practitioner can set up within the environment. Connection type The method by which you can access and interact with the running workload. It's essentially the \"doorway\" through which you can reach and use the tools the workload provide. (E.g node port, external URL, etc)"},{"location":"platform-admin/workloads/assets/environments/#workloads-associated-with-the-environment","title":"Workloads associated with the environment","text":"<p>Click one of the values in the Workload(s) column to view the list of workloads and their parameters.</p> Column Description Workload The workload that uses the environment Type The workload type (Workspace/Training/Inference) Status Represents the workload lifecycle. see the full list of workload status"},{"location":"platform-admin/workloads/assets/environments/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV</li> </ul>"},{"location":"platform-admin/workloads/assets/environments/#environments-created-by-runai","title":"Environments created by Run:ai","text":"<p>When installing Run:ai, you automatically get the environment created by Run:ai to ease up the onboarding process and support different use cases out of the box. These environments are created at the scope of the account.</p> Environment Image Jupiter-lab jupyter/scipy-notebook jupyter-tensorboard gcr.io/run-ai-demo/jupyter-tensorboard tensorboard tensorflow/tensorflow:latest llm-server runai.jfrog.io/core-llm/runai-vllm:v0.5.5-0.5.0 chatbot-ui runai.jfrog.io/core-llm/llm-app gpt2 runai.jfrog.io/core-llm/quickstart-inference:gpt2-cpu"},{"location":"platform-admin/workloads/assets/environments/#adding-a-new-environment","title":"Adding a new environment","text":"<p>Environment creation is limited to specific roles</p> <p>To add a new environment:</p> <ol> <li>Go to the Environments table  </li> <li>Click +NEW ENVIRONMENT </li> <li>Select under which cluster to create the environment  </li> <li>Select a scope </li> <li>Enter a name for the environment. The name must be unique.  </li> <li>Optional: Provide a description of the essence of the environment  </li> <li>Enter the Image URL    If a token or secret is required to pull the image, it is possible to create it via credentials of type docker registry. These credentials are automatically used once the image is pulled (which happens when the workload is submitted)  </li> <li>Set the image pull policy - the condition for when to pull the image from the registry  </li> <li>Set the workload architecture:  <ul> <li>Standard Only standard workloads can use the environment. A standard workload consists of a single process.  </li> <li>Distributed Only distributed workloads can use the environment. A distributed workload consists of multiple processes working together. These processes can run on different nodes.  </li> <li>Select a framework from the list.  </li> </ul> </li> <li>Set the workload type:  <ul> <li>Workspace </li> <li>Training </li> <li>Inference </li> <li>When inference is selected, define the endpoint of the model by providing both the protocol and the container\u2019s serving port  </li> </ul> </li> <li>Optional: Set the connection for your tool(s). The tools must be configured in the image. When submitting a workload using the environment, it is possible to connect to these tools  <ul> <li>Select the tool from the list (the available tools varies from IDE, experiment tracking, and more, including a custom tool for your choice)  </li> <li>Select the connection type  <ul> <li>External URL <ul> <li>Auto generate   A unique URL is automatically created for each workload using the environment  </li> <li>Custom URL   The URL is set manually  </li> </ul> </li> <li>Node port <ul> <li>Auto generate   A unique port is automatically exposed for each workload using the environment  </li> <li>Custom URL   Set the port manually  </li> </ul> </li> <li>Set the container port </li> </ul> </li> </ul> </li> <li>Optional: Set a command and arguments for the container running the pod  <ul> <li>When no command is added, the default command of the image is used (the image entrypoint)  </li> <li>The command can be modified while submitting a workload using the environment  </li> <li>The argument(s) can be modified while submitting a workload using the environment  </li> </ul> </li> <li>Optional: Set the environment variable(s) <ul> <li>The environment variable(s) are added to the default environment variables that are already set within the image  </li> <li>The environment variables can be modified and new variables can be added while submitting a workload using the environment</li> <li>You can configure a new Environment variable from your credentials (of type generic secret, access key or username &amp; password). When selecting an environment variable source from credentials, the predefined name for the credential assets are displayed as an option. In addition, you can select the type of the credential to be used (username / password or access key / access secret).</li> </ul> </li> <li>Optional: Set the container\u2019s working directory to define where the container\u2019s process starts running. When left empty, the default directory is used.  </li> <li>Optional: Set where the UID, GID and supplementary groups are taken from, this can be:  <ul> <li>From the image </li> <li>From the IdP token (only available in an SSO installations)  </li> <li>Custom (manually set) - decide whether the submitter can modify these value upon submission.  </li> </ul> </li> <li>Optional: Select Linux capabilities - Grant certain privileges to a container without granting all the privileges of the root user. </li> <li>Click CREATE ENVIRONMENT</li> </ol> <p>Note</p> <p>It is also possible to add environments directly when creating a specific workspace, training or inference workload</p>"},{"location":"platform-admin/workloads/assets/environments/#editing-an-environment","title":"Editing an environment","text":"<p>To edit an environment:</p> <ol> <li>Select the environment from the table  </li> <li>Click Rename to edit its name and description</li> </ol> <p>Note</p> <p>Additional fields can be edited using the API</p>"},{"location":"platform-admin/workloads/assets/environments/#copying-editing-an-environment","title":"Copying &amp; Editing an environment","text":"<p>To copy &amp; edit an environment:</p> <ol> <li>Select the project you want to duplicate  </li> <li>Click COPY &amp; EDIT. </li> <li>Update the environment and click SAVE.</li> </ol>"},{"location":"platform-admin/workloads/assets/environments/#deleting-an-environment","title":"Deleting an environment","text":"<p>To delete an environment:</p> <ol> <li>Select the environment you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol> <p>Note</p> <p>It is not possible to delete an environment being used by an existing workload and template.</p>"},{"location":"platform-admin/workloads/assets/environments/#using-api","title":"Using API","text":"<p>Go to the Environment API reference to view the available actions</p>"},{"location":"platform-admin/workloads/assets/overview/","title":"Overview","text":"<p>Workload assets enable organizations to:</p> <ul> <li>Create and reuse preconfigured setup for code, data, storage and resources to be used by AI practitioners to simplify the process of submitting workloads  </li> <li>Share the preconfigured setup with a wide audience of AI practitioners with similar needs</li> </ul> <p>Note</p> <ul> <li>The creation of assets is possible only via API and the Run:ai UI  </li> <li>The submission of workloads using assets, is possible only via the Run:ai UI</li> </ul>"},{"location":"platform-admin/workloads/assets/overview/#workload-asset-types","title":"Workload asset types","text":"<p>There are four workload asset types used by the workload:</p> <ul> <li>Environments   The container image, tools and connections for the workload  </li> <li>Data sources   The type of data, its origin and the target storage location such as PVCs or cloud storage buckets where datasets are stored  </li> <li>Compute resources   The compute specification, including GPU and CPU compute and memory  </li> <li>Credentials   The secrets to be used to access sensitive data, services, and applications such as docker registry or S3 buckets</li> </ul>"},{"location":"platform-admin/workloads/assets/overview/#asset-scope","title":"Asset scope","text":"<p>When a workload asset is created, a scope is required. The scope defines who in the organization can view and/or use the asset.</p> <p>Note</p> <p>When an asset is created via API, the scope can be the entire account, this is currently an experimental feature.</p>"},{"location":"platform-admin/workloads/assets/overview/#who-can-create-an-asset","title":"Who can create an asset?","text":"<p>Any subject (user, application, or SSO group) with a role that has permissions to Create an asset, can do so within their scope.</p>"},{"location":"platform-admin/workloads/assets/overview/#who-can-use-an-asset","title":"Who can use an asset?","text":"<p>Assets are used when submitting workloads. Any subject (user, application or SSO group) with a role that has permissions to Create workloads, can also use assets.</p>"},{"location":"platform-admin/workloads/assets/overview/#who-can-view-an-asset","title":"Who can view an asset?","text":"<p>Any subject (user, application, or SSO group) with a role that has permission to View an asset, can do so within their scope.  </p>"},{"location":"platform-admin/workloads/assets/templates/","title":"Templates","text":"<p>This article explains the procedure to manage templates.</p> <p>A template is a pre-set configuration that is used to quickly configure and submit workloads using existing assets. A template consists of all the assets a workload needs, allowing researchers to submit a workload in a single click, or make subtle adjustments to differentiate them from each other.</p>"},{"location":"platform-admin/workloads/assets/templates/#workspace-templates-table","title":"Workspace templates table","text":"<p>Access to the Templates table can be found on the left-hand menu in the Run:ai platform.</p> <p>The Templates table provides a list of all the templates defined in the platform, and allows you to manage them.</p> <p>Flexible Management</p> <p>It is also possible to manage templates directly for a specific user, application, project, or department.</p> <p></p> <p>The Templates table consists of the following columns:</p> Column Description Scope The scope to which the subject has access. Click the name of the scope to see the scope and its subordinates Environment The name of the environment related to the workspace template Compute resource The name of the compute resource connected to the workspace template Data source(s) The name of the data source(s) connected to the workspace template Created by The subject that created the template Creation time The timestamp for when the template was created Cluster The cluster name containing the template"},{"location":"platform-admin/workloads/assets/templates/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV  </li> <li>Refresh (optional) - Click REFRESH to update the table with the latest data  </li> <li>Show/Hide details (optional) - Click to view additional information on the selected row</li> </ul>"},{"location":"platform-admin/workloads/assets/templates/#adding-a-new-workspace-template","title":"Adding a new workspace template","text":"<p>To add a new template:</p> <ol> <li>Click +NEW TEMPLATE </li> <li>Set the scope for the template  </li> <li>Enter a name for the template  </li> <li>Select the environment for your workload  </li> <li> <p>Select the node resources needed to run your workload     - or -    Click +NEW COMPUTE RESOURCE</p> </li> <li> <p>Set the volume needed for your workload  </p> </li> <li>Create a new data source  </li> <li>Set auto-deletion, annotations and labels, as required  </li> <li>Click CREATE TEMPLATE</li> </ol>"},{"location":"platform-admin/workloads/assets/templates/#editing-a-template","title":"Editing a template","text":"<p>To edit a template:</p> <ol> <li>Select the template from the table  </li> <li>Click Rename to provide it with a new name  </li> <li>Click Copy &amp; Edit to make any changes to the template</li> </ol>"},{"location":"platform-admin/workloads/assets/templates/#deleting-a-template","title":"Deleting a template","text":"<p>To delete a template:</p> <ol> <li>Select the template you want to delete  </li> <li>Click DELETE </li> <li>Confirm you want to delete the template</li> </ol>"},{"location":"platform-admin/workloads/assets/templates/#using-api","title":"Using API**","text":"<p>Go to the Workload template API reference to view the available actions  </p>"},{"location":"platform-admin/workloads/overviews/managing-workloads/","title":"Managing Workloads","text":"<p>This article explains the procedure for managing workloads.</p>"},{"location":"platform-admin/workloads/overviews/managing-workloads/#workloads-table","title":"Workloads table","text":"<p>The Workloads table can be found under Workloads in the Run:ai platform.</p> <p>The workloads table provides a list of all the workloads scheduled on the run:ai Scheduler, and allows you to manage them.</p> <p></p> <p>The Workloads table consists of the following columns:</p> Column Description Workload The name of the workload Type The workload type Preemptible Is the workload preemptible Status The different phases in a workload life cycle. Project The project in which the workload runs. Department The department that the workload is associated with. this column is visible only if the department toggle is enabled by your administrator. Created by The user who created the workload Running/requested pods The number of running pods out of the requested Creation time The timestamp for when the workload was created Completion time The timestamp the workload reached a terminal state (failed/completed) Connection(s) The method by which you can access and interact with the running workload. It's essentially the \"doorway\" through which you can reach and use the tools the workload provide. (E.g node port, external URL, etc). Click one of the values in the column to view the list of connections and their parameters Data source(s) Data resources used by the workload Environment The environment used by the workload Workload architecture Standard or distributed. A standard workload consists of a single process. A distributed workload consists of multiple processes working together. These processes can run on different nodes. GPU compute request Amount of GPU devices Requested GPU compute allocation Amount of GPU devices allocated GPU memory request Amount of GPU memory Requested GPU memory allocation Amount of GPU memory allocated CPU compute request Amount of CPU cores requested CPU compute allocation Amount of CPU cores allocated CPU memory request Amount of CPU memory requested CPU memory allocation Amount of CPU memory allocated Cluster The cluster that the workload is associated with"},{"location":"platform-admin/workloads/overviews/managing-workloads/#workload-status","title":"Workload status","text":"<p>The following table describes the different phases in a workload life cycle.</p> Status Description Entry Condition Exit Condition Creating Workload setup is initiated in the cluster. Resources and pods are now provisioning. A workload is submitted. A multi-pod group is created. Pending Workload is queued and awaiting resource allocation. A pod group exists. All pods are scheduled. Initializing Workload is retrieving images, starting containers, and preparing pods. All pods are scheduled. All pods are initialized or a failure to initialize is detected. Running Workload is currently in progress with all pods operational. All pods initialized (all containers in pods are ready). Workload completion or failure. Degraded Pods may not align with specifications, network services might be incomplete, or persistent volumes may be detached. Check your logs for specific details. Pending - All pods are running but have issues. Running - All pods are running with no issues. Running - All resources are OK. Completed - Workload finished with fewer resources. Failed - Workload failure or user-defined rules. Deleting Workload and its associated resources are being decommissioned from the cluster. Deleting the workload. Resources are fully deleted. Stopped Workload is on hold and resources are intact but inactive. Stopping the workload without deleting resources. Transitioning back to the initializing phase or proceeding to deleting the workload. Failed Image retrieval failed or containers experienced a crash. Check your logs for specific details. An error occurs preventing the successful completion of the workload. Terminal state. Completed Workload has successfully finished its execution. The workload has finished processing without errors. Terminal state."},{"location":"platform-admin/workloads/overviews/managing-workloads/#pods-associated-with-workload","title":"Pods Associated with Workload","text":"<p>Click one of the values in the Running/requested pods column, to view the list of pods and their parameters.</p> Column Description Pod Pod name Status Pod lifecycle stages Node The node on which the pod resides Node pool The node pool in which the pod resides (applicable if node pools are enabled) Image The pod\u2019s main image GPU compute allocation Amount of GPU devices allocated for the pod GPU memory allocation Amount of GPU memory allocated for the pod"},{"location":"platform-admin/workloads/overviews/managing-workloads/#connections-associated-with-workload","title":"Connections Associated with Workload","text":"<p>A connection refers to the method by which you can access and interact with the running workloads. It is essentially the \"doorway\" through which you can reach and use the applications (tools) these workloads provide.</p> <p>Click one of the values in the Connection(s) column, to view the list of connections and their parameters. Connections are network interfaces that communicate with the application running in the workload. Connections are either the URL the application exposes or the IP and the port of the node that the workload is running on.</p> Column Description Name The name of the application running on the workload Connection type The network connection type selected for the workload Access Who is authorized to use this connection (everyone, specific groups/users) Address The connection URL Copy button Copy URL to clipboard Connect button Enabled only for supported tools"},{"location":"platform-admin/workloads/overviews/managing-workloads/#data-sources-associated-with-workload","title":"Data Sources Associated with Workload","text":"<p>Click one of the values in the Data source(s) column, to view the list of data sources and their parameters.</p> Column Description Data source The name of the data source mounted to the workload Type The data source type"},{"location":"platform-admin/workloads/overviews/managing-workloads/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Download table - Click MORE and then Click Download as CSV  </li> <li>Refresh - Click REFRESH to update the table with the latest data  </li> <li>Show/Hide details - Click to view additional information on the selected row</li> </ul>"},{"location":"platform-admin/workloads/overviews/managing-workloads/#showhide-details","title":"Show/Hide details","text":"<p>Click a row in the Workloads table and then click the SHOW DETAILS button at the upper-right side of the action bar. The details pane appears, presenting the following tabs:</p>"},{"location":"platform-admin/workloads/overviews/managing-workloads/#event-history","title":"Event History","text":"<p>Displays the workload status over time. It displays events describing the workload lifecycle and alerts on notable events. Use the filter to search through the history for specific events.</p>"},{"location":"platform-admin/workloads/overviews/managing-workloads/#metrics","title":"Metrics","text":"<ul> <li>GPU utilization   Per GPU graph and an average of all GPUs graph, all on the same chart, along an adjustable period allows you to see the trends of all GPUs compute utilization (percentage of GPU compute) in this node.  </li> <li>GPU memory utilization   Per GPU graph and an average of all GPUs graph, all on the same chart, along an adjustable period allows you to see the trends of all GPUs memory usage (percentage of the GPU memory) in this node.  </li> <li>CPU compute utilization   The average of all CPUs\u2019 cores compute utilization graph, along an adjustable period allows you to see the trends of CPU compute utilization (percentage of CPU compute) in this node.  </li> <li>CPU memory utilization   The utilization of all CPUs memory in a single graph, along an adjustable period allows you to see the trends of CPU memory utilization (percentage of CPU memory) in this node.  </li> <li> <p>CPU memory usage   The usage of all CPUs memory in a single graph, along an adjustable period allows you to see the trends of CPU memory usage (in GB or MB of CPU memory) in this node.</p> </li> <li> <p>For GPUs charts - Click the GPU legend on the right-hand side of the chart, to activate or deactivate any of the GPU lines.  </p> </li> <li>You can click the date picker to change the presented period  </li> <li>You can use your mouse to mark a sub-period in the graph for zooming in, and use Reset zoom to go back to the preset period  </li> <li>Changes in the period affect all graphs on this screen.</li> </ul>"},{"location":"platform-admin/workloads/overviews/managing-workloads/#logs","title":"Logs","text":"<p>Workload events are ordered in chronological order. The logs contain events from the workload\u2019s lifecycle to help monitor and debug issues.</p>"},{"location":"platform-admin/workloads/overviews/managing-workloads/#adding-new-workload","title":"Adding new workload","text":"<p>Before starting, make sure you have created a project or have one created for you to work with workloads.</p> <p>To create a new workload:</p> <ol> <li>Click +NEW WORKLOAD </li> <li>Select a workload type - Follow the links below to view the step-by-step guide for each workload type:  <ul> <li>Workspace. Used for data preparation and model-building tasks.  </li> <li>Training. Used for training tasks of all sorts  </li> <li>Inference. Used for inference and serving tasks  </li> <li>Job (legacy). This type is displayed only if enabled by your Administrator, under General Settings \u2192 Workloads \u2192 Workload policies  </li> </ul> </li> <li>Click CREATE WORKLOAD</li> </ol>"},{"location":"platform-admin/workloads/overviews/managing-workloads/#stopping-a-workload","title":"Stopping a workload","text":"<p>Stopping a workload kills the workload pods and releases the workload resources.</p> <ol> <li>Select the workload you want to stop  </li> <li>Click STOP</li> </ol>"},{"location":"platform-admin/workloads/overviews/managing-workloads/#running-a-workload","title":"Running a workload","text":"<p>Running a workload spins up new pods and resumes the workload work after it was stopped.</p> <ol> <li>Select the workload you want to run again  </li> <li>Click RUN</li> </ol>"},{"location":"platform-admin/workloads/overviews/managing-workloads/#connecting-to-a-workload","title":"Connecting to a workload","text":"<p>To connect to an application running in the workload (for example, Jupyter Notebook)</p> <ol> <li>Select the workload you want to connect  </li> <li>Click CONNECT </li> <li>Select the tool from the drop-down list  </li> <li>The selected tool is opened in a new tab on your browser</li> </ol>"},{"location":"platform-admin/workloads/overviews/managing-workloads/#deleting-a-workload","title":"Deleting a workload","text":"<ol> <li>Select the workload you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion  </li> </ol> <p>Note</p> <p>Once a workload is deleted you can view it in the Deleted tab in the workloads view. This tab is displayed only if enabled by your Administrator, under General Settings \u2192 Workloads \u2192 Deleted workloads</p> <ol> <li>Select the workload you want to copy and edit  </li> <li>Click COPY &amp; EDIT </li> <li>Update the workload and click CREATE WORKLOAD</li> </ol>"},{"location":"platform-admin/workloads/overviews/managing-workloads/#using-api","title":"Using API","text":"<p>Go to the Workloads API reference to view the available actions</p>"},{"location":"platform-admin/workloads/overviews/managing-workloads/#troubleshooting","title":"Troubleshooting","text":"<p>To understand the condition of the workload, review the workload status in the Workload table. For more information, see check the workload\u2019s event history.</p> <p>Listed below are a number of known issues when working with workloads and how to fix them:</p> Issue Mediation Cluster connectivity issues (there are issues with your connection to the cluster error message) Verify that you are on a network that has been granted access to the cluster.  Reach out to your cluster admin for instructions on verifying this.  If you are an admin, see the troubleshooting section in the cluster documentation Workload in \u201cInitializing\u201d status for some time Check that you have access to the Container image registry.  Check the statuses of the pods in the pods\u2019 modal.  Check the event history for more details Workload has been pending for some time Check that you have the required quota.  Check the project\u2019s available quota in the project dialog.  Check that all services needed to run are bound to the workload.  Check the event history for more details. PVCs created using the K8s API or <code>kubectl</code> are not visible or mountable in Run:ai. This is by design.  - Create a new data source of type PVC in the Run:ai UI  - In the Data mount section, select Existing PVC  - Select the PVC you created via the K8S API  You are now able to select and mount this PVC in your Run:ai submitted workloads. Workload is not visible in the UI. Check that the workload hasn\u2019t been deleted.  See the \u201cDeleted\u201d tab in the workloads view"},{"location":"platform-admin/workloads/overviews/workload-support/","title":"Workload Support","text":"<p>Workloads are the basic unit of work in Run:ai. Researchers and Engineers use workloads for every stage in their AI Project lifecycle. Workloads can be used to build, train, or deploy a model. Run:ai supports all types of Kubernetes workloads. Researchers can work with any workload in their organization but will get the largest value working with Run:ai native workloads.</p> <p>Run:ai offers three native types of workloads:</p> <ul> <li>Workspace. Used for data preparation and model-building tasks.  </li> <li>Training. Used for training tasks.  </li> <li>Inference. Used for inference and model serving tasks  </li> </ul> <p>Run:ai native workloads can be created via the Run:ai User interface, API or Command-line interface.</p>"},{"location":"platform-admin/workloads/overviews/workload-support/#levels-of-support","title":"Levels of support","text":"<p>Different types of workloads have different levels of support. Understanding what capabilities are needed before selecting the workload type to work with is important. The table below details the level of support for each workload type in Run:ai. The Run:ai native workloads are fully supported with all of Run:ai advanced features and capabilities. While third-party workloads are partially supported. The list of capabilities can change between different Run:ai versions.</p> Functionality Workload Type Run:ai workloads Third-party workloads Training - Standard Workspace Inference Training - distributed All K8s workloads Fairness v v v v v Priority and preemption v v v v v Over quota v v v v v Node pools v v v v v Bin packing / Spread v v v v v Fractions v v v v v Dynamic fractions v v v v v Node level scheduler v v v v v GPU swap v v v v v Elastic scaling NA NA v v v Gang scheduling v v v v v Monitoring v v v v v RBAC v v v v Workload awareness v v v v Workload submission v v v v Workload actions (stop/run) v v v Policies v v v v Scheduling rules v v v <p>Note</p> <p>Workload awareness</p> <p>Specific workload-aware visibility, so that different pods are identified and treated as a single workload (for example GPU utilization, workload view, dashboards).</p>"},{"location":"platform-admin/workloads/overviews/workload-support/#workload-scopes","title":"Workload scopes","text":"<p>Workloads must be created under a project. A project is the fundamental organization unit in the Run:ai account. To manage workloads, it\u2019s required to first create a project or have one created by the administrator.</p>"},{"location":"platform-admin/workloads/overviews/workload-support/#policies-and-rules","title":"Policies and rules","text":"<p>Policies and rules empower administrators to establish default values and implement restrictions on workloads allowing enhanced control, assuring compatibility with organizational policies, and optimizing resource usage and utilization.</p>"},{"location":"platform-admin/workloads/overviews/workload-support/#workload-statuses","title":"Workload statuses","text":"<p>The following table describes the different phases in a workload life cycle.</p> Phase Description Entry condition Exit condition Creating Workload setup is initiated in the Cluster. Resources and pods are now provisioning A workload is submitted A multi-pod group is created Pending Workload is queued and awaiting resource allocation. A pod group exists All pods are scheduled Initializing Workload is retrieving images, starting containers, and preparing pods All pods are scheduled All pods are initialized or a failure to initialize is detected Running Workload is currently in progress with all pods operational All pods initialized (all containers in pods are ready) workload completion or failure Degraded Pods may not align with specifications, network services might be incomplete, or persistent volumes may be detached. Check your logs for specific details. Pending: All pods are running but with issues Running: All pods are running with no issues. Running: All resources are OK Completed: Workload finished with fewer resources Failed: Workload failure or user-defined rules Deleting Workload and its associated resources are being decommissioned from the cluster Deleting the workload. Resources are fully deleted Stopped The workload is on hold and resources are intact but inactive Stopping the workload without deleting resources Transitioning back to the initializing phase or proceeding to deleting the workload Failed Image retrieval failed or containers experienced a crash. Check your logs for specific details. An error occurs preventing the successful completion of the workload Terminal State Completed Workload has successfully finished its execution The workload has finished processing without errors Terminal State"},{"location":"platform-admin/workloads/policies/old-policies/","title":"Policies (YAML-based)","text":"<p>Warning</p> <p>The below describes the old V1 Policies. While these still work, they have been replaced with Control-plane-based v2 policies which are accessible via API and user interface.  For a description of the new policies, see API-based Policies.</p>"},{"location":"platform-admin/workloads/policies/old-policies/#what-are-policies","title":"What are Policies?","text":"<p>Policies allow administrators to impose restrictions and set default values for Researcher Workloads. For example:</p> <ol> <li>Restrict researchers from requesting more than 2 GPUs, or less than 1GB of memory for an interactive workload.</li> <li>Set the default memory of each training job to 1GB, or mount a default volume to be used by any submitted Workload.</li> </ol> <p>Policies are stored as Kubernetes custom resources.</p> <p>Policies are specific to Workload type as such there are several kinds of Policies:</p> Workload Type Kubernetes Workload Name Kubernetes Policy Name Interactive <code>InteractiveWorkload</code> <code>InteractivePolicy</code> Training <code>TrainingWorkload</code> <code>TrainingPolicy</code> Distributed Training <code>DistributedWorkload</code> <code>DistributedPolicy</code> Inference <code>InferenceWorkload</code> <code>InferencePolicy</code> <p>A Policy can be created per Run:ai Project (Kubernetes namespace). Additionally, a Policy resource can be created in the <code>runai</code> namespace. This special Policy will take effect when there is no project-specific Policy for the relevant workload kind.</p> <p>When researchers create a new interactive workload or workspace, they see list of available node pools and their priority. Priority is set by dragging and dropping the node pools in the desired order of priority. When the node pool priority list is locked by an administrator policy, the node pool list isn't editable by the Researcher even if the workspace is created from a template or copied from another workspace.</p> <p>Note</p> <p>Policies on this page cannot be added to platform 2.16 or higher that have the New Policy Manager enabled.</p>"},{"location":"platform-admin/workloads/policies/old-policies/#creating-a-policy","title":"Creating a Policy","text":""},{"location":"platform-admin/workloads/policies/old-policies/#creating-your-first-policy","title":"Creating your First Policy","text":"<p>To create a sample <code>InteractivePolicy</code>, prepare a file (e.g. <code>policy.yaml</code>) containing the following YAML:</p> gpupolicy.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: InteractivePolicy\nmetadata:\n  name: interactive-policy1\n  namespace: runai-team-a # (1)\nspec:\n  gpu:\n    rules:\n      required: true\n      min: \"1\"  # (2)\n      max: \"4\"  \n    value: \"1\"\n</code></pre> <ol> <li>Set the Project namespace here.</li> <li>GPU values are quoted as they can contain non-integer values.</li> </ol> <p>The policy places a default and limit on the available values for GPU allocation. To apply this policy, run:</p> <pre><code>kubectl apply -f gpupolicy.yaml \n</code></pre> <p>Now, try the following command:</p> <pre><code>runai submit --gpu 5 --interactive -p team-a\n</code></pre> <p>The following message will appear:</p> <pre><code>gpu: must be no greater than 4\n</code></pre> <p>A similar message will appear in the New Job form of the Run:ai user interface, when attempting to enter the number of GPUs, which is out of range for a training job.</p>"},{"location":"platform-admin/workloads/policies/old-policies/#gpu-and-cpu-memory-limits","title":"GPU and CPU memory limits","text":"<p>The following policy places a default and limit on the available values for CPU and GPU memory allocation.</p> gpumemorypolicy.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: training-policy\n  namespace: runai\nspec:\n  gpuMemory:\n    rules:\n      min: 100M\n      max: 2G\nmemory:\n    rules:\n      min: 100M\n      max: 2G\n</code></pre>"},{"location":"platform-admin/workloads/policies/old-policies/#read-only-values","title":"Read-only values","text":"<p>When you do not want the user to be able to change a value, you can force the corresponding user interface control to become read-only by using the <code>canEdit</code> key. For example,</p> runasuserpolicy.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: train-policy1\n  namespace: runai-team-a # (1) \n\nspec:\n  runAsUser:\n    rules:\n      required: true  # (2)\n      canEdit: false  # (3)\n    value: true # (4)\n</code></pre> <ol> <li>Set the Project namespace here.</li> <li>The field is required.</li> <li>The field will be shown as read-only in the user interface.</li> <li>The field value is true.  </li> </ol>"},{"location":"platform-admin/workloads/policies/old-policies/#complex-values","title":"Complex Values","text":"<p>The example above illustrated rules for parameters of \"primitive\" types, such as GPU allocation, CPU memory, working directory, etc. These parameters contain a single value.</p> <p>Other workload parameters, such as ports or volumes, are \"complex\", in the sense that they may contain multiple values: a workload may contain multiple ports and multiple volumes.</p> <p>The following is an example of a policy containing the value <code>ports</code>, which is complex: The <code>ports</code> flag typically contains two values: The <code>external</code> port that is mapped to an internal <code>container</code> port. One can have multiple port tuples defined for a single Workload:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: InteractivePolicy\nmetadata:\n  name: interactive-policy\n  namespace: runai\nspec:\n  ports:\n    rules:\n      canAdd: true\n    itemRules:\n      container:\n        min: 30000\n        max: 32767\n      external:\n        max: 32767\n    items:\n      admin-port-a:\n        rules:\n          canRemove: false\n          canEdit: false\n        value:\n          container: 30100\n          external: 8080\n      admin-port-b:\n        value:\n          container: 30101\n          external: 8081\n</code></pre> <p>A policy for a complex field is composed of three parts:</p> <ul> <li>Rules: Rules apply to the <code>ports</code> parameter as a whole. In this example, the administrator specifies <code>canAdd</code> rule with <code>true</code> value, indicating that a researcher submitting an interactive job can add additional ports to the ports listed by the policy (true is the default for <code>canAdd</code>, so it actually could have been omitted from the policy above). When <code>canAdd</code> is set to <code>false</code>, the researcher will not be able to add any additional port except those already specified by the policy.</li> <li>itemRules: itemRules impose restrictions on the data members of each item, in this case - <code>container</code> and <code>external</code>. In the above example, the administrator has limited the value of <code>container</code> to 30000-32767, and the value of <code>external</code> to a maximum of 32767.</li> <li>Items: Specifies a list of default ports. Each port is an item in the ports list and given a label (e.g. <code>admin-port-b</code>). The administrator can also specify whether a researcher can change/delete ports from the submitted workload. In the above example, <code>admin-port-a</code> is hardwired and cannot be changed or deleted, while <code>admin-port-b</code> can be changed or deleted by the researcher when submitting the Workload. It is possible to specify a label using the reserved name of <code>DEFAULTS</code>. This item provides the defaults for all other items.</li> </ul> <p>The following is an example of a complex policy for PVCs which contains <code>DEFAULTS</code>.</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: tp # use your name.\n  namespace: runai-team-a # use your namespace\nspec:\n  pvcs:\n    itemRules:\n      existingPvc:\n        canEdit: false\n      claimName:\n        required: true\n    items:\n      DEFAULTS:\n        value:\n          existingPvc: true\n          path: nil\n</code></pre>"},{"location":"platform-admin/workloads/policies/old-policies/#syntax","title":"Syntax","text":"<p>The complete syntax of the policy YAML can be obtained using the <code>explain</code> command of kubectl. For example:</p> <p><pre><code>kubectl explain trainingpolicy.spec\n</code></pre> Should provide the list of all possible fields in the spec of training policies:</p> <pre><code>KIND:     TrainingPolicy\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: spec &lt;Object&gt;\n\nDESCRIPTION:\nThe specifications of this TrainingPolicy\n\nFIELDS:\nannotations &lt;Object&gt;\nSpecifies annotations to be set in the container running the created\nworkload.\n\narguments   &lt;Object&gt;\nIf set, the arguments are sent along with the command which overrides the\nimage's entry point of the created workload.\n\ncommand &lt;Object&gt;\nIf set, overrides the image's entry point with the supplied command.\n...\n</code></pre> <p>You can further drill down to get the syntax for <code>ports</code> by running:</p> <pre><code>kubectl explain trainingpolicy.spec.ports\n</code></pre> <pre><code>KIND:     TrainingPolicy\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: ports &lt;Object&gt;\n\nDESCRIPTION:\n     Specify the set of ports exposed from the container running the created\n     workload. Used together with --service-type.\n\nFIELDS:\n   itemRules    &lt;Object&gt;\n\n   items    &lt;map[string]Object&gt;\n\n   rules    &lt;Object&gt;\n     these rules apply to a value of type map (=non primitive) as a whole\n     additionally there are rules which apply for specific items of the map\n</code></pre> <p>Drill down into the <code>ports.rules</code> object by running:</p> <pre><code>kubectl explain trainingpolicy.spec.ports.rules\n</code></pre> <pre><code>KIND:     TrainingPolicy\nVERSION:  run.ai/\n\nRESOURCE: rules &lt;Object&gt;\n\nDESCRIPTION:\n     these rules apply to a value of type map (=non primitive) as a whole\n     additionally there are rules which apply for specific items of the map\n\nFIELDS:\n   canAdd   &lt;boolean&gt;\n     is it allowed for a workload to add items to this map\n\n   required &lt;boolean&gt;\n     if the map as a whole is required\n</code></pre> <p>Note that each kind of policy has a slightly different set of parameters. For example, an <code>InteractivePolicy</code> has a <code>jupyter</code> parameter that is not available under <code>TrainingPolicy</code>.</p>"},{"location":"platform-admin/workloads/policies/old-policies/#using-secrets-for-environment-variables","title":"Using Secrets for Environment Variables","text":"<p>It is possible to add values from Kubernetes secrets as the value of environment variables included in the policy. The secret will be extracted from the secret object when the Job is created. For example:</p> <pre><code>  environment:\n    items:\n      MYPASSWORD:\n        value: \"SECRET:my-secret,password\"\n</code></pre> <p>When submitting a workload that is affected by this policy, the created container will have an environment variable called <code>MYPASSWORD</code> whose value is the key <code>password</code> residing in Kubernetes secret <code>my-secret</code> which has been pre-created in the namespace where the workload runs.</p>"},{"location":"platform-admin/workloads/policies/old-policies/#prevent-data-storage-on-the-node","title":"Prevent Data Storage on the Node","text":"<p>You can configure policies to prevent the submission of workloads that use data sources that consist of a host path. This setting prevents data from being stored on the node so that in the event when a node is deleted, all data stored on that node is lost.</p> <p>Example for rejecting workloads requesting host path:</p> <pre><code>spec:\n  volumes:\n    itemRules:\n      nfsServer:\n        required: true\n</code></pre>"},{"location":"platform-admin/workloads/policies/old-policies/#terminate-runai-training-jobs-after-preemption-policy","title":"Terminate Run:ai training Jobs after preemption policy","text":"<p>Administrators can set a \u2018termination after preemption\u2019 policy to Run:ai training jobs. After applying this policy, a training job will be terminated once it has been preempted from any reason. For example, a training job that is using over-quota resources (e.g. GPUs) and the owner of those GPUs wants to reclaim them back, the Training job is preempted and typically goes back to the pending queue. However, if the termination policy is applied, the job is terminated instead of reinstated as pending. The Termination after Preemption Policy can be set as a cluster-wide policy (applicable to all namespaces/projects) or per project/namespace.</p> <p>To use this feature the administrator should configure either a cluster wide or namespace policy.</p> <p>For cluster wide (all namespaces/projects) use this YAML based policy:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: training-policy\n  namespace: runai\nspec:\n  terminateAfterPreemption:\n    value: true\n</code></pre> <p>For per namespace (project) use this YAML based policy:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: training-policy\n  namespace: runai-&lt;PROJECT_NAME&gt;\nspec:\n  terminateAfterPreemption:\n    value: false\n</code></pre>"},{"location":"platform-admin/workloads/policies/old-policies/#modifyingdeleting-policies","title":"Modifying/Deleting Policies","text":"<p>Use the standard kubectl get/apply/delete commands to modify and delete policies.</p> <p>For example, to view the global interactive policy:</p> <pre><code>kubectl get interactivepolicies -n runai\n</code></pre> <p>Should return the following:</p> <pre><code>NAME                 AGE\ninteractive-policy   2d3h\n</code></pre> <p>To delete this policy:</p> <pre><code>kubectl delete InteractivePolicy interactive-policy -n runai\n</code></pre> <p>To access project-specific policies, replace the <code>-n runai</code> parameter with the namespace of the relevant project.</p>"},{"location":"platform-admin/workloads/policies/old-policies/#see-also","title":"See Also","text":"<ul> <li>For creating workloads based on policies, see the Run:ai submitting workloads</li> </ul>"},{"location":"platform-admin/workloads/policies/overview/","title":"Overview","text":"<p>At Run:ai, Administrator can access a suite of tools designed to facilitate efficient account management. This article focuses on two key features: workload policies and workload scheduling rules. These features empower admins to establish default values and implement restrictions allowing enhanced control, assuring compatibility with organizational policies and optimizing resources usage and utilization.</p>"},{"location":"platform-admin/workloads/policies/overview/#workload-policies","title":"Workload policies","text":"<p>A workload policy is an end-to-end solution for AI managers and administrators to control and simplify how workloads are submitted. This solution allows them to set best practices, enforce limitations, and standardize processes for the submission of workloads for AI projects within their organization. It acts as a key guideline for data scientists, researchers, ML &amp; MLOps engineers by standardizing submission practices and simplifying the workload submission process.</p>"},{"location":"platform-admin/workloads/policies/overview/#older-and-newer-policy-technologies","title":"Older and Newer Policy technologies","text":"<p>Run:ai provides two policy technologies.</p> <p>YAML-Based policies are the older policies. These policies:</p> <ul> <li>Require access to Kubernetes to view or change.</li> <li>Contact Run:ai support to convert the old policies to the new V2 policies format.</li> </ul> <p>API-based policies which are the newer policies. These are:</p> <ul> <li>Show in the Run:ai user interface.</li> <li>Can be viewed and modified via the user interface and the Control-plane API.</li> <li>Enable new rules addressing differences between project, department and cluster policies.</li> <li>Only available with Run:ai clusters of version 2.18 and up. </li> </ul>"},{"location":"platform-admin/workloads/policies/overview/#why-use-a-workload-policy","title":"Why use a workload policy?","text":"<p>Implementing workload policies is essential when managing complex AI projects within an enterprise for several reasons:</p> <ol> <li>Resource control and management     Defining or limiting the use of costly resources across the enterprise via a centralized management system to ensure efficient allocation and prevent overuse.  </li> <li>Setting best practices     Provide managers with the ability to establish guidelines and standards to follow, reducing errors amongst AI practitioners within the organization.  </li> <li>Security and compliance    Define and enforce permitted and restricted actions to uphold organizational security and meet compliance requirements.  </li> <li>Simplified setup    Conveniently allow setting defaults and streamline the workload submission process for AI practitioners.  </li> <li>Scalability and diversity  <ol> <li>Multi-purpose clusters with various workload types that may have different requirements and characteristics for resource usage.  </li> <li>The organization has multiple hierarchies, each with distinct goals, objectives and degrees of flexibility.  </li> <li>Manage multiple users and projects with distinct requirements and methods, ensuring appropriate utilization of resources.</li> </ol> </li> </ol>"},{"location":"platform-admin/workloads/policies/overview/#understanding-the-mechanism","title":"Understanding the mechanism","text":"<p>The following sections provide details of how the workload policy mechanism works.</p>"},{"location":"platform-admin/workloads/policies/overview/#cross-interface-enforcement","title":"Cross-interface enforcement","text":"<p>The policy enforces the workloads regardless of whether they were submitted via UI, CLI, Rest APIs, or Kubernetes YAMLs.</p>"},{"location":"platform-admin/workloads/policies/overview/#policy-types","title":"Policy types","text":"<p>Run:ai\u2019s policies enforce Run:ai workloads. The policy type is per Run:ai workload type. This allows administrators to set different policies for each workload type.</p> Policy type Workload type Kubernetes name Workspace Workspace Interactive workload Training Standard Training Standard Training workload Distributed Distributed Distributed workload Inference* Inference Inference workload <p>* The submission of this policy type is supported currently via API only</p>"},{"location":"platform-admin/workloads/policies/overview/#policy-structure-rules-defaults-and-imposed-assets","title":"Policy structure - rules, defaults, and imposed assets","text":"<p>A policy consists of rules for limiting and controlling the values of fields of the workload. In addition to rules, some defaults allow the implementation of default values to different workload fields. These default values are not rules, as they simply suggest values that can be overridden during the workload submission.</p> <p>Furthermore, policies allow the enforcement of workload assets. For example, as an admin, you can impose a data source of type PVC to be used by any workload submitted.</p> <p>For more information see rules, defaults and imposed assets.</p>"},{"location":"platform-admin/workloads/policies/overview/#scope-of-effectiveness","title":"Scope of effectiveness","text":"<p>Numerous teams working on various projects require the use of different tools, requirements, and safeguards. One policy may not suit all teams and their requirements. Hence, administrators can select the scope to cover the effectiveness of the policy. When a scope is selected, all of its subordinate units are also affected. As a result, all workloads submitted within the selected scope are controlled by the policy.</p> <p>For example, if a policy is set for Department A, all workloads submitted by any of the projects within this department are controlled.</p> <p>A scope for a policy can be:  </p> <pre><code>    The entire account *  \n        L Specific cluster  \n            L Specific department  \n                L Specific project\n</code></pre> <p>* The policy submission to the entire account scope is supported via API only</p> <p>The different scoping of policies also allows the breakdown of the responsibility between different administrators. This allows delegation of ownership between different levels within the organization. The policies, containing rules and defaults, propagate* down the organizational tree, forming an \u201ceffective\u201d policy that enforces any workload submitted by users within the project.</p> <p></p>"},{"location":"platform-admin/workloads/policies/overview/#policy-rules-reconciliation","title":"Policy rules reconciliation","text":"<p>For situations where a rule or a default for a specific field is already governed by a policy, newly submitted policies for additional organizational units mentioning this existing field are not blocked from submission. For those instances, the effective rules and defaults are selected based on the following logic:</p> <ul> <li>For policy defaults - The lowest organizational hierarchy \u201cclosest\u201d to the actual workload becomes the effective policy defaults (project defaults &gt; department defaults &gt; cluster defaults &gt; tenant defaults).</li> <li>For policy rules - </li> <li>If the rule belongs to the compute and security sections in the workload spec of the Run:ai API, the highest hierarchy is chosen for the effective policy for the field (tenant rules &gt; cluster rules &gt; department rules &gt; project rules).</li> <li>If the rule does not belong to the compute or security sections, the lowest hierarchy \u201cclosest\u201d to the actual workload becomes the effective policy for the field (similar to defaults).</li> </ul> <p>While viewing the effective policy, for each rule and default the source of the policy origin is visible, allowing users to clearly understand the selected hierarchy of the effective policy.</p>"},{"location":"platform-admin/workloads/policies/overview/#runai-policies-vs-kyverno-policies","title":"Run:ai Policies vs. Kyverno Policies","text":"<p>Kyverno runs as a dynamic admission controller in a Kubernetes cluster. Kyverno receives validating and mutating admission webhook HTTP callbacks from the Kubernetes API server and applies matching policies to return results that enforce admission policies or reject requests. Kyverno policies can match resources using the resource kind, name, label selectors, and much more. For more information, see How Kyverno Works.</p>"},{"location":"platform-admin/workloads/policies/policy-examples/","title":"Policies Examples","text":"<p>This article provides examples of:</p> <ol> <li>Creating a new rule within a policy </li> <li>Best practices for adding sections to a policy </li> <li>A full example of a policy.</li> </ol>"},{"location":"platform-admin/workloads/policies/policy-examples/#creating-a-new-rule-within-a-policy","title":"Creating a new rule within a policy","text":"<p>This example shows how to add a new limitation to the GPU usage for workloads of type workspace:</p> <ol> <li> <p>Check the workload API fields documentation and select the field(s) that are most relevant for GPU usage.  </p> <pre><code>{\n\"spec\": {\n    \"compute\": {\n    \"gpuDevicesRequest\": 1,\n    \"gpuRequestType\": \"portion\",\n    \"gpuPortionRequest\": 0.5,\n    \"gpuPortionLimit\": 0.5,\n    \"gpuMemoryRequest\": \"10M\",\n    \"gpuMemoryLimit\": \"10M\",\n    \"migProfile\": \"1g.5gb\",\n    \"cpuCoreRequest\": 0.5,\n    \"cpuCoreLimit\": 2,\n    \"cpuMemoryRequest\": \"20M\",\n    \"cpuMemoryLimit\": \"30M\",\n    \"largeShmRequest\": false,\n    \"extendedResources\": [\n        {\n        \"resource\": \"hardware-vendor.example/foo\",\n        \"quantity\": 2,\n        \"exclude\": false\n        }\n    ]\n    },\n}\n}\n</code></pre> </li> <li> <p>Search the field in the Policy YAML fields - reference table. For example, gpuDevicesRequest appears under the Compute fields sub-table and appears as follow:</p> </li> </ol> Fields Description Value type Supported Run:ai workload type gpuDeviceRequest Specifies the number of GPUs to allocate for the created workload. Only if <code>gpuDeviceRequest = 1</code>, the gpuRequestType can be defined. integer Workspace &amp; Training <ol> <li> <p>Use the value type of the gpuDevicesRequest field indicated in the table - \u201cinteger\u201d and navigate to the Value types table to view the possible rules that can be applied to this value type - </p> <p>for integer, the options are: </p> <ul> <li>canEdit  </li> <li>required  </li> <li>min  </li> <li>max  </li> <li>step  </li> </ul> </li> <li> <p>Proceed to the Rule Type table, select the required rule for the limitation of the field - for example \u201cmax\u201d and use the examples syntax to indicate the maximum GPU device requested.</p> </li> </ol> <pre><code>compute:\n    gpuDevicesRequest:\n        max: 2\n</code></pre>"},{"location":"platform-admin/workloads/policies/policy-examples/#policy-yaml-best-practices","title":"Policy YAML best practices","text":"Create a policy that has multiple defaults and rules <p>Best practice description: Presentation of the syntax while adding a set of defaults and rules</p> <pre><code>defaults:\n  createHomeDir: true\n  environmentVariables:\n    instances:\n    - name: MY_ENV\n      value: my_value\nsecurity:\n  allowPrivilegeEscalation: false\n\nrules:\n  storage:\n    s3:\n      attributes:\n        url:\n          options:\n            - value: https://www.google.com\n            displayed: https://www.google.com\n            - value: https://www.yahoo.com\n            displayed: https://www.yahoo.com\n</code></pre> Allow only single selection out of many <p>Best practice description:  Blocking the option to create all types of data sources except the one that is allowed is the solution.</p> <pre><code>rules:\n  storage:\n    dataVolume:\n      instances:\n        canAdd: false\n    hostPath:\n      instances:\n        canAdd: false\n    pvc:\n      instances:\n        canAdd: false\n    git:\n      attributes:\n        repository:\n          required: true\n        branch:\n          required: true\n        path:\n          required: true\n    nfs:\n      instances:\n        canAdd: false\n    s3:\n      instances:\n        canAdd: false\n</code></pre> Create a robust set of guidelines <p>Best practice description: Set rules for specific compute resource usage, addressing most relevant spec fields </p> <pre><code>rules:\n  compute:\n    cpuCoreRequest:\n      required: true\n      min: 0\n      max: 8\n    cpuCoreLimit:\n      min: 0\n      max: 8\n    cpuMemoryRequest:\n      required: true\n      min: '0'\n      max: 16G\n    cpuMemoryLimit:\n      min: '0'\n      max: 8G\n    migProfile:\n      canEdit: false\n    gpuPortionRequest:\n      min: 0\n      max: 1\n    gpuMemoryRequest:\n      canEdit: false\n    extendedResources:\n      instances:\n        canAdd: false\n</code></pre> Environment creation (specific section) <pre><code>rules:\n  imagePullPolicy:\n    required: true\n    options:\n    - value: Always\n      displayed: Always\n    - value: Never\n      displayed: Never\n  createHomeDir:\n    canEdit: false\n</code></pre> Setting security measures (specific section) <pre><code>rules:\n  security:\n    runAsUid:\n      min: 1\n      max: 32700\n    allowPrivilegeEscalation:\n      canEdit: false\n</code></pre> Policy for distributed training workloads (specific section) <p>Best practice description: Set rules and defaults for a distributed training workload with different settings for master and worker </p> <pre><code>defaults:\n  worker:\n    command: my-command-worker-1\n    environmentVariables:\n      instances:\n        - name: LOG_DIR\n          value: policy-worker-to-be-ignored\n        - name: ADDED_VAR\n          value: policy-worker-added\n   security:\n    runAsUid: 500\n  storage:\n     s3:\n     attributes:\n       bucket: bucket1-worker\n master:\n   command: my-command-master-2\n   environmentVariables:\n     instances:\n       - name: LOG_DIR\n         value: policy-master-to-be-ignored\n       - name: ADDED_VAR\n         value: policy-master-added\n    security:\n      runAsUid: 800\n    storage:\n     s3:\n       attributes:\n         bucket: bucket1-master\n rules:\n   worker:\n     command:\n       options:\n         - value: my-command-worker-1\n           displayed: command1\n         - value: my-command-worker-2\n           displayed: command2\n     storage:\n       nfs:\n         instances:\n           canAdd: false\n       s3:\n         attributes:\n           bucket:\n             options:\n               - value: bucket1-worker\n               - value: bucket2-worker\n   master:\n     command:\n       options:\n         - value: my-command-master-1\n           displayed: command1\n         - value: my-command-master-2\n           displayed: command2\n     storage:\n       nfs:\n         instances:\n           canAdd: false\n       s3:\n         attributes:\n           bucket:\n             options:\n               - value: bucket1-master\n               - value: bucket2-master\n</code></pre> Impose an asset (specific section) <pre><code> defaults: null\n rules: null\n imposedAssets:\n   - f12c965b-44e9-4ff6-8b43-01d8f9e630cc\n</code></pre>"},{"location":"platform-admin/workloads/policies/policy-examples/#example-of-a-full-policy","title":"Example of a full policy","text":"<pre><code>defaults:\n  createHomeDir: true\n  imagePullPolicy: IfNotPresent\n  nodePools:\n    - node-pool-a\n    - node-pool-b\n  environmentVariables:\n    instances:\n      - name: WANDB_API_KEY\n        value: REPLACE_ME!\n      - name: WANDB_BASE_URL\n        value: https://wandb.mydomain.com\n  compute:\n    cpuCoreRequest: 0.1\n    cpuCoreLimit: 20\n    cpuMemoryRequest: 10G\n    cpuMemoryLimit: 40G\n    largeShmRequest: true\n  security:\n    allowPrivilegeEscalation: false\n  storage:\n    git:\n      attributes:\n        repository: https://git-repo.my-domain.com\n        branch: master\n    hostPath:\n      instances:\n        - name: vol-data-1\n          path: /data-1\n          mountPath: /mount/data-1\n        - name: vol-data-2\n          path: /data-2\n          mountPath: /mount/data-2\nrules:\n  createHomeDir:\n    canEdit: false\n  imagePullPolicy:\n    canEdit: false\n  environmentVariables:\n    instances:\n      locked:\n        - WANDB_BASE_URL\n  compute:\n    cpuCoreRequest:\n      max: 32\n    cpuCoreLimit:\n      max: 32\n    cpuMemoryRequest:\n      min: 1G\n      max: 20G\n    cpuMemoryLimit:\n      min: 1G\n      max: 40G\n    largeShmRequest:\n      canEdit: false\n    extendedResources:\n      instances:\n        canAdd: false\n  security:\n    allowPrivilegeEscalation:\n      canEdit: false\n    runAsUid:\n      min: 1\n  storage:\n    hostPath:\n      instances:\n        locked:\n          - vol-data-1\n          - vol-data-2\nimposedAssets:\n  - 4ba37689-f528-4eb6-9377-5e322780cc27\n</code></pre>"},{"location":"platform-admin/workloads/policies/policy-reference/","title":"Policies Reference","text":"<p>A workload policy is an end-to-end solution for AI managers and administrators to control and simplify how workloads are submitted, setting best practices, enforcing limitations, and standardizing processes for AI projects within their organization.</p> <p>This article explains the policy YAML fields and the possible rules and defaults that can be set for each field.</p>"},{"location":"platform-admin/workloads/policies/policy-reference/#policy-yaml-fields-reference-table","title":"Policy YAML fields - reference table","text":"<p>The policy fields are structured in a similar format to the workload API fields. The following tables represent a structured guide designed to help you understand and configure policies in a YAML format. It provides the fields, descriptions, defaults and rules for each workload type.</p> <p>Click the link to view the value type of each field.</p> Fields Description Value type Supported Run:ai workload type args When set, contains the arguments sent along with the command. These override the entry point of the image in the created workload string Workspace Training command A command to serve as the entry point of the container running the workspace string Workspace Training createHomeDir Instructs the system to create a temporary home directory for the user within the container. Data stored in this directory is not saved when the container exists. When the runAsUser flag is set to true, this flag defaults to true as well boolean Workspace Training environmentVariables Set of environmentVariables to populate the container running the workspace array Workspace Training image Specifies the image to use when creating the container running the workload string Workspace Training imagePullPolicy Specifies the pull policy of the image when starting t a container running the created workload. Options are: always, ifNotPresent, or never string Workspace Training workingDir Container\u2019s working directory. If not specified, the container runtime default is used, which might be configured in the container image string Workspace Training nodeType Nodes (machines) or a group of nodes on which the workload runs string Workspace Training nodePools A prioritized list of node pools for the scheduler to run the workspace on. The scheduler always tries to use the first node pool before moving to the next one when the first is not available. array Workspace Training annotations Set of annotations to populate into the container running the workspace itemized Workspace Training labels Set of labels to populate into the container running the workspace itemized Workspace Training terminateAfterPreemtpion Indicates whether the job should be terminated, by the system, after it has been preempted boolean Workspace Training autoDeletionTimeAfterCompletionSeconds Specifies the duration after which a finished workload (Completed or Failed) is automatically deleted. If this field is set to zero, the workload becomes eligible to be deleted immediately after it finishes. integer Workspace Training backoffLimit Specifies the number of retries before marking a workload as failed integer Workspace Training completions Used with Hyperparameter Optimization. Specifies the number of successful pods the job should reach to be completed. The Job is marked as successful once the specified amount of pods has succeeded. integer Workspace Training parallelism Used with Hyperparameters Optimization. Specifies the maximum desired number of pods the workload should run at any given time. itemized Workspace Training exposeUrls Specifies a set of exported URL (e.g. ingress) from the container running the created workload. itemized Workspace Training largeShmRequest Specifies a large /dev/shm device to mount into a container running the created workload. SHM is a shared file system mounted on RAM. boolean Workspace Training PodAffinitySchedulingRule Indicates if we want to use the Pod affinity rule as: the \u201chard\u201d (required) or the \u201csoft\u201d (preferred) option. This field can be specified only if PodAffinity is set to true. string Workspace Training podAffinityTopology Specifies the Pod Affinity Topology to be used for scheduling the job. This field can be specified only if PodAffinity is set to true. string Workspace Training ports Specifies a set of ports exposed from the container running the created workload. More information in Ports fields below. itemized Workspace Training probes Specifies the ReadinessProbe to use to determine if the container is ready to accept traffic. More information in Probes fields below - Workspace Training tolerations Toleration rules which apply to the pods running the workload. Toleration rules guide (but do not require) the system to which node each pod can be scheduled to or evicted from, based on matching between those rules and the set of taints defined for each Kubernetes node. itemized Workspace Training priorityClass Priority class of the workload. The values for workspace are build (default) or interactive-preemptible. For training only, use train. Enum: \"build\", \"train\", \"interactive-preemptible\" string Workspace storage Contains all the fields related to storage configurations. More information in Storage fields below. - Workspace Training security Contains all the fields related to security configurations. More information in Security fields below. - Workspace Training compute Contains all the fields related to compute configurations. More information in Compute fields below. - Workspace Training"},{"location":"platform-admin/workloads/policies/policy-reference/#ports-fields","title":"Ports fields","text":"Fields Description Value type Supported Run:ai workload type container The port that the container running the workload exposes. string Workspace Training serviceType Specifies the default service exposure method for ports. the default shall be sued for ports which do not specify service type. Options are: LoadBalancer, NodePort or ClusterIP. For more information see the External Access to Containers guide. string Workspace Training external The external port which allows a connection to the container port. If not specified, the port is auto-generated by the system. integer Workspace Training toolType The tool type that runs on this port. string Workspace Training toolName A name describing the tool that runs on this port. string Workspace Training"},{"location":"platform-admin/workloads/policies/policy-reference/#probes-fields","title":"Probes fields","text":"Fields Description Value type Supported Run:ai workload type readiness Specifies the Readiness Probe to use to determine if the container is ready to accept traffic. - Workspace Training Readiness field details Spec fields readiness Description Specifies the Readiness Probe to use to determine if the container is ready to accept traffic Supported Run:ai workload types Workspace Training Value type itemized Spec Readiness fields Description Value type initialDelaySeconds Number of seconds after the container has started before liveness or readiness probes are initiated. integer periodSeconds How often (in seconds) to perform the probe. integer timeoutSeconds Number of seconds after which the probe times out integer successThreshold Minimum consecutive successes for the probe to be considered successful after having failed. integer failureThreshod When a probe fails, the number of times to try before giving up. integer <p>Example workload snippet:</p> <pre><code>defaults:\n  probes:\n    readiness:\n        initialDelaySeconds: 2\n</code></pre>"},{"location":"platform-admin/workloads/policies/policy-reference/#security-fields","title":"Security fields","text":"Fields Description Value type Supported Run:ai workload type uidGidSource Indicates the way to determine the user and group ids of the container. The options are: <code>fromTheImage</code> - user and group IDs are determined by the docker image that the container runs. This is the default option. <code>custom</code> - user and group IDs can be specified in the environment asset and/or the workspace creation request. <code>idpToken</code> - user and group IDs are determined according to the identity provider (idp) access token. This option is intended for internal use of the environment UI form. For more information, see Non-root containers string Workspace Training capabilities The capabilities field allows adding a set of unix capabilities to the container running the workload. Capabilities are Linux distinct privileges traditionally associated with superuser which can be independently enabled and disabled Array Workspace Training seccompProfileType Indicates which kind of seccomp profile is applied to the container. The options are: RuntimeDefault - the container runtime default profile should be used Unconfined - no profile should be applied string Workspace Training runAsNonRoot Indicates that the container must run as a non-root user. boolean Workspace Training readOnlyRootFilesystem If true, mounts the container's root filesystem as read-only. boolean Workspace Training runAsUid Specifies the Unix user id with which the container running the created workload should run. integer Workspace Training runasGid Specifies the Unix Group ID with which the container should run. integer Workspace Training supplementalGroups Comma separated list of groups that the user running the container belongs to, in addition to the group indicated by runAsGid. string Workspace Training allowPrivilegeEscalation Allows the container running the workload and all launched processes to gain additional privileges after the workload starts boolean Workspace Training hostIpc Whether to enable hostIpc. Defaults to false. boolean Workspace Training hostNetwork Whether to enable host network. boolean Workspace Training"},{"location":"platform-admin/workloads/policies/policy-reference/#compute-fields","title":"Compute fields","text":"Fields Description Value type Supported Run:ai workload type cpuCoreRequest CPU units to allocate for the created workload (0.5, 1, .etc). The workload receives at least this amount of CPU. Note that the workload is not scheduled unless the system can guarantee this amount of CPUs to the workload. number Workspace Training cpuCoreLimit Limitations on the number of CPUs consumed by the workload (0.5, 1, .etc). The system guarantees that this workload is not able to consume more than this amount of CPUs. number Workspace Training cpuMemoryRequest The amount of CPU memory to allocate for this workload (1G, 20M, .etc). The workload receives at least this amount of memory. Note that the workload is not scheduled unless the system can guarantee this amount of memory to the workload quantity Workspace Training cpuMemoryLimit Limitations on the CPU memory to allocate for this workload (1G, 20M, .etc). The system guarantees that this workload is not be able to consume more than this amount of memory. The workload receives an error when trying to allocate more memory than this limit. quantity Workspace Training largeShmRequest A large /dev/shm device to mount into a container running the created workload (shm is a shared file system mounted on RAM). boolean Workspace Training gpuRequestType Sets the unit type for GPU resources requests to either portion, memory or mig profile. Only if <code>gpuDeviceRequest = 1</code>, the request type can be stated as <code>portion</code>, <code>memory</code> or <code>migProfile</code>. string Workspace Training migProfile Specifies the memory profile to be used for workload running on NVIDIA Multi-Instance GPU (MIG) technology. string Workspace Training (Deprecated) gpuPortionRequest Specifies the fraction of GPU to be allocated to the workload, between 0 and 1. For backward compatibility, it also supports the number of gpuDevices larger than 1, currently provided using the gpuDevices field. number Workspace Training gpuDeviceRequest Specifies the number of GPUs to allocate for the created workload. Only if <code>gpuDeviceRequest = 1</code>, the gpuRequestType can be defined. integer Workspace Training gpuPortionLimit When a fraction of a GPU is requested, the GPU limit specifies the portion limit to allocate to the workload. The range of the value is from 0 to 1. number Workspace Training gpuMemoryRequest Specifies GPU memory to allocate for the created workload. The workload receives this amount of memory. Note that the workload is not scheduled unless the system can guarantee this amount of GPU memory to the workload. quantity Workspace Training gpuMemoryLimit Specifies a limit on the GPU memory to allocate for this workload. Should be no less than the gpuMemory. quantity Workspace Training extendedResources Specifies values for extended resources. Extended resources are third-party devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that you want to allocate to your Job. itemized Workspace Training"},{"location":"platform-admin/workloads/policies/policy-reference/#storage-fields","title":"Storage fields","text":"Fields Description Value type Supported Run:ai workload type dataVolume Set of data volumes to use in the workload. Each data volume is mapped to a file-system mount point within the container running the workload. itemized Workspace Training hostPath Maps a folder to a file-system mount point within the container running the workload. itemized Workspace Training git Details of the git repository and items mapped to it. itemized Workspace Training pvc Specifies persistent volume claims to mount into a container running the created workload. itemized Workspace Training nfs Specifies NFS volume to mount into the container running the workload. itemized Workspace Training s3 Specifies S3 buckets to mount into the container running the workload. itemized Workspace Training configMapVolumes Specifies ConfigMaps to mount as volumes into a container running the created workload. itemized Workspace Training secretVolume Set of secret volumes to use in the workload. A secret volume maps a secret resource in the cluster to a file-system mount point within the container running the workload. itemized Workspace Training Storage field details Spec fields hostPath Description Maps a folder to a file system mount oint within the container running the workload Supported Run:ai workload types Workspace Training Value type itemized Git fields Description Value type name Unique name to identify the instance. primarily used for policy locked rules. string path Local path within the controller to which the host volume is mapped. string readOnly Force the volume to be mounted with read-only permissions. Defaults to false. boolean mountPath The path that the host volume is mounted to when in use. string mountPropagation Enum: \"None\" \"HostToContainer\" Share this volume mount with other containers. If set to HostToContainer, this volume mount receives all subsequent mounts that are mounted to this volume or any of its subdirectories. In case of multiple hostPath entries, this field should have the same value for all of them string <p>Example workload snippet:</p> <pre><code>defaults:\n  storage:\n    hostPath:\n      instances:\n        - path: h3-path-1\n          mountPath: h3-mount-1\n        - path: h3-path-2\n          mountPath: h3-mount-2\n      attributes:\n        - readOnly: true\n</code></pre> Spec fields git Description Details of the git repository and items mapped to it. Supported Run:ai workload types Workspace Training Value type itemized Git fields Description Value type repository URL to a remote git repository. The content of this repository is mapped to the container running the workload string revision Specific revision to synchronize the repository from string path Local path within the workspace to which the S3 bucket is mapped. string secretName Optional name of Kubernetes secret that holds your git username and password. string username If secretName is provided, this field should contain the key, within the provided Kubernetes secret, which holds the value of your git username. Otherwise, this field should specify your git username in plain text (example: myuser). string <p>Example workload snippet:</p> <pre><code>defaults:\n  storage:\n    git:\n      attributes:\n        Repository: https://runai.public.github.com\n      instances\n        - branch: \"master\"\n          path: /container/my-repository\n          passwordSecret: my-password-secret\n</code></pre> Spec fields pvc Description Specifies persistent volume claims to mount into a container running the created workload Supported Run:ai workload types Workspace Training Value type itemized Spec PVC fields Description Value type claimName (manadatory) A given name for the PVC. Allowed referencing it across workspaces. string ephemeral Use true to set PVC to ephemeral. If set to true, the PVC is deleted when the workspace is stopped. boolean path Local path within the workspace to which the PVC bucket is mapped. string readonly Permits read only from the PVC, prevents additions or modifications to its content. boolean ReadwriteOnce Requesting claim that can be mounted in read/write mode to exactly 1 host. If none of the modes are specified, the default is readWriteOnce. boolean size Requested size for the PVC. Mandatory when existing PVC is false. string storageClass Storage class name to associate with the PVC. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Further details at Kubernetes storage classes. string readOnlyMany Requesting claim that can be mounted in read-only mode to many hosts. boolean readWriteMany Requesting claim that can be mounted in read/write mode to many hosts. boolean <p>Example workload snippet:</p> <pre><code>defaults:\n  storage:\n    pvc:\n      instances:\n        - claimName: pvc-staging-researcher1-home\n          existingPvc: true\n          path: /myhome\n          readOnly: false\n          claimInfo:\n            accessModes:\n              readWriteMany: true\n</code></pre> Spec fields nfs Description Specifies NFS volume to mount into the container running the workload Supported Run:ai workload types Workspace Training Value type itemized Spec PVC fields Description Value type mountpath The path that the NFS volume is mounted to when in use. string path Path that is exported by the NFS server. string readOnly Whether to force the NFS export to be mounted with read-only permissions. boolean nfsServer The hostname or IP address of the NFS server. string <p>Example workload snippet:</p> <pre><code>defaults:\nstorage:\n  nfs:\n    instances:\n      - path: nfs-path\n        readOnly: true\n        server: nfs-server\n        mountPath: nfs-mount\nrules:\n  storage:\n    nfs:\n      instances:\n        canAdd: false\n</code></pre> Spec fields s3 Description Specifies S3 buckets to mount into the container running the workload Supported Run:ai workload types Workspace Training Value type itemized Spec PVC fields Description Value type Bucket The name of the bucket string path Local path within the workspace to which the S3 bucket is mapped string url The URL of the S3 service provider. The default is the URL of the Amazon AWS Se service string <p>Example workload snippet:</p> <pre><code>defaults:\n  storage:\n    s3:\n      instances:\n        - bucket: bucket-opt-1\n          path: /s3/path\n          accessKeySecret: s3-access-key\n          secretKeyOfAccessKeyId: s3-secret-id\n          secretKeyOfSecretKey: s3-secret-key\n      attributes:\n        url: https://amazonaws.s3.com\n</code></pre>"},{"location":"platform-admin/workloads/policies/policy-reference/#value-types","title":"Value types","text":"<p>Each field has a specific value type. The following value types are supported.</p> Value type Description Supported rule type Defaults Boolean A binary value that can be either True or False canEdit required true/false String A sequence of characters used to represent text. It can include letters, numbers, symbols, and spaces canEdit required options abc Itemized An ordered collection of items (objects), which can be of different types (all items in the list are of the same type). For further information see the chapter below the table. canAdd locked See below Integer An Integer is a whole number without a fractional component. canEdit required min max step 100 Number Capable of having non-integer values canEdit required min 10.3 Quantity Holds a string composed of a number and a unit representing a quantity canEdit required min max 5M Array Set of values that are treated as one, as opposed to Itemized in which each item can be referenced separately. canEdit required node-a node-b node-c"},{"location":"platform-admin/workloads/policies/policy-reference/#itemized","title":"Itemized","text":"<p>Workload fields of type itemized have multiple instances, however in comparison to objects, each can be referenced by a key field. The key field is defined for each field.</p> <p>Consider the following workload spec:</p> <pre><code>spec:\n  image: ubuntu\n  compute:\n    extendedResources:\n      - resource: added/cpu\n        quantity: 10\n      - resource: added/memory\n        quantity: 20M\n</code></pre> <p>In this example, extendedResources have two instances, each has two attributes: resource (the key attribute) and quantity.</p> <p>In policy, the defaults and rules for itemized fields have two sub sections:</p> <ul> <li>Instances: default items to be added to the policy or rules which apply to an instance as a whole.  </li> <li>Attributes: defaults for attributes within an item or rules which apply to attributes within each item.</li> </ul> <p>Consider the following example:</p> <pre><code>defaults:\n  compute:\n    extendedResources:\n      instances: \n        - resource: default/cpu\n          quantity: 5\n        - resource: default/memory\n          quantity: 4M\n      attributes:\n        quantity: 3\nrules:\n  compute:\n    extendedResources:\n      instances:\n        locked: \n          - default/cpu\n      attributes:\n        quantity: \n          required: true\n</code></pre> <p>Assume the following workload submission is requested:</p> <pre><code>spec:\n  image: ubuntu\n  compute:\n    extendedResources:\n      - resource: default/memory\n        exclude: true\n      - resource: added/cpu\n      - resource: added/memory\n        quantity: 5M\n</code></pre> <p>The effective policy for the above mentioned workload has the following extendedResources instances:</p> Resource Source of the instance Quantity Source of the attribute quantity default/cpu Policy defaults 5 The default of this instance in the policy defaults section added/cpu Submission request 3 The default of the quantity attribute from the attributes section added/memory Submission request 5M Submission request <p>Note</p> <p>The default/memory is not populated to the workload, this is because it has been excluded from the workload using \u201cexclude: true\u201d.</p> <p>A workload submission request cannot exclude the default/cpu resource, as this key is included in the locked rules under the instances section. {#a-workload-submission-request-cannot-exclude-the-default/cpu-resource,-as-this-key-is-included-in-the-locked-rules-under-the-instances-section.}</p>"},{"location":"platform-admin/workloads/policies/policy-reference/#rule-types","title":"Rule types","text":"Rule types Description Supported value types Rule type example canAdd Whether the submission request can add items to an itemized field other than those listed in the policy defaults for this field. itemized <code>storage:   hostPath:      instances:        canAdd: false</code> locked Set of items that the workload is unable to modify or exclude. In this example, a workload policy default is given to HOME and USER, that the submission request cannot modify or exclude from the workload. itemized <code>storage:   hostPath:     Instances:       locked:         - HOME         - USER</code> canEdit Whether the submission request can modify the policy default for this field. In this example, it is assumed that the policy has default for imagePullPolicy. As canEdit is set to false, submission requests are not able to alter this default. string boolean integer number quantity array <code>imagePullPolicy:     canEdit: false</code> required When set to true, the workload must have a value for this field. The value can be obtained from policy defaults. If no value specified in the policy defaults, a value must be specified for this field in the submission request. string boolean integer number quantity array <code>image:     required: true</code> min The minimal value for the field. integer number quantity <code>compute:   gpuDevicesRequest:     min: 3</code> max The maximal value for the field. integer number quantity <code>compute:   gpuMemoryRequest:      max: 2G</code> step The allowed gap between values for this field. In this example the allowed values are: 1, 3, 5, 7 integer number <code>compute:   cpuCoreRequest:     min: 1     max: 7     Step: 2</code> options Set of allowed values for this field. string <code>image:   options:     - value: image-1     - value: image-2</code>"},{"location":"platform-admin/workloads/policies/policy-reference/#policy-spec-sections","title":"Policy Spec Sections","text":"<p>For each field of a specific policy, you can specify both rules and defaults. A policy spec consists of the following sections:</p> <ul> <li>Rules  </li> <li>Defaults  </li> <li>Imposed Assets</li> </ul>"},{"location":"platform-admin/workloads/policies/policy-reference/#rules","title":"Rules","text":"<p>Rules set up constraints on workload policy fields. For example, consider the following policy:</p> <pre><code>rules:\n  compute:\n    gpuDevicesRequest: \n      max: 8\n  security:\n    runAsUid: \n      min: 500\n</code></pre> <p>Such a policy restricts the maximum value for gpuDeviceRequests to 8, and the minimal value for runAsUid, provided in the security section to 500.</p>"},{"location":"platform-admin/workloads/policies/policy-reference/#defaults","title":"Defaults","text":"<p>The defaults section is used for providing defaults for various workload fields. For example, consider the following policy:</p> <pre><code>defaults:\n  imagePullPolicy: Always\n  security:\n    runAsNonRoot: true\n    runAsUid: 500\n</code></pre> <p>Assume a submission request with the following values:</p> <ul> <li>Image: ubuntu  </li> <li>runAsUid: 501</li> </ul> <p>The effective workload that runs has the following set of values:</p> Field Value Source Image Ubuntu Submission request ImagePullPolicy Always Policy defaults security.runAsNonRoot true Policy defaults security.runAsUid 501 Submission request <p>Note</p> <p>It is possible to specify a rule for each field, which states if a submission request is allowed to change the policy default for that given field, for example:</p> <pre><code>defaults:\n  imagePullPolicy: Always\n  security:\n    runAsNonRoot: true\n    runAsUid: 500\nrules:\n  security:\n    runAsUid:\n      canEdit: false\n</code></pre> <p>If this policy is applied, the submission request above fails, as it attempts to change the value of secuirty.runAsUid from 500 (the policy default) to 501 (the value provided in the submission request), which is forbidden due to canEdit rule set to false for this field.</p>"},{"location":"platform-admin/workloads/policies/policy-reference/#imposed-assets","title":"Imposed Assets","text":"<p>Default instances of a storage field can be provided using a datasource containing the details of this storage instance. To add such instances in the policy, specify those asset IDs in the imposedAssets section of the policy.</p> <pre><code>defaults: null\nrules: null\nimposedAssets:\n  - f12c965b-44e9-4ff6-8b43-01d8f9e630cc\n</code></pre> <p>Assets with references to credentials assets (for example: private S3, containing reference to an AccessKey asset) cannot be used as imposedAssets.</p>"},{"location":"platform-admin/workloads/policies/workspaces-policy/","title":"Policies","text":"<p>This article explains the procedure to manage workload policies.</p>"},{"location":"platform-admin/workloads/policies/workspaces-policy/#workload-policies-table","title":"Workload policies table","text":"<p>The Workload policies table can be found under Tools &amp; Settings in the Run:ai platform.</p> <p>Note</p> <p>Workload policies are disabled by default. If you cannot see Workload policies in the menu, then it must be enabled by your administrator, under General Settings \u2192 Workloads \u2192 Policies</p> <p>The Workload policies table provides a list of all the policies defined in the platform, and allows you to manage them.</p> <p></p> <p>The Workload policies table consists of the following columns:</p> Column Description Policy The policy name which is a combination of the policy scope and the policy type Type The policy type is per Run:ai workload type. This allows administrators to set different policies for each workload type. Status Representation of the policy lifecycle (one of the following - \u201cCreating\u2026\u201d, \u201cUpdating\u2026\u201d, \u201cDeleting\u2026\u201d, Ready or Failed) Scope The scope the policy affects. Click the name of the scope to view the organizational tree diagram. You can only view the parts of the organizational tree for which you have permission to view. Created by The user who created the policy Creation time The timestamp for when the policy was created Last updated The last time the policy was updated"},{"location":"platform-admin/workloads/policies/workspaces-policy/#customizing-the-table-view","title":"Customizing the table view","text":"<ul> <li>Filter - Click ADD FILTER, select the column to filter by, and enter the filter values  </li> <li>Search - Click SEARCH and type the value to search by  </li> <li>Sort - Click each column header to sort by  </li> <li>Column selection - Click COLUMNS and select the columns to display in the table  </li> <li>Refresh - Click REFRESH to update the table with the latest data</li> </ul>"},{"location":"platform-admin/workloads/policies/workspaces-policy/#adding-a-policy","title":"Adding a policy","text":"<p>To create a new policy:</p> <ol> <li>Click +NEW POLICY </li> <li>Select a scope </li> <li>Select the workload type </li> <li>Click +POLICY YAML </li> <li>In the YAML editor type or paste a YAML policy with defaults and rules.     You can utilize the following references and examples:  </li> <li>Policy YAML reference </li> <li>Policy YAML examples </li> <li>Click SAVE POLICY</li> </ol>"},{"location":"platform-admin/workloads/policies/workspaces-policy/#editing-a-policy","title":"Editing a policy","text":"<ol> <li>Select the policy you want to edit  </li> <li>Click EDIT </li> <li>Update the policy and click APPLY </li> <li>Click SAVE POLICY</li> </ol>"},{"location":"platform-admin/workloads/policies/workspaces-policy/#troubleshooting","title":"Troubleshooting","text":"<p>Listed below are issues that might occur when creating or editing a policy via the YAML Editor:</p> Issue Message Mitigation Cluster connectivity issues There's no communication from cluster \u201ccluster_name\u201c. Actions may be affected, and the data may be stale. Verify that you are on a network that has been allowed access to the cluster. Reach out to your cluster administrator for instructions on verifying the issue. Policy can\u2019t be applied due to a rule that is occupied by a different policy Field \u201cfield_name\u201d already has rules in cluster: \u201ccluster_id\u201d Remove the rule from the new policy or adjust the old policy for the specific rule. Policy is not visible in the UI - Check that the policy hasn\u2019t been deleted. Policy syntax is no valid Add a valid policy YAML;json: unknown field \"field_name\" For correct syntax check the Policy YAML reference or the Policy YAML examples. Policy can\u2019t be saved for some reason The policy couldn't be saved due to a network or other unknown issue. Download your draft and try pasting and saving it again later. Possible cluster connectivity issues. Try updating the policy once again at a different time. Policies were submitted before version 2.18, you upgraded to version 2.18 or above and wish to submit new policies If you have policies and want to create a new one, first contact Run:ai support to prevent potential conflicts Contact Run:ai support. R&amp;D can migrate your old policies to the new version."},{"location":"platform-admin/workloads/policies/workspaces-policy/#viewing-a-policy","title":"Viewing a policy","text":"<p>To view a policy:</p> <ol> <li>Select the policy for which you want to view its policies.  </li> <li>Click VIEW POLICY </li> <li>In the Policy form per workload section, view the workload rules and defaults:  <ul> <li>Parameter   The workload submission parameter that Rules and Defaults are applied to  </li> <li>Type (applicable for data sources only)   The data source type (Git, S3, nfs, pvc etc.)  </li> <li>Default   The default value of the Parameter  </li> <li>Rule   Set up constraint on workload policy field  </li> <li>Source   The origin of the applied policy (cluster, department or project)  </li> </ul> </li> </ol> <p>Note</p> <p>Some of the rules and defaults may be derived from policies of a parent cluster and/or department. You can see the source of each rule in the policy form. For more information, check the Scope of effectiveness documentation</p>"},{"location":"platform-admin/workloads/policies/workspaces-policy/#deleting-a-policy","title":"Deleting a policy","text":"<ol> <li>Select the policy you want to delete  </li> <li>Click DELETE </li> <li>On the dialog, click DELETE to confirm the deletion</li> </ol>"},{"location":"platform-admin/workloads/policies/workspaces-policy/#using-api","title":"Using API","text":"<p>Go to the Policies API reference to view the available actions.</p>"}]}