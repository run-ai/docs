{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Run:ai Documentation Library","text":"<p>Welcome to the Run:ai documentation area. For an introduction about what is the Run:ai Platform see Run:ai platform on the run.ai website.</p> <p>The Run:ai documentation is targeting three personas:</p> <ul> <li> <p>Run:ai Administrator - Is responsible for the setup and the day-to-day administration of the product. Administrator documentation can be found here.</p> </li> <li> <p>Researcher - Using Run:ai to submit Jobs. Researcher documentation can be found here.</p> </li> <li> <p>Developer - Using various APIs to manipulate Jobs and integrate with other systems. Developer documentation can be found here.</p> </li> </ul>"},{"location":"#how-to-get-support","title":"How to get support","text":"<p>To get support use the following channels:</p> <ul> <li> <p>On the Run:ai user interface at <code>&lt;company-name&gt;.run.ai</code>, use the 'Contact Support' link on the top right.</p> </li> <li> <p>Or submit a ticket by clicking the button below:</p> </li> </ul> <p>Submit a Ticket</p>"},{"location":"#community","title":"Community","text":"<p>Run:ai provides its customers with access to the Run:ai Customer Community portal in order to submit tickets, track ticket progress and update support cases.</p> <p>Customer Community Portal</p> <p>Reach out to customer support for credentials.</p>"},{"location":"#runai-cloud-status-page","title":"Run:ai Cloud Status Page","text":"<p>Run:ai cloud availability is monitored at status.run.ai.</p>"},{"location":"#collect-logs-to-send-to-support","title":"Collect Logs to Send to Support","text":"<p>As an IT Administrator, you can collect Run:ai logs to send to support:</p> <ul> <li>Install the Run:ai Administrator command-line interface.</li> <li>Run <code>runai-adm collect-logs</code>. The command will generate a compressed file containing all of the existing Run:ai log files.</li> </ul> <p>Note</p> <p>The tar file packages the logs of Run:ai components only. It does not include logs of researcher containers that may contain private information. </p>"},{"location":"#example-code","title":"Example Code","text":"<p>Code for the Docker images referred to on this site is available at https://github.com/run-ai/docs/tree/master/quickstart.</p> <p>The following images are used throughout the documentation:</p> Image Description Source gcr.io/run-ai-demo/quickstart Basic training image. Multi-GPU support https://github.com/run-ai/docs/tree/master/quickstart/main gcr.io/run-ai-demo/quickstart-distributed Distributed training using MPI and Horovod https://github.com/run-ai/docs/tree/master/quickstart/distributed zembutsu/docker-sample-nginx Build (interactive) with Connected Ports https://github.com/zembutsu/docker-sample-nginx gcr.io/run-ai-demo/quickstart-hpo Hyperparameter Optimization https://github.com/run-ai/docs/tree/master/quickstart/hpo gcr.io/run-ai-demo/quickstart-x-forwarding Use X11 forwarding from Docker image https://github.com/run-ai/docs/tree/master/quickstart/x-forwarding gcr.io/run-ai-demo/pycharm-demo Image used for tool integration (PyCharm and VSCode) https://github.com/run-ai/docs/tree/master/quickstart/python%2Bssh gcr.io/run-ai-demo/example-triton-client and  gcr.io/run-ai-demo/example-triton-server Basic Inference https://github.com/run-ai/models/tree/main/models/triton"},{"location":"#contributing-to-the-documentation","title":"Contributing to the documentation","text":"<p>This documentation is made better by a number of individuals from our customer and partner community. If you see something worth fixing, please comment at the bottom of the page or create a pull request via GitHub. The public GitHub repository can be found on the top-right of this page. </p>"},{"location":"Researcher/overview-researcher/","title":"Overview: Researcher Documentation","text":"<p>Researchers use Run:ai to submit jobs. </p> <p>As part of the Researcher documentation you can find:</p> <ul> <li>Quickstart Guides which provide step-by-step guides to Run:ai technology.</li> <li>Command line interface reference documentation.</li> <li>Best Practices for Deep Learning with Run:ai.</li> <li>Information about the Run:ai Scheduler.</li> <li>Using Run:ai with various developer tools. </li> </ul>"},{"location":"Researcher/use-cases/","title":"Use Cases","text":"<p>This is a collection of various client-requested use cases. Each use case is accompanied by a short live-demo video, along with all the files used.</p> <p>Note</p> <p>For the most up-to-date information, check out the official Run:ai use-cases GitHub page.  </p> <ul> <li>MLflow with Run:ai: experiment management is important for Data Scientists. This is a demo of how to set up and use MLflow with Run:ai.  </li> <li>Introduction to Docker: Run:ai runs using Docker images. This is a brief introduction to Docker, image creation, and how to use them in the context of Run:ai. Please also check out the Persistent Environments use case if you wish to keep the creation of Docker images to a minimum.  </li> <li>Tensorboard with Jupyter (ResNet demo): Many Data Scientists like to use Tensorboard to keep an eye on the their current training experiments. They also like to have it side-by-side with Jupyter. In this demo, we will show how to integrate Tensorboard and Jupyter Lab within the context of Run:ai.  </li> <li>Persistent Environments (with Conda/Mamba &amp; Jupyter): Some Data Scientists find creating Docker images for every single one of their environments a bit of a hindrance. They would often prefer the ability to create and alter environments on the fly and to have those environments remain, even after an image has finished running in a job. This demo shows users how they can create and persist Conda/Mamba environments using an NFS.  </li> <li>Weights &amp; Biases with Run:ai: W&amp;B (Weights &amp; Biases) is one of the best tools for experiment tracking and management. W&amp;B is an official Run:ai partner. In this tutorial, we will demo how to use W&amp;B alongside Run:ai</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/","title":"Quickstart: Launch an Inference Workload","text":""},{"location":"Researcher/Walkthroughs/quickstart-inference/#introduction","title":"Introduction","text":"<p>Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output.</p> <p>With Inference, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time.</p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster. There are additional prerequisites for running inference. See cluster installation prerequisites for more information.</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> <li>You must have ML Engineer access rights. See Adding, Updating and Deleting Users for more information.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/quickstart-inference/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named \"team-a\".</li> <li>Allocate 2 GPUs to the Project.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#run-an-inference-workload","title":"Run an Inference Workload","text":"<ul> <li>In the Run:ai user interface go to <code>Inference</code>. If you do not see the <code>Inference</code> section you may not have the required access control, or the inference module is disabled.</li> <li>Select <code>New workload</code> -&gt; <code>Inference</code> on the top right.</li> <li>Select <code>team-a</code> as a project and add an arbitrary name. Use the image <code>gcr.io/run-ai-demo/example-triton-server</code>.</li> <li>Under <code>Resources</code> add 0.5 GPUs.</li> <li>Under <code>Autoscaling</code> select a minimum of 1, a maximum of 2. Use the <code>concurrency</code> autoscaling threshold method. Add a threshold of 3.</li> <li>Add a <code>Container port</code> of <code>8000</code>.</li> </ul> <p>This would start an inference workload for team-a with an allocation of a single GPU.</p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#query-the-inference-server","title":"Query the Inference Server","text":"<p>The specific inference server we just created is accepting queries over port 8000. You can use the Run:ai Triton demo client to send requests to the server:</p> <ul> <li>Find an IP address by running <code>kubectl get svc -n runai-team-a</code>. Use the <code>inference1-00001-private</code> Cluster IP.</li> <li>Replace <code>&lt;IP&gt;</code> below and run:</li> </ul> <pre><code> runai submit inference-client  -i gcr.io/run-ai-demo/example-triton-client \\\n    -- perf_analyzer -m inception_graphdef  -p 3600000 -u  &lt;IP&gt;\n</code></pre> <ul> <li>To see the result, run the following:</li> </ul> <pre><code>runai logs inference-client\n</code></pre>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#view-status-on-the-runai-user-interface","title":"View status on the Run:ai User Interface","text":"<ul> <li>Open the Run:ai user interface.</li> <li>Under Deployments you can view the new Workload. When clicking the workload, note the utilization graphs go up.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#stop-workload","title":"Stop Workload","text":"<p>Use the user interface to delete the workload.</p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#see-also","title":"See also","text":"<ul> <li>You can also create Inference deployments via API. For more information see Submitting Workloads via YAML.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-mig/","title":"Quickstart: Launch Workloads with NVIDIA Dynamic MIG","text":""},{"location":"Researcher/Walkthroughs/quickstart-mig/#introduction","title":"Introduction","text":"<p>A single GPU has a significant amount of memory. Ranging from a couple of gigabytes in older generations and up to 80GB per GPU in the later models of the latest NVIDIA GPU technology. A single GPU also has a vast amount of computing power. </p> <p>This amount of memory and computing power is important for processing large amounts of data, such as in training deep learning models. However, there are quite a few applications that do not need this power. Examples can be inference workloads and the model-creation phase. It would thus be convenient if we could divide up a GPU between various workloads, thus achieving better GPU utilization. </p> <p>Run:ai provides two alternatives for splitting GPUs: Fractions and Dynamic MIG allocation. The focus of this article is Dynamic MIG allocation.  A detailed explanation of the two Run:ai offerings can be found here.</p>"},{"location":"Researcher/Walkthroughs/quickstart-mig/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> <li>A machine with a single available NVIDIA A100 GPU. This can be achieved by allocating filler workloads to the other GPUs on the node, or by using Google Cloud which allows for the creation of a virtual node with a single A100 GPU. </li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-mig/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/quickstart-mig/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Allocate 2 GPUs to the Project.</li> <li>Mark the node as a dynamic MIG node as described here.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-mig/#run-an-inference-workload-single-replica","title":"Run an Inference Workload - Single Replica","text":"<p>At the GPU node level, run: <code>nvidia-smi</code>:</p> <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                   On |\n| N/A   32C    P0    42W / 400W |      0MiB / 40536MiB |     N/A      Default |\n|                               |                      |              Enabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  No MIG devices found                                                       |\n+-----------------------------------------------------------------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>In the highlighted text above, note that:</p> <ul> <li>MIG is enabled (if <code>Enabled</code> has a star next to it, you need to reboot your machine).</li> <li>The GPU is not yet divided into devices.</li> </ul> <p>At the command-line run:</p> <pre><code>runai config project team-a\nrunai submit mig1 -i gcr.io/run-ai-demo/quickstart-cuda  --gpu-memory 10GB\nrunai submit mig2 -i gcr.io/run-ai-demo/quickstart-cuda  --mig-profile 2g.10gb \nrunai submit mig3 -i gcr.io/run-ai-demo/quickstart-cuda  --mig-profile 2g.10gb \n</code></pre> <p>We used two different methods to create MIG partitions: </p> <ol> <li>Stating the amount of GPU memory we require </li> <li>Requiring a partition of explicit size using NVIDIA terminology. </li> </ol> <p>Both methods achieve the same effect. They result in three MIG partitions of 10GB each. You can verify that by running <code>nvidia-smi</code>, at the GPU node level:</p> <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                   On |\n| N/A   47C    P0   194W / 400W |  27254MiB / 40536MiB |     N/A      Default |\n|                               |                      |              Enabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  0    3   0   0  |   9118MiB /  9984MiB | 28      0 |  2   0    1    0    0 |\n|                  |      4MiB / 16383MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n|  0    4   0   1  |   9118MiB /  9984MiB | 28      0 |  2   0    1    0    0 |\n|                  |      4MiB / 16383MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n|  0    5   0   2  |   9016MiB /  9984MiB | 28      0 |  2   0    1    0    0 |\n|                  |      2MiB / 16383MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0    3    0     142213      C   ./quickstart                     9111MiB |\n|    0    4    0     146799      C   ./quickstart                     9111MiB |\n|    0    5    0     132219      C   ./quickstart                     9009MiB |\n+-----------------------------------------------------------------------------+\n</code></pre> <ul> <li>Highlighted above is a list of 3 MIG devices, each 10GB large. Total of 30GB (out of the 40GB on the GPU)</li> <li>You can also run the same command inside one of the containers: <code>runai exec mig1 nvidia-smi</code>. This will show a single device (the only one that the container sees from its point of view).</li> <li>Run: <code>runai list</code> to see the 3 jobs in <code>Running</code> state.</li> </ul> <p>We now want to allocate an interactive job with 20GB. Interactive jobs take precedence over the default training jobs:</p> <p><pre><code>runai submit mig1-int -i gcr.io/run-ai-demo/quickstart-cuda \\\n    --interactive --gpu-memory 20G \n</code></pre> or similarly, <pre><code>runai submit mig1-int -i gcr.io/run-ai-demo/quickstart-cuda \\\n    --interactive --mig-profile 3g.20gb  \n</code></pre></p> <p>Using <code>runai list</code> and <code>nvidia-smi</code> on the host machine, you can see that:</p> <ul> <li>One training job is preempted, and its device is deleted.</li> <li>The new, interactive job starts running.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-overview/","title":"Run:ai Quickstart Guides","text":"<p>Below are a set of Quickstart documents. The purpose of these documents is to get you acquainted with an aspect of Run:ai in the simplest possible form. Follow the Quickstart documents below to learn more:</p> <ul> <li>Unattended training sessions</li> <li>Interactive build sessions</li> <li>Interactive build sessions with externalized services</li> <li>Using GPU Fractions</li> <li>Distributed Training</li> <li>Hyperparameter Optimization</li> <li>Over-Quota, Basic Fairness &amp; Bin Packing</li> <li>Fairness</li> <li>Inference</li> <li>Dynamic MIG</li> </ul> <p>Most quickstarts rely on an image called <code>gcr.io/run-ai-demo/quickstart</code>. The image is based on  TensorFlow Release 20-08. This TensorFlow image has minimal requirements for CUDA and NVIDIA Compute Capability. </p> <p>If your GPUs do not meet these requirements, use <code>gcr.io/run-ai-demo/quickstart:legacy</code> instead. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/","title":"Quickstart: Launch Interactive Build Workloads with Connected Ports","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#introduction","title":"Introduction","text":"<p>This Quickstart is an extension of the Quickstart document: Start and Use Interactive Build Workloads </p> <p>When starting a container with the Run:ai Command-Line Interface (CLI), it is sometimes needed to expose internal ports to the user. Examples are: accessing a Jupyter notebook, using the container from a development environment such as PyCharm. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#exposing-a-container-port","title":"Exposing a Container Port","text":"<p>There are three ways to expose ports in Kubernetes: Port Forwarding, NodePort, and LoadBalancer. The first two will always work. The other requires a special setup by your administrator. The four methods are explained here. </p> <p>The document below provides an example based on Port Forwarding.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#port-forwarding-step-by-step-walkthrough","title":"Port Forwarding, Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named <code>team-a</code>.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#run-workload","title":"Run Workload","text":"<ul> <li>At the command-line run:</li> </ul> <pre><code>runai config project team-a\nrunai submit nginx-test -i zembutsu/docker-sample-nginx --interactive\nrunai port-forward nginx-test --port 8080:80\n</code></pre> <ul> <li>The Job is based on a sample NGINX webserver docker image <code>zembutsu/docker-sample-nginx</code>. Once accessed via a browser, the page shows the container name. </li> <li>Note the interactive flag which means the Job will not have a start or end. It is the Researcher's responsibility to close the Job.  </li> <li>In this example, we have chosen the simplest scheme to expose ports which is port forwarding. We temporarily expose port 8080 to localhost as long as the <code>runai port-forward</code> command is not stopped</li> <li>It is possible to forward traffic from multiple IP addresses by using the \"--address\" parameter. Check the CLI reference for further details. </li> </ul> <p>The result will be:</p> <pre><code>The job 'nginx-test-0' has been submitted successfully\nYou can run `runai describe job nginx-test-0 -p team-a` to check the job status\n\nForwarding from 127.0.0.1:8080 -&gt; 80\nForwarding from [::1]:8080 -&gt; 80\n</code></pre>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#access-the-webserver","title":"Access the Webserver","text":"<p>Open the following in the browser at http://localhost:8080.</p> <p>You should see a web page with the name of the container.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#stop-workload","title":"Stop Workload","text":"<p>Press Ctrl-C in the shell to stop port forwarding. Then delete the Job by running <code>runai delete job nginx-test</code></p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#see-also","title":"See Also","text":"<ul> <li>Develop on Run:ai using Visual Studio Code</li> <li>Develop on Run:ai using PyCharm</li> <li>Use a Jupyter notbook with Run:ai.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/","title":"Quickstart: Launch Interactive Build Workloads","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build/#introduction","title":"Introduction","text":"<p>Deep learning workloads can be divided into two generic types:</p> <ul> <li>Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. </li> <li>Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results.</li> </ul> <p>With this Quickstart you will learn how to:</p> <ul> <li>Use the Run:ai command-line interface (CLI) to start a deep learning Build workload</li> <li>Open an ssh session to the Build workload</li> <li>Stop the Build workload</li> </ul> <p>It is also possible to open ports to specific services within the container. See \"Next Steps\" at the end of this article.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#step-by-step-quickstart","title":"Step by Step Quickstart","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named \"team-a\".</li> <li>Allocate 2 GPUs to the Project.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#run-workload","title":"Run Workload","text":"<ul> <li> <p>At the command-line run:</p> <pre><code>runai config project team-a\nrunai submit build1 -i ubuntu -g 1 --interactive -- sleep infinity\n</code></pre> </li> <li> <p>The job is based on a sample docker image <code>ubuntu</code></p> </li> <li>We named the job build1.</li> <li>Note the interactive flag which means the job will not have a start or end. It is the Researcher's responsibility to close the job. </li> <li>The job is assigned to team-a with an allocation of a single GPU. </li> <li>The command provided is <code>sleep infinity</code>. You must provide a command or the container will start and then exit immediately. Alternatively, replace these flags with <code>--attach</code> to attach immediately to a session.</li> </ul> <p>Follow up on the job's status by running:</p> <pre><code>runai list jobs\n</code></pre> <p>The result:</p> <p></p> <p>Typical statuses you may see:</p> <ul> <li>ContainerCreating - The docker container is being downloaded from the cloud repository</li> <li>Pending - the job is waiting to be scheduled</li> <li>Running - the job is running</li> </ul> <p>A full list of Job statuses can be found here</p> <p>To get additional status on your job run:</p> <pre><code>runai describe job build1\n</code></pre>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#get-a-shell-to-the-container","title":"Get a Shell to the container","text":"<p>Run:</p> <pre><code>runai bash build1\n</code></pre> <p>This should provide a direct shell into the computer</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#view-status-on-the-runai-user-interface","title":"View status on the Run:ai User Interface","text":"<ul> <li>Open the Run:ai user interface.</li> <li>Under \"Jobs\" you can view the new Workload:</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> <pre><code>runai delete job build1\n</code></pre> <p>This would stop the training workload. You can verify this by running <code>runai list jobs</code> again.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#next-steps","title":"Next Steps","text":"<ul> <li>Expose internal ports to your interactive build workload: Quickstart Launch an Interactive Build Workload with Connected Ports.</li> <li>Follow the Quickstart document: Launch Unattended Training Workloads.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/","title":"Quickstart: Launch Distributed Training Workloads","text":""},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#introduction","title":"Introduction","text":"<p>Distributed Training is the ability to split the training of a model among multiple processors. Each processor is called a worker node. Worker nodes work in parallel to speed up model training. Distributed Training should not be confused with multi-GPU training. Multi-GPU training is the allocation of more than a single GPU to your workload which runs on a single container.</p> <p>Getting Distributed Training to work is more complex than multi-GPU training as it requires syncing of data and timing between the different workers. However, it is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Several Deep Learning frameworks support Distributed Training. Horovod is a good example.</p> <p>Run:ai provides the ability to run, manage, and view Distributed Training workloads. The following is a Quickstart document for such a scenario.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster</li> <li>During the installation, you have installed the Kubeflow MPI Operator as specified here</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named \"team-a\".</li> <li>Allocate 2 GPUs to the Project.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#run-training-distributed-workload","title":"Run Training Distributed Workload","text":"<ul> <li>At the command-line run:</li> </ul> <pre><code>runai config project team-a\nrunai submit-dist mpi --workers=2 -g 1 \\\n        -i gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 -e RUNAI_SLEEP_SECS=60\n</code></pre> <ul> <li>We named the Job dist</li> <li>The Job is assigned to team-a</li> <li>There will be two worker pods (--workers=2), each allocated with a single GPU (-g 1)</li> <li>The Job is based on a sample docker image <code>gcr.io/run-ai-demo/quickstart-distributed:v0.3.0</code>.</li> <li>The image contains a startup script that runs a deep learning Horovod-based workload.</li> </ul> <p>Follow up on the Job's status by running:</p> <pre><code>    runai list jobs\n</code></pre> <p>The result:</p> <p></p> <p>The Run:ai scheduler ensures that all pods can run together. You can see the list of workers as well as the main \"launcher\" pod by running:</p> <pre><code>    runai describe job dist\n</code></pre> <p>You will see two worker pods, their status, and on which node they run:</p> <p></p> <p>To see the merged logs of all pods run:</p> <pre><code>    runai logs dist\n</code></pre> <p>Finally, you can delete the distributed training workload by running:</p> <pre><code>    runai delete job dist\n</code></pre>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#run-an-interactive-distributed-workload","title":"Run an Interactive Distributed Workload","text":"<p>It is also possible to run a distributed training Job as \"interactive\". This is useful if you want to test your distributed training Job before committing on a long, unattended training session. To run such a session use:</p> <pre><code>runai submit-dist mpi dist-int --workers=2 -g 1 \\\n        -i gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 --interactive \\\n        -- sh -c \"sleep infinity\" \n</code></pre> <p>When the workers are running run:</p> <pre><code>    runai bash dist-int\n</code></pre> <p>This will provide shell access to the launcher process. From there, you can run your distributed workload. For Horovod version smaller than 0.17.0 run:</p> <pre><code>horovodrun -np $RUNAI_MPI_NUM_WORKERS \\\n        python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n        --model=resnet20 --num_batches=1000000 --data_name cifar10 \\\n        --data_dir /cifar10 --batch_size=64 --variable_update=horovod\n</code></pre> <p>For Horovod version 0.17.0 or later, add the <code>-hostfile</code> flag as follows:</p> <pre><code>horovodrun -np $RUNAI_MPI_NUM_WORKERS -hostfile /etc/mpi/hostfile \\\n        python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n        --model=resnet20 --num_batches=1000000 --data_name cifar10 \\\n        --data_dir /cifar10 --batch_size=64 --variable_update=horovod \n</code></pre> <p>The environment variable <code>RUNAI_MPI_NUM_WORKERS</code> is passed by Run:ai and contains the number of workers provided to the <code>runai submit-dist mpi</code> command (in the above example the value is 2).</p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#see-also","title":"See Also","text":"<ul> <li>The source code of the image used in this Quickstart document is in Github</li> <li>For a full list of the <code>submit-dist mpi</code> options see runai submit-dist mpi</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/","title":"Quickstart: Launch Workloads with GPU Fractions","text":""},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#introduction","title":"Introduction","text":"<p>Run:ai provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU, enabling companies to run more workloads such as computer vision, voice recognition and natural language processing on the same hardware, lowering costs.</p> <p>Run:ai\u2019s fractional GPU system effectively creates logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. This enables several workloads to run in containers side-by-side on the same GPU without interfering with each other. The solution is transparent, simple, and portable; it requires no changes to the containers themselves.</p> <p>A typical use-case could see 2-8 Jobs running on the same GPU, meaning you could do eight times the work with the same hardware. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named \"team-a\".</li> <li>Allocate 1 GPU to the Project.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#run-workload","title":"Run Workload","text":"<ul> <li> <p>At the command-line run:</p> <pre><code>runai config project team-a\n\nrunai submit frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5 --interactive\nrunai submit frac03 -i gcr.io/run-ai-demo/quickstart -g 0.3\n</code></pre> </li> <li> <p>The Jobs are based on a sample docker image <code>gcr.io/run-ai-demo/quickstart</code> the image contains a startup script that runs a deep learning TensorFlow-based workload.</p> </li> <li>We named the Jobs frac05 and frac03 respectively. </li> <li>Note that fractions may or may not use the <code>--interactive</code> flag. Setting the flag means that the Job will not automatically finish. Rather, it is the Researcher's responsibility to delete the Job. Fractions support both Interactive and non-interactive Jobs. </li> <li>The Jobs are assigned to team-a with an allocation of a single GPU. </li> </ul> <p>Follow up on the Job's status by running:</p> <pre><code>runai list jobs\n</code></pre> <p>The result:</p> <p></p> <p>Note that both Jobs were allocated to the same node.</p> <p>When both Jobs are running, bash into one of them:</p> <pre><code>runai bash frac05\n</code></pre> <p>Now, inside the container,  run: </p> <pre><code>nvidia-smi\n</code></pre> <p>The result:</p> <p></p> <p>Notes:</p> <ul> <li>The total memory is circled in red. It should be 50% of the GPUs memory size. In the picture above we see 8GB which is half of the 16GB of Tesla V100 GPUs.</li> <li>The script running on the container is limited by 8GB. In this case, TensorFlow, which tends to allocate almost all of the GPU memory has allocated 7.7GB RAM (and not close to 16 GB). Overallocation beyond 8GB will lead to an out-of-memory exception </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#use-exact-gpu-memory","title":"Use Exact GPU Memory","text":"<p>Instead of requesting a fraction of the GPU, you can ask for specific GPU memory requirements. For example:</p> <pre><code>runai submit  -i gcr.io/run-ai-demo/quickstart --gpu-memory 5G\n</code></pre> <p>Which will provide 5GB of GPU memory. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/","title":"Quickstart: Over-Quota and Bin Packing","text":""},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#goals","title":"Goals","text":"<p>The goal of this Quickstart is to explain the concepts of over-quota and bin-packing (consolidation) and how they help in maximizing cluster utilization: </p> <ul> <li>Show the simplicity of resource provisioning, and how resources are abstracted from users.</li> <li>Show how the system eliminates compute bottlenecks by allowing teams/users to go over their resource quota if there are free GPUs in the cluster.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#setup-and-configuration","title":"Setup and configuration:","text":"<ul> <li>4 GPUs on 2 machines with 2 GPUs each</li> <li>2 Projects: team-a and team-b with 2 allocated GPUs each</li> <li>Run:ai canonical image gcr.io/run-ai-demo/quickstart</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-i-over-quota","title":"Part I: Over-quota","text":"<p>Run the following commands:</p> <pre><code>runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a\nrunai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a\nrunai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <ul> <li>team-a has 3 GPUs allocated. Which is over its quota by 1 GPU. </li> <li>The system allows this over-quota as long as there are available resources</li> <li>The system is at full capacity with all GPUs utilized. </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-2-basic-fairness-via-preemption","title":"Part 2: Basic Fairness via Preemption","text":"<p>Run the following command:</p> <pre><code>runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <ul> <li>team-a can no longer remain in over-quota. Thus, one Job, must be preempted: moved out to allow team-b to grow.</li> <li>Run:ai scheduler chooses to preempt Job a1.</li> <li>It is important that unattended Jobs will save checkpoints. This will ensure that whenever Job a1 resume, it will do so from where it left off.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-3-bin-packing","title":"Part 3: Bin Packing","text":"<p>Run the following command:</p> <pre><code>runai delete job a2 -p team-a\n</code></pre> <p>a1 is now going to start running again.</p> <p>Run:</p> <pre><code>runai list jobs -A\n</code></pre> <p>You have two Jobs that are running on the first node and one Job that is running alone the second node. </p> <p>Choose one of the two Jobs from the full node and delete it:</p> <pre><code>runai delete job &lt;job-name&gt; -p &lt;project&gt;\n</code></pre> <p>The status now is: </p> <p>Now, run a 2 GPU Job:</p> <pre><code>runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a\n</code></pre> <p>The status now is: </p> <p>Discussion</p> <p>Note that Job a1 has been preempted and then restarted on the second node, in order to clear space for the new a2 Job. This is bin-packing or consolidation</p>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/","title":"Quickstart: Queue Fairness","text":""},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#goal","title":"Goal","text":"<p>The goal of this Quickstart is to explain fairness. The over-quota Quickstart shows basic fairness where allocated GPUs per Project are adhered to such that if a Project is in over-quota, its Job will be preempted once another Project requires its resources.</p> <p>This Quickstart is about queue fairness. It shows that Jobs will be scheduled fairly regardless of the time they have been submitted. As such, if a person in Project A has submitted 50 Jobs and soon after that, a person in Project B has submitted 25 Jobs, the Jobs in the queue will be processed fairly.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#setup-and-configuration","title":"Setup and configuration:","text":"<ul> <li>4 GPUs on 2 machines with 2 GPUs each.</li> <li>2 Projects: team-a and team-b with 1 allocated GPU each.</li> <li>Run:ai canonical image gcr.io/run-ai-demo/quickstart</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-i-immediate-displacement-of-over-quota","title":"Part I: Immediate Displacement of Over-Quota","text":"<p>Run the following commands:</p> <pre><code>runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a\nrunai submit a2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a\nrunai submit a3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a\nrunai submit a4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <p>team-a, even though it has a single GPU as quota, is now using all 4 GPUs.</p> <p>Run the following commands:</p> <pre><code>runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b\nrunai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b\nrunai submit b3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b\nrunai submit b4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <ul> <li>Two team-b Jobs have immediately displaced team-a. </li> <li>team-a and team-b each have a quota of 1 GPU, thus the remaining over-quota (2 GPUs) is distributed equally between the Projects.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-2-queue-fairness","title":"Part 2: Queue Fairness","text":"<p>Now lets start deleting Jobs. Alternatively, you can wait for Jobs to complete.</p> <pre><code>runai delete job b2 -p team-b\n</code></pre> <p>Discussion</p> <p>As the quotas are equal (1 for each Project, the remaining pending Jobs will get scheduled one by one alternating between Projects, regardless of the time in which they were submitted. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/","title":"Quickstart: Launch Unattended Training Workloads","text":""},{"location":"Researcher/Walkthroughs/walkthrough-train/#introduction","title":"Introduction","text":"<p>Deep learning workloads can be divided into two generic types:</p> <ul> <li>Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly.</li> <li>Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results.</li> </ul> <p>With this Quickstart you will learn how to:</p> <ul> <li>Use the Run:ai command-line interface (CLI) to start a deep learning training workload.</li> <li>View training status and resource consumption using the Run:ai user interface and the Run:ai CLI.</li> <li>View training logs.</li> <li>Stop the training.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-train/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named \"team-a\".</li> <li>Allocate 2 GPUs to the Project.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#run-workload","title":"Run Workload","text":"<ul> <li>At the command-line run:<pre><code>runai config project team-a\nrunai submit train1 -i gcr.io/run-ai-demo/quickstart -g 1\n</code></pre> </li> </ul> <p>This would start an unattended training Job for team-a with an allocation of a single GPU. The Job is based on a sample docker image <code>gcr.io/run-ai-demo/quickstart</code>. We named the Job <code>train1</code></p> <ul> <li>Follow up on the Job's progress by running:<pre><code>runai list jobs\n</code></pre> </li> </ul> <p>The result:</p> <p></p> <p>Typical statuses you may see:</p> <ul> <li>ContainerCreating - The docker container is being downloaded from the cloud repository</li> <li>Pending - the Job is waiting to be scheduled</li> <li>Running - the Job is running</li> <li>Succeeded - the Job has ended</li> </ul> <p>A full list of Job statuses can be found here </p> <p>To get additional status on your Job run:</p> <pre><code>runai describe job train1\n</code></pre>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#view-logs","title":"View Logs","text":"<p>Run the following:</p> <pre><code>runai logs train1\n</code></pre> <p>You should see a log of a running deep learning session:</p> <p></p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#view-status-on-the-runai-user-interface","title":"View status on the Run:ai User Interface","text":"<ul> <li>Open the Run:ai user interface.</li> <li>Under \"Jobs\" you can view the new Workload:</li> </ul> <p>The image we used for training includes the Run:ai Training library. Among other features, this library allows the reporting of metrics from within the deep learning Job. Metrics such as progress, accuracy, loss, and epoch and step numbers.  </p> <ul> <li>Progress can be seen in the status column above. </li> <li>To see other metrics, press the settings wheel on the top right  and select additional deep learning metrics from the list</li> </ul> <p>Under Nodes you can see node utilization:</p> <p></p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> <pre><code>runai delete job train1\n</code></pre> <p>This would stop the training workload. You can verify this by running <code>runai list jobs</code> again.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Quickstart document: Launch Interactive Workloads</li> <li>Use your container to run an unattended training workload</li> </ul>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/","title":"Best Practice: From Bare Metal to Docker Images","text":""},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#introduction","title":"Introduction","text":"<p>Some Researchers do data science on bare metal. The term bare-metal relates to connecting to a server and working directly on its operating system and disks.</p> <p>This is the fastest way to start working, but it introduces problems when the data science organization scales:</p> <ul> <li>More Researchers mean that the machine resources need to be efficiently shared</li> <li>Researchers need to collaborate and share data, code, and results</li> </ul> <p>To overcome that, people working on bare-metal typically write scripts to gather data, code as well as code dependencies. This soon becomes an overwhelming task.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#why-use-docker-images","title":"Why Use Docker Images?","text":"<p>Docker images and containerization in general provide a level of abstraction which, by large, frees developers and Researchers from the mundane tasks of setting up an environment. The image is an operating system by itself and thus the 'environment' is by large, a part of the image.</p> <p>When a docker image is instantiated, it creates a container. A container is the running manifestation of a docker image.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#moving-a-data-science-environment-to-docker","title":"Moving a Data Science Environment to Docker","text":"<p>A data science environment typically includes:</p> <li>Training data</li> <li>Machine Learning (ML) code and inputs</li> <li>Libraries: Code dependencies that must be installed before the ML code can be run</li>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#training-data","title":"Training data","text":"<p>Training data is usually significantly large (from several Gigabytes to Petabytes) and is read-only in nature. Thus, training data is typically left outside of the docker image. Instead, the data is mounted onto the image when it is instantiated. Mounting a volume allows the code within the container to access the data as though it was within a directory on the local file system.</p> <p>The best practice is to store the training data on a shared file system. This allows the data to be accessed uniformly on whichever machine the Researcher is currently using, allowing the Researcher to easily migrate between machines. </p> <p>Organizations without a shared file system typically write scripts to copy data from machine to machine.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#machine-learning-code-and-inputs","title":"Machine Learning Code and Inputs","text":"<p>As a rule, code needs to be saved and versioned in a code repository.</p> <p>There are two alternative practices:</p> <ul> <li>The code resides in the image and is being periodically pulled from the repository. This practice requires building a new container image each time a change is introduced to the code.</li> <li>When a shared file system exists, the code can reside outside the image on a shared disk and mounted via a volume onto the container. </li> </ul> <p>Both practices are valid.</p> <p>Inputs to machine learning models and artifacts of training sessions, like model checkpoints, are also better stored in and loaded from a shared file system.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#code-dependencies","title":"Code Dependencies","text":"<p>Any code has code dependencies. These libraries must be installed for the code to run. As the code is changing, so do the dependencies.</p> <p>ML Code is typically python and python dependencies are typically declared together in a single <code>requirements.txt</code> file which is saved together with the code.</p> <p>The best practice is to have your docker startup script (see below) run this file using <code>pip install -r requirements.txt</code>. This allows the flexibility of adding and removing code dependencies dynamically.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#ml-lifecycle-build-and-train","title":"ML Lifecycle: Build and Train","text":"<p>Deep learning workloads can be divided into two generic types:</p> <li>Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter Notebook, remote PyCharm, or similar and accesses GPU resources directly. Build workloads are typically meant for debugging and development sessions. </li> <li>Unattended \"training\" sessions. Training is characterized by a machine learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the data scientist can examine the results. A Training session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU and at the end of the run automatically frees the resources. </li> <p>Getting your docker ready is also a matter of which type of workload you are currently running.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#build-workloads","title":"Build Workloads","text":"<p>With \"build\" you are actually coding and debugging small experiments. You are interactive. In that mode, you can typically take a well known standard image (e.g. https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow) and use it directly.</p> <p>Start a docker container by running:</p> <pre><code>docker run -it .... \"the well known image\" -v /where/my/code/resides bash </code></pre> <p>You get a shell prompt to a container with a mounted volume of where your code is. You can then install your prerequisites and run your code via ssh.</p> <p>You can also access the container remotely from tools such as PyCharm, Jupyter Notebook, and more. In this case, the docker image needs to be customized to install the \"server software\" (e.g. a Jupyter Notebook service).</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#training-workloads","title":"Training Workloads","text":"<p>For training workloads, you can use a well-known image (e.g. the TensorFlow image from the link above) but more often than not, you want to create your own docker image. The best practice is to use the well-known image (e.g. TensorFlow from above) as a base image and add your own customizations on top of it. To achieve that, you create a Dockerfile. A Dockerfile is a declarative way to build a docker image and is built in layers. e.g.:</p> <ol><li>Base image is nvidia-tensorflow</li> <li>Install popular software</li> <li>(Optional) Run a script</li> </ol> <p>The script can be part of the image or can be provided as part of the command line to run the docker. It will typically include additional dependencies to install as well as a reference to the ML code to be run. </p> <p>The best practice for running training workloads is to test the container image in a \"build\" session and then send it for execution as a training Job. For further information on how to set up and parameterize a training workload via docker or Run:ai see Converting your Workload to use Unattended Training Execution.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/","title":"Best Practice: Convert your Workload to Run Unattended","text":""},{"location":"Researcher/best-practices/convert-to-unattended/#motivation","title":"Motivation","text":"<p>Run:ai allows non-interactive training workloads to extend beyond guaranteed quotas and into over-quota as long as computing resources are available. To achieve this kind of flexibility, the system needs to be able to safely stop a workload and restart it again later. This requires Researchers to switch workloads from running interactively, to running unattended, thus allowing Run:ai to pause/resume the run.</p> <p>Unattended workloads are a good fit for long-duration runs, or sets of smaller hyperparameter optimization runs.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#best-practices","title":"Best Practices","text":""},{"location":"Researcher/best-practices/convert-to-unattended/#docker-image","title":"Docker Image","text":"<p>A docker container is based on a docker image. Some Researchers use generic images such as ones provided by Nvidia, for example: NVIDIA NGC TensorFlow. Others, use generic images as the base image to a more customized image using Dockerfiles.</p> <p>Realizing that Researchers are not always proficient with building docker files, as a best practice, you will want to:</p> <ul> <li>Use the same docker image both for interactive and unattended jobs. In this way, you can keep the difference between both methods of invocation to a minimum. This can be a stock image from Nvidia or a custom image.</li> <li>Leave some degree of flexibility, which allows the Researcher to add/remove python dependencies without re-creating images.</li> </ul>"},{"location":"Researcher/best-practices/convert-to-unattended/#code-location","title":"Code Location","text":"<p>You will want to minimize the cycle of code change-and-run. There are a couple of best practices which you can choose from:</p> <ol> <li>Code resides on the network file storage. This way you can change the code and immediately run the Job. The Job picks up the new files from the network.</li> <li>Use the <code>runai submit</code> flag <code>--git-sync</code>. The flag allows the Researcher to provide details of a Git repository. The repository will be automatically cloned into a specified directory when the container starts.</li> <li>The code can be embedded within the image. In this case, you will want to create an automatic CI/CD process, which packages the code into a modified image.</li> </ol> <p>The document below assumes option #1.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#create-a-startup-script","title":"Create a Startup Script","text":"<p>Gather the commands you ran inside the interactive Job into a single script. The script will be provided with the command-line at the start of the unattended execution (see the section running the job below). This script should be kept next to your code, on a shared network drive (e.g. /nfs/john).</p> <p>An example of a common startup script start.sh:</p> <pre><code>pip install -r requirements.txt\n...\npython training.py\n</code></pre> <p>The first line of this script is there to make sure that all required python libraries are installed before the training script executes, it also allows the Researcher to add/remove libraries without needing changes to the image itself.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#support-variance-between-different-runs","title":"Support Variance Between Different Runs","text":"<p>Your training script must be flexible enough to support variance in execution without changing the code. For example, you will want to change the number of epochs to run, apply a different set of hyperparameters, etc. There are two ways to handle this in your script. You can use one or both methods:</p> <ol> <li> <p>Your script can read arguments passed to the script:</p> <p><pre><code>python training.py --number-of-epochs=30</code></pre></p> </li> </ol> <p>In which case, change your start.sh script to:</p> <pre><code>pip install -r requirements.txt\n...\npython training.py $@</code></pre> <ol> <li>Your script can read from environment variables during script execution. In case you use environment variables, the variables will be passed to the training script automatically. No special action is required in this case.</li> </ol>"},{"location":"Researcher/best-practices/convert-to-unattended/#checkpoints","title":"Checkpoints","text":"<p>Run:ai can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:ai will give you back the resources and restore your workload. Thus, it is a good practice to save your weights at various checkpoints and start a workload from the latest checkpoint (typically between epochs).</p> <p>TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for PyTorch).</p> <p>It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node</p> <p>For more information on best practices for saving checkpoints, see Saving Deep Learning Checkpoints.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#running-the-job","title":"Running the Job","text":"<p>Using <code>runai submit</code>, drop the flag <code>--interactive</code>. For submitting a Job using the script created above, please use <code>-- [COMMAND]</code> flag to specify a command, use the <code>--</code> syntax to pass arguments, and pass environment variables using the flag <code>--environment</code>.</p> <p>Example with Environment variables:</p> <pre><code>runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3  \n    -v /nfs/john:/mydir -g 1  --working-dir /mydir/  \n    -e 'EPOCHS=30'  -e 'LEARNING_RATE=0.02'  \n    -- ./startup.sh  \n</code></pre> <p>Example with Command-line arguments:</p> <pre><code>runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3  \n    -v /nfs/john:/mydir -g 1  --working-dir /mydir/  \n    -- ./startup.sh batch-size=64 number-of-epochs=3\n</code></pre> <p>Please refer to Command-Line Interface, runai submit for a list of all arguments accepted by the Run:ai CLI.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#use-cli-policies","title":"Use CLI Policies","text":"<p>Different run configurations may vary significantly and can be tedious to be written each time on the command-line. To make life easier, our CLI offers a way to set administrator policies for these configurations and use pre-configured configuration when submitting a Workload. Please refer to Configure Command-Line Interface Policies.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#attached-files","title":"Attached Files","text":"<p>The 3 relevant files mentioned in this document can be downloaded from Github</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#see-also","title":"See Also","text":"<p>See the unattended training Quickstart: Launch Unattended Training Workloads</p>"},{"location":"Researcher/best-practices/env-variables/","title":"Environment Variables inside a Run:ai Workload","text":""},{"location":"Researcher/best-practices/env-variables/#identifying-a-job","title":"Identifying a Job","text":"<p>There may be use cases where your container may need to uniquely identify the Job it is currently running in. A typical use case is for saving Job artifacts under a unique name.  Run:ai provides pre-defined environment variables you can use. These variables are guaranteed to be unique even if the Job is preempted or evicted and then runs again. </p> <p>Run:ai provides the following environment variables:</p> <ul> <li><code>JOB_NAME</code> - the name of the Job.</li> <li><code>JOB_UUID</code> - a unique identifier for the Job. </li> </ul> <p>Note that the Job can be deleted and then recreated with the same name. A Job UUID will be different even if the Job names are the same.</p>"},{"location":"Researcher/best-practices/env-variables/#identifying-a-pod","title":"Identifying a Pod","text":"<p>With Hyperparameter Optimization, experiments are run as Pods within the Job. Run:ai provides the following environment variables to identify the Pod.</p> <ul> <li><code>POD_INDEX</code> -  An index number (0, 1, 2, 3....) for a specific Pod within the Job. This is useful for Hyperparameter Optimization to allow easy mapping to individual experiments. The Pod index will remain the same if restarted (due to a failure or preemption). Therefore, it can be used by the Researcher to identify experiments. </li> <li><code>POD_UUID</code> - a unique identifier for the Pod. if the Pod is restarted, the Pod UUID will change.</li> </ul>"},{"location":"Researcher/best-practices/env-variables/#gpu-allocation","title":"GPU Allocation","text":"<p>Run:ai provides an environment variable, visible inside the container, to help identify the number of GPUs allocated for the container. Use <code>RUNAI_NUM_OF_GPUS</code></p>"},{"location":"Researcher/best-practices/env-variables/#node-name","title":"Node Name","text":"<p>There may be use cases where your container may need to identify the node it is currently running on. Run:ai provides an environment variable, visible inside the container, to help identify the name of the node on which the pod was scheduled. Use <code>NODE_NAME</code></p>"},{"location":"Researcher/best-practices/env-variables/#usage-example-in-python","title":"Usage Example in Python","text":"<pre><code>import os\n\njobName = os.environ['JOB_NAME']\njobUUID = os.environ['JOB_UUID']\n</code></pre>"},{"location":"Researcher/best-practices/save-dl-checkpoints/","title":"Best Practice: Save Deep-Learning Checkpoints","text":""},{"location":"Researcher/best-practices/save-dl-checkpoints/#introduction","title":"Introduction","text":"<p>Run:ai can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:ai will give you back the resources and restore your workload. Thus, it is a good practice to save the state of your run at various checkpoints and start a workload from the latest checkpoint (typically between epochs).</p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#how-to-save-checkpoints","title":"How to Save Checkpoints","text":"<p>TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for PyTorch).</p> <p>This document uses Keras as an example. The code itself can be found here</p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#where-to-save-checkpoints","title":"Where to Save Checkpoints","text":"<p>It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node. Example:</p> <pre><code>runai submit train-with-checkpoints -i tensorflow/tensorflow:1.14.0-gpu-py3 \\\n  -v /mnt/nfs_share/john:/mydir -g 1  --working-dir /mydir --command -- ./startup.sh\n</code></pre> <p>The command saves the checkpoints in an NFS checkpoints folder <code>/mnt/nfs_share/john</code></p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#when-to-save-checkpoints","title":"When to Save Checkpoints","text":""},{"location":"Researcher/best-practices/save-dl-checkpoints/#save-periodically","title":"Save Periodically","text":"<p>It is a best practice to save checkpoints at intervals. For example, every epoch as the Keras code below shows:</p> <pre><code>checkpoints_file = \"weights.best.hdf5\"\ncheckpoint = ModelCheckpoint(checkpoints_file, monitor='val_acc', verbose=1, \n    save_best_only=True, mode='max')\n</code></pre>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#save-on-exit-signal","title":"Save on Exit Signal","text":"<p>If periodic checkpoints are not enough, you can use a signal-hook provided by Run:ai (via Kubernetes). The hook is python code that is called before your Job is suspended and allows you to save your checkpoints as well as other state data you may wish to store.</p> <pre><code>import signal\nimport time\n\ndef graceful_exit_handler(signum, frame):\n    # save your checkpoints to shared storage\n\n    # exit with status \"1\" is important for the Job to return later.  \n    exit(1)\n\nsignal.signal(signal.SIGTERM, graceful_exit_handler)\n</code></pre> <p>By default, you will have 30 seconds to save your checkpoints.</p> <p>Important</p> <p>For the signal to be captured, it must be propagated from the startup script to the python child process. See code here</p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#resuming-using-saved-checkpoints","title":"Resuming using Saved Checkpoints","text":"<p>A Run:ai unattended workload that is resumed, will run the same startup script as on the first run. It is the responsibility of the script developer to add code that:</p> <ul> <li>Checks if saved checkpoints exist (see above)</li> <li>If saved checkpoints exist, load them and start the run using these checkpoints</li> </ul> <pre><code>import os\n\ncheckpoints_file = \"weights.best.hdf5\"\nif os.path.isfile(checkpoints_file):\n    print(\"loading checkpoint file: \" + checkpoints_file)\n    model.load_weights(checkpoints_file)\n</code></pre>"},{"location":"Researcher/cli-reference/Introduction/","title":"Introduction","text":"<p>The Run:ai Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc.</p> <p>To install and configure the Run:ai CLI see Researcher Setup - Start Here</p>"},{"location":"Researcher/cli-reference/runai-attach/","title":"runai attach","text":""},{"location":"Researcher/cli-reference/runai-attach/#description","title":"Description","text":"<p>Attach to a running Job.</p> <p>The command attaches to the standard input, output, and error streams of a running Job. If the Job has multiple pods the job will attach to the first pod unless otherwise set.</p>"},{"location":"Researcher/cli-reference/runai-attach/#synopsis","title":"Synopsis","text":"<pre><code>runai attach &lt;job-name&gt;\n    [--no-stdin ]\n    [--no-tty]   \n    [--pod string]\n    .\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-attach/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-attach/#-no-stdin","title":"--no-stdin","text":"<p>Do not attach STDIN.</p>"},{"location":"Researcher/cli-reference/runai-attach/#-no-tty","title":"--no-tty","text":"<p>Do not allocate a pseudo-TTY</p>"},{"location":"Researcher/cli-reference/runai-attach/#-pod-string","title":"--pod string","text":"<p>Attach to a specific pod within the Job. To find the list of pods run <code>runai describe job &lt;job-name&gt;</code> and then use the pod name with the <code>--pod</code> flag.</p>"},{"location":"Researcher/cli-reference/runai-attach/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-attach/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-attach/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-attach/#output","title":"Output","text":"<p>None</p>"},{"location":"Researcher/cli-reference/runai-bash/","title":"runai bash","text":""},{"location":"Researcher/cli-reference/runai-bash/#description","title":"Description","text":"<p>Get a bash session inside a running Job</p> <p>This command is a shortcut to runai exec (<code>runai exec -it job-name bash</code>). See runai exec for full documentation of the exec command.</p>"},{"location":"Researcher/cli-reference/runai-bash/#synopsis","title":"Synopsis","text":"<pre><code>runai bash &lt;job-name&gt; \n    [--pod string]\n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-bash/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-bash/#-pod-string","title":"--pod string","text":"<p>Specify a pod of a running Job. To get a list of the pods of a specific Job, run <code>runai describe job &lt;job-name&gt;</code> command</p>"},{"location":"Researcher/cli-reference/runai-bash/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-bash/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>"},{"location":"Researcher/cli-reference/runai-bash/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-bash/#-help-h","title":"--help | -h","text":"<p>Show help text</p>"},{"location":"Researcher/cli-reference/runai-bash/#output","title":"Output","text":"<p>The command will access the container that should be currently running in the current cluster and attempt to create a command-line shell based on bash.</p> <p>The command will return an error if the container does not exist or has not been in a running state yet.</p>"},{"location":"Researcher/cli-reference/runai-bash/#see-also","title":"See also","text":"<p>Build Workloads. See Quickstart document: Launch Interactive Build Workloads.</p>"},{"location":"Researcher/cli-reference/runai-config/","title":"runai config","text":""},{"location":"Researcher/cli-reference/runai-config/#description","title":"Description","text":"<p>Set a default Project or Cluster</p>"},{"location":"Researcher/cli-reference/runai-config/#synopsis","title":"Synopsis","text":"<pre><code>runai  config project &lt;project-name&gt;\n    [--loglevel value] \n    [--help | -h]\n\nrunai  config cluster &lt;cluster-name&gt;\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-config/#options","title":"Options","text":"<p>&lt;project-name&gt;  - The name of the Project you want to set as default. Mandatory.</p> <p>&lt;cluster-name&gt; - The name of the cluster you want to set as the current cluster. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-config/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-config/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-config/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-config/#output","title":"Output","text":"<p>None</p>"},{"location":"Researcher/cli-reference/runai-delete/","title":"runai delete","text":""},{"location":"Researcher/cli-reference/runai-delete/#description","title":"Description","text":"<p>Delete a Workload and its associated Pods.</p> <p>Note that once you delete a Workload, its entire data will be gone:</p> <ul> <li>You will no longer be able to enter it via bash.</li> <li>You will no longer be able to access logs.</li> <li>Any data saved on the container and not stored in a shared location will be lost.</li> </ul>"},{"location":"Researcher/cli-reference/runai-delete/#synopsis","title":"Synopsis","text":"<pre><code>runai delete job &lt;job-name&gt; \n    [--all | -A]\n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-delete/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Workload to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-delete/#-all-a","title":"--all | -A","text":"<p>Delete all Workloads.</p>"},{"location":"Researcher/cli-reference/runai-delete/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-delete/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-delete/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-delete/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-delete/#output","title":"Output","text":"<ul> <li> <p>The Workload will be deleted and not available via the command runai list jobs.</p> </li> <li> <p>The Workloads will show as <code>deleted</code> from the Run:ai user interface Job list.</p> </li> </ul>"},{"location":"Researcher/cli-reference/runai-delete/#see-also","title":"See Also","text":"<ul> <li> <p>Build Workloads. See Quickstart document: Launch Interactive Build Workloads.</p> </li> <li> <p>Training Workloads. See Quickstart document:  Launch Unattended Training Workloads.</p> </li> </ul>"},{"location":"Researcher/cli-reference/runai-describe/","title":"runai describe","text":""},{"location":"Researcher/cli-reference/runai-describe/#description","title":"Description","text":"<p>Display details of a Workload or Node.</p>"},{"location":"Researcher/cli-reference/runai-describe/#synopsis","title":"Synopsis","text":"<pre><code>runai describe job &lt;job-name&gt; \n    [--output value | -o value]  \n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n    [--output string | -o string]  \n\n\nrunai describe node [node-name] \n\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-describe/#options","title":"Options","text":"<ul> <li>&lt;job-name&gt; - The name of the Workload to run the command with. Mandatory.</li> <li>&lt;node-name&gt; - The name of the Node to run the command with. If a Node name is not specified, a description of all Nodes is shown.</li> </ul> <p>-o | --output</p> <p>Output format. One of: json|yaml|wide. Default is 'wide'</p>"},{"location":"Researcher/cli-reference/runai-describe/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-describe/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-describe/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project, use: <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-describe/#-help-h","title":"--help | -h","text":"<p>Show help text</p>"},{"location":"Researcher/cli-reference/runai-describe/#output","title":"Output","text":"<ul> <li>The <code>runai describe job</code> command will show Workload properties and status as well as lifecycle events and the list of related resources and pods.</li> <li>The <code>runai describe node</code> command will show Node properties. </li> </ul>"},{"location":"Researcher/cli-reference/runai-exec/","title":"runai exec","text":""},{"location":"Researcher/cli-reference/runai-exec/#description","title":"Description","text":"<p>Execute a command inside a running Job</p> <p>Note: to execute a bash command, you can also use the shorthand runai bash</p>"},{"location":"Researcher/cli-reference/runai-exec/#synopsis","title":"Synopsis","text":"<pre><code>runai exec &lt;job-name&gt; &lt;command&gt; \n    [--stdin | -i] \n    [--tty | -t]\n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-exec/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p> <p>&lt;command&gt; the command itself (e.g. bash).</p>"},{"location":"Researcher/cli-reference/runai-exec/#-stdin-i","title":"--stdin | -i","text":"<p>Keep STDIN open even if not attached.</p>"},{"location":"Researcher/cli-reference/runai-exec/#-tty-t","title":"--tty | -t","text":"<p>Allocate a pseudo-TTY.</p>"},{"location":"Researcher/cli-reference/runai-exec/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-exec/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-exec/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-exec/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-exec/#output","title":"Output","text":"<p>The command will run in the context of the container.</p>"},{"location":"Researcher/cli-reference/runai-exec/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-list/","title":"runai list","text":""},{"location":"Researcher/cli-reference/runai-list/#description","title":"Description","text":"<p>Show lists of Workloads, Projects, Clusters or Nodes.</p>"},{"location":"Researcher/cli-reference/runai-list/#synopsis","title":"Synopsis","text":"<pre><code>runai list jobs \n    [--all-projects | -A]  \n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n\nrunai list projects \n    [--loglevel value] \n    [--help | -h]\n\nrunai list clusters  \n    [--loglevel value] \n    [--help | -h]\n\nrunai list nodes [node-name]\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-list/#options","title":"Options","text":"<p><code>node-name</code> - Name of a specific node to list (optional).</p>"},{"location":"Researcher/cli-reference/runai-list/#-all-projects-a","title":"--all-projects | -A","text":"<p>Show Workloads from all Projects.</p>"},{"location":"Researcher/cli-reference/runai-list/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-list/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-list/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-list/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-list/#output","title":"Output","text":"<ul> <li>A list of Workloads, Nodes, Projects, or Clusters. </li> <li>To filter 'runai list nodes' for a specific Node, add the Node name.</li> </ul>"},{"location":"Researcher/cli-reference/runai-list/#see-also","title":"See Also","text":"<p>To show details for a specific Workload or Node see runai describe.</p>"},{"location":"Researcher/cli-reference/runai-login/","title":"runai login","text":""},{"location":"Researcher/cli-reference/runai-login/#description","title":"Description","text":"<p>Login to Run:ai</p> <p>When Researcher Authentication is enabled, you will need to login to Run:ai using your username and password before accessing resources </p>"},{"location":"Researcher/cli-reference/runai-login/#synopsis","title":"Synopsis","text":"<pre><code>runai login \n    [--loglevel value]\n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-login/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-login/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-login/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-login/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-login/#output","title":"Output","text":"<p>You will be prompted for a user name and password</p>"},{"location":"Researcher/cli-reference/runai-login/#see-also","title":"See Also","text":"<ul> <li>runai logout.</li> </ul>"},{"location":"Researcher/cli-reference/runai-logout/","title":"runai logout","text":""},{"location":"Researcher/cli-reference/runai-logout/#description","title":"Description","text":"<p>Log out from Run:ai</p>"},{"location":"Researcher/cli-reference/runai-logout/#synopsis","title":"Synopsis","text":"<pre><code>runai logout \n    [--loglevel value]\n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-logout/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-logout/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-logout/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-logout/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-logout/#output","title":"Output","text":"<p>You will be logged out from Run:ai</p>"},{"location":"Researcher/cli-reference/runai-logout/#see-also","title":"See Also","text":"<ul> <li>runai login.</li> </ul>"},{"location":"Researcher/cli-reference/runai-logs/","title":"runai logs","text":""},{"location":"Researcher/cli-reference/runai-logs/#description","title":"Description","text":"<p>Show the logs of a Job.</p>"},{"location":"Researcher/cli-reference/runai-logs/#synopsis","title":"Synopsis","text":"<pre><code>runai logs &lt;job-name&gt; \n    [--follow | -f] \n    [--pod string | -p string] \n    [--since duration] \n    [--since-time date-time] \n    [--tail int | -t int] \n    [--timestamps]  \n\n    [--loglevel value] \n    [--project string | -p string] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-logs/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-follow-f","title":"--follow | -f","text":"<p>Stream the logs.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-pod-p","title":"--pod | -p","text":"<p>Specify a specific pod name. When a Job fails, it may start a couple of times in an attempt to succeed. The flag allows you to see the logs of a specific instance (called 'pod'). Get the name of the pod by running <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-instance-string-i-string","title":"--instance (string) | -i (string)","text":"<p>Show logs for a specific instance in cases where a Job contains multiple pods.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-since-duration","title":"--since (duration)","text":"<p>Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. The flags since and since-time cannot be used together.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-since-time-date-time","title":"--since-time (date-time)","text":"<p>Return logs after specified date. Date format should be RFC3339, example: <code>2020-01-26T15:00:00Z</code>.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-tail-int-t-int","title":"--tail (int) | -t (int)","text":"<p># of lines of recent log file to display.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-timestamps","title":"--timestamps","text":"<p>Include timestamps on each line in the log output.</p>"},{"location":"Researcher/cli-reference/runai-logs/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-logs/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-logs/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-logs/#output","title":"Output","text":"<p>The command will show the logs of the first process in the container. For training Jobs, this would be the command run at startup. For interactive Jobs, the command may not show anything.</p>"},{"location":"Researcher/cli-reference/runai-logs/#see-also","title":"See Also","text":"<ul> <li>Training Workloads. See Quickstart document:  Launch Unattended Training Workloads.</li> </ul>"},{"location":"Researcher/cli-reference/runai-port-forwarding/","title":"runai port-forward","text":""},{"location":"Researcher/cli-reference/runai-port-forwarding/#description","title":"Description","text":"<p>Forward one or more local ports to the selected job or a pod within the job. The forwarding session ends when the selected job terminates or the terminal is interrupted.</p>"},{"location":"Researcher/cli-reference/runai-port-forwarding/#examples","title":"Examples","text":"<ol> <li> <p>Port forward connections from localhost:8080 (localhost is the default) to  on port 8090. <p><code>runai port-forward &lt;job-name&gt; --port 8080:8090</code></p> <li> <p>Port forward connections from 192.168.1.23:8080 to  on port 8080. <p><code>runai port-forward &lt;job-name&gt; --port 8080 --address 192.168.1.23</code></p> <li> <p>Port forward multiple connections from localhost:8080 to  on port 8090 and localhost:6443 to  on port 443. <p><code>runai port-forward &lt;job-name&gt; --port 8080:8090  --port 6443:443</code></p> <li> <p>Port forward into a specific pod in a multi-pod job.</p> <p><code>runai port-forward &lt;job-name&gt; --port 8080:8090 --pod &lt;pod-name&gt;</code></p> </li>"},{"location":"Researcher/cli-reference/runai-port-forwarding/#global-flags","title":"Global flags","text":"<p><code>--loglevel &lt;string&gt;</code>\u2014Set the logging level. Choose:  (default \"info\"). <p><code>-p | --project &lt;string&gt;</code>\u2014Specify the project name. To change the default project use <code>runai config project &lt;project name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-port-forwarding/#flags","title":"Flags","text":"<p><code>--address &lt;string&gt; | [local-interface-ip\\host] |localhost | 0.0.0.0 [privileged]</code>\u2014The listening address of your local machine. (default \"localhost\").</p> <p><code>-h | --help</code>\u2014Help for the command.</p> <p><code>--port</code>\u2014forward ports based on one of the following arguments:</p> <ul> <li> <p><code>&lt;stringArray&gt;</code>\u2014a list of port forwarding combinations.</p> </li> <li> <p><code>[local-port]:[remote-port]</code>\u2014different local and remote ports.</p> </li> <li> <p><code>[local-port=remote-port]</code>\u2014the same port is used for both local and remote.</p> </li> </ul> <p><code>--pod</code>\u2014Specify a pod of a running job. To get a list of the pods of a specific job, run the command <code>runai describe &lt;job-name&gt;</code>.</p> <p><code>--pod-running-timeout</code>\u2014The length of time (like 5s, 2m, or 3h, higher than zero) to wait until the pod is running. Default is 10 minutes.</p> <p>Filter based flags</p> <p><code>--mpi</code>\u2014search only for mpi jobs.</p> <p><code>--interactive</code>\u2014search only for interactive jobs.</p> <p><code>--pytorch</code>\u2014search only for pytorch jobs.</p> <p><code>--tf</code>\u2014search only for tensorflow jobs.</p> <p><code>--train</code>\u2014search only for training jobs.</p>"},{"location":"Researcher/cli-reference/runai-resume/","title":"runai resume","text":""},{"location":"Researcher/cli-reference/runai-resume/#description","title":"Description","text":"<p>Resume a suspended Job</p> <p>Resuming a previously suspended Job will return it to the queue for scheduling. The Job may or may not start immediately, depending on available resources. </p> <p>Suspend and resume do not work with mpi Jobs. </p>"},{"location":"Researcher/cli-reference/runai-resume/#synopsis","title":"Synopsis","text":"<pre><code>runai resume &lt;job-name&gt;\n    [--all | -A]\n\n    [--loglevel value]\n    [--project string | -p string]\n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-resume/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-resume/#-all-a","title":"--all | -A","text":"<p>Resume all suspended Jobs in the current Project.</p>"},{"location":"Researcher/cli-reference/runai-resume/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-resume/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-resume/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-resume/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-resume/#output","title":"Output","text":"<ul> <li>The Job will be resumed. When running runai list jobs the Job status will no longer by Suspended.</li> </ul>"},{"location":"Researcher/cli-reference/runai-resume/#see-also","title":"See Also","text":"<ul> <li>Suspending Jobs: Suspend.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/","title":"runai submit-dist tf","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#description","title":"Description","text":"<p> Version 2.10 and later.</p> <p>Submit a distributed TensorFlow training run:ai job to run.</p> <p>Note</p> <p>To use distributed training you need to have installed the TensorFlow operator as specified in Distributed training.</p> <p>Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#examples","title":"Examples","text":"<pre><code>runai submit-dist tf --name distributed-job --workers=2 -g 1 \\\n    -i &lt;image_name&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#distributed","title":"Distributed","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-clean-pod-policy-string","title":"--clean-pod-policy &lt; string &gt;","text":"<p>The\u00a0CleanPodPolicy\u00a0controls deletion of pods when a job terminates. The policy can be one of the following values:</p> <ul> <li>Running\u2014only pods still running when a job completes (for example, parameter servers) will be deleted immediately. Completed pods will not be deleted so that the logs will be preserved. (Default)</li> <li>All\u2014all (including completed) pods will be deleted immediately when the job finishes.</li> <li>None\u2014no pods will be deleted when the job completes.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-workers-int","title":"--workers &lt; int&gt;","text":"<p>Number of replicas for Inference jobs</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-args-string-string","title":"--master-args string <code>&lt;string&gt;</code>  <p>Arguments to pass to the master pod container command. If used together with <code>--command</code>, overrides the image's entrypoint of the master pod container with the given command.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-environment-stringarray","title":"--master-environment <code>&lt;stringArray&gt;</code>  <p>Set environment variables in the master pod container. To prevent from a worker environment variable from being set in the master, use the format: <code>name=-</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-extended-resource-stringarray","title":"--master-extended-resource <code>&lt;stringArray&gt;</code>  <p>Request access to an extended resource in the master pod. Use the format: <code>resource_name=quantity</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-gpu-float","title":"--master-gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-master-no-pvcs","title":"--master-no-pvcs  <p>Do not mount any persistent volumes in the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-no-master","title":"--no-master  <p>Do not create a separate pod for the master.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-mig-profile-string","title":"--mig-profile <code>&lt;string&gt;</code>  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-ttl-after-finish-duration","title":"--ttl-after-finish &lt; duration &gt;  <p>The duration, after which a finished job is automatically deleted (e.g. 5s, 2m, 3h).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#output","title":"Output","text":"<p>The command will attempt to submit an mpi Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-TF/#see-also","title":"See Also","text":"<ul> <li>See Quickstart document Running Distributed Training.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/","title":"runai submit-dist mpi","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#description","title":"Description","text":"<p>Submit a Distributed Training (MPI) Run:ai Job to run.</p> <p>Note</p> <p>To use distributed training you need to have installed the Kubeflow MPI Operator as specified in Distributed training.</p> <p>Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#examples","title":"Examples","text":"<p>You can start an unattended mpi training Job of name dist1, based on Project team-a using a quickstart-distributed image:</p> <pre><code>runai submit-dist mpi --name dist1 --workers=2 -g 1 \\\n    -i gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 -e RUNAI_SLEEP_SECS=60\n</code></pre> <p>(see: distributed training Quickstart).</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#distributed","title":"Distributed","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-clean-pod-policy-string","title":"--clean-pod-policy &lt; string &gt;","text":"<p>The\u00a0CleanPodPolicy\u00a0controls deletion of pods when a job terminates. The policy can be one of the following values:</p> <ul> <li>Running\u2014only pods still running when a job completes (for example, parameter servers) will be deleted immediately. Completed pods will not be deleted so that the logs will be preserved. (Default)</li> <li>All\u2014all (including completed) pods will be deleted immediately when the job finishes.</li> <li>None\u2014no pods will be deleted when the job completes.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-workers-int","title":"--workers &lt; int &gt;","text":"<p>Number of replicas for Inference jobs.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-slots-per-worker-int","title":"--slots-per-worker &lt; int &gt;","text":"<p>Number of slots to allocate for each worker.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-args-string-string","title":"--master-args string <code>&lt;string&gt;</code>  <p>Arguments to pass to the master pod container command. If used together with <code>--command</code>, overrides the image's entrypoint of the master pod container with the given command.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-environment-stringarray","title":"--master-environment <code>&lt;stringArray&gt;</code>  <p>Set environment variables in the master pod container. To prevent from a worker environment variable from being set in the master, use the format: <code>name=-</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-extended-resource-stringarray","title":"--master-extended-resource <code>&lt;stringArray&gt;</code>  <p>Request access to an extended resource in the master pod. Use the format: <code>resource_name=quantity</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-gpu-float","title":"--master-gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-master-no-pvcs","title":"--master-no-pvcs  <p>Do not mount any persistent volumes in the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-mig-profile-string","title":"--mig-profile <code>&lt;string&gt;</code>  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-ttl-after-finish-duration","title":"--ttl-after-finish &lt; duration &gt;  <p>The duration, after which a finished job is automatically deleted (e.g. 5s, 2m, 3h).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#output","title":"Output","text":"<p>The command will attempt to submit an mpi Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-mpi/#see-also","title":"See Also","text":"<ul> <li>See Quickstart document Running Distributed Training.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/","title":"runai submit-dist pytorch","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#description","title":"Description","text":"<p> Version 2.10 and later.</p> <p>Submit a distributed PyTorch training run:ai job to run.</p> <p>Note</p> <p>To use distributed training you need to have installed the Pytorch operator as specified in Distributed training.</p> <p>Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#examples","title":"Examples","text":"<pre><code>runai submit-dist pytorch --name distributed-job --workers=2 -g 1 \\\n    -i &lt;image_name&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#distributed","title":"Distributed","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-clean-pod-policy-string","title":"--clean-pod-policy &lt; string &gt;","text":"<p>The\u00a0CleanPodPolicy\u00a0controls deletion of pods when a job terminates. The policy can be one of the following values:</p> <ul> <li>Running\u2014only pods still running when a job completes (for example, parameter servers) will be deleted immediately. Completed pods will not be deleted so that the logs will be preserved. (Default)</li> <li>All\u2014all (including completed) pods will be deleted immediately when the job finishes.</li> <li>None\u2014no pods will be deleted when the job completes.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-max-replicas-int","title":"--max-replicas &lt; int &gt;","text":"<p>Maximum number of replicas for elastic PyTorch job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-min-replicas-int","title":"--min-replicas &lt; int &gt;","text":"<p>Minimum number of replicas for elastic PyTorch job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-workers-int","title":"--workers &lt; int&gt;","text":"<p>Number of replicas for Inference jobs</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-args-string-string","title":"--master-args string <code>&lt;string&gt;</code>  <p>Arguments to pass to the master pod container command. If used together with <code>--command</code>, overrides the image's entrypoint of the master pod container with the given command.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-environment-stringarray","title":"--master-environment <code>&lt;stringArray&gt;</code>  <p>Set environment variables in the master pod container. To prevent from a worker environment variable from being set in the master, use the format: <code>name=-</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-extended-resource-stringarray","title":"--master-extended-resource <code>&lt;stringArray&gt;</code>  <p>Request access to an extended resource in the master pod. Use the format: <code>resource_name=quantity</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-gpu-float","title":"--master-gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-master-no-pvcs","title":"--master-no-pvcs  <p>Do not mount any persistent volumes in the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-no-master","title":"--no-master  <p>Do not create a separate pod for the master.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-mig-profile-string","title":"--mig-profile <code>&lt;string&gt;</code>  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-ttl-after-finish-duration","title":"--ttl-after-finish &lt; duration &gt;  <p>The duration, after which a finished job is automatically deleted (e.g. 5s, 2m, 3h).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-pytorch/#output","title":"Output","text":"<p>The command will attempt to submit a distributed pytorch workload. You can follow up on the workload by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/","title":"runai submit-dist xgboost","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#description","title":"Description","text":"<p>Submit a distributed XGBoost training run:ai job to run.</p> <p>Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#examples","title":"Examples","text":"<pre><code>runai submit-dist xgboost --name distributed-job --workers=2 -g 1 \\\n    -i &lt;image_name&gt;\n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#distributed","title":"Distributed","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-clean-pod-policy-string","title":"--clean-pod-policy &lt; string &gt;","text":"<p>The\u00a0CleanPodPolicy\u00a0controls deletion of pods when a job terminates. The policy can be one of the following values:</p> <ul> <li>Running\u2014only pods still running when a job completes (for example, parameter servers) will be deleted immediately. Completed pods will not be deleted so that the logs will be preserved. (Default)</li> <li>All\u2014all (including completed) pods will be deleted immediately when the job finishes.</li> <li>None\u2014no pods will be deleted when the job completes.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-workers-int","title":"--workers &lt; int&gt;","text":"<p>Number of replicas for Inference jobs</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-args-string-string","title":"--master-args string <code>&lt;string&gt;</code>  <p>Arguments to pass to the master pod container command. If used together with <code>--command</code>, overrides the image's entrypoint of the master pod container with the given command.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-environment-stringarray","title":"--master-environment <code>&lt;stringArray&gt;</code>  <p>Set environment variables in the master pod container. To prevent from a worker environment variable from being set in the master, use the format: <code>name=-</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-extended-resource-stringarray","title":"--master-extended-resource <code>&lt;stringArray&gt;</code>  <p>Request access to an extended resource in the master pod. Use the format: <code>resource_name=quantity</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-gpu-float","title":"--master-gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-master-no-pvcs","title":"--master-no-pvcs  <p>Do not mount any persistent volumes in the master pod.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-mig-profile-string","title":"--mig-profile <code>&lt;string&gt;</code>  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-ttl-after-finish-duration","title":"--ttl-after-finish &lt; duration &gt;  <p>The duration, after which a finished job is automatically deleted (e.g. 5s, 2m, 3h).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#output","title":"Output","text":"<p>The command will attempt to submit an mpi Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-dist-xgboost/#see-also","title":"See Also","text":"<ul> <li>See Quickstart document Running Distributed Training.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit/","title":"Description","text":"<p>Submit a Run:ai Job for execution.</p> <p>Syntax notes:</p> <ul> <li>Flags of type stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit/#examples","title":"Examples","text":"<p>All examples assume a Run:ai Project has been setup using <code>runai config project &lt;project-name&gt;</code>.</p> <p>Start an interactive Job:</p> <pre><code>runai submit -i ubuntu --interactive --attach -g 1\n</code></pre> <p>Or</p> <pre><code>runai submit --name build1 -i ubuntu -g 1 --interactive -- sleep infinity \n</code></pre> <p>(see: build Quickstart).</p> <p>Externalize ports:</p> <pre><code>runai submit --name build-remote -i rastasheep/ubuntu-sshd:14.04 --interactive \\\n   --service-type=nodeport --port 30022:22\n   -- /usr/sbin/sshd -D\n</code></pre> <p>(see: build with ports Quickstart).</p> <p>Start a Training Job</p> <pre><code>runai submit --name train1 -i gcr.io/run-ai-demo/quickstart -g 1 \n</code></pre> <p>(see: training Quickstart).</p> <p>Use GPU Fractions</p> <pre><code>runai submit --name frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5\n</code></pre> <p>(see: GPU fractions Quickstart).</p> <p>Hyperparameter Optimization</p> <pre><code>runai submit --name hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1  \\\n   --parallelism 3 --completions 12 -v /nfs/john/hpo:/hpo \n</code></pre> <p>(see: hyperparameter optimization Quickstart).</p> <p>Submit a Job without a name (automatically generates a name)</p> <pre><code>runai submit -i gcr.io/run-ai-demo/quickstart -g 1 \n</code></pre> <p>Submit a job using the system autogenerated name to an external URL:</p> <pre><code>runai submit -i ubuntu --interactive --attach -g 1 service-type=external-url --port 3745 --custom-url=&lt;destination_url&gt;\n</code></pre> <p>Submit a job without a name to a system generated a URL :</p> <pre><code>runai submit -i ubuntu --interactive --attach -g 1 service-type=external-url --port 3745\n</code></pre> <p>Submit a Job without a name with a pre-defined prefix and an incremental index suffix</p> <pre><code>runai submit --job-name-prefix -i gcr.io/run-ai-demo/quickstart -g 1 \n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit/#job-type","title":"Job Type","text":""},{"location":"Researcher/cli-reference/runai-submit/#-interactive","title":"--interactive","text":"<p>Mark this Job as interactive.</p>"},{"location":"Researcher/cli-reference/runai-submit/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit/#-completions-int","title":"--completions &lt; int &gt;","text":"<p>Number of successful pods required for this job to be completed. Used with HPO.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-parallelism-int","title":"--parallelism &lt; int &gt;","text":"<p>Number of pods to run in parallel at any given time.  Used with HPO.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-preemptible","title":"--preemptible","text":"<p>Interactive preemptible jobs can be scheduled above guaranteed quota but may be reclaimed at any time.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-auto-deletion-time-after-completion","title":"--auto-deletion-time-after-completion","text":"<p>The timeframe after which a completed or failed job is automatically deleted. Configured in seconds, minutes, or hours (for example 5s, 2m, or 3h). If set to 0, the job will be deleted immediately after completing or failing.</p>"},{"location":"Researcher/cli-reference/runai-submit/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"Researcher/cli-reference/runai-submit/#container-definition","title":"Container Definition","text":""},{"location":"Researcher/cli-reference/runai-submit/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-e-stringarray-environment-stringarray","title":"-e <code>&lt;stringArray&gt;</code> | --environment <code>&lt;stringArray&gt;</code>","text":"<p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>"},{"location":"Researcher/cli-reference/runai-submit/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>","text":"<p>Image to use when creating the container for this Job</p>"},{"location":"Researcher/cli-reference/runai-submit/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>","text":"<p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>","text":"<p>Set labels variables in the container.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>","text":"<p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>","text":"<p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-stdin","title":"--stdin","text":"<p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-t-tty","title":"-t | --tty","text":"<p>Allocate a pseudo-TTY.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>","text":"<p>Starts the container with the specified directory as the current directory.</p>"},{"location":"Researcher/cli-reference/runai-submit/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>","text":"<p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>","text":"<p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-extended-resource-stringarray","title":"--extended-resource <code>&lt;stringArray&gt;</code>","text":"<p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>","text":"<p>GPU units to allocate for the Job (0.5, 1).</p>"},{"location":"Researcher/cli-reference/runai-submit/#-gpu-memory","title":"--gpu-memory","text":"<p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-memory-string","title":"--memory <code>&lt;string&gt;</code>","text":"<p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-memory-limit-string","title":"--memory-limit <code>&lt;string&gt;</code>","text":"<p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-mig-profile-string","title":"--mig-profile <code>&lt;string&gt;</code>","text":"<p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>"},{"location":"Researcher/cli-reference/runai-submit/#job-lifecycle_1","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>","text":"<p>The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the <code>--interactive</code> flag is not specified).</p>"},{"location":"Researcher/cli-reference/runai-submit/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>","text":"<p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-large-shm","title":"--large-shm","text":"<p>Mount a large /dev/shm device.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-mount-propagation","title":"--mount-propagation","text":"<p>Enable HostToContainer mount propagation for all container volumes</p>"},{"location":"Researcher/cli-reference/runai-submit/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>","text":"<p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>"},{"location":"Researcher/cli-reference/runai-submit/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>","text":"<p>Mount a persistent volume claim into a container.</p> <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--new-pvc</code>.</p> <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class.</p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p> <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only</p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only</p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write</p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write</p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>"},{"location":"Researcher/cli-reference/runai-submit/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>","text":"<p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running</li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-new-pvc-stringarray","title":"--new-pvc  <code>&lt;stringArray&gt;</code>","text":"<p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p> <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>","text":"<p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'","text":"<p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-configmap-volume-namepath","title":"--configmap-volume name=,path= ...'  <p>Mount a <code>ConfigMap</code> object for use as a data volume.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to jobs. Options are:</p> <ul> <li><code>nodeport</code> - add one or more ports using <code>--port</code>.</li> <li><code>external-url</code> - add one port and an optional custom URL using <code>--custom-url</code>.</li> </ul> <p>For example:</p> <p><code>runai submit test-jup -p team-a -i gcr.io/run-ai-demo/jupyter-tensorboard --service-type external-url --port 8888</code></p> <p><code>runai submit test-np -p team-a -i ubuntu --service-type nodeport --port 30000:7070</code></p> <p>This flag supports more than one <code>service-type</code>. Multiple service types are supported in CSV style using multiple instances of the same option and commas to separate the values for them.</p> <p>For example:</p> <p><code>runai submit test-np -p team-a -i ubuntu --service-type nodeport,port=30000:7070 --service-type external-url,port=30001</code></p> <p><code>runai submit test-np -p team-a -i ubuntu --service-type nodeport,port=30000:7070,port=9090 --service-type external-url,port=8080,custom-url=https://my.domain.com/url</code> </p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container. You can use a port number (for example 9090) or use the numbers of <code>hostport:containerport</code> (for example, 30000:7070).</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-custom-url-string","title":"--custom-url <code>&lt;string&gt;</code>  <p>An optional argument that specifies a custom URL when using the <code>external-url</code> service type. If not provided, the system will generate a URL automatically.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node. This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"Researcher/cli-reference/runai-submit/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"Researcher/cli-reference/runai-submit/#output","title":"Output","text":"<p>The command will attempt to submit a Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p> <p>Note that the submit call may use a policy to provide defaults to any of the above flags.</p>"},{"location":"Researcher/cli-reference/runai-submit/#see-also","title":"See Also","text":"<ul> <li>See any of the Quickstart documents here:.</li> <li>See policy configuration for a description on how policies work.</li> </ul>"},{"location":"Researcher/cli-reference/runai-suspend/","title":"runai suspend","text":""},{"location":"Researcher/cli-reference/runai-suspend/#description","title":"Description","text":"<p>Suspend a Job</p> <p>Suspending a Running Job will stop the Job and will not allow it to be scheduled until it is resumed using <code>runai resume</code>. This means that,</p> <ul> <li>You will no longer be able to enter it via <code>runai bash</code>.</li> <li>The Job logs will be deleted.</li> <li>Any data saved on the container and not stored in a shared location will be lost.</li> </ul> <p>Technically, the command deletes the Kubernetes pods associated with the Job and marks the Job as suspended until it is manually released. </p> <p>Suspend and resume do not work with MPI and Inference </p>"},{"location":"Researcher/cli-reference/runai-suspend/#synopsis","title":"Synopsis","text":"<pre><code>runai suspend &lt;job-name&gt;\n    [--all | -A]\n\n    [--loglevel value]\n    [--project string | -p string]\n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-suspend/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#-all-a","title":"--all | -A","text":"<p>Suspend all Jobs in the current Project.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-suspend/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-suspend/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#output","title":"Output","text":"<ul> <li>The Job will be suspended. When running runai list jobs the Job will be marked as Suspended.</li> </ul>"},{"location":"Researcher/cli-reference/runai-suspend/#see-also","title":"See Also","text":"<ul> <li>Resuming Jobs: Resume.</li> </ul>"},{"location":"Researcher/cli-reference/runai-top-node/","title":"runai top node","text":""},{"location":"Researcher/cli-reference/runai-top-node/#description","title":"Description","text":"<p>Show list of Nodes (machines), their capacity and utilization.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#synopsis","title":"Synopsis","text":"<pre><code>runai top node \n    [--help | -h]\n    [--details | -d]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-top-node/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-top-node/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-top-node/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-top-node/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#-details-d","title":"--details | -d","text":"<p>Show additional details.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#output","title":"Output","text":"<p>Shows a list of Nodes their capacity and utilization.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-update/","title":"runai update","text":""},{"location":"Researcher/cli-reference/runai-update/#description","title":"Description","text":"<p>Find and install the latest version of the runai command-line utility. The command must be run with sudo permissions.</p> <pre><code>sudo runai update\n</code></pre>"},{"location":"Researcher/cli-reference/runai-update/#synopsis","title":"Synopsis","text":"<pre><code>runai update \n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-update/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-update/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-update/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-update/#output","title":"Output","text":"<p>Update of the Run:ai command-line interface.</p>"},{"location":"Researcher/cli-reference/runai-update/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-version/","title":"runai version","text":""},{"location":"Researcher/cli-reference/runai-version/#description","title":"Description","text":"<p>Show the version of this utility.</p>"},{"location":"Researcher/cli-reference/runai-version/#synopsis","title":"Synopsis","text":"<pre><code>runai version \n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-version/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-version/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-version/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-version/#output","title":"Output","text":"<p>The version of the Run:ai command-line interface.</p>"},{"location":"Researcher/cli-reference/runai-version/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-whoami/","title":"runai whoami","text":""},{"location":"Researcher/cli-reference/runai-whoami/#description","title":"Description","text":"<p>Show the user name currently logged in</p>"},{"location":"Researcher/cli-reference/runai-whoami/#synopsis","title":"Synopsis","text":"<pre><code>runai whoami \n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-whoami/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-whoami/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-whoami/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-whoami/#output","title":"Output","text":"<p>The name of the User currently logged in with the Run:ai command-line interface.</p>"},{"location":"Researcher/cli-reference/runai-whoami/#see-also","title":"See Also","text":""},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/","title":"GPU Time Slicing Scheduler","text":""},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#new-time-slicing-scheduler-by-runai","title":"New Time-slicing scheduler by Run:ai","text":"<p>To provide customers with predictable and accurate GPU compute resources scheduling, Run:ai is introducing a new feature called Time-slicing GPU scheduler which adds fractional compute capabilities on top of other existing Run:ai memory fractions capabilities. Unlike the default NVIDIA GPU orchestrator which doesn\u2019t provide the ability to split or limit the runtime of each workload, Run:ai created a new mechanism that gives each workload exclusive access to the full GPU for a limited amount of time (lease time) in each scheduling cycle (plan time). This cycle repeats itself for the lifetime of the workload.</p> <p>Using the GPU runtime this way guarantees a workload is granted its requested GPU compute resources proportionally to its requested GPU fraction.</p> <p>Run:ai offers two new Time-slicing modes:</p> <ol> <li>Strict\u2014each workload gets its precise GPU compute fraction, which equals to its requested GPU (memory) fraction. In terms of official Kubernetes resource specification, this means:</li> </ol> <pre><code>gpu-compute-request = gpu-compute-limit = gpu-(memory-)fraction\n</code></pre> <ol> <li>Fair\u2014each workload is guaranteed at least its GPU compute fraction, but at the same time can also use additional GPU runtime compute slices that are not used by other idle workloads. Those excess time slices are divided equally between all workloads running on that GPU (after each got at least its requested GPU compute fraction). In terms of official Kubernetes resource specification, this means:</li> </ol> <pre><code>gpu-compute-request = gpu-(memory-)fraction\n\ngpu-compute-limit = 1.0\n</code></pre> <p>The figure below illustrates how Strict time-slicing mode is using the GPU from Lease (slice) and Plan (cycle) perspective:</p> <p></p> <p>The figure below illustrates how Fair time-slicing mode is using the GPU from Lease (slice) and Plan (cycle) perspective:</p> <p></p>"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#setting-the-time-slicing-scheduler-policy","title":"Setting the Time-slicing scheduler policy","text":"<p>Time-slicing is a cluster flag which changes the default behavior of Run:ai GPU fractions feature.</p> <p>Enable time-slicing by setting the following cluster flag in the <code>runaiconfig</code> file:</p> <pre><code>global: \n    core: \n        timeSlicing: \n            mode: fair/strict\n</code></pre> <p>If the <code>timeSlicing</code> flag is not set, the system continues to use the default NVidia GPU orchestrator to maintain backward compatability.</p>"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#time-slicing-plan-and-lease-times","title":"Time-slicing Plan and Lease Times","text":"<p>Each GPU scheduling cycle is a plan, the plan time is determined by the lease time and granularity (precision). By default, basic lease time is 250ms with 5% granularity (precision), which means the plan (cycle) time is: 250 / 0.05 = 5000ms (5 Sec). Using these values, a workload that asked to get gpu-fraction=0.5 gets 2.5s runtime out of 5s cycle time.</p> <p>Different workloads requires different SLA and precision, so it also possible to tune the lease time and precision for customizing the time-slicing capabilities to your cluster.</p> <p>Note</p> <p>Decreasing the lease time makes time-slicing less accurate. Increasing the lease time make the system more accurate, but each workload is less responsive.</p> <p>Once timeSlicing is enabled, all submitted GPU fraction or GPU memory workloads will have their gpu-compute-request\\limit set automatically by the system, depending on the annotation used on the timeSlicing mode:</p>"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#strict-compute-resources","title":"Strict Compute Resources","text":"Annotation Value GPU Compute Request GPU Compute Limit <code>gpu-fraction</code> x x x <code>gpu-memory</code> x 0 1.0"},{"location":"Researcher/scheduling/GPU-time-slicing-scheduler/#fair-compute-resources","title":"Fair Compute Resources","text":"Annotation Value GPU Compute Request GPU Compute Limit <code>gpu-fraction</code> x x 1.0 <code>gpu-memory</code> x 0 1.0 <p>Note</p> <p>The above tables show that when submitting a workload using gpu-memory annotation, the system will split the GPU compute time between the different workloads running on that GPU. This means the workload can get anything from very little compute time (&gt;0) to full GPU compute time (1.0).</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/","title":"Introduction","text":"<p>When we discuss the allocation of deep learning compute resources, the discussion tends to focus on GPUs as the most critical resource. But two additional resources are no less important:</p> <ul> <li>CPUs. Mostly needed for preprocessing and postprocessing tasks during a deep learning training run.</li> <li>Memory. Has a direct influence on the quantities of data a training run can process in batches.</li> </ul> <p>GPU servers tend to come installed with a significant amount of memory and CPUs.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#requesting-cpu-memory","title":"Requesting CPU &amp; Memory","text":"<p>When submitting a Job, you can request a guaranteed amount of CPUs and memory by using the --cpu and --memory flags in the runai submit command. For example:</p> <pre><code>runai submit job1 -i ubuntu --gpu 2 --cpu 12 --memory 1G\n</code></pre> <p>The system guarantees that if the Job is scheduled, you will be able to receive this amount of CPU and memory.</p> <p>For further details on these flags see: runai submit</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#cpu-over-allocation","title":"CPU over allocation","text":"<p>The number of CPUs your Job will receive is guaranteed to be the number defined using the <code>--cpu</code> flag. In practice, however, you may receive more CPUs than you have asked for:</p> <ul> <li>If you are currently alone on a node, you will receive all the node CPUs until such time when another workload has joined.</li> <li>However, when a second workload joins, each workload will receive a number of CPUs proportional to the number requested via the <code>--cpu</code> flag. For example, if the first workload asked for 1 CPU and the second for 3 CPUs, then on a node with 40 cpus, the workloads will receive 10 and 30 CPUs respectively. If the flag <code>--cpu</code> is not specified, it will be taken from the cluster default (see the section below)</li> </ul>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#memory-over-allocation","title":"Memory over allocation","text":"<p>The amount of Memory your Job will receive is guaranteed to be the number defined using the --memory flag. In practice, however, you may receive more memory than you have asked for. This is along the same lines as described with CPU over allocation above.</p> <p>It is important to note, however, that if you have used this memory over-allocation, and new workloads have joined, your Job may receive an out-of-memory exception and terminate.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#cpu-and-memory-limits","title":"CPU and Memory limits","text":"<p>You can limit your Job's allocation of CPU and memory by using the --cpu-limit and --memory-limit flags in the runai submit command. For example:</p> <pre><code>runai submit job1 -i ubuntu --gpu 2 --cpu 12 --cpu-limit 24 \\\n    --memory 1G --memory-limit 4G\n</code></pre> <p>The limit behavior is different for CPUs and memory.</p> <ul> <li>Your Job will never be allocated with more than the amount stated in the <code>--cpu-limit</code> flag</li> <li>If your Job tries to allocate more than the amount stated in the <code>--memory-limit</code> flag it will receive an out-of-memory exception.</li> </ul> <p>The limit (for both CPU and memory) overrides the cluster default described in the section below</p> <p>For further details on these flags see: runai submit</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#flag-defaults","title":"Flag Defaults","text":""},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-cpu-flag","title":"Defaults for --cpu flag","text":"<p>If your Job has not specified <code>--cpu</code>, the system will use a default. The default is cluster-wide and is defined as a ratio of GPUs to CPUs.</p> <p>If, for example, the default has been defined as 1:6 and your Job has specified <code>--gpu 2</code> and has not specified <code>--cpu</code>, then the implied <code>--cpu</code> flag value is 12 CPUs.</p> <p>The system comes with a cluster-wide default of 1:1. To change the ratio see below.</p> <p>If you didn't request any GPUs for your job and has not specified <code>--cpu</code>, the default is defined as a ratio of CPU limit to CPUs.</p> <p>If, for example, the default has been defined as 1:0.2 and your Job has specified <code>--cpu-limit 10</code> and has not specified <code>--cpu</code>, then the implied <code>--cpu</code> flag value is 2 CPUs.</p> <p>The system comes with a cluster-wide default of 1:0.1. To change the ratio see below.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-memory-flag","title":"Defaults for --memory flag","text":"<p>If your Job has not specified <code>--memory</code>, the system will use a default. The default is cluster-wide and is proportional to the number of requested GPUs.</p> <p>The system comes with a cluster-wide default of 100MiB of allocated CPU memory per GPU. To change the ratio see below.</p> <p>If you didn't request any GPUs for your job and has not specified <code>--memory</code>, the default is defined as a ratio of CPU Memory limit to CPU Memory Request.</p> <p>The system comes with a cluster-wide default of 1:0.1. To change the ratio see below.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-cpu-limit-flag","title":"Defaults for --cpu-limit flag","text":"<p>If your Job has not specified <code>--cpu-limit</code>, then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to CPUs. See below on how to change the ratio.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-memory-limit-flag","title":"Defaults for --memory-limit flag","text":"<p>If your Job has not specified <code>--memory-limit</code>, then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to Memory. See below on how to change the ratio.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#changing-the-ratios","title":"Changing the ratios","text":"<p>To change the cluster wide-ratio use the following process. The example shows:</p> <ul> <li>a CPU request with a default ratio of 2:1 CPUs to GPUs.</li> <li>a CPU Memory request with a default ratio of 200MB per GPU.</li> <li>a CPU limit with a default ratio of 4:1 CPU to GPU.</li> <li>a Memory limit with a default ratio of 2GB per GPU.</li> <li>a CPU request with a default ratio of 0.1 CPUs per 1 CPU limit.</li> <li>a CPU Memory request with a default ratio of 0.1:1 request per CPU Memory limit.</li> </ul> <p>You must edit the cluster installation values file:</p> <ul> <li>When installing the Run:ai cluster, edit the values file.</li> <li>On an existing installation, use the upgrade cluster instructions to modify the values file.</li> <li>You must specify at least the first 4 values as follows:</li> </ul> <pre><code>runai-operator:\n  config:\n    limitRange:\n      cpuDefaultRequestGpuFactor: 2\n      memoryDefaultRequestGpuFactor: 200Mi\n      cpuDefaultLimitGpuFactor: 4\n      memoryDefaultLimitGpuFactor: 2Gi\n      cpuDefaultRequestCpuLimitFactorNoGpu: 0.1\n      memoryDefaultRequestMemoryLimitFactorNoGpu: 0.1\n</code></pre>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#validating-cpu-memory-allocations","title":"Validating CPU &amp; Memory Allocations","text":"<p>To review CPU &amp; Memory allocations you need to look into Kubernetes. A Run:ai Job creates a Kubernetes pod. The pod declares its resource requests and limits. To see the memory and CPU consumption in Kubernetes:</p> <ul> <li>Get the pod name for the Job by running:</li> </ul> <p><code>runai describe job &lt;JOB_NAME&gt;</code></p> <p>the pod will appear under the <code>PODS</code> category.</p> <ul> <li>Run:</li> </ul> <p><code>kubectl describe pod &lt;POD_NAME&gt;</code></p> <p>The information will appear under <code>Requests</code> and <code>Limits</code>. For example:</p> <pre><code>Limits:\n    nvidia.com/gpu:  2\nRequests:\n    cpu:             1\n    memory:          104857600\n    nvidia.com/gpu:  2\n</code></pre>"},{"location":"Researcher/scheduling/dynamic-gpu-fractions/","title":"Dynamic GPU Fractions","text":""},{"location":"Researcher/scheduling/dynamic-gpu-fractions/#introduction","title":"Introduction","text":"<p>Many AI workloads are using GPU resources intermittently and sometimes these resources are not used at all. These AI workloads need these resources when they are running AI applications, or debugging a model in development. Other workloads such as Inference, might be using GPU resources at a lower utilization rate than requested, and may suddenly ask for higher guaranteed resources at peak utilization times.</p> <p>This pattern of resource request vs. actual resource utilization causes lower utilization of GPUs. This mainly happens if there are many workloads requesting resources to match their peak demand, even though the majority of the time they operate far below that peak.</p> <p>Run:ai has introduced Dynamic GPU fractions in v2.15 to cope with resource request vs. actual resource utilization which enables users to optimize GPU resource usage.</p> <p>Dynamic GPU fractions is part of Run:ai's core capabilities to enable workloads to optimize the use of GPU resources. This works by providing the ability to specify and consume GPU memory and compute resources dynamically by leveraging Kubernetes Request and Limit notations.</p> <p>Dynamic GPU fractions allow a workload to request a guaranteed fraction of GPU memory or GPU compute resource (similar to a Kubernetes request), and at the same time also request the ability to grow beyond that guaranteed request up to a specific limit (similar to a Kubernetes limit), if the resources are available.</p> <p>For example, with Dynamic GPU Fractions, a user can specify a workload with a GPU fraction Request of 0.25 GPU, and add the parameter <code>gpu-fraction-limit</code> of up to 0.80 GPU. The cluster/node-pool scheduler schedules the workload to a node that can provide the GPU fraction request (0.25), and then assigns the workload to a GPU. The GPU scheduler monitors the workload and allows it to occupy memory between 0 to 0.80 of the GPU memory (based on the parameter <code>gpu-fraction-limit</code>), where only 0.25 of the GPU memory is guaranteed to that workload. The rest of the memory (from 0.25 to 0.8) is \u201cloaned\u201d to the workload, as long as it is not needed by other workloads.</p> <p>Run:ai automatically manages the state changes between <code>request</code> and <code>Limit</code> as well as the reverse (when the balance need to be \"returned\"), updating the metrics and workloads' states and graphs.</p>"},{"location":"Researcher/scheduling/dynamic-gpu-fractions/#setting-fractional-gpu-memory-limit","title":"Setting Fractional GPU Memory Limit","text":"<p>With the fractional GPU memory limit, users can submit workloads using GPU fraction <code>Request</code> and <code>Limit</code>.</p> <p>You can either:</p> <ol> <li> <p>Use a GPU Fraction parameter (use the <code>gpu-fraction</code> annotation)</p> <p>or</p> </li> <li> <p>Use an absolute GPU Memory parameter (<code>gpu-memory</code> annotation)</p> </li> </ol> <p>When setting a GPU memory limit either as GPU fraction, or GPU memory size, the <code>Limit</code> must be equal or greater than the GPU fraction memory request.</p> <p>Both GPU fraction and GPU memory are translated into the actual requested memory size of the Request (guaranteed resources) and the Limit (burstable resources).</p> <p>To guarantee fair quality of service between different workloads using the same GPU, Run:ai developed an extendable GPU <code>OOMKiller</code> (Out Of Memory Killer) component that guarantees the quality of service using Kubernetes semantics for resources Request and Limit.</p> <p>The <code>OOMKiller</code> capability requires adding <code>CAP_KILL</code> capabilities to the Dynamic GPU fraction and to the Run:ai core scheduling module (toolkit daemon). This capability is disabled by default.</p> <p>To change the state of Dynamic GPU Fraction in the cluster, edit the <code>runaiconfig</code> file and set:</p> <pre><code>spec: \n  global: \n    core: \n      dynamicFraction: \n        enabled: true # Boolean field default is true.\n</code></pre> <p>To set the gpu memory limit per workload, add the <code>RUNAI_GPU_MEMORY_LIMIT</code> environment variable to the first container in the pod. This is the GPU consuming container.</p> <p>To use <code>RUNAI_GPU_MEMORY_LIMIT</code> environment variable:</p> <ol> <li> <p>Submit a workload yaml directly, and set the <code>RUNAI_GPU_MEMORY_LIMIT</code> environment variable.</p> </li> <li> <p>Create a policy, per Project or globally. For example, set all Interactive workloads of <code>Project=research_vision1</code> to always set the environment variable of <code>RUNAI_GPU_MEMORY_LIMIT</code> to 1.</p> </li> <li> <p>Pass the environment variable through the CLI or the UI.</p> </li> </ol> <p>The supported values depend on the label used. You can use them in either the UI or the CLI. Use only one of the variables in the following table (they cannot be mixed):</p> Variable Input format <code>gpu-fraction</code> A fraction value (for example: 0.25, 0.75). <code>gpu-memory</code> Kubernetes resources quantity which must be larger than <code>gpu-memory</code>. For example, 500000000, 2500M, 4G. NOTE: The <code>gpu-memory</code> label values are always in MB, unlike the env variable."},{"location":"Researcher/scheduling/dynamic-gpu-fractions/#compute-resources-ui-with-dynamic-fractions-support","title":"Compute Resources UI with Dynamic Fractions support","text":"<p>To enable the UI elements for Dynamic Fractions, press Settings, General, then open the Resources pane and toggle GPU Resource Optimization. This enables all the UI features related to GPU Resource Optimization for the whole tenant. There are other per cluster or per node-pool configurations that should be configured in order to use the capabilities of \u2018GPU Resource Optimization\u2019 See the documentation for each of these features. Once the \u2018GPU Resource Optimization\u2019 feature is enabled, you will be able to create Compute Resources with the GPU Portion (Fraction) Limit and GPU Memory Limit. In addition, you will be able to view the workloads\u2019 utilization vs. Request and Limit parameters in the Metrics pane for each workload.</p> <p></p> <p>Note</p> <p>To use Dynamic Fractions, GPU devices per pod must be equal to 1. If more than 1 GPU device is used per pod, or if a MIG profile is selected, Dynamic Fractions cannot be used for that Compute Resource (and any related pods).</p> <p>Note</p> <p>When setting a workload with Dynamic Fractions, (for example, when using it with GPU Request or GPU memory Limits), you practically make the workload burstable. This means it can use memory that is not guaranteed for that workload and is susceptible to an \u2018OOM Kill\u2019 signal if the actual owner of that memory requires it back. This applies to non-preemptive workloads as well. For that reason, its recommended that you use Dynamic Fractions with Interactive workloads running Notebooks. Notebook pods are not evicted when their GPU process is OOM Kill\u2019ed. This behavior is the same as standard Kubernetes burstable CPU workloads.</p>"},{"location":"Researcher/scheduling/fractions/","title":"Allocation of GPU Fractions","text":""},{"location":"Researcher/scheduling/fractions/#introduction","title":"Introduction","text":"<p>A single GPU has a significant amount of memory. Ranging from a couple of gigabytes in older generations and up to 80GB per GPU in the later models of the latest NVIDIA GPU technology. A single GPU also has a vast amount of computing power. </p> <p>This amount of memory and computing power is important for processing large amounts of data, such as in training deep learning models. However, there are quite a few applications that do not need this power. Examples can be inference workloads and the model-creation phase. It would thus be convenient if we could divide up a GPU between various workloads, thus achieving better GPU utilization. </p> <p>This article describes two complementary technologies that allow the division of GPUs and how to use them with Run:ai.</p> <ol> <li>Run:ai Fractions. </li> <li>Dynamic allocation using NVIDIA Multi-instance GPU (MIG)</li> </ol>"},{"location":"Researcher/scheduling/fractions/#runai-fractions","title":"Run:ai Fractions","text":"<p>Run:ai provides the capability to allocate a container with a specific amount of GPU RAM. As a researcher, if you know that your code needs 4GB of RAM. You can submit a job using the flag <code>--gpu-memory 4G</code> to specify the exact portion of the GPU memory that you need. Run:ai will allocate your container that specific amount of GPU RAM. Attempting to reach beyond your allotted RAM will result in an out-of-memory exception. </p> <p>You can also use the flag <code>--gpu 0.2</code> to get 20% of the GPU memory on the GPU assigned for you. </p> <p>For more details on Run:ai fractions see the fractions quickstart.</p> <p>Limitation</p> <p>With the fraction technology all running workloads, which utilize the GPU, share the compute in parallel and on average get an even share of the compute. For example, assuming two containers, one with 0.25 GPU workload and the other with 0.75 GPU workload - both will get (on average) an equal part of the computation power. If one of the workloads does not utilize the GPU, the other workload will get the entire GPU's compute power.</p> <p>Info</p> <p>For interoperability with other Kubernetes schedulers, Run:ai creates special reservation pods. Once a workload has been allocated a fraction of a GPU, Run:ai will create a pod in a dedicated <code>runai-reservation</code> namespace with the full GPU as a resource. This would cause other schedulers to understand that the GPU is reserved.    </p>"},{"location":"Researcher/scheduling/fractions/#dynamic-mig","title":"Dynamic MIG","text":"<p>NVIDIA MIG allows GPUs based on the NVIDIA Ampere architecture (such as NVIDIA A100) to be partitioned into separate GPU Instances:</p> <ul> <li>When divided, the portion acts as a fully independent GPU.</li> <li>The division is static, in the sense that you have to call NVIDIA API or the <code>nvidia-smi</code> command to create or remove the MIG partition. </li> <li>The division is both of compute and memory.</li> <li>The division has fixed sizes.  Up to 7 units of compute and memory in fixed sizes. The various MIG profiles can be found in the NVIDIA documentation. A typical profile can be <code>MIG 2g.10gb</code> which provides 2/7 of the compute power and 10GB of RAM</li> <li>Reconfiguration of MIG profiles on the GPU requires administrator permissions and the draining of all running workloads. </li> </ul> <p>Run:ai provides a way to dynamically create a MIG partition:</p> <ul> <li>Using the same experience as the Fractions technology above, if you know that your code needs 4GB of RAM. You can use the flag <code>--gpu-memory 4G</code> to specify the portion of the GPU memory that you need. Run:ai will call the NVIDIA MIG API to generate the smallest possible MIG profile for your request, and allocate it to your container. </li> <li>MIG is configured on the fly according to workload demand, without needing to drain workloads or to involve an IT administrator.</li> <li>Run:ai will automatically deallocate the partition when the workload finishes. This happens in a lazy fashion in the sense that the partition will not be removed until the scheduler decides that it is needed elsewhere. </li> <li>Run:ai provides an additional flag to dynamically create the specific MIG partition in NVIDIA terminology. As such, you can specify <code>--mig-profile 2g.10gb</code>.  </li> <li>In a single GPU cluster you have some MIG nodes that are dynamically allocated and some that are not.</li> </ul> <p>For more details on Run:ai fractions see the dynamic MIG quickstart.</p>"},{"location":"Researcher/scheduling/fractions/#setting-up-dynamic-mig","title":"Setting up Dynamic MIG","text":"<p>As described above, MIG is only available in the latest NVIDIA architecture. </p> <ul> <li>When working with Kubernetes, NVIDIA defines a concept called MIG Strategy. With Run:ai you must set the MIG strategy to <code>mixed</code>. See NVIDIA prerequisites on how to set this flag. </li> <li> <p>The administrator needs to specifically enable dynamic MIG on the node by running: </p> <p><pre><code>runai-adm set node-role --dynamic-mig-enabled &lt;node-name&gt;\n</code></pre> (use <code>runai-adm remove</code> to unset)</p> </li> <li> <p>Make sure that MIG is enabled on the node level by running <code>nvidia-smi</code> on the node and verifying that MIG Mode is enabled (see highlight below):</p> </li> </ul> <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                   On |\n| N/A   32C    P0    42W / 400W |      0MiB / 40536MiB |     N/A      Default |\n|                               |                      |              Enabled |\n+-------------------------------+----------------------+----------------------+\n</code></pre> <ul> <li> <p>To enable MIG Mode see NVIDIA documentation.</p> </li> <li> <p>Set:     <pre><code>kubectl label node &lt;node-name&gt; node-role.kubernetes.io/runai-mig-enabled=true\n</code></pre>    (use <code>kubectl</code> to unset)</p> </li> </ul> <p>Limitations</p> <ul> <li>Once a node has been marked as dynamic MIG enabled, it can only be used via the Run:ai scheduler.</li> <li>Run:ai currently supports H100 or A100 nodes with 40GB/80GB RAM.</li> <li>GPU utilization, shown on the Run:ai dashboards, may not be accurate while MIG jobs are running.</li> </ul>"},{"location":"Researcher/scheduling/fractions/#mixing-fractions-and-dynamic-mig","title":"Mixing Fractions and Dynamic MIG","text":"<p>Given a specific node, the IT administrator can decide whether to use one technology or the other. When the Researcher asks for a specific amount of GPU memory, Run:ai will either provide it on an annotated node by dynamically allocating a MIG partition, or use a different node using the fractions technology.</p>"},{"location":"Researcher/scheduling/fractions/#see-also","title":"See Also","text":"<ul> <li>Fractions quickstart.</li> <li>Dynamic MIG quickstart</li> </ul>"},{"location":"Researcher/scheduling/job-statuses/","title":"Job Statuses","text":""},{"location":"Researcher/scheduling/job-statuses/#introduction","title":"Introduction","text":"<p>The runai submit function and its sibling the runai submit-dist mpi function submit Run:ai Jobs for execution.</p> <p>A Job has a status. Once a Job is submitted it goes through several statuses before ending in an End State. Most of these statuses originate in the underlying Kubernetes infrastructure, but some are Run:ai-specific. </p> <p>The purpose of this document is to explain these statuses as well as the lifecycle of a Job. </p>"},{"location":"Researcher/scheduling/job-statuses/#successful-flow","title":"Successful Flow","text":"<p>A regular, training Job that has no errors and executes without preemption would go through the following statuses:</p> <p></p> <ul> <li>Pending - the Job is waiting to be scheduled.</li> <li>ContainerCreating - the Job has been scheduled, the Job docker image is now downloading.</li> <li>Running - the Job is now executing.</li> <li>Succeeded - the Job has finished with exit code 0 (success).</li> </ul> <p>The Job can be preempted, in which case it can go through other statuses:</p> <ul> <li>Terminating - the Job is now being preempted.</li> <li>Pending - the Job is waiting in queue again to receive resources.</li> </ul> <p>An interactive Job, by definition, needs to be closed by the Researcher and will thus never reach the Succeeded status. Rather, it would be moved by the Researcher to status Deleted.</p> <p>For a further explanation of the additional statuses, see the table below.</p>"},{"location":"Researcher/scheduling/job-statuses/#error-flow","title":"Error flow","text":"<p>A regular, training Job may encounter an error inside the running process (exit code is non-zero). In which case the following will happen:</p> <p></p> <p>The Job enters an Error status and then immediately tries to reschedule itself for another attempted run. The reschedule can happen on another node in the system. After a specified number of retries, the Job will enter a final status of Fail</p> <p>An interactive Job, enters an Error status and then moves immediately to CrashLoopBackOff trying to reschedule itself. The reschedule attempt has no 'back-off' limit and will continue to retry indefinitely </p> <p></p> <p>Jobs may be submitted with an image that cannot be downloaded. There are special statuses for such Jobs. See table below </p>"},{"location":"Researcher/scheduling/job-statuses/#status-table","title":"Status Table","text":"<p>Below is a list of statuses. For each status the list shows:</p> <ul> <li> <p>Name</p> </li> <li> <p>End State - this status is the final status in the lifecycle of the Job</p> </li> <li> <p>Resource Allocation - when the Job is in this status, does the system allocate resources to it</p> </li> <li> <p>Description</p> </li> <li> <p>Color - Status color as can be seen in the Run:ai User Interface Job list</p> </li> </ul> <p>Status</p> <p>End State</p> <p>Resource Allocation</p> <p>Description</p> <p>Color</p> <p>Running</p> <p></p> <p>Yes</p> <p>Job is running successfully</p> <p></p> <p>Terminating</p> <p></p> <p>Yes</p> <p>Pod is being evicted at the moment (e.g. due to an over-quota allocation, the reason will be written once eviction finishes). A new pod will be created shortly</p> <p></p> <p>ContainerCreating</p> <p></p> <p>Yes</p> <p>Image is being pulled from registry.</p> <p></p> <p>Pending</p> <p></p> <p>-</p> <p>Job is pending. Possible reasons:</p> <p>- Not enough resources</p> <p>- Waiting in Queue (over-quota etc).</p> <p></p> <p>Succeeded</p> <p>Yes</p> <p>-</p> <p>An Unattended (training) Job has ran and finished successfully.</p> <p></p> <p>Deleted</p> <p>Yes</p> <p>-</p> <p>Job has been deleted.</p> <p></p> <p>TimedOut</p> <p>Yes</p> <p>-</p> <p>Interactive Job has reached the defined timeout of the project.</p> <p></p> <p>Preempted</p> <p>Yes</p> <p>-</p> <p>Interactive preemptible Job has been evicted.</p> <p></p> <p>ContainerCannotRun</p> <p>Yes</p> <p>-</p> <p>Container has failed to start running. This is typically a problem within the docker image itself.</p> <p></p> <p>Error</p> <p></p> <p>Yes for interactive only </p> <p>The Job has returned an exit code different than zero. It is now waiting for another run attempt (retry).</p> <p></p> <p>Fail</p> <p>Yes</p> <p>-</p> <p>Job has failed after a number of retries (according to \"--backoffLimit\" field) and will not be trying again.</p> <p></p> <p>CrashLoopBackOff</p> <p></p> <p>Yes</p> <p>Interactive Only: During backoff after Error, before a retry attempt to run pod on the same node.</p> <p></p> <p>ErrImagePull, ImagePullBackOff</p> <p></p> <p>Yes</p> <p>Failing to retrieve docker image</p> <p></p> <p>Unknown</p> <p>Yes</p> <p>-</p> <p>The Run:ai Scheduler wasn't running when the Job has finished.</p> <p></p>"},{"location":"Researcher/scheduling/job-statuses/#how-to-get-more-information","title":"How to get more information","text":"<p>The system stores various events during the Job's lifecycle. These events can be helpful in diagnosing issues around Job scheduling. To view these events run:</p> <pre><code>runai describe job &lt;workload-name&gt;\n</code></pre> <p>Sometimes, useful information can be found by looking at  logs emitted from the process running inside the container. For example, Jobs that have exited with an exit code different than zero may write an exit reason in this log. To see Job logs run:</p> <pre><code>runai logs &lt;job-name&gt;\n</code></pre>"},{"location":"Researcher/scheduling/job-statuses/#distributed-training-mpi-jobs","title":"Distributed Training (mpi) Jobs","text":"<p>A distributed (mpi) Job, which has no errors will be slightly more complicated and has additional statuses associated with it. </p> <ul> <li> <p>Distributed Jobs start with an \"init container\" which sets the stage for a distributed run.</p> </li> <li> <p>When the init container finishes, the main \"launcher\" container is created. The launcher is responsible for coordinating between the different workers</p> </li> <li> <p>Workers run and do the actual work.</p> </li> </ul> <p>A successful flow of distribute training would look as:</p> <p></p> <p>Additional Statuses:</p> <p>Status</p> <p>End State</p> <p>Resource Allocation</p> <p>Description</p> <p>Color</p> <p>Init:&lt;number A&gt;/&lt;number B&gt;</p> <p></p> <p>Yes</p> <p>The Pod has B Init Containers, and A have completed so far.</p> <p></p> <p>PodInitializing</p> <p></p> <p>Yes</p> <p>The pod has finished executing Init Containers. The system is creating the main 'launcher' container</p> <p></p> <p>Init:Error</p> <p></p> <p></p> <p>An Init Container has failed to execute.</p> <p></p> <p>Init:CrashLoopBackOff</p> <p></p> <p></p> <p>An Init Container has failed repeatedly to execute</p> <p></p>"},{"location":"Researcher/scheduling/node-level-scheduler/","title":"Optimize performance with Node Level Scheduler","text":"<p>The Node Level Scheduler optimizes the performance of your pods and maximizes the utilization of GPUs by making optimal local decisions on GPU allocation to your pods. While the Cluster Scheduler chooses the specific node for a POD, but has no visibility to node\u2019s GPUs internal state, the Node Level Scheduler is aware of the local GPUs states and makes optimal local decisions such that it can optimize both the GPU utilization and pods\u2019 performance running on the node\u2019s GPUs.</p> <p>Node Level Scheduler applies to all workload types, but will best optimize the performance of burstable workloads, giving those more GPU memory than requested and up to the limit specified. Be aware, burstable workloads are always susceptible to an OOM Kill signal if the owner of the excess memory requires it back. This means that using the Node Level Scheduler with Inference or Training workloads may cause pod preemption. Interactive workloads that are using notebooks behave differently since the OOM Kill signal will cause the Notebooks' GPU process to exit but not the notebook itself. This keeps the Interactive pod running and retrying to attach a GPU again. This makes Interactive workloads with notebooks a great use case for burstable workloads and Node Level Scheduler.</p>"},{"location":"Researcher/scheduling/node-level-scheduler/#interactive-notebooks-use-case","title":"Interactive Notebooks Use Case","text":"<p>Consider the following example of a node with 2 GPUs and 2 interactive pods that are submitted and want GPU resources.</p> <p></p> <p>The Scheduler instructs the node to put the two pods on a single GPU, bin packing a single GPU and leaving the other free for a workload that might want a full GPU or more than half GPU. However that would mean GPU#2 is idle while the two notebooks can only use up to half a GPU, even if they temporarily need more.</p> <p></p> <p>However, with Node Level Scheduler enabled, the local decision will be to spread those two pods on two GPUs and allow them to maximize bot pods\u2019 performance and GPUs\u2019 utilization by bursting out up to the full GPU memory and GPU compute resources.</p> <p></p> <p>The Cluster Scheduler still sees a node with a full empty GPU. When a 3rd pod is scheduled, and it requires a full GPU (or more than 0.5 GPU), the scheduler will send it to that node, and Node Level Scheduler will move one of the Interactive workloads to run with the other pod in GPU#1, as was the Cluster Scheduler initial plan.</p> <p></p> <p>This is an example of one scenario that shows how Node Level Scheduler locally optimizes and maximizes GPU utilization and pods\u2019 performance.</p>"},{"location":"Researcher/scheduling/node-level-scheduler/#how-to-configure-node-level-scheduler","title":"How to configure Node Level Scheduler","text":"<p>Node Level Scheduler can be enabled per Node-Pool, giving the Administrator the option to decide which Node-Pools will be used with this new feature.</p> <p>To use Node Level Scheduler the Administrator should follow the steps:</p> <ol> <li> <p>Enable Node Level Scheduler at the cluster level (per cluster), edit the <code>runaiconfig</code> file and set:</p> <pre><code>spec: \n  global: \n      core: \n        nodeScheduler:\n          enabled: true\n</code></pre> <p>The Administrator can also use this patch command to perform the change:</p> <pre><code>kubectl patch -n runai runaiconfigs.run.ai/runai --type='merge' --patch '{\"spec\":{\"global\":{\"core\":{\"nodeScheduler\":{\"enabled\": true}}}}}'\n</code></pre> </li> <li> <p>To enable \u2018GPU resource optimization\u2019 on your tenant\u2019s, go to your tenant\u2019s UI and press Tools &amp; Settings, General, the open the Resources pane and toggle Resource Optimization to on.</p> </li> <li> <p>To enable \u2018Node Level Scheduler\u2019 on any of the Node Pools you want to use this feature, go to the tenant\u2019s UI \u2018Node Pools\u2019 tab (under \u2018Nodes\u2019), and either create a new Node-Pool or edit an existing Node-Pool. In the Node-Pool\u2019s form, under the \u2018Resource Utilization Optimization\u2019 tab, change the \u2018Number of workloads on each GPU\u2019 to any value other than \u2018Not Enforced\u2019 (i.e. 2, 3, 4, 5).</p> </li> </ol> <p>The Node Level Scheduler is now ready to be used on that Node-Pool.</p>"},{"location":"Researcher/scheduling/schedule-to-aws-groups/","title":"Scheduling workloads to AWS placement groups","text":"<p>Run:ai supports AWS placement groups when building and submitting a job. AWS Placement Groups are used to maximize throughput and performance of distributed training workloads.</p> <p>To enable and configure this feature:</p> <ol> <li>Press <code>Jobs | New job</code>.</li> <li>In <code>Scheduling and lifecycle</code> enable the <code>Topology aware scheduling</code>.</li> <li>In <code>Topology key</code>, enter the label of the topology of the node.</li> <li> <p>In <code>Scheduling rule</code> choose <code>Required</code> or <code>Preferred</code> from the drop down.</p> <ul> <li><code>Required</code>\u2014when enabled, all PODs must be scheduled to the same placement group.</li> <li><code>Preferred</code>\u2014when enabled, this is a best-effort, to place as many PODs on the same placement group.</li> </ul> </li> </ol>"},{"location":"Researcher/scheduling/strategies/","title":"Introduction","text":"<p>When the Run:ai scheduler schedules Jobs, it can use two alternate placement strategies:</p> Strategy Description Bin Packing Fill up a GPU or CPU and/or a node before moving on to the next one Spreading Equally spread Jobs amongst GPUs, CPUs and nodes"},{"location":"Researcher/scheduling/strategies/#bin-packing","title":"Bin Packing","text":"<p>Bin packing is the default strategy. With bin packing, the scheduler tries to:</p> <ul> <li>Fill up a node (GPUSs or CPUs) with Jobs before allocating Jobs to second and third nodes.</li> <li>In a multi GPU node, when using fractions, fill up a GPU before allocating Jobs to a second GPU.</li> </ul> <p>The advantage of this strategy is that the scheduler, over time, can package more Jobs into the cluster. As the strategy minimizes fragmentation.</p> <p>In a GPU node, for example, if we have 2 GPUs in a single node on the cluster, and 2 tasks requiring 0.5 GPUs each, using bin-packing, we would place both Jobs on the same GPU and remain with a full GPU ready for the next Job.</p> <p>In a CPU node, for example, if we have 4 CPUs in a single node on the cluster, and 2 tasks requiring 1 CPU each, using bin-packing, we would place both Jobs on the same node and still have more capacity for the next Job.</p>"},{"location":"Researcher/scheduling/strategies/#spreading","title":"Spreading","text":"<p>There are disadvantages to bin-packing:</p> <ul> <li>Within a single GPU, two fractional Jobs compete for the same onboard compute power.</li> <li>Within a single node, two Jobs (even on separate GPUs) compete for networking resources, compute power and memory.</li> </ul> <p>When there are more resources available than requested, it sometimes makes sense to spread Jobs amongst nodes and GPUs, to allow higher utilization of computing resources and network resources.</p> <p>Returning to the example above, if we have 2 GPUs in a single node on the cluster, and 2 Jobs requiring 0.5 GPUs each, using spread scheduling we would place each Job on a separate GPU, allowing both to benefit from the computing power of a full GPU.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/","title":"Introduction","text":"<p>At the heart of the Run:ai solution is the Run:ai scheduler. The scheduler is the gatekeeper of your organization's hardware resources. It makes decisions on resource allocations according to pre-created rules.</p> <p>The purpose of this document is to describe the Run:ai scheduler and explain how resource management works.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#terminology","title":"Terminology","text":""},{"location":"Researcher/scheduling/the-runai-scheduler/#workload-types","title":"Workload Types","text":"<p>Run:ai differentiates between three types of deep learning workloads:</p> <ul> <li>Interactive build workloads. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. Build workloads typically do not tax the GPU for a long duration. There are also typically real users behind an interactive workload that need an immediate scheduling response.</li> <li> <p>Unattended (or \"non-interactive\") training workloads. Training is characterized by a deep learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. Training workloads typically utilize large percentages of the GPU. During the execution, the Researcher can examine the results. A Training session can take anything from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. It follows that a good practice for the Researcher is to save checkpoints and allow the code to restore from the last checkpoint.</p> </li> <li> <p>Inference workloads. These are production workloads that serve requests. The Run:ai scheduler treats these workloads as Interactive workloads.</p> </li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#projects","title":"Projects","text":"<p>Projects are quota entities that associate a Project name with a deserved GPU quota as well as other preferences.</p> <p>A Researcher submitting a workload must associate a Project with any workload request. The Run:ai scheduler will then compare the request against the current allocations and the Project's deserved quota and determine whether the workload can be allocated with resources or whether it should remain in a pending state.</p> <p>For further information on Projects and how to configure them, see: Working with Projects</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#departments","title":"Departments","text":"<p>A Department is the second hierarchy of resource allocation above Project. A Department quota supersedes a Project quota in the sense that if the sum of Project quotas for Department A exceeds the Department quota -- the scheduler will use the Department quota rather than the Projects' quota.  </p> <p>For further information on Departments and how to configure them, see: Working with Departments</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#pods","title":"Pods","text":"<p>Pods are units of work within a Job.</p> <ul> <li>Typically, each Job has a single Pod. However, in some scenarios (see Hyperparameter Optimization and Distribute Training below) there will be multiple Pods per Job.</li> <li>All Pods execute with the same arguments as added via <code>runai submit</code>. E.g. The same image name, the same code script, the same number of Allocated GPUs, memory.</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#basic-scheduling-concepts","title":"Basic Scheduling Concepts","text":""},{"location":"Researcher/scheduling/the-runai-scheduler/#interactive-training-and-inference","title":"Interactive, Training and Inference","text":"<p>The Researcher uses the --interactive flag to specify whether the workload is an unattended \"train\" workload or an interactive \"build\" workload.</p> <ul> <li>Interactive &amp; Inference workloads will get precedence over training workloads.</li> <li>Training workloads can be preempted when the scheduler determines a more urgent need for resources. Interactive workloads are never preempted.</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#guaranteed-quota-and-over-quota","title":"Guaranteed Quota and Over-Quota","text":"<p>There are two use cases for Quota and Over-Quota:</p> <p>Node pools are disabled</p> <p>Every new workload is associated with a Project. The Project contains a deserved GPU quota. During scheduling:</p> <ul> <li>If the newly required resources, together with currently used resources, end up within the Project's quota, then the workload is ready to be scheduled as part of the guaranteed quota.</li> <li>If the newly required resources together with currently used resources end up above the Project's quota, the workload will only be scheduled if there are 'spare' GPU resources. There are nuances in this flow that are meant to ensure that a Project does not end up with an over-quota made fully of interactive workloads. For additional details see below.</li> </ul> <p>Node pools are enabled</p> <p>Every new workload is associated with a Project. The Project contains a deserved GPU quota that is the sum of all node pools GPU quotas. During scheduling:</p> <ul> <li>If the newly required resources, together with currently used resources, end up within the overall Project's quota and the requested node pool(s) quota, then the workload is ready to be scheduled as part of the guaranteed quota.</li> <li>If the newly required resources together with currently used resources end up above the Project's quota or the requested node pool(s) quota, the workload will only be scheduled if there are 'spare' GPU resources within the same node pool but not part of this Project. There are nuances in this flow that are meant to ensure that a Project does not end up with an over-quota made entirely of interactive workloads. For additional details see below.</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#limit-quota-over-or-under-subscription","title":"Limit Quota Over or Under Subscription","text":"<p>Run:ai provides the ability to limit over subscription of quotas by Projects or Departments.</p> <p>Over quota will be limited under the following circumstances:</p> <ul> <li>If a project\u2019s quota request (with or without nodepools) increases the sum of all the department projects quotas to more than the department quota (for GPU and CPU, compute and memory).</li> <li>If a department\u2019s quota decrease request (with or without nodepools) causes the sum of all projects quotas to surpass the department quota (for GPU and CPU, compute and memory).</li> <li>If a project\u2019s quota decrease request (with or without nodepools) causes the sum of all allocated non-preemptible workloads in that project to surpass it (for GPU and CPU, compute and memory).</li> </ul> <p>To configure limiting over or under quota subscriptions:</p> <ol> <li>Press the Tools and settings icon, then go to General.</li> <li>Enable the Limit quota over/under subscription toggle.</li> </ol> <p>To disable the limiting of over quota subscription, disable the toggle.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#quota-with-multiple-resources","title":"Quota with Multiple Resources","text":"<p>A project may have a quota set for more than one resource (GPU, CPU or CPU Memory). For a project to be \"Over-quota\" it will have to have at least one resource over its quota. For a project to be under-quota it needs to have all of its resources under-quota.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#scheduler-details","title":"Scheduler Details","text":""},{"location":"Researcher/scheduling/the-runai-scheduler/#allocation-preemption","title":"Allocation &amp; Preemption","text":"<p>The Run:ai scheduler wakes up periodically to perform allocation tasks on pending workloads:</p> <ul> <li>The scheduler looks at each Project separately and selects the most 'deprived' Project.</li> <li> <p>For this deprived Project it chooses a single workload to work on:</p> <ul> <li>Interactive &amp; Inference workloads are tried first, but only up to the Project's guaranteed quota. If such a workload exists, it is scheduled even if it means preempting a running unattended workload in this Project.</li> <li>Else, it looks for an unattended workload and schedules it on guaranteed quota or over-quota.</li> </ul> </li> <li> <p>The scheduler then recalculates the next 'deprived' Project and continues with the same flow until it finishes attempting to schedule all workloads</p> </li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#node-pools","title":"Node Pools","text":"<p>A Node Pool is a set of nodes grouped by an Administrator into a distinct group of resources from which resources can be allocated to Projects and Departments. By default, any node pool created in the system is automatically associated with all Projects and Departments using zero quota resource (GPUs, CPUs, Memory) allocation. This allows any Project and Department to use any node pool with Over-Quota (for Preemptible workloads), thus maximizing the system resource utilization.</p> <ul> <li>An Administrator can allocate resources from a specific node pool to chosen Projects and Departments. See Project Setup</li> <li>The Researcher can use node pools in two ways. The first one is where a Project has guaranteed resources on node pools - The Researcher can then submit a workload and specify a single node pool or a prioritized list of node pools to use and receive guaranteed resources. The second is by using node-pool(s) with no guaranteed resource for that Project (zero allocated resources), and in practice using Over-Quota resources of node-pools. This means a Workload must be Preemptible as it uses resources out of the Project or node pool quota. The same scenario occurs if a Researcher uses more resources than allocated to a specific node pool and goes Over-Quota.</li> <li>By default, if a Researcher doesn't specify a node-pool to use by a workload, the scheduler assigns the workload to run using the Project's 'Default node-pool list'.</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#node-affinity","title":"Node Affinity","text":"<p>Both the Administrator and the Researcher can provide limitations as to which nodes can be selected for the Job. Limits are managed via Kubernetes labels:</p> <ul> <li>The Administrator can set limits at the Project level. Example: Project <code>team-a</code> can only run <code>interactive</code> Jobs on machines with a label of <code>v-100</code> or <code>a-100</code>. See Project Setup for more information.</li> <li>The Researcher can set a limit at the Job level, by using the command-line interface flag <code>--node-type</code>. The flag acts as a subset to the Project setting.</li> </ul> <p>Node affinity constraints are used during the Allocation phase to filter out candidate nodes for running the Job. For more information on how nodes are filtered see the <code>Filtering</code> section under Node selection in kube-scheduler. The Run:ai scheduler works similarly.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#reclaim","title":"Reclaim","text":"<p>During the above process, there may be a pending workload whose Project is below the deserved capacity. Still, it cannot be allocated due to the lack of GPU resources. The scheduler will then look for alternative allocations at the expense of another Project which has gone over-quota while preserving fairness between Projects.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#fairness","title":"Fairness","text":"<p>The Run:ai scheduler determines fairness between multiple over-quota Projects according to their GPU quota. Consider for example two Projects, each spawning a significant amount of workloads (e.g. for Hyperparameter tuning) all of which wait in the queue to be executed. The Run:ai Scheduler allocates resources while preserving fairness between the different Projects regardless of the time they entered the system. The fairness works according to the relative portion of the GPU quota for each Project. To further illustrate that, suppose that:</p> <ul> <li>Project A has been allocated a quota of 3 GPUs.</li> <li>Project B has been allocated a quota of 1 GPU.</li> </ul> <p>Then, if both Projects go over-quota, Project A will receive 75% (=3/(1+3)) of the idle GPUs and Project B will receive 25% (=1/(1+3)) of the idle GPUs. This ratio will be recalculated every time a new Job is submitted to the system or an existing Job ends.</p> <p>This fairness equivalence will also be maintained amongst running Jobs. The scheduler will preempt training sessions to maintain this balance.</p> <p>Large distributed workloads from one Department may be able to preempt smaller workloads from another Department. You can preserve in-quota workloads even if the department quota or over-quota-fairness is not preserved. This is done by adding the following to the <code>Spec:</code> section of the <code>RunaiConfig</code> file:</p> <pre><code>runai-scheduler:\n     disable_department_fairness: false\n\u00a0\u00a0\u00a0\u00a0\u00a0fullHierarchyFairness: false\n</code></pre> <p>If you want to change the behavior of the scheduler so that you always preserve project guaranteed quota, change <code>runaiconfig.disable_department_fairness</code> to TRUE. This behavior allows you to go back to the original current behavior.</p> <p>By default, the Run:ai software at the cluster level maintains the current fairness mode after\u00a0an upgrade. To change to the fairness mode, the Admin must change the `RunaiConfig file and change the parameters above.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#over-quota-priority","title":"Over-Quota Priority","text":"<p>Over-Quota Priority prioritizes the distribution of unused resources so that they are allocated to departments and projects fairly. Priority is determined by an assigned weight. Over-Quota Priority values are translated into numeric values as follows: None-0, Low-1, Medium-2, High-3.</p> <p>For Projects</p> <p>When Over-Quota Priority is enabled, the excess resources are split between the Projects according to the over-quota-priority weight set by the admin. When Over-Quota Priority is disabled, excess resources are split between the Projects according to weights, and the weights are equal to the deserved quota. This means Projects with a higher deserved quota will get a larger portion of the unused resources.</p> <p>An example with Over-Quota Weights for projects:</p> <ul> <li>Project A has been allocated with a quota of 3 GPUs and GPU over-quota weight is set to Low.</li> <li>Project B has been allocated with a quota of 1 GPU and GPU over-quota weight is set to High.</li> </ul> <p>For Departments</p> <p>When Departments are enabled, excess resources are first split to the departments, then internally to the projects.Excess resources between Departments are always split between departments according to the weights which are equal to the deserved quota regardless of the Over-Quota-Priority flag.</p> <p>Then, Project A is allocated with 3 GPUs and project B is allocated with 1 GPU. If both Projects go over-quota, Project A will receive an additional 25% (=1/(1+3)) of the idle GPUs and Project B will receive an additional 75% (=3/(1+3)) of the idle GPUs.</p> <p>With the addition of node pools, the principles of Over-Quota and Over-Quota priority remain unchanged. However, the number of resources that are allocated with Over-Quota and Over-Quota Priority is calculated against node pool resources instead of the whole Project resources.</p> <p>Note</p> <p>Over-Quota On/Off and Over-Quota Priority settings remain at the Project and Department level.  </p> <p>When the Over Quota Priority is disabled, over-quota weights are equal to deserved quota and any excess resources are divided in the same proportion as the in-quota resources.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#bin-packing-consolidation","title":"Bin-packing &amp; Consolidation","text":"<p>Part of an efficient scheduler is the ability to eliminate fragmentation:</p> <ul> <li>The first step in avoiding fragmentation is bin packing: try and fill nodes (machines) up before allocating workloads to new machines.</li> <li>The next step is to consolidate Jobs on demand. If a workload cannot be allocated due to fragmentation, the scheduler will try and move unattended workloads from node to node in order to get the required amount of GPUs to schedule the pending workload.</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#advanced","title":"Advanced","text":""},{"location":"Researcher/scheduling/the-runai-scheduler/#gpu-fractions","title":"GPU Fractions","text":"<p>Run:ai provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU.</p> <p>Run:ai\u2019s fractional GPU system effectively creates logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors.</p> <p>One important thing to note is that fraction scheduling divides up GPU memory. As such the GPU memory is divided up between Jobs. If a Job asks for 0.5 GPU, and the GPU has 32GB of memory, then the Job will see only 16GB. An attempt to allocate more than 16GB will result in an out-of-memory exception.</p> <p>GPU Fractions are scheduled as regular GPUs in the sense that:</p> <ul> <li>Allocation is made using fractions such that the total of the GPU allocation for a single GPU is smaller or equal to 1.</li> <li>Preemption is available for non-interactive workloads.  </li> <li>Bin-packing &amp; Consolidation work the same for fractions.</li> </ul> <p>Support:</p> <ul> <li>Hyperparameter Optimization supports fractions.</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#distributed-training","title":"Distributed Training","text":"<p>Distributed Training, is the ability to split the training of a model among multiple processors. It is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Each such split is a pod (see definition above). Run:ai spawns an additional launcher process that manages and coordinates the other worker pods.</p> <p>Distribute Training utilizes a practice sometimes known as Gang Scheduling:</p> <ul> <li>The scheduler must ensure that multiple pods are started on what are typically multiple Nodes before the Job can start.</li> <li>If one pod is preempted, the others are also immediately preempted.</li> <li>When node pools are enabled, all pods must be scheduled to the same node pool.</li> </ul> <p>Gang Scheduling essentially prevents scenarios where part of the pods are scheduled while other pods belonging to the same Job are pending for resources to become available; scenarios that can cause deadlock situations and major inefficiencies in cluster utilization.</p> <p>The Run:ai system provides:</p> <ul> <li>Inter-pod communication.</li> <li>Command-line interface to access logs and an interactive shell.</li> </ul> <p>For more information on Distributed Training in Run:ai see here</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, and the number of layers.</p> <p>To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine the results to decide what works best.</p> <p>With HPO, the Researcher provides a single script that is used with multiple, varying, parameters. Each run is a pod (see definition above). Unlike Gang Scheduling, with HPO, pods are independent. They are scheduled independently, started, and end independently, and if preempted, the other pods are unaffected. The scheduling behavior for individual pods is exactly as described in the Scheduler Details section above for Jobs. In case node pools are enabled, if the HPO workload has been assigned with more than one node pool, the different pods might end up running on different node pools.</p> <p>For more information on Hyperparameter Optimization in Run:ai see here</p>"},{"location":"Researcher/scheduling/using-node-pools/","title":"Introduction","text":"<p> Version 2.8 and up.</p> <p>Node pools assist in managing heterogeneous resources effectively. A node pool is a set of nodes grouped into a bucket of resources using a predefined (e.g. GPU-Type) or administrator-defined label (key &amp; value). Typically, those nodes share a common feature or property, such as GPU type or other HW capability (such as Infiniband connectivity) or represent a proximity group (i.e. nodes interconnected via a local ultra-fast switch). Those nodes would typically be used by researchers to run specific workloads on specific resource types, or by MLops engineers to run specific Inference workloads that require specific node types.</p>"},{"location":"Researcher/scheduling/using-node-pools/#enabling-node-pools","title":"Enabling Node-Pools","text":"<p>The Node Pools feature is enabled by default:</p> <ul> <li>To manage CPU resources - enable this feature under  <code>Settings</code> | <code>General</code>. Turn on <code>Enable CPU Resources Quota</code>.</li> </ul> <p>Once the feature is enabled by the administrator, all nodes in each of your upgraded clusters are associated with the <code>Default</code> node pool.</p>"},{"location":"Researcher/scheduling/using-node-pools/#creating-and-using-node-pools","title":"Creating and Using Node-Pools","text":"<p>An administrator creates logical groups of nodes by specifying a unique label (key &amp; value) and associating it with a node pool. Run:ai allows an administrator to use any label key and value as the designated node-pool label (e.g. <code>gpu-type = A100</code> or <code>faculty = computer-science</code>). Each node pool has a unique name and label used to identify and group nodes into a node pool. Once a new node pool is created, it is automatically assigned to all Projects and Departments with a quota of zero GPU resources and CPU resources. This allows any Project and Department to use any node pool when over-quota is enabled, even if the administrator has not assigned a quota for a specific node pool in a Project or Department.</p> <p>Using resources with over-quota means these resources might be reclaimed by other Projects or Departments that have an assigned quota in place for those node pools. On the other hand, this pattern allows for maximizing the utilization of GPU and CPU resources by the system. An administrator should assign resources from a node pool to a project for which the administrator wants to guarantee reserved resources on that node pool. The reservation should be done for GPU resources and CPU resources. Projects and Departments with no reserved resources for a specific node pool can still use node pool resources, but the resources are not reserved and can be reclaimed by the resources owner Project (or Department).</p> <p>Creating a new node pool and assigning resources from a node pool to Projects and Departments is an operation limited to Administrators only. Researchers can use node pools when submitting a new workload. By specifying the node pool from which a workload allocates resources, the scheduler shell launch that workload on a node that is part of the specified node pool. If no node-pool is selected by a workload, the \u2018Default\u2019 node-pool is used.</p>"},{"location":"Researcher/scheduling/using-node-pools/#creating-new-node-pools","title":"Creating New Node Pools","text":"<p>To create a node pool:</p> <ol> <li>From the left menu select Nodes then Node Pools.</li> <li>Press New Nodepool</li> <li>Enter a name, label, and value for the node pool.</li> <li>Select a GPU or CPU placement strategy. Press Save when complete.</li> </ol> <p>To assign nodes to a node pool:</p> <ol> <li> <p>Get the list of nodes and their current labels using the following command:</p> <pre><code>kubectl get nodes --show-labels\n</code></pre> </li> <li> <p>Annotate a specific node with a new label using the following command:</p> <pre><code>kubectl label node &lt;node-name&gt; &lt;key&gt;=&lt;value&gt;\n</code></pre> </li> </ol> <p>Note</p> <ul> <li>You can annotate multiple nodes with the same label.</li> </ul>"},{"location":"Researcher/scheduling/using-node-pools/#node-and-node-pool-status","title":"Node and Node Pool Status","text":"<p>The <code>Status</code> column in the Node and Node pools table enable you to quickly view and diagnose potential issues that you may run into. The following table describes the possible issues you may have with a node or node-pools.</p> Status Description Not ready (Disk pressure) Disk capacity is low. Not ready (Memory pressure) Node memory is low. Not ready (PID pressure) Too many processes on the node. Not ready (Network unavailable) Network is not configured correctly for the node. Not ready (Scheduling disabled) Node might be cordoned and marked as unavailable to the scheduler. Remove the cordon to make it available. Not ready (Undrained migrated node) Evict all pod to make the node ready. Not ready (Missing Nvidia Container Toolkit ) The NVIDIA Container Toolkit enables users to build and run GPU-accelerated containers. Not ready ( Missing Nvidia DCGM Exporter) DCGM-Exporter allows users to gather GPU metrics and understand workload behavior or monitor GPUs in clusters."},{"location":"Researcher/scheduling/using-node-pools/#download-node-and-node-pools-table","title":"Download Node and Node-pools Table","text":"<p>You can download the Node and Node-Pools tables to a CSV file. Downloading a CSV can provide a snapshot history of your node and node-pools over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>To download the Nodes table to a CSV:</p> <ol> <li>From the left menu, press Nodes.</li> <li>From the Columns icon, select the columns you would like to have displayed in the table.</li> <li>Click on the ellipsis labeled More, and download the CSV.</li> </ol> <p>To download the Node-Pools table to a CSV:</p> <ol> <li>In the Nodes, table select Node Pools.</li> <li>From the Columns icon, select the columns you would like to have displayed in the table.</li> <li>Click on the ellipsis labeled More, and download the CSV.</li> </ol>"},{"location":"Researcher/scheduling/using-node-pools/#multiple-node-pools-selection","title":"Multiple Node Pools Selection","text":"<p> Version 2.9 and up</p> <p>Starting version 2.9, Run:ai system supports scheduling workloads to a node pool using a list of prioritized node pools. The scheduler will try to schedule the workload to the most prioritized node pool first, if it fails, it will try the second one and so forth. If the scheduler tried the entire list and failed to schedule the workload, it will start from the most prioritized node pool again. This pattern allows for maximizing the odds that a workload will be scheduled.</p>"},{"location":"Researcher/scheduling/using-node-pools/#defining-project-level-default-node-pool-priority-list","title":"Defining Project level 'default node pool priority list'","text":"<p>If the Researcher did not specify any node pool within the workload specification, the system will use the default node pool priority list as defined by the administrator. If the administrator did not define the *default node pool priority list_, the system will use the <code>Default</code> node pool.</p>"},{"location":"Researcher/scheduling/using-node-pools/#node-pools-best-practices","title":"Node-Pools Best Practices","text":"<p>Node pools give administrators the ability to manage quotas in a more granular manner than the Project level, allowing them to specify which Projects are assigned guaranteed resources on specific sets of nodes to be then used by Workloads that need specific node characteristics. Any Project can use any node pool, even if a quota was not assigned to the Node-Pool, it can still be used in an Over-Quota manner.</p> <p>As a rule of thumb, it is best for the administrator to split the organization's GPU deployment to the smallest number of node pools that still serves its purpose, this would help in keeping each pool large enough and minimize the probability that the Run:ai scheduler would not be able to find available resources on a specific node-pool.</p> <p>It is a good practice for researchers to use multiple node pools where applicable, to maximize their workload odds to get scheduled promptly or in cases where resources are scarce in a specific node pool.</p> <p>Administrators should set Projects' default node pool priority list' to make sure that in case a workload was scheduled with no node pool selection, it is scheduled to the preferences of the Administrator, and to increase the workload's odds to get scheduled and promptly.</p>"},{"location":"Researcher/scheduling/using-node-pools/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Training workloads that require specific GPU-type nodes, either because of the scale of parameters (computation time) or for other specific GPU capabilities</li> <li>Inference workloads that require specific GPU-type nodes to comply with constraints such as execution time</li> <li>Workloads that require proximity of nodes for purposes of local ultra-fast networking</li> <li>Organizations where specific nodes belong to specific a  department, and while assuring quota for that department and its subordinated projects, the administrator also wants to let other departments and projects use those nodes when not used by the resource owner</li> <li>Projects that need to use specific resources, but also ensure others will not occupy those resources</li> </ul> <p>While the upper use cases refer to a single node pool, they are also applicable to multiple node pools. In cases where a workload's specification could be satisfied by more than one type of node, using multiple node pool selection potentially increases the probability of a workload finding resources to allocate and shortening the time it will take to get those resources.</p>"},{"location":"Researcher/tools/dev-jupyter/","title":"Use a Jupyter Notebook with a Run:ai Job","text":"<p>A Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code. Uses include data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Jupyter Notebooks are popular with Researchers as a way to code and run deep-learning code. A Jupyter Notebook runs inside the user container.</p> <p>This document is about accessing the remote container created by Run:ai via such a notebook. Alternatively, Run:ai provides integration with JupyterHub. JupyterHub is a separate service that makes it possible to serve pre-configured data science environments. For more information see Connecting JupyterHub with Run:ai.</p>"},{"location":"Researcher/tools/dev-jupyter/#submit-a-jupyter-notebook-workload","title":"Submit a Jupyter Notebook Workload","text":"<p>There are two ways to submit a Jupyter Notebook Job: via the Command-line interface or the user interface</p>"},{"location":"Researcher/tools/dev-jupyter/#submit-via-the-user-interface","title":"Submit via the User interface","text":"<ul> <li>Within the user interface go to the Job list.</li> <li>Select <code>New Job</code> on the top right.</li> <li>Select <code>Interactive</code> at the top.</li> <li>Add an image that supports Jupyter Notebook. For example <code>jupyter/scipy-notebook</code>.</li> <li>Select the <code>Jupyter Notebook</code> button.</li> </ul> <p>Submit the Job. When running, select the job and press <code>Connect</code> on the top right.</p>"},{"location":"Researcher/tools/dev-jupyter/#submit-a-workload","title":"Submit a Workload","text":"<p>Run the following command to connect to the Jupyter Notebook container as if it were running locally:</p> <pre><code>runai submit build-jupyter --jupyter -g 1\n</code></pre> <p>The terminal will show the following:</p> <pre><code>~&gt; runai submit build-jupyter --jupyter -g 1 --attach\nINFO[0001] Exposing default jupyter notebook port 8888\nINFO[0001] Using default jupyter notebook image \"jupyter/scipy-notebook\"\nINFO[0001] Using default jupyter notebook service type portforward\nThe job 'build-jupyter' has been submitted successfully\nYou can run `runai describe job build-jupyter -p team-a` to check the job status\nINFO[0006] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0081] Job started\nJupyter notebook token: 428dc561a5431bd383eff17714460de478d673deec57c045\nOpen access point(s) to service from localhost:8888\nForwarding from 127.0.0.1:8888 -&gt; 8888\nForwarding from [::1]:8888 -&gt; 8888\n</code></pre> <ul> <li>The Job starts a Jupyter notebook container.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 8888</li> </ul> <p>Browse to http://localhost:8888. Use the token in the output to log into the notebook.</p>"},{"location":"Researcher/tools/dev-jupyter/#alternatives","title":"Alternatives","text":"<p>The above flag <code>--jupyter</code> is a shortcut with a predefined image. If you want to run your own notebook, use the quickstart on running a build workload with connected ports.</p>"},{"location":"Researcher/tools/dev-pycharm/","title":"Use PyCharm with a Run:ai Job","text":"<p>Once you launch a workload using Run:ai, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook</p> <p>This document is about accessing the remote container created by Run:ai, from JetBrain's PyCharm. </p>"},{"location":"Researcher/tools/dev-pycharm/#submit-a-workload","title":"Submit a Workload","text":"<p>You will need your image to run an SSH server  (e.g OpenSSH). For the purposes of this document, we have created an image named <code>gcr.io/run-ai-demo/pycharm-demo</code>. The image runs both python and ssh. Details on how to create the image are here. The image is configured to use the <code>root</code> user and password for SSH.</p> <p>Run the following command to connect to the container as if it were running locally:</p> <pre><code>runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo --interactive  \\\n        --service-type=portforward --port 2222:22\n</code></pre> <p>The terminal will show the connection: </p> <pre><code>The job 'build-remote' has been submitted successfully\nYou can run `runai describe job build-remote -p team-a` to check the job status\nINFO[0007] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0045] Job started\nOpen access point(s) to service from localhost:2222\nForwarding from [::1]:2222 -&gt; 22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 2222</li> </ul> <p>Note</p> <pre><code>It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run:\n\n```\nrunai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22\n```\n\n* The Job starts an sshd server on port 22.\n* The Job redirects the external port 30022 to port 22 and uses a [Node Port](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types){target=_blank} service type.\n* Run: `runai list worklaods`\n\n* Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222\n</code></pre>"},{"location":"Researcher/tools/dev-pycharm/#pycharm","title":"PyCharm","text":"<ul> <li>Under PyCharm | Preferences go to: Project | Python Interpreter </li> <li>Add a new SSH Interpreter. </li> <li>As Host, use the IP address above. Change the port to the above and use the Username <code>root</code></li> <li>You will be prompted for a password. Enter <code>root</code></li> <li>Apply settings and run the code via this interpreter. You will see your project uploaded to the container and running remotely. </li> </ul>"},{"location":"Researcher/tools/dev-tensorboard/","title":"Connecting to TensorBoard","text":"<p>Once you launch a Deep Learning workload using Run:ai, you may want to view its progress. A popular tool for viewing progress is TensorBoard.</p> <p>The document below explains how to use TensorBoard to view the progress or a Run:ai Job.</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-a-workload","title":"Submit a Workload","text":"<p>When you submit a workload, your workload must save TensorBoard logs which can later be viewed. Follow this document on how to do this. You can also view the Run:ai sample code here.</p> <p>The code shows:</p> <ul> <li>A reference to a log directory:</li> </ul> <pre><code>log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n</code></pre> <ul> <li>A registered Keras callback for TensorBoard:</li> </ul> <pre><code>tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n\nmodel.fit(x_train, y_train,\n        ....\n        callbacks=[..., tensorboard_callback])\n</code></pre> <p>The <code>logs</code> directory must be saved on a Network File Server such that it can be accessed by the TensorBoard Job. For example, by running the Job as follows:</p> <pre><code>runai submit train-with-logs -i tensorflow/tensorflow:1.14.0-gpu-py3 \\\n  -v /mnt/nfs_share/john:/mydir -g 1  --working-dir /mydir --command -- ./startup.sh\n</code></pre> <p>Note the volume flag (<code>-v</code>) and working directory flag (<code>--working-dir</code>). The logs directory will be created on <code>/mnt/nfs_share/john/logs/fit</code>.</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-a-tensorboard-workload","title":"Submit a TensorBoard Workload","text":"<p>There are two ways to submit a TensorBoard Workload: via the Command-line interface or the user interface</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-via-the-user-interface","title":"Submit via the User interface","text":"<ul> <li>Within the user interface go to the Job list.</li> <li>Select <code>New Job</code> on the top right.</li> <li>Select <code>Interactive</code> at the top. </li> <li>Add an image that supports TensorBoard. For example: <code>tensorflow/tensorflow:latest</code>.</li> <li>Select the <code>TensorBoard</code> button.</li> <li>Add a mounted volume on which TensorBoard logs exist. The example above uses <code>/mnt/nfs_share/john</code>. Map to <code>/mydir</code></li> <li>Add <code>/mydir</code> to the <code>TensorBoard Logs Directory</code>. </li> </ul> <p>Submit the Job. When running, select the job and press <code>Connect</code> on the top right.</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-via-the-command-line-interface","title":"Submit via the Command-line interface","text":"<p>Run the following:</p> <pre><code>runai submit tb -i tensorflow/tensorflow:latest --interactive --service-type=portforward --port 8888:8888  --working-dir /mydir  -v /mnt/nfs_share/john:/mydir  -- tensorboard --logdir logs/fit --port 8888 --host 0.0.0.0\n</code></pre> <p>The terminal will show the following: </p> <pre><code>The job 'tb' has been submitted successfully\nYou can run `runai describe job tb -p team-a` to check the job status\nINFO[0006] Waiting for job to start\nWaiting for job to start\nINFO[0014] Job started\nOpen access point(s) to service from localhost:8888\nForwarding from 127.0.0.1:8888 -&gt; 8888\nForwarding from [::1]:8888 -&gt; 8888\n</code></pre> <p>Browse to http://localhost:8888/ to view TensorBoard.</p> <p>Note</p> <p>A single TensorBoard Job can be used to view multiple deep learning Jobs, provided it has access to the logs directory for these Jobs. </p> <p>You can also submit a TensorBoard Job via the user interface. In which case, instead of <code>portforward</code> you will need to select a different service type. If the URL to the TensorBoard job includes a path, you may need to use the TensorBoard flag <code>--path_prefix</code>. For example, if your access point is acme.com/tensorboard1 add  <code>--path_prefix /tensorboard1</code>.</p>"},{"location":"Researcher/tools/dev-vscode/","title":"Use Visual Studio Code with a Run:ai Job","text":"<p>Once you launch a workload using Run:ai, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook</p> <p>This document is about accessing the remote container created by Run:ai, from Visual Studio Code. </p>"},{"location":"Researcher/tools/dev-vscode/#submit-a-workload","title":"Submit a Workload","text":"<p>You will need your image to run an SSH server  (e.g OpenSSH). For the purposes of this document, we have created an image named <code>gcr.io/run-ai-demo/pycharm-demo</code>. The image runs both python and ssh. Details on how to create the image are here. The image is configured to use the <code>root</code> user and password for SSH.</p> <p>Run the following command to create the interactive workload: <pre><code>runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo --interactive\n</code></pre></p> <p>Output: <pre><code>Job build-remote submitted successfully.\nYou can check the status of the job by running:\n    runai describe job build-remote -p ziggy\n</code></pre></p> <p>Run the following command to connect to the container as if it were running locally: <pre><code>runai port-forward build-remote --port 2222:22\n</code></pre></p> <p>The terminal will show the connection:  <pre><code>open access point(s) to service from localhost:2222\nForwarding from 127.0.0.1:2222 -&gt; 22\nForwarding from [::1]:2222 -&gt; 22\nHandling connection for 2222\n</code></pre></p> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 2222</li> </ul> <p>Note</p> <pre><code>It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run:\n\n```\nrunai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22\n```\n\n* The Job starts an sshd server on port 22.\n* The Job redirects the external port 30022 to port 22 and uses a [Node Port](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types){target=_blank} service type.\n* Run: `runai list jobs`\n\n* Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222\n</code></pre>"},{"location":"Researcher/tools/dev-vscode/#visual-studio-code","title":"Visual Studio Code","text":"<ul> <li>Under Visual Studio code install the Remote SSH extension.</li> <li>Create an ssh entry to the service by editing .ssh/config file or use the command Remote-SSH: Connect to Host... from the Command Palette.  Enter the IP address and port from above (e.g. ssh root@35.34.212.12 -p 30022 or ssh root@127.0.0.1 -p 2222). User and password are <code>root</code> </li> <li>Using VS Code, install the Python extension on the remote machine </li> <li>Write your first python code and run it remotely.</li> </ul>"},{"location":"Researcher/tools/dev-x11forward-pycharm/","title":"Use PyCharm with X11 Forwarding and Run:ai","text":"<p>X11 is a window system for the Unix operating systems. X11 forwarding allows executing a program remotely through an SSH connection. Meaning, the executable file itself is hosted on a different machine than where the graphical interface is being displayed. The graphical windows are forwarded to your local machine through the SSH connection.</p> <p>This section is about setting up X11 forwarding from a Run:ai-based container to a PyCharm IDE on a remote machine.</p>"},{"location":"Researcher/tools/dev-x11forward-pycharm/#submit-a-workload","title":"Submit a Workload","text":"<p>You will need your image to run an SSH server  (e.g OpenSSH). For the purposes of this document, we have created an image named <code>gcr.io/run-ai-demo/quickstart-x-forwarding</code>. The image runs:</p> <ul> <li>Python</li> <li>SSH Daemon configured for X11Forwarding </li> <li>OpenCV python library for image handling</li> </ul> <p>Details on how to create the image are here. The image is configured to use the <code>root</code> user and password for SSH.</p> <p>Run the following command to connect to the container as if it were running locally:</p> <pre><code>runai submit xforward-remote -i gcr.io/run-ai-demo/quickstart-x-forwarding --interactive  \\\n        --service-type=portforward --port 2222:22\n</code></pre> <p>The terminal will show the connection:</p> <pre><code>The job 'xforward-remote' has been submitted successfully\nYou can run `runai describe job xforward-remote -p team-a` to check the job status\nINFO[0007] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0045] Job started\nOpen access point(s) to service from localhost:2222\nForwarding from [::1]:2222 -&gt; 22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 2222</li> </ul>"},{"location":"Researcher/tools/dev-x11forward-pycharm/#setup-the-x11-forwarding-tunnel","title":"Setup the X11 Forwarding Tunnel","text":"<p>Connect to the new Job by running:</p> <pre><code>ssh -X root@127.0.0.1 -p 2222\n</code></pre> <p>Note the <code>-X</code> flag. </p> <p>Run:</p> <p><pre><code>echo $DISPLAY\n</code></pre> Copy the value. It will be used as a PyCharm environment variable.</p> <p>Important</p> <p>The ssh terminal should remain active throughout the session.</p>"},{"location":"Researcher/tools/dev-x11forward-pycharm/#pycharm","title":"PyCharm","text":"<ul> <li>Under PyCharm | Preferences go to: Project | Python Interpreter</li> <li>Add a new SSH Interpreter.</li> <li>As Host, use <code>localhost</code>. Change the port to the above (<code>2222</code>) and use the Username <code>root</code>.</li> <li>You will be prompted for a password. Enter <code>root</code>.</li> <li>Make sure to set the correct path of the Python binary. In our case it's <code>/usr/local/bin/python</code>.</li> <li> <p>Apply your settings.</p> </li> <li> <p>Under PyCharm configuration set the following environment variables:</p> <ol> <li><code>DISPLAY</code> - set environment variable you copied before</li> <li><code>HOME</code> - In our case it's <code>/root</code>. This is required for the X11 authentication to work.</li> </ol> </li> </ul> <p>Run your code. You can use our sample code here.</p>"},{"location":"Researcher/user-interface/trainings/","title":"Trainings","text":"<p>The Trainings interface provides a wizard to make submitting workloads easy.</p>"},{"location":"Researcher/user-interface/trainings/#prerequisites","title":"Prerequisites","text":"<p>You must have:</p> <ul> <li>Workspaces enabled.</li> <li>At least one Project configured.</li> </ul> <p>Note</p> <p>See your system administrator to ensure the prerequisites are enabled and configured.</p>"},{"location":"Researcher/user-interface/trainings/#adding-trainings","title":"Adding Trainings","text":"<p>Note</p> <p>Where there is a card gallery, use the search bar to find specific cards based on title or field values.</p> <p>To add a training:</p> <ol> <li>Press Tranings in the menu.</li> <li>In the Projects pane, select the destination project. Use the search box to find projects that are not listed. If you can't find the project, you can create your own, or see your system administrator.</li> <li>In the Multi-node pane, choose <code>Single node</code> for a single node training, or <code>Multi-node (distributed)</code> for distributed training. When you choose <code>Multi-node</code>, select a framework that is listed, then select the <code>multi-node</code> training configuration by selecting either <code>Workers &amp; master</code> or <code>Workers only</code>.</li> <li>In the Templates pane, select a template from the list. Use the search box to find templates that are not listed. If you can't find the specific template you need, see Creating a new template, or see your system administrator.</li> <li>In the Training name pane, enter a name for the Training, then press continue.</li> <li> <p>Select an environment from the tiles. If your environment is not listed, use the Search environments box to find it or press New environment to create a new environment. Press  to create an environment if needed. In the Set the connection for your tool(s), enter the URL of the tool if a custom URL has been enabled in the selected environment. Use the Private toggle to lock access to the tool to only the creator of the environment.</p> <p>In the Runtime Settings:</p> <ol> <li>Press Commands and Arguments to add special commands and arguments to your environment selection.</li> <li>Press Environment variable to add an environment variable. Press again if you need more environment variables.</li> </ol> </li> <li> <p>In the Compute resource pane:</p> <ol> <li>Select the number of workers for your training.</li> <li>Select Compute resources for your training or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> </ol> <p>Note</p> <p>The number of compute resources for the workers is based on the number of workers selected.</p> </li> <li> <p>(Optional) Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a *Volume persistency.</li> </ol> </li> <li> <p>(Optional) In the Data sources pane, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> </li> <li> <p>(Optional) In the General pane, add special settings for your training (optional):</p> <ol> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minutes, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails.</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>If you if selected  <code>Workers &amp; master</code> Press Continue to <code>Configure the master</code> and go to the next step. If not, then press Create training.</p> </li> <li> <p>If you do not want a different setup for the master, press Create training. If you would like to have a different setup for the master, toggle the switch to enable to enable a different setup.</p> <ol> <li>In the Environment pane select or create a new environment. Use the search box to find environments that are not listed. Press More settings to add an <code>Environment variable</code> or to edit the Command and Arguments field for the environment you selected.</li> <li>In the Compute resource pane, select a Compute resources for your training or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> <li> <p>(Optional) Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a *Volume persistency.</li> </ol> </li> <li> <p>(Optional) In the Data sources pane, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> </li> <li> <p>(Optional) In the General pane, add special settings for your training (optional):</p> <ol> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minutes, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails.</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> </ol> </li> <li> <p>When your training configuration is complete. press Create training.</p> </li> </ol>"},{"location":"Researcher/user-interface/trainings/#managing-trainings","title":"Managing Trainings","text":"<p>The Trainings list contains a list of training jobs that you have created or have access to.</p> <p>To manage your trainings:</p> <ol> <li>Press Tranings in the left menu.</li> <li>Select a Training from the list.</li> <li>Choose from the following actions:<ul> <li>Activate\u2014activates the selected training job.</li> <li>Stop\u2014stops the selected training job.</li> <li>Connect\u2014connects to the training job's configured environment.</li> <li>Copy &amp; edit\u2014copies the details of the selected training job to a new training job.</li> <li>Delete\u2014deletes the current training session.</li> <li>Show details\u2014displays details about the training job.</li> </ul> </li> </ol>"},{"location":"Researcher/user-interface/trainings/#training-details","title":"Training details","text":"<p>Training details are displayed using the Show details action. The details available per training job include;</p> <ul> <li>Event history\u2014a graph of the job's status over time along with a list of events found in the log.</li> <li> <p>Metrics\u2014a graph of available metrics for the job. Use the drop down select a date and a time slice. Metrics include:</p> </li> <li> <p>GPU utilization</p> </li> <li>GPU memory usage</li> <li>CPU usage</li> <li> <p>CPU memory usage</p> </li> <li> <p>Logs\u2014a log file of the current status. Use the download button to save the logs.</p> </li> </ul> <p>To hide the training details, press Hide details.</p>"},{"location":"Researcher/user-interface/trainings/#download-trainings-table","title":"Download Trainings Table","text":"<p>You can download the Trainings table to a CSV file. Downloading a CSV can provide a snapshot history of your Trainings over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>To download the Trainings table to a CSV: 1. Open Trainings. 2. From the Columns icon, select the columns you would like to have displayed in the table. 3. Click on the ellipsis labeled More, and download the CSV.</p>"},{"location":"Researcher/user-interface/workspaces/overview/","title":"Getting familiar with workspaces","text":"<p> Version 2.9</p> <p>Workspace is a simplified tool for researchers to conduct experiments, build AI models, access standard MLOps tools, and collaborate with their peers.</p> <p>Run:ai workspaces abstract complex concepts related to running containerized workloads in a Kubernetes environment. Aspects such as networking, storage, and secrets, are built from predefined abstracted setups, that ease and streamline the researcher's AI model development.</p> <p>A workspace consists of all the setup and configuration needed for the research, including container images, data sets, resource requests, as well as all required tools for the research, in a single place.  This setup is set to facilitate the research needs and yet to ensure infrastructure owners keep control and efficiency when supporting the various needs.</p> <p>A workspace is associated with a specific Run:ai project (internally: a Kubernetes namespace). A researcher can create multiple workspaces under a specific project.</p> <p>Researchers can only view and use workspaces that are created under projects they are assigned to.</p> <p></p> <p>Workspaces can be created with just a few clicks of a button. See Workspace creation.  </p> <p>Workspaces can be stopped and started to save expensive resources without losing complex environment configurations.</p> <p>Only when a workspace is in status active (see also Workspace Statuses) does it consume resources. </p> <p>When the workspace is active it exposes the connections to the tools (for example, a Jupyter notebook) within the workspace. </p> <p></p> <p>An active workspace is a Run:ai interactive workload. The interactive workload starts when the workspace is started and stopped when the workspace is stopped. </p> <p>Workspaces can be used via the user interface or programmatically via the Run:ai Admin API. Workspaces are not supported via the command line interface. You can still run an interactive workload via the command line. </p>"},{"location":"Researcher/user-interface/workspaces/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Workspaces are made from building blocks. Read about the various building blocks</li> <li>See how to create a Workspace.  </li> </ul>"},{"location":"Researcher/user-interface/workspaces/statuses/","title":"Workspace Statuses","text":"<p>The Workspace\u2019s status mechanism displays the state of the workspace by aggregating various Kubernetes statuses into the following list:</p> Status Description Pending The workspace is waiting in queue and does not consume any resources. Initializing The workspace has been scheduled and it is consuming resources. Active The workspace is ready to be used and allows the researcher to connect. Stopped The workspace is currently unused and does not consume any resources Failed Something went wrong and the workspace is not usable. <p>This allows the researcher to quickly understand whether the workspace is ready to use and if resources are allocated to it. You can hover over the status column to see additional details about the workspace status.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/statuses/#pending-workspace","title":"Pending workspace","text":"<p>The Pending status indicates that the workspace is waiting in queue and does not consume any resources. The workspace will always end up in this state if the workspace was successfully activated but the relevant resources are unavailable.</p>"},{"location":"Researcher/user-interface/workspaces/statuses/#initializing-workspace","title":"Initializing workspace","text":"<p>The Initializing status indicates that the workspace has been scheduled and is consuming resources. However, it is not active yet as its container is still initializing (so it is not possible to connect to the container tools). This step can take anything from a few seconds to a couple of minutes depending on several factors such as the image size to be pulled. The workspace always goes through this state before the workspace turns active.</p>"},{"location":"Researcher/user-interface/workspaces/statuses/#active-workspace","title":"Active workspace","text":"<p>The Active status indicates that the workspace is ready to be used and allows the researcher to connect to its tools. At this status, the workspace is consuming resources and affecting the project\u2019s quota. The workspace will turn to active status once the <code>Active</code> button is pressed, the activation process ends up successfully and relevant resources are available and vacant.</p>"},{"location":"Researcher/user-interface/workspaces/statuses/#stopped-workspace","title":"Stopped workspace","text":"<p>The Stopped status indicates that the workspace is currently unused and does not consume any resources. A workspace can be stopped either manually, or automatically if triggered by idleness criteria set by the admin (see Limit duration of interactive Jobs).</p>"},{"location":"Researcher/user-interface/workspaces/statuses/#failed-workspace","title":"Failed workspace","text":"<p>The Failed status indicates that something went wrong and the workspace is not usable. You must recreate the workspace and try again.</p>"},{"location":"Researcher/user-interface/workspaces/statuses/#transitioning-states","title":"Transitioning states","text":"<p>When the user attempts to delete, stop, or activate a workspace, the status column indicates a transition state which will either be successful or will fail. If the action fails, the workspace will stay in its original status. For example, if the user tries to delete an active workspace and fails, the workspace is left in active status. Transitioning states are only visible in the browser of the user.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/statuses/#download-workspaces-table","title":"Download Workspaces Table","text":"<p>You can download the Workspaces table to a CSV file. Downloading a CSV can provide a snapshot history of your workspaces over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>To download the Workspaces table to a CSV:</p> <ol> <li>Open Workspaces.</li> <li>From the Columns icon, select the columns you would like to have displayed in the table.</li> <li>Click on the ellipsis labeled More, and download the CSV.</li> </ol>"},{"location":"Researcher/user-interface/workspaces/blocks/building-blocks/","title":"Workspace Building Blocks","text":"<p>Workspace building blocks are a layer that abstracts complex containers and Kubernetes concepts and provides simple and reusable tools to quickly allocate resources to the workspace. This way researchers need to interact only with the building blocks, and do not need to be aware of technical setups and configurations.</p> <p>Workspaces are built from the following building blocks:</p> <ol> <li>Environment</li> <li>Data source</li> <li>Compute resource</li> </ol> <p></p> <p>When a workspace is created, the researcher chooses from preconfigured building blocks or can create a new one on the fly. For example, a workspace can be composed of the following blocks:</p> <ul> <li>Environment: Jupyter, Tensor Board and Cuda 11.2</li> <li>Compute resource: 0.5 GPU, 8 cores and 200 Megabytes of CPU memory</li> <li>Data source: A Git branch with the relevant dataset needed</li> </ul> <p></p> <p>A building block has a scope. The scope links a building block to a specific Run:ai project or to all projects:   </p> <ul> <li>When a building block scope is a specific project. It can be viewed and used only within the project.</li> <li>A building block scope can also be set to all projects (current projects and also any future ones).</li> </ul> <p></p> <p>Typically, building blocks are created by the administrator and then assigned to a project. You can grant permission to the researchers to create their own building blocks. These building blocks will only be available to the projects that are assigned to the researcher that created them.</p>"},{"location":"Researcher/user-interface/workspaces/blocks/building-blocks/#next-steps","title":"Next Steps","text":"<p>Read about the various building blocks Environments, Compute Resources and Data Sources.</p>"},{"location":"Researcher/user-interface/workspaces/blocks/compute/","title":"Compute resource introduction","text":"<p>A compute resource building block represents a resource request to be used by the workspace (for example 0.5 GPU, 8 cores and 200 Megabytes of CPU memory). When a workspace is activated, the scheduler looks for a node that can fullfil the request. </p> <p>The compute resource is a mandatory building block for Workspace. A request is composed of the following resources: </p> <ul> <li>GPU resources</li> <li>CPU memory resources</li> <li>CPU cores resources</li> </ul> <p></p> <p>Note</p> <p>GPU resources can be requested as either a memory request, a full GPU request or a fraction of a GPU. A fraction of a GPU also supports the selection of a dynamic MIG profile if configured</p>"},{"location":"Researcher/user-interface/workspaces/blocks/compute/#see-also","title":"See Also","text":"<ul> <li>Create a Compute resource. </li> </ul>"},{"location":"Researcher/user-interface/workspaces/blocks/datasources/","title":"Data source introduction","text":"<p>A data source is a location where data sets relevant to the research are stored. Workspaces can be attached to several data sources for reading and writing. The data can be located locally or in the cloud. Run:ai data sources can use a variety of storage technologies such as Git, S3, NFS, PVC, and more.  </p> <p>The data source is an optional building block for the creation of a workspace.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/blocks/datasources/#see-also","title":"See Also","text":"<ul> <li>Create a Data source. </li> </ul>"},{"location":"Researcher/user-interface/workspaces/blocks/environments/","title":"Environment introduction","text":"<p>The environment block consists of the URL path for the container image and the image pull policy. It exposes all the necessary tools (open source, 3rd party, or custom tools) along with their connection interfaces (See also external node port and the container ports.</p> <p>An environment is a mandatory building block for the creation of a workspace. </p> <p></p> <p>You can also include commands, arguments, and environment variables, as well as the user identity with permission to run the commands in the container.</p> <p>Note</p> <p>Additional arguments and environment variables can be added to workspaces even if they were not defined in the environment building block used by the workspace. This ensures that the same environment can still serve many workspaces, even if they differ in their arguments and environment variables.</p>"},{"location":"Researcher/user-interface/workspaces/blocks/environments/#see-also","title":"See Also","text":"<ul> <li>Create an Environment. </li> </ul>"},{"location":"Researcher/user-interface/workspaces/create/create-compute/","title":"Compute Resource","text":"<p>A compute resource, is assigned to a single project or all projects (current and future ones). The latter option can only be created by a Run:ai administrator. A compute resource, by design, is shared with all project members.</p> <p>You can select one or more resources. For example, one compute resource may consist of a CPU resource request only, whereas a different request can consist of a CPU memory request and a GPU request.</p> <p>Note</p> <p>Selecting resources more than the cluster can supply will result in a permanently failed workspace.</p> <p>Use the Cluster filter at the top of the table to see compute resources that are assigned to specific clusters.</p> <p>Note</p> <p>The cluster filter will be in the top bar when there are clusters that are installed with version 2.16 or lower.</p> <p>Use the Add filter to add additional filters to the table.</p>"},{"location":"Researcher/user-interface/workspaces/create/create-compute/#set-gpu-resources","title":"Set GPU resources","text":"<p>GPU resources can be expressed in various ways:</p> <ol> <li>Request GPU devices: this option supports whole GPUs (for example, 1 GPU, 2 GPUs, 3 GPUs) or a fraction of GPU (for example, 0.1 GPU, 0.5 GPU, 0.93 GPU, etc.)</li> <li>Request partial memory of a single GPU device: this option allows to explicitly state the amount of memory needed (for example, 5GB GPU RAM).</li> <li>Request a MIG profile: this option will dynamically provision the requested MIG profile (if the relevant hardware exists).</li> </ol> <p>Note</p> <ul> <li>Selecting a GPU fraction (for example, 0.5 GPU) in a heterogeneous cluster may result in inconsistent results: For example, half of a V100 16GB GPU memory is different than A100 with 40GB). In such scenarios. Requesting specific GPU memory is a better strategy.</li> <li>When selecting partial memory of a single GPU device, if NVIDIA MIG is enabled on a node, then the memory can be provided as a MIG profile. For more information see Dynamic MIG.</li> <li>If GPUs are not requested, they will not be allocated even if resources are available. In that case, the project's GPU quota will not be affected.</li> </ul>"},{"location":"Researcher/user-interface/workspaces/create/create-compute/#set-cpu-resources","title":"Set CPU resources","text":"<p>A CPU resource consists of cores and memory. When GPU resources are requested the user interface will automatically present a proportional amount of CPU cores and memory (as set on the cluster side).</p> <p>Note</p> <p>If no GPU, CPU and memory resources are defined, the request will not be allocated any GPUs. The scheduler will create a container with no minimal CPU and memory. Such a job will run but is likely to be preempted at any time by other jobs. The scheme is relevant for testing and debugging purposes.  </p>"},{"location":"Researcher/user-interface/workspaces/create/create-compute/#create-a-new-compute-resource","title":"Create a new Compute Resource","text":"<p>To create a compute resource:</p> <ol> <li>Select the <code>New Compute Resource</code> button.</li> <li>In the Scope pane, choose a cluster, department, or project from the tree. The compute resource is assigned to that item and all its subsidiaries.</li> <li>Give the resource a meaningful name.</li> <li>In the resources pane, set the resource request.<ol> <li>To add GPU resources, enter the number of GPUs to request. You can then enter the amount of GPU memory by selecting a percentage of the GPU, memory size in MB or GB, or multi-instance GPUs.</li> <li>To add CPU resources, in the CPU compute pane enter the number of requested resources by choosing cores or millicores. In the CPU memory pane, enter the amount of memory to request, and choose from MB or GB. Select limit if you want limit the number of resources, and enter the limit.</li> </ol> </li> <li>Press More settings to add additional settings to the resource.<ol> <li>Enable Increased shared memory size to increase the default memory size.</li> <li>Press Extended resource to add additional resources. Enter the resource subdomain and name and the quantity of resources to request. For more information, see Extended resources and Quantity.</li> </ol> </li> </ol>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/","title":"Create a new data source","text":"<p>When you select <code>New Compute Resource</code> you will be presented with various data source options described below.</p>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-an-nfs-data-source","title":"Create an NFS data source","text":"<p>To create an NFS data source, provide:</p> <ul> <li>A data source name.</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li>An NFS server.</li> <li>The path to the data within the server.</li> <li>The path within the container where the data will be mounted.</li> </ul> <p>The data can be set as read-write or limited to read-only permission regardless of any other user privileges.</p>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-a-pvc-data-source","title":"Create a PVC data source","text":"<p>To create an PVC data source, provide:</p> <ul> <li>A data source name</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li> <p>Select an existing PVC or create a new one by providing:</p> </li> <li> <p>A claim name</p> </li> <li>A storage class</li> <li>Access mode</li> <li>Required storage size</li> <li> <p>Volume system mode</p> </li> <li> <p>The path within the container where the data will be mounted.</p> </li> </ul> <p>You can see the status of the resources created in the Data sources table.</p>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-an-s3-data-source","title":"Create an S3 data source","text":"<p>S3 storage saves data in buckets. S3 is typically attributed to AWS cloud service but can also be used as a separate service unrelated to Amazon.</p> <p>To create an S3 data source, provide</p> <ul> <li>A data source name</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li>The relevant S3 service URL server</li> <li>The bucket name of the data.</li> <li>The path within the container where the data will be mounted.</li> </ul> <p>An S3 data source can be public or private. For the latter option, please select the relevant credentials associated with the project to allow access to the data. S3 buckets that use credentials will have a status associated with it. For more information, see Data sources table.</p>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-a-git-data-source","title":"Create a Git data source","text":"<p>To create a Git data source, provide:</p> <ul> <li>A data source name.</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li>The relevant repository URL.</li> <li>The path within the container where the data will be mounted.</li> </ul> <p>The Git data source can be public or private. To allow access to a private Git data source, you must select the relevant credentials associated with the project. Git data sources that use credentials will have a status associated with it. For more information, see Data sources table.</p>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-a-host-path-data-source","title":"Create a host path data source","text":"<p>To create a host path data source, provide:</p> <ul> <li>A data source name.</li> <li>A Run:ai scope (cluster, department, or project) which is assigned to that item and all its subsidiaries.</li> <li>The relevant path on the host.</li> <li>The path within the container where the data will be mounted.</li> </ul> <p>Note</p> <p>The data can be limited to read-only permission regardless of any other user privileges.</p>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-a-configmap-data-source","title":"Create a ConfigMap data source","text":"<ul> <li> <p>A Run:ai project scope which is assigned to that item and all its subsidiaries.</p> <p>Note</p> <p>You can only choose a project as a scope.</p> </li> <li> <p>A data source name.</p> </li> <li> <p>A data mount consisting of:</p> </li> <li> <p>A ConfigMap name\u2014select from the drop down.</p> </li> <li>A target location\u2014the path to the container.</li> </ul>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#data-sources-table","title":"Data sources table","text":"<p>The Data sources table contains a column for the status of the data source. The following statuses are supported:</p> Status Description No issues found No issues were found when propagating the data source to the PROJECTS. Issues found Failed to create the data source for some or all of the PROJECTS. Issues found Failed to access the cluster. Deleting The data source is being removed. <p>Note</p> <ul> <li>The Status column in the table shows statuses based on your level of permissions. For example, a user that has create permissions for the scope, will see statuses that are calculated from the entire scope, while users who have only view and use permissions, will only be able to see statuses from a subset of the scope (assets that they have permissions to).</li> <li>The status of \u201c-\u201d indicates that there is no status because this asset is not cluster-syncing.</li> </ul> <p>You can download the Data Sources table to a CSV file. Downloading a CSV can provide a snapshot history of your Data Sources over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>Use the Cluster filter at the top of the table to see data sources that are assigned to specific clusters.</p> <p>Note</p> <p>The cluster filter will be in the top bar when there are clusters that are installed with version 2.16 or lower.</p> <p>Use the Add filter to add additional filters to the table.</p> <p>To download the Data Sources table to a CSV:</p> <ol> <li>Open Data Sources.</li> <li>From the Columns icon, select the columns you would like to have displayed in the table.</li> <li>Click on the ellipsis labeled More, and download the CSV.</li> </ol>"},{"location":"Researcher/user-interface/workspaces/create/create-env/","title":"Creating a new environment","text":"<p>To create an environment:</p> <ol> <li>In the left menu, press New Environment.</li> <li>In the Scope pane, choose a cluster, department, or project from the tree. The environment is assigned to that item and all its subsidiaries.</li> <li>Enter an Environment name.</li> <li>Enter the image URL path and an image pull policy.</li> <li> <p>Choose a supported workload type. Configure this section based on the type of workload you expect to run in this environment. Choose from:</p> <ul> <li>Standard\u2014use for running workloads on a single node.</li> <li>Distributed\u2014use for running distributed workloads on multiple nodes.</li> </ul> <p>Then choose the workload that can use the environment:</p> <ul> <li>Workspace</li> <li>Training</li> <li>Inference</li> </ul> <p>If you selected Inference, in the endpoint pane, select a Protocol from the dropdown, then enter the Container port.</p> </li> <li> <p>Select a tool from the list. You can add multiple tools by pressing + Tool. Selecting a tool is optional.</p> <p>Tools can be:</p> <ul> <li>Different applications such as Code editor IDEs (for example, VS Code), Experiment tracking (for example, Weight and Biases), visualization tools (for example, Tensor Board), and more.</li> <li>Open source tools (for example, Jupyter notebook) or commercial 3rd party tools (for example,. MATLAB)</li> </ul> <p>Note</p> <p>Tool configuration is not supported with Inference environments.</p> <p>It is also possible to set up a custom tool used by the organization.</p> <p>For each tool, you must set the type of connection interface and port. If not set, default values are provided. The supported connection types are:</p> <ul> <li>External URL:  This connection type allows you to connect to your tool either by inserting a custom URL or having one generated for you. Either way, the URL should be unique per workspace as many workspaces may use the same environment. If the URL type was set to custom, the URL will be requested from the Researcher upon creating the workspace.</li> <li>External node port: A NodePort exposes your application externally on every host of the cluster, access the tool using <code>http://&lt;HOST_IP&gt;:&lt;NODEPORT&gt;</code> (for example, http://203.0.113.20:30556).</li> </ul> <p>Note</p> <p>Selecting a tool requires a configuration to be up and running.</p> <p>To configure a tool:</p> <ul> <li>The container image needs to support the tool.</li> <li>The administrator must configure a DNS record and certificate. For more information, see Workspaces configuration.</li> </ul> </li> <li> <p>Configure runtime settings with:</p> <ol> <li>Commands and arguments\u2014visible, but not editable in the workspace creation form.</li> <li>Environment variables\u2014visible and editable in the workspace creation form.</li> <li>Set the container's working directory.</li> </ol> <p>Note</p> <p>The value of an environment variable can remain empty for the researcher to fill in when creating a workspace.</p> </li> <li> <p>Configure the security settings from:</p> <ol> <li>Settings in the image\u2014security settings that come with the image file. </li> <li> <p>Custom settings:</p> <ol> <li>User ID.</li> <li>Group ID.</li> <li>Supplementary Groups.</li> <li>Values modification settings.</li> </ol> </li> <li> <p>Add linux capabilities.</p> </li> </ol> </li> </ol>"},{"location":"Researcher/user-interface/workspaces/create/create-env/#download-environments-table","title":"Download Environments Table","text":"<p>You can download the Environments table to a CSV file. Downloading a CSV can provide a snapshot history of your environments over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>Use the Cluster filter at the top of the table to see environments that are assigned to specific clusters.</p> <p>Note</p> <p>The cluster filter will be in the top bar when there are clusters that are installed with version 2.16 or lower.</p> <p>Use the Add filter to add additional filters to the table.</p> <p>To download the Environments table to a CSV:</p> <ol> <li>In the left menu, press Environments.</li> <li>From the Columns icon, select the columns you would like to have displayed in the table.</li> <li>Click on the ellipsis labeled More, and download the CSV.</li> </ol>"},{"location":"Researcher/user-interface/workspaces/create/workspace-v2/","title":"Create a new workspace","text":"<p>A Workspace is assigned to a project and is affected by the project\u2019s quota just like any other workload. A workspace is shared with all project members for collaboration.</p> <p>Note</p> <ul> <li>You must have at least one project configured in the system. To configure a project, see Creating a project.</li> <li>You must have at least 1 researcher assigned to the project.</li> </ul> <p>Use the Jobs form below if you have not enabled the Workloads feature.</p> Jobs enabledWorkloads enabled <p>To create a new workspace:</p> <ol> <li>Press Workspaces on left menu, then press New workspace.</li> <li>Select a project from the project tiles. If your project is not listed, use the Search projects box to find a project.</li> <li> <p>Select a template from the template tiles. If your template is not listed, use the Search templates box to find a template. Choose Start from scratch if you do not have, or want to use a template.</p> <p>A template contains a set of predefined building blocks as well as additional configurations which allow the user to immediately create a templated-based workspace.</p> </li> <li> <p>Enter a name for your workspace and press Continue.</p> </li> <li> <p>Select an environment from the tiles. If your environment is not listed, use the Search environments box to find it or press New environment to create a new environment. Press  to create an environment if needed. In the Set the connection for your tool(s), enter the URL of the tool if a custom URL has been enabled in the selected environment. Use the Private toggle to lock access to the tool to only the creator of the environment.</p> <p>In the Runtime Settings:</p> <ol> <li>Press Commands and Arguments to add special commands and arguments to your environment selection.</li> <li>Press Environment variable to add an environment variable. Press again if you need more environment variables.</li> </ol> </li> <li> <p>Select a compute resource from the tiles. If your compute resource is not listed, use the Search compute resources box to find it. Press New compute resource to create a compute resource if needed.</p> </li> <li> <p>Open the Volume pane, and press Volume to add a volume to your workspace.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a Volume persistency. Choose from Persistent or Ephemeral.</li> </ol> </li> <li> <p>Select a data source from the tiles. If your data source is not listed, use the Search compute resources box to find it. Press New data source to create a new data source if needed.</p> </li> <li> <p>In the General pane, add special settings for your workspace:</p> <ol> <li>Press Auto-deletion to delete the workspace automatically when it either completes or fails. You can configure the timeframe in days, hours, minutes, and seconds. If the timeframe is set to 0, the workspace will be deleted immediately after it completes or fails.</li> <li>Press Annotation to a name and value to annotate the workspace. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the workspace. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>Press Create workspace</p> </li> </ol> <p>To create a new workspace:</p> <ol> <li>Press Workloads on left menu, then press New workload, then choose Workspace.</li> <li>Select a project from the project tiles. If your project is not listed, use the Search projects box to find a project.</li> <li> <p>Select a template from the template tiles. If your template is not listed, use the Search templates box to find a template. Choose Start from scratch if you do not have, or want to use a template.</p> <p>A template contains a set of predefined building blocks as well as additional configurations which allow the user to immediately create a templated-based workspace.</p> </li> <li> <p>Enter a name for your workspace and press Continue.</p> </li> <li> <p>Select an environment from the tiles. If your environment is not listed, use the Search environments box to find it. Press New environment to create an environment if needed. In the Set the connection for your tool(s), enter the URL of the tool if a custom URL has been enabled in the selected environment. Use the Private toggle to lock access to the tool to only the creator of the environment.</p> <p>In the Runtime Settings:</p> <ol> <li>Press Commands and Arguments to add special commands and arguments to your environment selection.</li> <li>Press Environment variable to add an environment variable. Press again if you need more environment variables.</li> </ol> </li> <li> <p>Select a compute resource from the tiles. If your compute resource is not listed, use the Search compute resources box to find it. Press New compute resource to create a compute resource if needed.</p> </li> <li> <p>Open the Volume pane, and press Volume to add a volume to your workspace.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a Volume persistency. Choose from Persistent or Ephemeral.</li> </ol> </li> </ol> <ol> <li>Select a data source from the tiles. If your data source is not listed, use the Search data resources box to find it. Press New data source to create a new data source if needed.</li> <li> <p>In the General pane, add special settings for your workspace:</p> <ol> <li>Press Auto-deletion to delete the workspace automatically when it either completes or fails. You can configure the timeframe in days, hours, minuets, and seconds. If the timeframe is set to 0, the workspace will be deleted immediately after it completes or fails.</li> <li>Press Annotation to a name and value to annotate the workspace. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the workspace. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>Press Create workspace</p> </li> </ol>"},{"location":"Researcher/user-interface/workspaces/create/workspace/","title":"Workspaces actions and use cases","text":""},{"location":"Researcher/user-interface/workspaces/create/workspace/#create-a-new-workspace","title":"Create a new workspace","text":"<p>A Workspace is assigned to a project and is affected by the project\u2019s quota just like any other workload. A workspace is shared with all project members for collaboration.</p> <p>To create a workspace, you must provide:</p> <ul> <li>At least one project</li> <li>A researcher assigned to at least one project</li> </ul> <p>To create a workspace, the researcher must select building blocks  in one of two ways:</p> <ul> <li>Create a workspace from scratch:  this allows you to either select an existing building block or create them on the fly (pending the right permissions).</li> <li>Create a workspace from a template: a template contains a set of predefined building blocks as well as additional configurations which allow the user to immediately create a templated-based workspace.</li> </ul> <p>Note</p> <p>Where there is a card gallery, use the search bar to find specific cards based on title or field values.</p> <p>To create a workspace:</p> <ul> <li>Press <code>New Workspace</code></li> <li>Select a project for the new workspace. The project visualization contains information about the project such as how much of the quota is being allocated and indicates the likelihood of the workspace being scheduled or left in the queue</li> </ul>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#create-a-new-workspace-from-scratch","title":"Create a new workspace from scratch","text":"<p>See picture:</p> <p></p> <p>Note</p> <p>The building block can also be created (and then selected) directly from within the workspace creation form.</p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#select-an-environment-for-a-new-workspace","title":"Select an Environment for a new workspace","text":"<p>An environment is a mandatory element of a workspace. All environments created for the project will be shown to researchers in the form of a gallery view (see also Creating a new environment). Each tile shows the tools as well as the image. When selecting an environment, the command, arguments and environment variables defined in the environment are visible for review. The researcher can edit arguments and environment variables that are specific to the current workspace and that are not part of the common shared environment. In some cases, it would even be expected that the researcher will provide additional information (for example, values for environment variables) to successfully create the workspace (see also Create new environment).</p> <p></p> <p>You can also decide whether the workspace is preemptible or not (see also create a preemptible worksapce). By default, interactive sessions are limited to the project\u2019s GPU, meaning that they can only be scheduled (and activated) when there is an available and sufficient GPU quota.  With the following parameter, the researcher can determine whether the workspace is allowed to go over-quota with the understanding that it can be preempted if other projects would demand back their quota.</p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#select-a-compute-resource-for-a-new-workspace","title":"Select a compute resource for a new workspace","text":"<p>Selecting compute resources for the workspace is a mandatory step. If compute resources are created for the project (see also creating a new compute resource), those will be offered to researchers in the form of a gallery view. Each tile shows the amount of GPU, CPU and Memory in the request.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#select-a-data-source-for-a-new-workspace","title":"Select a data source for a new workspace","text":"<p>Selecting a data source for the workspace is a non-mandatory step. If data sources are created for the project (see also creating a new compute resource), those will be offered to researchers in the form of a gallery view. Each tile shows the unique name of the building block and the type of data source.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#create-a-new-workspace-from-a-template","title":"Create a new workspace from a template","text":"<p>Templates ease the way of creating a new workspace in a few clicks. In contrast to creating a workspace from scratch (selecting manually which building blocks to use in your workspace), a template aggregates all building blocks under a single entity for researchers to use for the creation of workspaces.</p> <p></p> <p>A Template consists of the building blocks and other parameters that are exposed in a workspace creation form. Templates can be fully defined to a point researcher can select and create the workspace without providing any additional information or partially defined, hence, leaving some degree of freedom in the creation of the workspace via the template. This can help in cases where only part of the configuration is selected in the template and the rest is expected to be provided by the user creating a workspace from the template.</p> <p>Few examples:</p> <ul> <li>A template can have the value of an environment variable empty for the researcher to edit later during the workspace creation.</li> <li>A template can consist of an environment with a tool that requests a custom URL. This URL field stays empty until the researcher fills it upon creating the workspace</li> </ul> <p>For collaboration purposes, templates are assigned to a specific project and are shared with all project members by design.</p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#create-a-preemptible-workspace","title":"Create a preemptible workspace","text":"<p>For a better experience, workspaces, as they are built for interactive research, are designed to not be preempted (because the researchers actively interact with GPU resources). Thus, non-preemptible workspaces can be only scheduled if the project has a sufficient vacant quota. However, if that\u2019s not the case (the project does not have a sufficient vacant quota) and the researcher still needs to create and activate a workspace (if cluster resources are available) he/she can allow the workspace to go over-quota, thus be scheduled, but with the cost of preemption without prior notice.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#download-workspaces-table","title":"Download Workspaces Table","text":"<p>You can download the Workspaces table to a CSV file. Downloading a CSV can provide a snapshot history of your workspaces over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>To download the Workspaces table to a CSV: 1. Open Workspaces. 2. From the Columns icon, select the columns you would like to have displayed in the table. 3. Click on the ellipsis labeled More, and download the CSV.</p>"},{"location":"admin/overview-administrator/","title":"Overview: Administrator Documentation","text":"<p>The role of Administrators is to set up Run:ai and perform day-to-day monitoring and maintenance. </p> <p>As part of the Administrator documentation you will find:</p> <ul> <li>Run:ai Setup How to set up and modify a GPU cluster with Run:ai.</li> <li>Researcher Setup How to set up Researchers to work with Run:ai.</li> <li>How to configure Workloads and Workload Policies.</li> <li>Setting and maintaining the cluster via the  Run:ai User Interface.</li> <li>Troubleshooting Run:ai and understanding cluster health.</li> <li>Integrations of Run:ai with a variety of other systems.</li> </ul>"},{"location":"admin/admin-ui-setup/admin-ui-users/","title":"Adding, Updating and Deleting Users","text":""},{"location":"admin/admin-ui-setup/admin-ui-users/#introduction","title":"Introduction","text":"<p>The Run:ai UI allows you to manage all of the users in the Run:ai platform. There are two types of users, local users and SSO users. Local users are users that are created and managed in the Run:ai platform and SSO users are authorized to use the Run:ai platform using an identity provider. All users are assigned levels of access to all aspects of the UI including submitting jobs on the cluster.</p> <p>Tip</p> <p>It is possible to connect the Run:ai UI to the organization's directory and use single sign-on (SSO). This allows you to set Run:ai roles for users and groups from the organizational directory. For further information see single sign-on configuration.</p>"},{"location":"admin/admin-ui-setup/admin-ui-users/#create-a-user","title":"Create a User","text":"<p>Note</p> <ul> <li>To be able to review, add, update and delete users, you must have System Administrator access. To upgrade your access, contact a system administrator.</li> <li>When SSO is enabled, users created using the procedure below will be local users only. You can only provide SSO users with access to the system. You cannot create new SSO users.</li> </ul> <p>To create a new user:</p> <ol> <li>Login to the Run:ai UI at <code>company-name.run.ai</code>.</li> <li>Press the  icon, then select Users.</li> <li>Press New user and enter the user's email address, then press Create.</li> <li>Review the new user information and note the temporary password that has been assigned.</li> <li>Press Done when complete.</li> </ol>"},{"location":"admin/admin-ui-setup/admin-ui-users/#users-table","title":"Users Table","text":"<p>The Users table displays a list of users who are authorized to login to the Run:ai platform.</p> <p>Note</p> <p>SSO users who have not logged into the Run:ai platform will not appear in the table until they do.</p> <p>The Users table contains information about local users which have been created and SSO users who have logged into the Run:ai Platform. The table includes columns for:</p> <ul> <li>User\u2014the email address which is the user identifier.</li> <li>Type\u2014the type of user (local or SSO).</li> <li>Last login\u2014the last time the user logged into the platform.</li> <li>Access rule(s)\u2014press VIEW to view the access rule(s) assigned to the user.</li> <li>Created by\u2014indicates who created the user in the able.</li> <li>Creation time\u2014the timestamp for when the user was created.</li> <li>Last updated\u2014the last time the user information was updated.</li> </ul> <p>When you press View in the Access rule(s) column, a pop-up will appear that displays the rules assigned to the user. In the popup are the following columns:</p> <ul> <li>Role\u2014the name of the role assigned to the user.</li> <li>Scope\u2014the scope to which the user has rights.</li> <li>Type\u2014the type of subject assigned to the access rule (User or SSO group)</li> <li>Group\u2014the name of the group assigned to the access rule, in case the access rule is inherited from an SSO group.</li> <li>Authorized by\u2014the user who granted the access rule.</li> <li>Creation time\u2014the timestamp for when the rule was created.</li> <li>Last updated\u2014the last time the rule information was updated.</li> </ul>"},{"location":"admin/admin-ui-setup/admin-ui-users/#assigning-access-rules-to-users","title":"Assigning access rules to users","text":"<p>Once you have created the users you can assign them Access rules. This provides the needed authorization to access system assets and resources.</p>"},{"location":"admin/admin-ui-setup/admin-ui-users/#roles-and-permissions","title":"Roles and permissions","text":"<p>Roles provide a way for administrators to group and identify collections of permissions that administrators assign to subjects. Permissions define the actions that can be performed on managed entities. The Roles table shows the default roles and permissions that come with the system. See Role based access control for more information.</p> <p>To add an Access rule to a user:</p> <ol> <li>Select the user, then press Access rules, then press +Access rule.</li> <li>Select a Role from the dropdown.</li> <li>Press  then select a scope for the user. You can select multiple scopes.</li> <li>After selecting all the required scopes, press Save rule.</li> <li>To add another rule, use the +Access rule.</li> <li>Press Done when all the rules are configured.</li> </ol>"},{"location":"admin/admin-ui-setup/credentials-setup/","title":"Credentials","text":"<p>Credentials are used to unlock protected resources such as applications, containers, and other assets.</p> <p>The Credentials manager in the Run:ai environment supports 3 types of credentials:</p> <ol> <li>Docker registry</li> <li>Access key</li> <li>User name and password</li> </ol>"},{"location":"admin/admin-ui-setup/credentials-setup/#secrets","title":"Secrets","text":"<p>Credentials are built using <code>Secrets</code>. A <code>Secret</code> is an object that contains a small amount of sensitive data so that you don't need to include confidential data in your application code. When creating a credential you can either create a new secret or use an existing secret.</p>"},{"location":"admin/admin-ui-setup/credentials-setup/#existing-secrets","title":"Existing secrets","text":"<p>An existing secret is a secret that you have created before creating the credential. One way to create a secret is to use the Kubernetes Secrets creation tool to create a pre-existing secret for the credential. You must <code>label</code> these secrets so that they are registered in the Run:ai environment.</p> <p>The following command makes the secret available to all projects in the cluster.</p> <pre><code>kubectl label secret -n runai &lt;SECRET_NAME&gt; run.ai/cluster-wide-credentials=true\n</code></pre> <p>The following command makes the secret available to the entire scope of a department.</p> <pre><code>kubectl label secret -n runai &lt;SECRET_NAME&gt; run.ai/resource=&lt;credential_type&gt; run.ai/department=&lt;department-id&gt;\n</code></pre> <p><code>credential_type</code> is one of the following: <code>password</code> / <code>access-key</code> / <code>docker-registry</code></p> <p>The following command makes the secret available to a specific project in the cluster.</p> <pre><code>kubectl label secret -n &lt;NAMESPACE_OF_PROJECT&gt; &lt;SECRET_NAME&gt; run.ai/credentials=true\n</code></pre>"},{"location":"admin/admin-ui-setup/credentials-setup/#user-id-and-password","title":"User-id and password","text":"<p>You can create a credential using a user-id and password. Use the user-id and password of the target resource.</p>"},{"location":"admin/admin-ui-setup/credentials-setup/#configuring-credentials","title":"Configuring Credentials","text":"<p>Important</p> <p>To configure Credentials you need to make sure <code>Workspaces</code> are enabled.</p> <p>To configure Credentials:</p> <ol> <li>Press <code>Credentials</code> in the left menu.</li> <li>Press <code>New Credential</code> and select one from the list.</li> </ol>"},{"location":"admin/admin-ui-setup/credentials-setup/#docker-registry","title":"<code>Docker registry</code>","text":"<ol> <li>Select a <code>Scope</code> (cluster, department, or project) for the credential.</li> <li>In the <code>Credential name</code> field, enter a name for the credential.</li> <li> <p>In the <code>Secret</code> field, choose from <code>Existing secret</code> or <code>New secret</code>.</p> <ul> <li> <p>If you select <code>Existing secret</code>, select an unused secret from the drop down.</p> <p>Note</p> <p>Existing secrets can't be used more than once.</p> </li> <li> <p>If you choose <code>New secret</code>, enter a username and password.</p> </li> </ul> </li> <li> <p>Enter a URL for the docker registry, then press <code>Create credential</code> to create the credential.</p> </li> </ol>"},{"location":"admin/admin-ui-setup/credentials-setup/#access-key","title":"<code>Access key</code>","text":"<ol> <li>Select a <code>Scope</code> (cluster, department, or project) for the credential.</li> <li>In the <code>Credential name</code> field, enter a name for the credential.</li> <li> <p>In the <code>Secret</code> field, choose from <code>Existing secret</code> or <code>New secret</code>.</p> <ul> <li> <p>If you select <code>Existing secret</code>, select an unused secret from the drop down.</p> <p>Note</p> <p>Existing secrets can't be used more than once.  </p> </li> <li> <p>If you choose <code>New secret</code>, enter an access key and access secret.</p> </li> </ul> </li> <li> <p>Press <code>Create credential</code> to create the credential.</p> </li> </ol>"},{"location":"admin/admin-ui-setup/credentials-setup/#username-and-password","title":"<code>Username and password</code>","text":"<ol> <li>Select a <code>Scope</code> (cluster, department, or project) for the credential.</li> <li>In the <code>Credential name</code> field, enter a name for the credential.</li> <li> <p>In the <code>Secret</code> field, choose from <code>Existing secret</code> or <code>New secret</code>.</p> <ul> <li> <p>If you select <code>Existing secret</code>, select an unused secret from the drop down.</p> <p>Note</p> <p>Existing secrets can't be used more than once.</p> </li> <li> <p>If you choose <code>New secret</code>, enter a username and password.</p> </li> </ul> </li> <li> <p>Press <code>Create credential</code> to create the credential.</p> </li> </ol>"},{"location":"admin/admin-ui-setup/credentials-setup/#credentials-table","title":"Credentials Table","text":"<p>The Credentials table contains a column that shows the status of the credential. The following statuses are supported:</p> Status Description No issues found No issues were found when propagating the credential to the configured scope. Issues found Issues were found while propagating the credentials to the configured scope. Issues found The credential could not be created in the cluster. No status Status could not be displayed because the credentials scope is an account. No Status Status could not be displayed because the current version of the cluster is not up to date. <p>You can download the Credentials table to a CSV file. Downloading a CSV can provide a snapshot history of your credentials over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>Use the Cluster filter at the top of the table to see credentials that are assigned to specific clusters.</p> <p>Note</p> <p>The cluster filter will be in the top bar when there are clusters that are installed with version 2.16 or lower.</p> <p>Use the Add filter to add additional filters to the table.</p> <p>To download the Credentials table to a CSV:</p> <ol> <li>Open Credentials.</li> <li>From the Columns icon, select the columns you would like to have displayed in the table.</li> <li>Click on the ellipsis labeled More, and download the CSV.</li> </ol>"},{"location":"admin/admin-ui-setup/dashboard-analysis/","title":"Introduction","text":"<p>The Run:ai Administration User Interface provides a set of dashboards that help you monitor Clusters, Cluster Nodes, Projects, and Workloads. This document provides the key metrics to monitor, how to assess them as well as suggested actions.</p> <p>Dashboards are used by system administrators to analyze and diagnose issues that relate to:</p> <ul> <li>Physical Resources.</li> <li>Organization resource allocation and utilization.</li> <li>Usage characteristics.</li> </ul> <p>System administrators need to know important information about the physical resources that are currently being used. Important information such as:</p> <ul> <li>Resource health.</li> <li>Available resources and their distribution.</li> <li>Is there a lack of resources.</li> <li>Are resources being utilized correctly.</li> </ul> <p>With this information, system administrators can hone in on:</p> <ul> <li>How resources are allocated across the organization.</li> <li>How the different organizational units utilized quotas and resources within those quotas.</li> <li>The actual performance of the organizational units.</li> </ul> <p>These dashboards give system administrators the ability to drill down to see details of the different types of workloads that each of the organizational units is running. These usage and performance metrics ensure that system administrators can then take actions to correct issues that affect performance.</p> <p>There are 5 dashboards:</p> <ul> <li>GPU/CPU Overview dashboard\u2014Provides information about what is happening right now in the cluster.</li> <li>Quota Management dashboard\u2014Provides information about quota utilization.</li> <li>Analytics dashboard\u2014Provides long term analysis of cluster behavior.</li> <li>Multi-Cluster Overview dashboard\u2014Provides a more holistic, multi-cluster view of what is happening right now. The dashboard is intended for organizations that have more than one connected cluster.</li> <li>Consumption dashboard\u2014Provides information about resource consumption.</li> </ul>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#gpucpu-overview-dashboard-new-and-legacy","title":"GPU/CPU Overview Dashboard (New and legacy)","text":"<p>The Overview dashboard provides information about what is happening right now in the cluster.  Administrators can view high-level information on the state of the cluster. The dashboard has two tabs that change the display to provide a focused view for GPU Dashboards (default view) and CPU Dashboards.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#gpu-dashboard","title":"GPU Dashboard","text":"<p>The GPU dashboard displays specific information for GPU based nodes, node-pools, clusters, or tenants. These dashboards also include additional metrics that specific to GPU based environments. The dashboard contains tiles that show information about specific resource allocation and performance metrics. The tiles are interactive allowing you to link directly to the assets or drill down to specific scopes. Use the time frame selector to choose a time frame for all the tiles in the dashboard.</p> <p>The dashboard has the following tiles:</p> <ul> <li>Ready nodes\u2014displays GPU nodes that are in the ready state.</li> <li>Ready GPU devices\u2014displays the number of GPUs in nodes that are in the ready state.</li> <li>Allocated GPU compute\u2014displays the total number of GPUs allocated from all the nodes.</li> <li>Running workloads\u2014displays the number of running workloads.</li> <li>Pending workloads\u2014displays the number of workloads in the pending status.</li> <li>Allocation ration by node pool\u2014displays the percentage of GPUs allocated per node pool. Hover over the bar for detailed information. Use the scope selected at the bottom of the graph to drill down for more details.</li> <li>Free resources by node pool\u2014the graph displays the amount of free resources per node pool. Press a entry in the graph for more details. Hover over the resource bubbles for specific details for the workers in the node. Use the ellipsis to download the graph as a CSV file.</li> <li>Resource allocation by workload type\u2014displays the resource allocation by workload type. Hover over the bar for detailed information. Use the scope selected at the bottom of the graph to drill down for more details. Use the ellipsis to download the graph as a CSV file.</li> <li>Workload by status\u2014displays the number of workloads for each status in the workloads table. Hover over the bar for detailed information. Use the scope selected at the bottom of the graph to drill down for more details. Use the ellipsis to download the graph as a CSV file.</li> <li>Resources utilization\u2014displays the resource utilization over time. The right pane of the graph shows the average utilization of the selected time frame of the dashboard. Hover over the graph to see details of a specific time in the graph. Use the ellipsis to download the graph as a CSV file.</li> <li>Resource allocation\u2014displays the resource allocation over time. The right pane of the graph shows the average allocation of the selected time frame of the dashboard. Hover over the graph to see details of a specific time in the graph. Use the ellipsis to download the graph as a CSV file.</li> </ul>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#cpu-dashboard","title":"CPU Dashboard","text":"<p>The CPU dashboards display specific information for CPU based nodes, node-pools, clusters, or tenants. These dashboards also include additional metrics that specific to CPU based environments.</p> <p>To enable CPU Dashboards:</p> <ol> <li>Press the <code>Tools &amp; Settings</code> icon, then press <code>General</code>.</li> <li>Open the <code>Analytics</code> pane and toggle the Show CPU dashboard switch to enable the feature.</li> </ol> <p>Toggle the switch to <code>disable</code> to disable CPU Dashboards option.</p> <p>The dashboard contains the following tiles:</p> <ul> <li>Total CPU Nodes\u2014displays the total amount of CPU nodes.</li> <li>Ready CPU nodes\u2014displays the total amount of CPU nodes in the ready state.</li> <li>Total CPUs\u2014displays the total amount of CPUs.</li> <li>Ready CPUs\u2014displays the total amount of CPUs in the ready state.</li> <li>Allocated CPUs\u2014displays the amount of allocated CPUs.</li> <li>Running workloads\u2014displays the amount of workloads in the running state.</li> <li>Pending workloads\u2014displays the amount of workloads in the pending state.</li> <li>Allocated CPUs per project\u2014displays the amount of CPUs allocated per project.</li> <li>Active projects\u2014displays the active projects  with the CPU allocation and amount of running and pending workloads.</li> <li>Utilization per resource type\u2014displays the CPU compute and CPU memory utilization over time.</li> <li>CPU compute utilization\u2014displays the current CPU compute utilization.</li> <li>CPU memory utilization\u2014displays the current CPU memory utilization.</li> <li>Pending workloads\u2014displays the requested resources and wait time for workloads in the pending status.</li> <li>Workloads with error\u2014displays the amount of workloads that are currently not running due to an error.</li> <li>Workload Count per CPU Compute Utilization\u2014</li> <li>5 longest running workloads\u2014displays up to 5 of workloads that have the longest running time.</li> </ul> <p>Analysis and Suggested actions:</p> Review Analysis  &amp; Actions Interactive Workloads are too frequently idle Consider setting time limits for interactive Workloads through the Projects tab.\u00a0  Consider also reducing GPU/CPU quotas for specific Projects to encourage users to run more training Workloads as opposed to interactive Workloads (note that interactive Workloads can not use more than the GPU/CPU quota assigned to their Project). Training Workloads are too frequently idle Identify and notify the right users and work with them to improve the utilization of their training scripts"},{"location":"admin/admin-ui-setup/dashboard-analysis/#workloads-with-an-error","title":"Workloads with an Error","text":"<p>Search for Workloads with an error status. These Workloads may be holding GPUs/CPUs without actually using them.</p> <p>Analysis and Suggested actions:</p> <p>Search for workloads with an Error status on the Workloads view and discuss with the Job owner. Consider deleting these Workloads to free up the resources for other users.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#workloads-with-a-long-duration","title":"Workloads with a Long Duration","text":"<p>View list of 5 longest Workloads.</p> <p>Analysis and Suggested actions:</p> Review Analysis &amp; Actions Training Workloads run for too long Ask users to view their Workloads and analyze whether useful work is being done. If needed, stop their Workloads. Interactive Workloads run for too long Consider setting time limits for interactive Workloads via the Project editor."},{"location":"admin/admin-ui-setup/dashboard-analysis/#job-queue","title":"Job Queue","text":"<p>Identify queueing bottlenecks.</p> <p>Analysis and Suggested actions:</p> Review Analysis &amp; Actions Cluster is fully loaded Go over the table of active Projects and check that fairness between Projects was enforced, by reviewing the number of allocated GPUs/CPUs for each Project, ensuring each Project was allocated with its fair-share portion of the cluster. Cluster is not fully loaded Go to the Workloads view to review the resources requested for that Job (CPU, CPU memory, GPU, GPU memory). Go to the Nodes view to verify that there is no Node with enough free resources that can host that Job. <p>Also, check the command that the user used to submit the job. The Researcher may have requested a specific Node for that Job.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#analytics-dashboard","title":"Analytics Dashboard","text":"<p>The Analytics dashboard provides means for viewing historical data on cluster information such as:</p> <ul> <li>Utilization across the cluster</li> <li>GPU usage by different Projects, including allocation and utilization, broken down into interactive and training Workloads</li> <li>Breakdown of running Workloads into interactive, training, and GPU versus CPU-only Workloads, including information on queueing (number of pending Workloads and requested GPUs),</li> <li>Status of Nodes in terms of availability and allocated and utilized resources.</li> </ul> <p>The dashboard has a dropdown filter for node pools and Departments. From the dropdown, select one or more node pools. The default setting is <code>all</code>.</p> <p>The information presented in Analytics can be used in different ways for identifying problems and fixing them. Below are a few examples.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#node-downtime","title":"Node Downtime","text":"<p>View the overall available resources per Node and identify cases where a Node is down and there was a reduction in the number of available resources.</p> <p>How to: view the following panel.</p> <p></p> <p>Analysis and Suggested actions:</p> <p>Filter according to time range to understand for how long the Node is down.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#gpu-allocation","title":"GPU Allocation","text":"<p>Track GPU allocation across time.</p> <p>How to: view the following panels.</p> <p></p> <p>The panel on the right-hand side shows the cluster-wide GPU allocation and utilization versus time, whereas the panels on the left-hand side show the cluster-wide GPU allocation and utilization averaged across the filtered time range.</p> <p>Analysis and Suggested actions:</p> <p>If the allocation is too low for a long period, work with users to run more workloads and to better utilize the Cluster.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#track-gpu-utilization","title":"Track GPU utilization","text":"<p>Track whether Researchers efficiently use the GPU resources they have allocated for themselves.</p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>If utilization is too low for a long period, you will want to identify the source of the problem:</p> <ul> <li>Go to \u201cAverage GPU Allocation &amp; Utilization\u201d</li> <li>Look for Projects with large GPU allocations for interactive Workloads or Projects that poorly utilize their training Workloads. Users tend to poorly utilize their GPUs in interactive sessions because of the dev &amp; debug nature of their work which typically is an iterative process with long idle GPU time. On many occasions users also don\u2019t shut down their interactive Workloads, holding their GPUs idle and preventing others from using them.</li> </ul> Review Analysis &amp; Actions Low GPU utilization is due to interactive Workloads being used too frequently Consider setting time limits for interactive Workloads through the Projects tab or reducing GPU quotas to encourage users to run more training Workloads as opposed to interactive Workloads (note that interactive Workloads can not use more than the GPU quota assigned to their Project). Low GPU utilization is due to users poorly utilizing their GPUs in training sessions Identify Projects with bad GPU utilization in training Workloads, notify the users and work with them to improve their code and the way they utilize their GPUs."},{"location":"admin/admin-ui-setup/dashboard-analysis/#training-vs-interactive-researcher-maturity","title":"Training vs. Interactive -- Researcher maturity","text":"<p>Track the number of running Workloads and the breakdown into interactive, training, and CPU-only Workloads.</p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>We would want to encourage users to run more training Workloads than interactive Workloads, as it is the key to achieving high GPU utilization across the Cluster:</p> <ul> <li>Training Workloads run to completion and free up their resources automatically when training ends</li> <li>Training Workloads can be preempted, queued, and resumed automatically by the Run:ai system according to predefined policies which increases fairness and Cluster utilization.</li> </ul>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#pending-queue-size","title":"Pending Queue Size","text":"<p>Track how long is the queue for pending Workloads</p> <p>How to: view the following panels:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>Consider buying more GPUs:</p> <ul> <li>When there are too many Workloads are waiting in queue for too long.</li> <li>With a large number of requested GPUs.</li> <li>While the Cluster is fully loaded and well utilized.</li> </ul>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#cpu-memory-utilization","title":"CPU &amp; Memory Utilization","text":"<p>Track CPU and memory Node utilization and identify times where the load on specific Nodes is high.</p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>If the load on specific Nodes is too high, it may cause problems with the proper operation of the Cluster and the way workloads are running.</p> <p>Consider adding more CPUs, or adding additional CPU-only nodes for Workloads that do only CPU processing.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#multi-cluster-overview-dashboard","title":"Multi-Cluster overview dashboard","text":"<p>Provides a holistic, aggregated view across Clusters, including information about Cluster and Node utilization, available resources, and allocated resources. With this dashboard, you can identify Clusters that are down or underutilized and go to the Overview of that Cluster to explore further.</p> <p></p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#consumption-dashboard","title":"Consumption dashboard","text":"<p>This dashboard enables users and admins to view consumption usage using run:AI services. The dashboard provides views based on configurable filters and timelines. The dashboard also provides costing analysis for GPU, CPU, and memory costs for the system.</p> <p></p> <p>The dashboard has 4 tiles for:</p> <ul> <li>Cumulative GPU allocation per Project or Department</li> <li>Cumulative CPU allocation per Project or Department</li> <li>Cumulative memory allocation per Project or Department</li> <li>Consumption types</li> </ul> <p>Use the drop down menus at the top of the dashboard to apply filters for:</p> <ul> <li>Project or department</li> <li>Per project (single, multiple, or all)</li> <li>Per department (single, multiple or all)</li> <li>Per cluster (single, multiple, all)</li> </ul> <p>To enable the Consumption Dashboard:</p> <ol> <li>Press the <code>Tools &amp; Settings</code> icon, then press <code>General</code>.</li> <li>Open the <code>Analytics</code> pane and toggle the Consumption switch to enable the feature.</li> <li>Enter the cost of:</li> <li>GPU compute / Hour</li> <li>CPU compute / Hour</li> <li>CPU memory / Hour</li> </ol> <p>Use the time picker dropdown to select relative time range options and set custom absolute time ranges. You can change the Timezone and fiscal year settings from the time range controls by clicking the Change time settings button.</p> <p>Note</p> <p>Dashboard data updates once an hour.</p> <p></p> <p>You can change the refresh interval using the refresh interval drop down.</p> <p>The dashboard has a 2 consumption tables that display the total consumption of resources. Hover over an entry in the table to filter it in or out of the table.</p> <p>The Total consumption table includes consumption details based on the filters selected. Fields include:</p> <ul> <li>Project</li> <li>Department</li> <li>GPU hours</li> <li>CPU hours</li> <li>Memory hours</li> <li>GPU Idle allocated hours\u2014the portion of time the GPUs spend idle from the total allocation hours.</li> <li>CPU usage hours\u2014the actual usage time of CPU.</li> <li>Memory usage time\u2014the actual usage time of CPU memory.</li> <li>GPU cost (only when configured)</li> <li>CPU cost (only when configured)</li> <li>CPU memory (only when configured)</li> </ul> <p>The Total department consumption table includes consumption details for each department, or details for departments selected in the filters. Fields include:</p> <ul> <li>Department</li> <li>GPU hours</li> <li>CPU hours</li> <li>Memory hours</li> <li>GPU Idle allocated hours\u2014the portion of time the GPUs spend idle from the total allocation hours.</li> <li>CPU usage hours\u2014the actual usage time of CPU.</li> <li>Memory usage time\u2014the actual usage time of CPU memory.</li> <li>GPU cost (only when configured)</li> <li>CPU cost (only when configured)</li> <li>CPU memory (only when configured)</li> </ul> <p>The dashboard has a graph of the GPU allocation over time.</p> <p>!</p> <p>The dashboard has a graph of the Project over-quota GPU consumption.</p> <p>!</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#metrics-used-for-consumption-calculation","title":"Metrics used for consumption calculation","text":"<ul> <li><code>runai_allocated_gpu_count_per_pod:hourly</code>\u2014GPU allocation hourly</li> <li><code>runai_allocated_millicpus_per_pod:hourly</code>\u2014CPU allocation hours</li> <li><code>runai_allocated_memory_per_pod:hourly</code>\u2014Memory allocation hours </li> <li><code>runai_used_cpu_cores_per_pod:hourly</code>\u2014CPU usage hours</li> <li><code>runai_used_memory_bytes_per_pod:hourly</code>\u2014Memory usage hours</li> <li><code>runai_gpu_idle_hours_per_queue:hourly</code>\u2014GPU Idle allocated hours</li> </ul>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#quota-management-dashboard","title":"Quota management dashboard","text":"<p>The Quota management dashboard provides an efficient means to monitor and manage resource utilization within the AI cluster. The dashboard is divided into sections with essential metrics and data visualizations to identify resource usage patterns, potential bottlenecks, and areas for optimization. The sections of the dashboard include:</p> <ul> <li>Add Filter</li> <li>Quota / Total</li> <li>Allocated / Quota</li> <li>Pending workloads</li> <li>Quota by node pool</li> <li>Allocation by node pool</li> <li>Pending workloads by node pool</li> <li>Departments with lowest allocation by node pool</li> <li>Projects with lowest allocation ratio by node pool</li> <li>Over time allocation / quota</li> </ul>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#add-filter","title":"Add Filter","text":"<p>Use the Add Filter dropdown to select filters for the dashboard. The filters will change the data shown on the dashboard. Available filters are:</p> <ul> <li>Departments</li> <li>Projects</li> <li>Nodes</li> </ul> <p>Select a filter from the dropdown, then select a item from the list, and press apply.</p> <p>Note</p> <p>You can create a filter with multiple categories, but you can use each category and item only once.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#quota-total","title":"Quota / Total","text":"<p>This section shows the number of GPUs that are in the quota based on the filter selection. The quota of GPUs is the number of GPUs that are reserved for use.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#allocated-quota","title":"Allocated / Quota","text":"<p>This section shows the number of GPUs that are allocated based on the filter selection. Allocated GPUs are the number of GPUs that are being used.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#pending-workloads","title":"Pending workloads","text":"<p>This section shows the number workloads that are pending based on the filter selection. Pending workloads are workloads that have not started.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#quota-by-node-pool","title":"Quota by node pool","text":"<p>This section shows the quota of GPUs by node pool based on the filter. The quota is the number of GPUs that are reserved for use. You can drill down into the data in this section by pressing on the graph or the link at the bottom of the section.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#allocation-by-node-pool","title":"Allocation by node pool","text":"<p>This section shows the allocation of GPUs by node pool based on the filter. The allocation is the number of GPUs that are being used. You can drill down into the data in this section by pressing on the graph or the link at the bottom of the section.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#pending-workloads-by-node-pool","title":"Pending workloads by node pool","text":"<p>This section shows the number of pending workloads by node pool. You can drill down into the data in this section by pressing on the graph or the link at the bottom of the section.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#departments-with-lowest-allocation-by-node-pool","title":"Departments with lowest allocation by node pool","text":"<p>This section shows the departments with the lowest allocation of GPUs by percentage relative to the total number of GPUs.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#projects-with-lowest-allocation-ratio-by-node-pool","title":"Projects with lowest allocation ratio by node pool","text":"<p>This section shows the projects with the lowest allocation of GPUS by percentage relative to the total number of GPUs.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#over-time-allocation-quota","title":"Over time allocation / quota","text":"<p>This section shows the allocation of GPUs from the quota over a period of time.</p>"},{"location":"admin/admin-ui-setup/department-setup/","title":"Introduction","text":"<p>Researchers submit Jobs. To streamline resource allocation and prioritize work, Run:ai introduces the concept of Projects. Projects are the tool to implement resource allocation policies as well as create segregation between different initiatives. A project in most cases represents a team, an individual, or an initiative that shares resources or has a specific resources budget (quota).</p> <p>A Researcher submitting a Job needs to associate a Project name with the request. The Run:ai scheduler will compare the request against the current allocations and the Project and determine whether the workload can be allocated resources or whether it should remain in the queue for future allocation.</p> <p>In some organizations, Projects may not be enough, this is because:</p> <ul> <li>There are simply too many individual entities that are attached to a quota.</li> <li>There are organizational quotas at a higher level.</li> </ul>"},{"location":"admin/admin-ui-setup/department-setup/#departments","title":"Departments","text":"<p>Departments create a secondary hierarchy of resource allocation:</p> <ul> <li>A Project is associated with a single Department. Multiple Projects can be associated with the same Department.</li> <li>A Department, like a Project is associated with a Quota. </li> <li>It is recommended that a Department's quota supersedes the sum of all its associated Projects' quota.</li> </ul>"},{"location":"admin/admin-ui-setup/department-setup/#node-pools-and-quota-settings","title":"Node Pools and Quota settings","text":"<p>For detailed information on node pools, see Using node pools.</p> <p>By default, all nodes in a cluster are part of the <code>Default</code> node pool. The administrator can choose to create new node pools and include a set of nodes in a node pool by associating the nodes with a label.</p> <p>If the node pools feature is disabled, all GPU and CPU resources are directly associated with the Department's Quota. </p> <p>Once an Administrator enables node pools, all GPU and CPU resources will be included in the <code>Default</code> node pool and summed up to the Department's overall Quotas.</p> <p>An administrator can create a new node pool and associate nodes into this pool. Any new pool is automatically associated with all Departments and Projects within a cluster, with a GPU and CPU resource Quota of zero. The Administrator can then change the Quota of any node-pool resource per Department and Project. The Quota of node-pool X within Department Y should be at least the sum of the same node-pool X Quota across all associated Projects. This means an administrator should carefully plan the resource Quota allocation from the Department to its descendent Projects. The overall Quota of the Department is the sum of all its associated node pools. </p>"},{"location":"admin/admin-ui-setup/department-setup/#over-quota-behavior","title":"Over-quota behavior","text":"<p>Consider an example from an academic use case: the Computer Science Department and the GeoPhysics Department have each purchased 10 nodes with 8 GPUs for each node, totaling a cluster of 160 GPUs for both departments. The two Departments do not mind sharing GPUs as long as they always get their 80 GPUs when they truly need them. As such, there could be many Projects in the GeoPhysics Department, totaling an allocation of 100 GPUs, but anything above 80 GPUs will be considered by the Run:ai scheduler as over-quota. For more details on over-quota scheduling see the Run:ai Scheduler. In case node pools are enabled, the same rule applies per node pool, i.e. if a job tries to use resources that supersede a node pool Department's quota - it will be considered as Over-Quota.</p> <p>Important</p> <p>Best practice: As a rule, the sum of the Departments' Quota allocations should be equal to the number of GPUs in the cluster.</p>"},{"location":"admin/admin-ui-setup/department-setup/#creating-and-managing-departments","title":"Creating and Managing Departments","text":""},{"location":"admin/admin-ui-setup/department-setup/#enable-departments","title":"Enable Departments","text":"<p>Departments are disabled by default. To start working with Departments:</p> <ul> <li>Go to <code>Settings</code> | <code>General</code>.</li> <li>Enable <code>Departments</code>.</li> </ul> <p>Once Departments are enabled, the left-side menu will have a new item named Departments.</p> <p>Under Departments there will be a single department named default. All Projects created before the Department feature was enabled will belong to the default department.</p>"},{"location":"admin/admin-ui-setup/department-setup/#adding-departments","title":"Adding Departments","text":"<p>To add a new department:</p> <ol> <li>In the Departments grid, press New Department.</li> <li>Enter a name.</li> <li>In Quota management configure the number GPUs, CPUs, and CPU memory, then press Save.</li> </ol>"},{"location":"admin/admin-ui-setup/department-setup/#download-departments-table","title":"Download Departments Table","text":"<p>You can download the Departments table to a CSV file. Downloading a CSV can provide a snapshot history of your departments over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>To download the Departments table to a CSV: 1. Open Departments. 2. From the Columns icon, select the columns you would like to have displayed in the table. 3. Click on the ellipsis labeled More, and download the CSV.</p>"},{"location":"admin/admin-ui-setup/department-setup/#assigning-department-administrator-role","title":"Assigning Department Administrator role","text":"<p>There are two ways to add Department Administrator roles to a department.</p> <p>The first is through the Users UI, and the second is through the Access rules that you can assign to a department.</p>"},{"location":"admin/admin-ui-setup/department-setup/#users-ui","title":"Users UI","text":"<p>You can create a new user with the Department Administrator role, or add the role to existing users. To create a new user with this role, see Create a user. To add this role to an existing user:</p> <ol> <li>Press the  icon, then select Users..</li> <li>Select a user, then press Access rules, then press +Access rule.</li> <li>Select the <code>Department Administrator</code> role from the list.</li> <li>Press on the  and select one or more departments.</li> <li>Press Save rule and then Close.</li> </ol>"},{"location":"admin/admin-ui-setup/department-setup/#assigning-the-access-rule-to-the-department","title":"Assigning the access rule to the department","text":"<p>To assign the Access rule to the department:</p> <ol> <li>Select a department from the list, then press Access rules, then press then press +Access rule.</li> <li>From the Subject type dropdown choose User or Application, then enter the user name or the application name.</li> <li>From the Role dropdown, select Department administrator, then press Save rule.</li> <li>If you want to add another rule, use the +Access rule.</li> <li>When all the rules are configured, press Close.</li> </ol>"},{"location":"admin/admin-ui-setup/department-setup/#assigning-projects-to-departments","title":"Assigning Projects to Departments","text":"<p>Under Projects edit an existing Project. You will see a new Department drop-down with which you can associate a Project with a Department.</p>"},{"location":"admin/admin-ui-setup/jobs/","title":"Viewing and Submitting Jobs","text":"<p>The Run:ai User interface Job area allows the viewing of Jobs and Job details. It also allows the Researcher to submit Jobs, suspend and resume Jobs and delete Jobs.</p>"},{"location":"admin/admin-ui-setup/jobs/#job-list","title":"Job List","text":"<p>The main view shows a list of Jobs. The list can be filtered and sorted:</p> <p></p>"},{"location":"admin/admin-ui-setup/jobs/#submit-a-job","title":"Submit a Job","text":"<p>On the top right, you can choose to Submit a new Job. A Job form will open:</p> <p></p> <p>Note</p> <p>If the Submit Job button is disabled or does not exist, then your cluster is not installed or configured to connect to the cluster see here for more information.</p>"},{"location":"admin/admin-ui-setup/jobs/#job-properties","title":"Job Properties","text":"<p>When selecting a single Job, a right-pane appears:</p> <p></p> <p>This multi-tab view provides information about Job details, related Pods, Job status history, and various utilization graphs. You can also view internal Job logs as shown here:</p> <p></p>"},{"location":"admin/admin-ui-setup/jobs/#other-operations","title":"Other Operations","text":"<p>You can also delete a selected Job or suspend/resume a selected Job.</p>"},{"location":"admin/admin-ui-setup/jobs/#workloads-toggle","title":"Workloads Toggle","text":"<p>The workloads toggle changes the current Jobs view to Workloads. For more information, see the Workloads page.</p>"},{"location":"admin/admin-ui-setup/overview/","title":"User Interface Overview","text":"<p>Run:ai provides a single user interface that, depending on your role, serves both as a control-plane management tool and a researcher workbench.</p> <p>The control-plane part of the tool allows the administrator to:</p> <ul> <li>Analyze cluster status using dashboards.</li> <li>Manage Run:ai metadata such as users, departments, and projects.</li> <li>View Job details to be able to help researchers solve Job-related issues.</li> </ul> <p>The researcher workbench part of the tool allows Researchers to submit, delete and pause Jobs, view Job logs etc.</p>"},{"location":"admin/admin-ui-setup/overview/#setup","title":"Setup","text":"<p>The cluster installation process requires configuring a new cluster. On SaaS-based installations, the cluster creation wizard requires a URL to the cluster as explained here.</p>"},{"location":"admin/admin-ui-setup/overview/#architecture","title":"Architecture","text":"<ul> <li>Run:ai saves metadata such as users, projects, departments, clusters, and tenant settings, in the control plane residing on the Run:ai cloud.</li> <li>Workload information resides on (sometimes multiple) GPU clusters.</li> <li>The Run:ai user interface needs to work with both sources of information.</li> </ul> <p>As such, the chosen architecture of the user interface is:</p> <p></p> <ul> <li>The user interface is served from the management backend.</li> <li>The user interface connects directly to multiple GPU clusters using cross-origin access. This works using CORS: Cross-origin resource sharing. This allows submitting workloads and getting extended logging information directly from the GPU clusters.</li> <li>Meta-data, such as Projects, Settings, and Job information is synced into the management backend via a cluster-sync service. Cluster-sync creates an outbound-only channel with no incoming HTTPS connections.  </li> </ul> <p>Important</p> <p>One corollary of this architecture is that for SaaS-based tenants, the user interface will only be able to access the cluster when the browser is inside the corporate firewall. When working outside the firewall. Workload-related functionality such as Submitting a Job, viewing Job lots etc, is disabled.</p>"},{"location":"admin/admin-ui-setup/overview/#administrator-messages","title":"Administrator Messages","text":"<p>System administrators can use Administrator messages to make announcements to users once they have logged in. These messages typically are used to keep user informed about different aspects of the platform.</p> <p>To configure an Administrator message:</p> <ol> <li>Press <code>Settings | General</code>.</li> <li>Expand the Message from administrator pane.</li> <li>Press Message.</li> <li>Enter your message in the text box. Use the formatting tools in the toolbar to add special formatting or links to the message.</li> <li>Enable the <code>Display \"Don't show this again\" checkbox on message to users</code> to allow the users to see the message only once.</li> <li>Press Publish when complete.</li> </ol>"},{"location":"admin/admin-ui-setup/project-setup/","title":"Introduction","text":"<p>Researchers submit Workloads. To streamline resource allocation and prioritize work, Run:ai introduces the concept of Projects. Projects are the tool to implement resource allocation policies as well as create segregation between different initiatives. A project in most cases represents a team, an individual, or an initiative that shares resources or has a specific resources budget (quota).</p> <p>A Researcher submitting a Job needs to associate a Project name with the request. The Run:ai scheduler will compare the request against the current allocations and the Project and determine whether the workload can be allocated resources or whether it should remain in the queue for future allocation.</p>"},{"location":"admin/admin-ui-setup/project-setup/#modeling-projects","title":"Modeling Projects","text":"<p>As an Admin, you need to determine how to model Projects. You can:</p> <ul> <li>Set a Project per user.</li> <li>Set a Project per team of users.</li> <li>Set a Project per a real organizational Project.</li> </ul>"},{"location":"admin/admin-ui-setup/project-setup/#node-pools","title":"Node Pools","text":"<p>For detailed information on node pools, see Using node pools.</p> <p>By default, all nodes in a cluster are part of the <code>Default</code> node pool. The administrator can choose to create new node pools and include a set of nodes in a node pool by associating the nodes with a label.</p> <p>Each node pool is automatically associated with all Projects and Departments with zero resource allocation (Quotas). When submitting a Workload (or Inference), the Researcher can choose one or more node pools. When choosing more than one node pool, the researcher sets the order of priority between the chosen node pools. The scheduler will try to schedule the Job to the first node pool. If not successful the scheduler will try the second node pool in the list, and so forth until it finds a node pool that can provide the Job's specification.</p> <p>An administrator can set a Project's <code>default priority list</code> of node pools. In case the Researcher did not specify any node pool (or node pool list), the scheduler will use the Project's default node pool priority list to determine the order that the scheduler will use when scheduling the Job.</p>"},{"location":"admin/admin-ui-setup/project-setup/#project-quotas","title":"Project Quotas","text":"<p>Each Project is associated with a total quota of GPU and CPU resources (CPU Compute &amp; CPU Memory) that can be allocated for the Project at the same time. This total is the sum of all node pools' quotas associated with this Project. This is guaranteed quota in the sense that Researchers using this Project are guaranteed to get this amount of GPU and CPU resources, no matter what the status in the cluster is.</p> <p>Beyond that, a user of this Project can receive an over-quota (The administrator needs to enable over-quota per project). As long as GPUs are unused, a Researcher using this Project can get more GPUs. However, these GPUs can be taken away at a moment's notice. When the node pools flag is enabled, over-quota is effective and calculated per node pool, this means that a workload requesting resources from a certain node pool can get its resources from a quota that belongs to another Project for the same node pool if the resources are exhausted for this Project and available on another Project. For more details on over-quota scheduling see the Run:ai Scheduler.</p> <p>Important</p> <p>Best practice: As a rule, the sum of the Projects' allocations should be equal to the number of GPUs in the cluster.</p>"},{"location":"admin/admin-ui-setup/project-setup/#controlling-over-quota-behavior","title":"Controlling Over-Quota Behavior","text":"<p>By default, the amount of over-quota available for Project members is proportional to the original quota provided above. The Run:ai scheduler document provides further examples which show how over-quota is distributed amongst competing Projects.</p> <p>As an administrator, you may want to disconnect the two parameters. So, for example, a Project with a high quota will receive little or no over-quota. To perform this:</p> <ul> <li>Under <code>Settings | General</code> turn on the <code>Enable Over-quota Priority</code> feature</li> <li>When creating a new Project, you can now see a slider for over-quota priority ranging from <code>None</code> to <code>High</code></li> </ul>"},{"location":"admin/admin-ui-setup/project-setup/#create-a-project","title":"Create a Project","text":"<p>Note</p> <p>To be able to create or edit Projects, you must have Editor access. See the Users documentation.</p> <ol> <li>In the left-menu, press Projects, then press +Add New Project.</li> <li>Choose a Department from the drop-down. The default is <code>default</code>.</li> <li>Enter a Project name. Press Namespace to set the namespace associated with the project. You can either create the namespace from the project name (default) or enter an existing namespace.</li> <li> <p>In Quota management, configure the node pool priority (if editable), the GPUs, CPUs, CPU memory, and Over-quota priority settings. Configure the following:</p> <ul> <li>Order of priority\u2014the priority the node pool will receive when trying to schedule workloads. For more information, see Node pool priority.</li> <li>GPUs\u2014the number of GPUs in the node pool. Press GPUs and enter the number of GPUs, then press Apply to save.</li> <li>CPUs(Cores)\u2014the number of CPU cores in the node pool. Press CPUs and enter the number of GPUs, then press Apply to save.</li> <li>CPU Memory\u2014the amount of memory the CPUs will be allocated. Press CPU Memory, enter an amount of memory, then press Apply to save.</li> <li>Over-quota priority\u2014the priority for the specific node pool to receive over-quota allocations.</li> </ul> </li> <li> <p>(Optional) In the Scheduling rules pane, use the dropdown arrow to open the pane. Press on the + Rule button to add a new rule to the project. Add one (or more) of the following rule types:</p> <ul> <li>Idle GPU timeout\u2014controls the amount of time that specific workload GPUs which are idle will be remain assigned to the project before getting reassigned.</li> <li>Workspace duration\u2014limit the length of time a workspace will before being terminated.</li> <li>Training duration\u2014limit the length of time training workloads will run.</li> <li>Node type (Affinity)\u2014limits specific workloads to run on specific node types.</li> </ul> </li> </ol>"},{"location":"admin/admin-ui-setup/project-setup/#assign-access-rules-to-a-project","title":"Assign Access Rules to a Project","text":"<p>To assign Access rules to the project:</p> <ol> <li>Select a project from the list, then press Access rules, then press then press +Access rule.</li> <li>From the Subject dropdown choose User or Application, then enter the user name or the application name.</li> <li>From the Role dropdown, select the desired role, then press Save rule.</li> <li>If you want to add another rule, use the +Access rule.</li> <li>When all the rules are configured, press Close.</li> </ol> <p>If you are using Single-sign-on, you can also assign Groups.</p>"},{"location":"admin/admin-ui-setup/project-setup/#viewing-project-policies","title":"Viewing Project Policies","text":"<p>Run:ai now provides the ability to view how policies effect your Project.</p> <p>To view a project policy:</p> <ol> <li>In the left menu, press Projects, then select a project from the list.</li> <li> <p>Press View Policy, and select a policy type. The Policy Viewer opens.</p> <p>Note</p> <p>Policy types are greyed out if there are no policies applied in the platform.</p> </li> <li> <p>To view the a policy restriction, expand a section of the policy. The following categories are available:</p> <ul> <li>Parameter</li> <li>Rule</li> <li>Default</li> <li>Source</li> </ul> </li> </ol>"},{"location":"admin/admin-ui-setup/project-setup/#other-project-properties","title":"Other Project Properties","text":""},{"location":"admin/admin-ui-setup/project-setup/#download-project-table","title":"Download Project Table","text":"<p>You can download the Projects table to a CSV file. Downloading a CSV can provide a snapshot history of your projects over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>To download the Projects table to a CSV:</p> <ol> <li>Open Projects.</li> <li>From the Columns icon, select the columns you would like to have displayed in the table.</li> <li>Click on the ellipsis labeled More, and download the CSV.</li> </ol>"},{"location":"admin/admin-ui-setup/project-setup/#limit-jobs-to-run-on-specific-node-groups","title":"Limit Jobs to run on Specific Node Groups","text":"<p>You can assign a Project to run on specific nodes (machines). This is achieved by two different mechanisms:</p> <ul> <li> <p>Node Pools:         All node pools in the system are associated with each Project. Each node pool can allocate GPU and CPU resources (CPU Compute &amp; CPU Memory) to a Project. By associating a quota on specific node pools for a Project, you can control which nodes a Project can utilize and which default priority order the scheduler will use (in case the workload did choose so by itself). Each workload should choose the node pool(s) to use, if no choice is made, it will use the Project's default 'node pool priority list'. Note that node pools with zero resources associated with a Project or node pools with exhausted resources can still be used by a Project when the Over-quota flag is enabled.</p> </li> <li> <p>Node Affinities (aka Node Type)         Administrator can associate specific node sets characterized by a shared run-ai/node-type label value to a Project. This means descendant workloads can only use nodes from one of those node affinity groups. A workload can specify which node affinity to use, out of the list is bounded to its parent Project.</p> </li> </ul> <p>There are many use cases and reasons to use specific nodes for a Project and its descendant workloads, here are some examples:</p> <ul> <li>The project team needs specialized hardware (e.g. with enough memory).</li> <li>The project team is the owner of specific hardware which was acquired with a specialized budget.</li> <li>We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes.</li> </ul>"},{"location":"admin/admin-ui-setup/project-setup/#the-difference-between-node-pools-and-affinities","title":"The difference between Node Pools and Affinities","text":"<p>Node pools represent an independent scheduling domain per Project, therefore are completely segregated from each other. To use a specific node pool (or node pools), any workload must specify the node pool(s) it would like to use. While for affinities, workloads that ask for a specific affinity will only be scheduled to nodes marked with that affinity, while workloads that did not specify any affinity might be scheduled as well to those nodes with an affinity. Therefore the scheduler cannot guarantee quota for node affinities, only to node pools.</p> <p>Note that using node pools and affinities narrows down the scope of nodes a specific project is eligible to use. It, therefore, reduces the odds of a specific workload under that Project getting scheduled. In some cases, this may reduce the overall system utilization.</p>"},{"location":"admin/admin-ui-setup/project-setup/#grouping-nodes-using-node-pools","title":"Grouping Nodes using Node Pools","text":"<p>To create a node pool you must first annotate nodes with a label or use an existing node label, as the key for grouping nodes into pools. You can use any unique label (in the format <code>key:value</code>) to form a node pool. a node pool is characterized by a label but also has its own unique node pool name.</p> <p>To get the list of nodes and their current labels, run:</p> <pre><code>kubectl get nodes --show-labels\n</code></pre> <p>To annotate a specific node with the label <code>dgx-2</code>, run:</p> <pre><code>kubectl label node &lt;node-name&gt; node-model=dgx-2\n</code></pre> <p>You can annotate multiple nodes with the same label.</p> <p>To create a node pool with the chosen common label use the create node pool Run:ai API.</p>"},{"location":"admin/admin-ui-setup/project-setup/#setting-node-pools-for-a-specific-project","title":"Setting Node Pools for a Specific Project","text":"<p>By default, all node pools are associated with every Project and Department using zero resource allocation. This means that by default any Project can use any node-pool if Over-Quota is set for that Project, but only for preemptible workloads (i.e. Training workloads or Interactive using Preemptible flag).</p> <ul> <li>To guarantee resources for all workloads including non-preemptible workloads, the administrator should allocate resources in node pools.</li> <li>Go to the Node Pools tab under Project and set a quota to any of the node pools (GPU resources, CPU resources) you want to use.</li> <li>To set the Project's default node pool's order of priority, you should set the precedence of each node pool, this is done in the Project's node pool tab.</li> <li>The node pool default priority order is used if the workload did not specify its preferred node pool(s) list of priority.</li> <li>To mandate a Workload to run on a specific node pool, the Researcher should specify the node pool to use for a workload.</li> <li>If no node-pool is specified - the Project's 'Default' node-pool priority list is used.</li> <li>Press 'Save' to save your changes.</li> </ul>"},{"location":"admin/admin-ui-setup/project-setup/#grouping-nodes-using-node-affinities","title":"Grouping Nodes using Node Affinities","text":"<p>To set node affinities, you must first annotate nodes with labels. These labels will later be associated with Projects.</p> <p>To get the list of nodes, run:</p> <pre><code>kubectl get nodes\n</code></pre> <p>To annotate a specific node with the label \"dgx-2\", run:</p> <pre><code>kubectl label node &lt;node-name&gt; run.ai/type=dgx-2\n</code></pre> <ul> <li>Each node can only be annotated with a single label.</li> <li>You can annotate multiple nodes with the same label.</li> </ul>"},{"location":"admin/admin-ui-setup/project-setup/#setting-affinity-for-a-specific-project","title":"Setting Affinity for a Specific Project","text":"<p>To mandate training Jobs to run on specific node groups:</p> <ul> <li>Create a Project or edit an existing Project.</li> <li>Go to the Node Affinity tab and set a limit to specific node groups.</li> <li>If the label does not yet exist, press the + sign and add the label.</li> <li>Press Enter to save the label.</li> <li>Select the label.</li> </ul> <p>To mandate interactive Jobs to run on specific node groups, perform the same steps under the \"interactive\" section in the Project dialog.</p>"},{"location":"admin/admin-ui-setup/project-setup/#further-affinity-refinement-by-the-researcher","title":"Further Affinity Refinement by the Researcher","text":"<p>The Researcher can limit the selection of node groups by using the CLI flag <code>--node-type</code> with a specific label. When setting specific Project affinity, the CLI flag can only be used with a node group out of the previously chosen list.  See CLI reference for further information runai submit</p>"},{"location":"admin/admin-ui-setup/project-setup/#limit-duration-of-interactive-and-training-jobs","title":"Limit Duration of Interactive and Training Jobs","text":"<p>As interactive sessions involve human interaction, Run:ai provides an additional tool to enforce a policy that sets the time limit for such sessions. This policy is often used to handle situations like researchers leaving sessions open even when they do not need to access the resources.</p> <p>Warning</p> <p>This feature will cause containers to automatically stop. Any work not saved to a shared volume will be lost.</p> <p>To set a duration limit for interactive Jobs:</p> <ul> <li>Create a Project or edit an existing Project.</li> <li>Go to the Time Limit tab</li> <li>You can limit interactive Jobs using two criteria:</li> <li>Set a hard time limit (day, hour, minute) to an Interactive Job, regardless of the activity of this Job, e.g. stop the Job after 1 day of work.</li> <li>Set a time limit for Idle Interactive Jobs, i.e. an Interactive Job idle for X time is stopped. Idle means no GPU activity.</li> <li>You can set if this idle time limit is effective for Interactive Jobs that are Preemptible, non-Preemptible, or both.</li> </ul> <p>The setting only takes effect for Jobs that have started after the duration has been changed.</p> <p>In some use cases, you would like to stop Training Jobs if X time elapsed since they have started to run. This can be to clean up stale Training Jobs or Jobs that are running for too long probably because of wrong parameters set or other errors of the model.</p> <p>To set a duration limit for Training Jobs:</p> <ul> <li>Create a Project or edit an existing Project.</li> <li>Go to the Time Limit tab:</li> <li>Set a time limit for Idle Training Jobs, i.e. a Training Job idle for X time is stopped. Idle means no GPU activity.</li> </ul> <p>The setting only takes effect for Jobs that have started after the duration has been changed.</p>"},{"location":"admin/admin-ui-setup/project-setup/#setting-runai-as-default-scheduler-per-projectnamespace","title":"Setting Run:ai as default scheduler per Project/Namespace","text":"<p>By default, Kubernetes will use its native scheduler to schedule any type of submitted workload. However, Kubernetes also provides a standard way to use other schedulers such as Run:ai. This is done by adding to the submitted container workload\u2019s YAML file:  </p> <p><code>schedulerName: runai-scheduler</code></p> <p>There may be cases where you cannot change the YAML file and still want to use the Run:ai Scheduler to schedule those workloads. For these cases, another option is to configure the Run:ai Scheduler as the default scheduler for a specific namespace (Project). This will now make any workload type that is submitted to that namespace (Project) use the Run:ai scheduler. To configure this, add the following annotation on the namespace itself:</p> <p><code>runai/enforce-scheduler-name: true</code></p>"},{"location":"admin/admin-ui-setup/project-setup/#example","title":"Example","text":"<p>To annotate a project named <code>proj-a</code>, use the following command:</p> <pre><code>kubectl annotate ns runai-proj-a runai/enforce-scheduler-name=true\n</code></pre> <p>Verify the namespace in yaml format to see the annotation:</p> <pre><code>kubectl get ns runai-proj-a -o yaml\n</code></pre> <p>Output: </p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  annotations:\n    runai/enforce-scheduler-name: \"true\"\n  creationTimestamp: \"2024-04-09T08:15:50Z\"\n  labels:\n    kubernetes.io/metadata.name: runai-proj-a\n    runai/namespace-version: v2\n    runai/queue: proj-a\n  name: runai-proj-a\n  resourceVersion: \"388336\"\n  uid: c53af666-7989-43df-9804-42bf8965ce83\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Active\n</code></pre>"},{"location":"admin/admin-ui-setup/project-setup/#see-also","title":"See Also","text":"<p>Run:ai supports an additional (optional) level of resource allocation called Departments.</p>"},{"location":"admin/admin-ui-setup/templates/","title":"Templates","text":"<p>A template is a pre-set configuration that is used to quickly configure and submit workloads using existing assets. A template consists of all the assets a workload needs, allowing researchers to submit a workload in a single click, or make subtle adjustments to differentiate them from each other.</p>"},{"location":"admin/admin-ui-setup/templates/#creating-templates","title":"Creating Templates","text":"<p>To create a template:</p> <ol> <li>In the left menu, press Templates, then press New Template.</li> <li>In the Scope pane, select a cluster, department, or project.</li> <li>In the Template Name pane, enter a name for the template.</li> <li> <p>Select an environment from the tiles. If your environment is not listed, use the Search environments box to find it or press New environment to create a new environment. Press  to create an environment if needed. In the Set the connection for your tool(s), enter the URL of the tool if a custom URL has been enabled in the selected environment. Use the Private toggle to lock access to the tool to only the creator of the environment.</p> <p>In the Runtime Settings:</p> <ol> <li>Press Commands and Arguments to add special commands and arguments to your environment selection.</li> <li>Press Environment variable to add an environment variable. Press again if you need more environment variables.</li> </ol> </li> <li> <p>In the Compute resource pane, select a compute resource. Use the Search compute resources if you do not see your resource listed. Press New compute resource to add a new compute resource to the system. Press More settings to add a node type (node affinity) to the compute resource selected.</p> </li> <li> <p>(Optional) In the Volume pane, press +volume to add a new volume to the template.</p> <p>From the drop down menus select: * Storage class * Access mode * Claim size and units * Volume mode</p> <p>Set the Volume target location, then select from either a Persistent volume or an Ephemeral volume.</p> </li> <li> <p>In the Data sources pane, select a data source. Press New data source to add a new data source to the system.</p> </li> <li> <p>In the General pane, choose to add the following:</p> <ul> <li>Auto-deletion\u2014the time after which a workload that has completed or failed will be deleted. Press +Auto-deletion then configure the time in days, hours, minutes, and seconds.</li> <li>Annotation\u2014press + Annotation then enter a name and a value. You can add multiple annotations by pressing the + Annotation.</li> <li>Label\u2014press +Label then enter a name and a value. You can add multiple labels by pressing the +Label.</li> </ul> </li> <li> <p>Press Create template when your configuration is complete.</p> </li> </ol>"},{"location":"admin/admin-ui-setup/templates/#download-templates-table","title":"Download Templates Table","text":"<p>You can download the templates table to a CSV file. Downloading a CSV can provide a snapshot history of your templates over the course of time, and help with compliance tracking. All the columns that are selected (displayed) in the table will be downloaded to the file.</p> <p>Use the Cluster filter at the top of the table to see templates that are assigned to specific clusters.</p> <p>Note</p> <p>The cluster filter will be in the top bar when there are clusters that are installed with version 2.16 or lower.</p> <p>Use the Add filter to add additional filters to the table.</p> <p>To download the templates table to a CSV:</p> <ol> <li>Open Templates.</li> <li>From the Columns icon, select the columns you would like to have displayed in the table.</li> <li>Click on the ellipsis labeled More, and download the CSV.</li> </ol>"},{"location":"admin/researcher-setup/cli-install/","title":"Install the Run:ai Command-line Interface","text":"<p>The Run:ai Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc.</p> <p>The instructions below will guide you through the process of installing the CLI. The Run:ai CLI runs on Mac, Linux and Windows. </p>"},{"location":"admin/researcher-setup/cli-install/#researcher-authentication","title":"Researcher Authentication","text":"<p>When enabled, Researcher authentication requires additional setup when installing the CLI. To configure authentication see Setup Project-based Researcher Access Control. Use the modified Kubernetes configuration file described in the article.</p>"},{"location":"admin/researcher-setup/cli-install/#prerequisites","title":"Prerequisites","text":"<ul> <li>When installing the command-line interface, it is worth considering future upgrades:<ul> <li>Install the CLI on a dedicated Jumpbox machine. Researchers will connect to the Jumpbox from which they can submit Run:ai commands</li> <li>Install the CLI on a shared directory that is mounted on Researchers' machines.  </li> </ul> </li> <li>A Kubernetes configuration file.</li> </ul>"},{"location":"admin/researcher-setup/cli-install/#setup","title":"Setup","text":""},{"location":"admin/researcher-setup/cli-install/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<ul> <li>In the Researcher's root folder, create a directory .kube. Copy the Kubernetes configuration file into the directory. Each Researcher should have a separate copy of the configuration file. The Researcher should have write access to the configuration file as it stores user defaults.</li> <li>If you choose to locate the file at a different location than <code>~/.kube/config</code>, you must create a shell variable to point to the configuration file as follows:</li> </ul> <pre><code>export KUBECONFIG=&lt;Kubernetes-config-file&gt;\n</code></pre> <ul> <li>Test the connection by running:</li> </ul> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#install-runai-cli","title":"Install Run:ai CLI","text":"<ul> <li>Go to the Run:ai user interface. On the top right select <code>Researcher Command Line Interface</code>.</li> <li>Select <code>Mac</code>, <code>Linux</code> or <code>Windows</code>.</li> <li>Download directly using the button or copy the file to run it on a remote machine</li> </ul> Mac or LinuxWindows <p>Run:</p> <pre><code>chmod +x runai\nsudo mv runai /usr/local/bin/runai\n</code></pre> <p>Rename the downloaded file to have a <code>.exe</code> extension and move the file to a folder that is a part of the <code>PATH</code>.</p> <p>Note</p> <p>An alternative way of downloading the CLI is provided under the CLI Troubleshooting section.</p> <p>To verify the installation run:</p> <pre><code>runai list jobs\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#install-command-auto-completion","title":"Install Command Auto-Completion","text":"<p>It is possible to configure your Linux/Mac shell to complete Run:ai CLI commands. This feature works on bash and zsh shells only.</p>"},{"location":"admin/researcher-setup/cli-install/#zsh","title":"Zsh","text":"<p>Edit the file <code>~/.zshrc</code>. Add the lines:</p> <pre><code>autoload -U compinit; compinit -i\nsource &lt;(runai completion zsh)\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#bash","title":"Bash","text":"<p>Install the bash-completion package:</p> <ul> <li>Mac: <code>brew install bash-completion</code></li> <li>Ubuntu/Debian: <code>sudo apt-get install bash-completion</code></li> <li>Fedora/Centos: <code>sudo yum install bash-completion</code></li> </ul> <p>Edit the file <code>~/.bashrc</code>. Add the lines:</p> <pre><code>[[ -r \u201c/usr/local/etc/profile.d/bash_completion.sh\u201d ]] &amp;&amp; . \u201c/usr/local/etc/profile.d/bash_completion.sh\u201d\nsource &lt;(runai completion bash)\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#troubleshoot-the-cli-installation","title":"Troubleshoot the CLI Installation","text":"<p>See Troubleshooting a CLI installation</p>"},{"location":"admin/researcher-setup/cli-install/#update-the-runai-cli","title":"Update the Run:ai CLI","text":"<p>To update the CLI to the latest version perform the same install process again.</p>"},{"location":"admin/researcher-setup/cli-install/#delete-the-runai-cli","title":"Delete the Run:ai CLI","text":"<p>If you have installed using the default path, run:</p> <pre><code>sudo rm /usr/local/bin/runai\n</code></pre>"},{"location":"admin/researcher-setup/cluster-wide-pvc/","title":"Cluster wide PVCs","text":"<p> Version 2.10 and later.</p> <p>A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes. For more information about PVCs, see Persistent Volumes.</p> <p>PVCs are namespace-specific. If your PVC relates to all run:ai Projects, do the following to propagate the PVC to all Projects:</p> <p>Create a PVC within the run:ai namespace, then run the following once to propagate the PVC to all run:ai Projects:</p> <pre><code>kubectl label persistentvolumeclaims -n runai &lt;PVC_NAME&gt; runai/cluster-wide=true\n</code></pre> <p>To delete a PVC from all run:ai Projects, run:</p> <pre><code>kubectl label persistentvolumeclaims -n runai &lt;PVC_NAME&gt; runai/cluster-wide-\n</code></pre> <p>You can add a PVC to a job using the <code>New job</code> form.</p> <p>To add a PVC to a new job:</p> <ol> <li>On the <code>New job</code> form, press <code>Storage</code>.</li> <li>In <code>Persistent Volume Claims</code> press <code>Add</code>.</li> <li>Enable <code>Existing PVC</code>.</li> <li>Enter the name (claim name) of the PVC.</li> <li>Enter the storage class. (Optional)</li> <li>Enter the size.</li> <li>Enable / disable access modes.</li> </ol>"},{"location":"admin/researcher-setup/docker-to-runai/","title":"From Docker to Run:ai","text":""},{"location":"admin/researcher-setup/docker-to-runai/#dockers-images-and-kubernetes","title":"Dockers, Images, and Kubernetes","text":"<p>Researchers are typically proficient in working with Docker. Docker is an isolation level above the operating system which allows creating your own bundle of the operating system + deep learning environment and packaging it within a single file. The file is called a docker image.</p> <p>You create a container by starting a docker image on a machine.</p> <p>Run:ai is based on Kubernetes. At its core, Kubernetes is an orchestration software above Docker: Among other things, it allows location abstraction as to where the actual container is running. This calls for some adaptation to the Researcher's workflow as follows.</p>"},{"location":"admin/researcher-setup/docker-to-runai/#image-repository","title":"Image Repository","text":"<p>If your Kubernetes cluster contains a single GPU node (machine), then your image can reside on the node itself (in which case, when runai submit workloads, the Researcher must use the flag <code>--local-image</code>).</p> <p>If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the image can no longer reside on the node itself.  It must be relocated to an image repository. There are quite a few repository-as-a-service, most notably Docker hub. Alternatively, the organization can install a private repository on-prem.</p> <p>Day-to-day work with the image located remotely is almost identical to local work. The image name now contains its location. For example, <code>nvcr.io/nvidia/pytorch:19.12-py_3</code> is a PyTorch image that is located in nvcr.io. This is the Nvidia image repository as found on the web. </p>"},{"location":"admin/researcher-setup/docker-to-runai/#data","title":"Data","text":"<p>Deep learning is about data. It can be your code, the training data, saved checkpoints, etc.</p> <p>If your Kubernetes cluster contains a single GPU node (machine), then your data can reside on the node itself.</p> <p>If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the data must sit outside the machine, typically on network storage. The storage must be uniformly mapped to your container when it starts (using the -v command).</p>"},{"location":"admin/researcher-setup/docker-to-runai/#working-with-containers","title":"Working with Containers","text":"<p>Starting a container using docker usually involves a single command-line with multiple flags. A typical example: </p> <pre><code>docker run --runtime=nvidia --shm-size 16G -it --rm -e HOSTNAME='hostname' \\\n    -v /raid/public/my_datasets:/root/dataset:ro   -i  nvcr.io/nvidia/pytorch:19.12-py3\n</code></pre> <p>The docker command <code>docker run</code> should be replaced with a Run:ai command <code>runai submit</code>. The flags are usually the same but some adaptation is required. A complete list of flags can be found here: runai submit. </p> <p>There are similar commands to get a shell into the container (runai bash), get the container logs (runai logs), and more. For a complete list see the Run:ai CLI reference. </p>"},{"location":"admin/researcher-setup/docker-to-runai/#schedule-an-onboarding-session","title":"Schedule an Onboarding Session","text":"<p>It is highly recommended to schedule an onboarding session for Researchers with a Run:ai customer success professional. Run:ai can help with the above transition, but adding to that, we at Run:ai have also acquired a large body of knowledge on data science best practices which can help streamline  Researchers' work as well as save money for the organization.</p>"},{"location":"admin/researcher-setup/limit-to-node-group/","title":"Group Nodes","text":""},{"location":"admin/researcher-setup/limit-to-node-group/#why","title":"Why?","text":"<p>In some business scenarios, you may want to direct the Run:ai scheduler to schedule a Workload to a specific node or a node group. For example, in some academic institutions, Hardware is bought using a specific grant and thus \"belongs\" to a specific research group. Another example is an inference workload that is optimized to a specific GPU type and must have dedicated resources reserved to ensure enough capacity.</p> <p>Run:ai provides two methods to designate, and group, specific resources:</p> <ul> <li>Node Pools: Run:ai allows administrators to group specific nodes into a node pool. A node pool is a group of nodes identified by a given name (node pool name) and grouped by any label (key and value combination). The label can be chosen by the administrator or can be an existing, pre-set, label (such as an NVIDIA GPU type label).</li> <li>Node Affinity: Run:ai allows this \"taint\" by labeling a node, or a set of nodes and then during scheduling, using the flag <code>--node-type &lt;label&gt;</code> to force this allocation.</li> </ul> <p>Important</p> <p>One can set and use both node pool and node affinity combined as a prerequisite to the scheduler, for example, if a researcher wants to use a T4 node with an Infiniband card - he or she can use a node pool of T4 and from that group, choose only the nodes with Infiniband card (node-type = infiniband).</p> <p>There is a tradeoff in place when allowing Researchers to designate specific nodes. Overuse of this feature limits the scheduler in finding an optimal resource and thus reduces overall cluster utilization.</p>"},{"location":"admin/researcher-setup/limit-to-node-group/#configuring-node-groups","title":"Configuring Node Groups","text":"<p>To configure a node pool:</p> <ul> <li>Find the label key &amp; value you want to use for Run:ai to create the node pool.</li> <li>Check that the nodes you want to group as a pool have a unique label to use, otherwise you should mark those nodes with your own uniquely identifiable label.</li> <li>Get the names of the nodes you want Run:ai to group together. To get a list of nodes, run:</li> </ul> <pre><code>kubectl get nodes\nKubectl get nodes --show-labels\n</code></pre> <ul> <li>If you chose to set your own label, run the following:</li> </ul> <pre><code>kubectl label node &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;\n</code></pre> <p>The same value can be set to a single node or multiple nodes. Node Pool can only use one label (key &amp; value) at a time.</p> <ul> <li>To create a node pool use the create node pool Run:ai API.</li> </ul> <p>To configure a node affinity:</p> <ul> <li>Get the names of the nodes where you want to limit Run:ai. To get a list of nodes, run:</li> </ul> <pre><code>kubectl get nodes\n</code></pre> <ul> <li>For each node run the following:</li> </ul> <pre><code>kubectl label node &lt;node-name&gt; run.ai/type=&lt;label&gt;\n</code></pre> <p>The same value can be set to a single node, or for multiple nodes. A node can only be set with a single value.</p>"},{"location":"admin/researcher-setup/limit-to-node-group/#using-node-groups-via-the-cli","title":"Using Node Groups via the CLI","text":"<p>To use Run:ai node pool with a workload, use Run:ai CLI command \u2018node-pool\u2019: </p> <pre><code>runai submit job1 ... --node-pools \"my-pool\" ...\n</code></pre> <p>To use multiple node pools with a workload, use the Run:ai CLI command:</p> <pre><code>runai submit job1 ... --node-pools \"my-pool my-pool2 my-pool3\" ...\n</code></pre> <p>With multiple node pools, the researcher creates a list of prioritized node pools and lets the scheduler try and choose from any of the node pools in the list, according to the given priority. </p> <p>To use node affinity, use the node type label with the <code>--node-type</code> flag:</p> <pre><code>runai submit job1 ... --node-type \"my-nodes\"\n</code></pre> <p>A researcher may combine the two flags to select both a node pool and a specific set of nodes out of that node pool (e.g. gpu-type=t4 and node-type=infiniband):</p> <pre><code>runai submit job1 ... --node-pool-name \u201cmy pool\u201d --node-type \"my-nodes\"\n</code></pre> <p>Note</p> <p>When submitting a workload, if you choose a node pool label and a node affinity (node type) label which does not intersect, the Run:ai scheduler will not be able to schedule that workload as it represents an empty nodes group.</p> <p>See the runai submit documentation for further information.</p>"},{"location":"admin/researcher-setup/limit-to-node-group/#assigning-node-groups-to-a-project","title":"Assigning Node Groups to a Project","text":"<p>Node Pools are automatically assigned to all Projects and Departments with zero resource allocation as default. Allocating resources to a node pool can be done for each Project and Department. Submitting a workload to a node pool that has zero allocation for a specific project (or department) results in that workload running as an over-quota workload.</p> <p>To assign and configure specific node affinity groups or node pools to a Project see working with Projects.</p> <p>When the command-line interface flag is used in conjunction with Project-based affinity, the flag is used to refine the list of allowable node groups set in the Project.</p>"},{"location":"admin/researcher-setup/registry-integration/","title":"Registry integration (alpha feature)","text":"<p>run:ai now provides the ability to integrate container registry images into jobs and workspaces. Enabling this features provides you with a selection of container images from a pre-configured registry.</p>"},{"location":"admin/researcher-setup/registry-integration/#configure-the-registry","title":"Configure the registry","text":"<p>To configure the registry:</p> <ol> <li>Press <code>Settings | General</code>.</li> <li>Enable <code>Enable registry integration</code>.</li> <li>Enter the URL of the registry.</li> <li>Enter the UserId and password.</li> </ol> <p>Note</p> <p>You can configure only one container registry.</p> <p>Once you have configured the container registry, container images and tags will be available to add to jobs.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/","title":"Researcher Setup Overview","text":"<p>Following is a step-by-step guide for getting a new Researcher up to speed with Run:ai and Kubernetes.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#change-of-paradigms-from-docker-to-kubernetes","title":"Change of Paradigms: from Docker to Kubernetes","text":"<p>As part of Run:ai, the organization is typically moving from Docker-based workflows to Kubernetes. This document is an attempt to help the Researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:ai CLI.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#setup-the-runai-command-line-interface","title":"Setup the Run:ai Command-Line Interface","text":"<p>Run:ai CLI needs to be installed on the Researcher's machine. This document provides step by step instructions.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#provide-the-researcher-with-a-gpu-quota","title":"Provide the Researcher with a GPU Quota","text":"<p>To submit workloads with Run:ai, the Researcher must be provided with a Project that contains a GPU quota. Please see Working with Projects document on how to create Projects and set a quota.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#provide-access-to-the-runai-user-interface","title":"Provide access to the Run:ai User Interface","text":"<p>See Setting up users for further information on how to provide access to users.  </p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#schedule-an-onboarding-session","title":"Schedule an Onboarding Session","text":"<p>It is highly recommended to schedule an onboarding session for Researchers with a Run:ai customer success professional. Run:ai can help with the above transition, but adding to that, we at Run:ai have also acquired a large body of knowledge on data science best practices which can help streamline the Researchers' work as well as save money for the organization. </p>"},{"location":"admin/runai-setup/installation-types/","title":"Installation Types","text":"<p>Run:ai consists of two components:</p> <ul> <li>The Run:ai Cluster. One or more data-science GPU clusters hosted by the customer (on-prem or cloud).</li> <li>The Run:ai Control plane. A single entity that monitors clusters, sets priorities, and business policies.</li> </ul> <p>There are two main installation options:</p> Installation Type Description Classic (SaaS) Run:ai is installed on the customer's data science GPU clusters. The cluster connects to the Run:ai control plane on the cloud (https://<code>&lt;tenant-name&gt;</code>.run.ai).  With this installation, the cluster requires an outbound connection to the Run:ai cloud. Self-hosted The Run:ai control plane is also installed in the customer's data center <p>The self-hosted option is for organizations that cannot use a SaaS solution due to data leakage concerns. The self-hosted installation is priced differently. For further information please talk to Run:ai sales.</p> <p></p>"},{"location":"admin/runai-setup/installation-types/#self-hosted-installation","title":"Self-hosted Installation","text":"<p>Run:ai self-hosting comes with two variants:</p> Self-hosting Type Description Connected The organization can freely download from the internet (though upload is not allowed) Air-gapped The organization has no connection to the internet"},{"location":"admin/runai-setup/installation-types/#self-hosting-with-kubernetes-vs-openshift","title":"Self-hosting with Kubernetes vs OpenShift","text":"<p>Kubernetes has many Certified Kubernetes Providers. Run:ai has been certified with several of them (see the Kubernetes prerequisites section). The OpenShift installation is different from the rest. As such, the Run:ai self-hosted installation instructions are divided into two separate sections:</p> <ul> <li>OpenShift-based installation. See Run:ai OpenShift installation.</li> <li>Kubernetes-based installation. See Run:ai Kubernetes installation.</li> </ul>"},{"location":"admin/runai-setup/installation-types/#secure-installation","title":"Secure Installation","text":"<p>In many organizations, Kubernetes is governed by IT compliance rules. In this scenario, there are strict access control rules during the installation and running of workloads:</p> <ul> <li>OpenShift is secured using Security Context Constraints (SCC). The Run:ai installation supports SCC.</li> <li>Run:ai provides limited support for Kubernetes Pod Security Admission (PSA). For more information see Kubernetes prerequisites.</li> </ul>"},{"location":"admin/runai-setup/access-control/rbac/","title":"Role based access control","text":"<p>User authorization to system resources and entities is managed using Role-based access control (RBAC). RBAC is a policy-neutral access control mechanism defined around roles and privileges. The components of RBAC make it simple to manage access to system resources and entities.</p>"},{"location":"admin/runai-setup/access-control/rbac/#rbac-components","title":"RBAC components","text":"<p>Run:ai uses the following components for RBAC:</p>"},{"location":"admin/runai-setup/access-control/rbac/#subjects","title":"Subjects","text":"<p>A Subject is an entity that receives the rule. Subjects are:</p> <ul> <li>Users</li> <li>Applications</li> <li>Groups (For tenants that use SSO authentication)</li> </ul>"},{"location":"admin/runai-setup/access-control/rbac/#roles","title":"Roles","text":"<p>A role is a group of permissions that can be granted. Permissions are a set of actions that can be applied to entities. Run:ai supports the following roles:</p> Role Description Environment administrator Create, view, edit, and delete Environments. View Jobs, Workspaces, Dashboards, Data sources, Compute resources, and Templates. Credentials administrator Create view, edit, and delete Credentials. View Jobs, Workspaces, Dashboards, Data sources, Compute resources, Templates*, and environments. Data source administrator Create, view, edit, and delete Data sources. View Jobs, Workspaces, Dashboards, Environments, Compute resources, and Templates. Compute resource administrator Create, view, edit, and delete Compute resources. View Jobs, Workspaces, Dashboards, Environments, Data sources, and Templates. System administrator Controls all aspects of the system. This role has global system control and should be limited to a small group of skilled IT administrators. Department administrator Create, view, edit, and delete: Departments and Projects.Assign Roles (Researcher, ML engineer, Research manager, Viewer) within those departments and projects.View Dashboards (including the Consumption dashboard). Editor View Screens and DashboardsManage Departments and Projects. Research manager Create, view, edit, and delete: Environments, Data sources, Compute resources, Templates and Projects.View related Jobs and Workspaces, and Dashboards. L1 researcher Create, view, edit, delete: Workspaces, Trainings, Environments, Data sources, Compute resources, Templates, Credentials, Deployments  View: Clusters, Projects, Node pools, Inference, Policies, Dashboards ML engineer Create, edit, view, and delete Inference.View Departments, Projects, Clusters, Node-pools, Nodes, Dashboards. Viewer View Departments, Projects, Respective subordinates (Jobs, Inference, Workspaces, Environments, Data sources, Compute resources, Templates), Dashboards. A viewer cannot edit Configurations. L2 researcher Create, view, edit, and delete Jobs, Workspaces.An L2 researcher cannot create, edit, or delete Environments, Data sources, Compute resources, and Templates. Template administrator Create, view, edit, and delete Templates.View Jobs, Workspaces, Dashboards, Environments, Compute resources, and Data sources. Department viewer View Departments, Projects, assigned subordinates (Jobs, Inference, Workspaces, Environments, Data sources, Compute resources, Templates), and Dashboards. <p>Note</p> <p>Keep the following in mind when upgrading from versions 2.13 or earlier:</p> <ol> <li>Admin becomes System Admin with full access to all managed objects and scopes.</li> <li>Research Manager is not automatically assigned to all projects but to Projects set by the relevant Admin when assigning this role to a user, group, or app.</li> <li>To preserve backward compatibility, users with the role of Research Manager are assigned to all current projects, but not to new projects.</li> <li>To allow the Department Admin to assign a Researcher role to a user, group, or app, the Department Admin must have VECD permissions for Jobs and Workspaces. This creates a broader span of managed objects.</li> <li>To preserve backward compatibility, users with the role Editor, are assigned to the same scope they had before the upgrade. However, with new user assignments, the Admin can limit the scope to only part of the organizational scope.</li> </ol>"},{"location":"admin/runai-setup/access-control/rbac/#scope","title":"Scope","text":"<p>A Scope is an organizational component which accessible based on assigned roles. Scopes include:</p> <ul> <li>Projects</li> <li>Departments</li> <li>Clusters</li> <li>Tenant (all clusters)</li> </ul>"},{"location":"admin/runai-setup/access-control/rbac/#asset","title":"Asset","text":"<p>RBAC uses rules to ensure that only authorized users or applications can gain access to system entities. entities that can have RBAC rules applied are:</p> <ul> <li>Departments</li> <li>Projects</li> <li>Inference</li> <li>Workspaces</li> <li>Environments</li> <li>Quota management dashboard</li> <li>Training</li> </ul>"},{"location":"admin/runai-setup/access-control/rbac/#rbac-enforcement","title":"RBAC enforcement","text":"<p>RBAC ensures that user have access to system entities based on the rules that are applied to those entities. Should an asset be part of a larger scope of entities to which the user does not have access. The scope shown to the user will appear to be incomplete because the user is able to access only the entities to which they are authorized.</p>"},{"location":"admin/runai-setup/access-control/rbac/#access-rules","title":"Access rules","text":"<p>An Access rule is the assignment of a Role to a Subject in a Scope. Access rules are expressed as follows:</p> <p><code>&lt;subject&gt; is a &lt;role&gt; in a &lt;scope&gt;</code>.</p> <p>For example: User user@domain.com is a department admin in Department A.</p>"},{"location":"admin/runai-setup/access-control/rbac/#access-rules-table","title":"Access Rules Table","text":"<p>The Access Rules table provides a list of subjects (users, SSO groups, applications) that have been assigned access to the platform. Use Add filter to add one or more filter results based on the columns that are in the table. In the Contains pane, you can use partial or complete text. Filtered text is not case sensitive. To remove the filter, press X next to the filter.</p> <p>The table contains the following columns:</p> <ul> <li>Type\u2014the type of subject assigned to the access rule (User, SSO group, or Application).</li> <li>Subject\u2014the user, SSO group, or application id of the subject with role assignments.</li> <li>Role\u2014the name of the role assigned to the user.</li> <li>Scope\u2014the scope to which the user has rights. Press the name of the scope to see the scope and related children.</li> <li>Authorized by\u2014the user who granted the access rule.</li> <li>Creation time\u2014the timestamp for when the rule was created.</li> <li>Last updated\u2014the last time the rule information was updated.</li> </ul>"},{"location":"admin/runai-setup/access-control/rbac/#create-or-delete-rules","title":"Create or delete rules","text":"<p>To create a new access rule:</p> <ol> <li>Press the  icon, then Access rules &amp; Roles.</li> <li>Choose the ACCESS RULES tab, then press NEW ACCESS RULE.</li> <li> <p>Select a subject type from the dropdown. Choose from:</p> <ol> <li>User\u2014a user that has been created in the platform, or a known SSO user listed in your IDP. Enter an email address to select a user.</li> <li>SSO Group\u2014a known group listed in your IDP server.</li> </ol> <p>Note</p> <p>To add SSO users and groups, you must enter a user id, or group id that is recognized by the configured IDP.</p> <ol> <li>Application\u2014an application that has been created in the platform.</li> </ol> </li> <li> <p>Select a [Role] from the dropdown.</p> </li> <li>Press the  icon and select a scope, and press SAVE RULE when done.</li> </ol> <p>Note</p> <p>You cannot edit access rules. To change an access rules, you need to delete the rule, then create a new rule to replace it. You can also add multiple rules for the same user.</p> <p>To delete a rule:</p> <ol> <li>Press the  icon, then Roles and Access rules.</li> <li>Choose Access rules, then select a rule and press Delete.</li> </ol>"},{"location":"admin/runai-setup/authentication/authentication-overview/","title":"Overview","text":""},{"location":"admin/runai-setup/authentication/authentication-overview/#authentication-overview","title":"Authentication Overview","text":"<p>To access Run:ai resources, you have to authenticate. The purpose of this document is to explain how authentication works at Run:ai.</p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#authentication-endpoints","title":"Authentication Endpoints","text":"<p>Generally speaking, there are two authentication endpoints:</p> <ul> <li>The Run:ai control plane.</li> <li>Run:ai GPU clusters.</li> </ul> <p>Both endpoints are accessible via APIs as well as a user interface. </p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#identity-service","title":"Identity Service","text":"<p>Run:ai includes an internal identity service. The identity service ensures users are who they claim to be and gives them the right kind of access to Run:ai.</p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#users","title":"Users","text":"<p>Out of the box, The Run:ai identity service provides a way to create users and associate them with access roles. </p> <p>It is also possible to configure the Run:ai identity service to connect to a company directory using the SAML protocol. For more information see single sign-on.</p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#authentication-method","title":"Authentication Method","text":"<p>Both endpoints described above are protected via time-limited oauth2-like JWT authentication tokens.</p> <p>There are two ways of getting a token:</p> <ul> <li>Using a user/password combination.</li> <li>Using client applications for API access.</li> </ul>"},{"location":"admin/runai-setup/authentication/authentication-overview/#authentication-flows","title":"Authentication Flows","text":""},{"location":"admin/runai-setup/authentication/authentication-overview/#runai-control-plane","title":"Run:ai control plane","text":"<p>You can use the Run:ai user interface to provide user/password. These are validated against the identity service. Run:ai will return a token with the right access rights for continued operation. </p> <p>You can also use a client application to get a token and then connect directly to the administration API endpoint. </p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#runai-gpu-clusters","title":"Run:ai GPU Clusters","text":"<p>The Run:ai GPU cluster is a Kubernetes cluster. All communication into Kubernetes flows through the Kubernetes API server.</p> <p>To facilitate authentication via Run:ai the Kubernetes API server must be configured to use the Run:ai identity service to validate authentication tokens. For more information on how to configure the Kubernetes API server see Kubernetes configuration under researcher authentication.</p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#inactivity-timeout","title":"Inactivity timeout","text":"<p> Version 2.10 and later.</p> <p>Run:ai session should timeout after 1 hour of inactivity.</p> <p>Note</p> <p>Timeout settings are configured in minutes.</p> <p>To configure the inactivity timeout: 1. Open <code>Settings | General</code>. 2. Set the inactivity timeout in minutes. (Default is 60)</p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#see-also","title":"See also","text":"<ul> <li>To configure authentication for researchers researcher authentication.</li> <li>To configure single sign-on, see single sign-on.</li> </ul>"},{"location":"admin/runai-setup/authentication/researcher-authentication/","title":"Setup Researcher Access Control","text":""},{"location":"admin/runai-setup/authentication/researcher-authentication/#introduction","title":"Introduction","text":"<p>The following instructions explain how to complete the configuration of access control for Researchers. Run:ai access control is at the Project level. When you assign Users to Projects, only these users are allowed to submit Jobs and access Jobs details.</p> <p>This requires several steps:</p> <ul> <li>Assign users to their Projects.</li> <li>(Mandatory) Modify the Kubernetes entry point (called the <code>Kubernetes API server</code>) to validate credentials of incoming requests against the Run:ai Authentication authority.</li> <li>(Command-line Interface usage only) Modify the Kubernetes profile to prompt the Researcher for credentials when running <code>runai login</code> (or <code>oc login</code> for OpenShift).</li> </ul>"},{"location":"admin/runai-setup/authentication/researcher-authentication/#administration-user-interface-setup","title":"Administration User Interface Setup","text":""},{"location":"admin/runai-setup/authentication/researcher-authentication/#assign-users-to-projects","title":"Assign Users to Projects","text":"<p>Assign Researchers to Projects:</p> <ul> <li>Open the Run:ai user interface and navigate to <code>Users</code>. Add a Researcher and assign it a <code>Researcher</code> role.</li> <li>Navigate to <code>Projects</code>. Edit or create a Project. Use the <code>Access Control</code> tab to assign the Researcher to the Project.</li> <li>If you are using Single Sign-On, you can also assign Groups. For more information see the Single Sign-On documentation.</li> </ul>"},{"location":"admin/runai-setup/authentication/researcher-authentication/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<p>Important</p> <p>As of Run:ai version 2.15, you only need to perform this step when accessing Run:ai from the command-line interface or sending YAMLs directly to Kubernetes</p> <p>As described in authentication overview, you must direct the Kubernetes API server to authenticate via Run:ai. This requires adding flags to the Kubernetes API Server. The flags show in the Run:ai user interface under <code>Settings</code> | <code>General</code> | <code>Researcher Authentication</code> | <code>Server configuration</code>.</p> <p>Modifying the API Server configuration differs between Kubernetes distributions:</p> Vanilla KubernetesOpenShiftRKERKE2GKEEKSBCMAKSOther <ul> <li>Locate the Kubernetes API Server configuration file. The file's location may differ between different Kubernetes distributions. The location for vanilla Kubernetes is <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code></li> <li>Edit the document, under the <code>command</code> tag, add the server configuration text described above.</li> <li>Verify that the <code>kube-apiserver-&lt;master-node-name&gt;</code> pod in the <code>kube-system</code> namespace has been restarted and that changes have been incorporated. Run the below and verify that the oidc flags you have added:</li> </ul> <pre><code>kubectl get pods -n kube-system kube-apiserver-&lt;master-node-name&gt; -o yaml\n</code></pre> <p>No configuration is needed. Instead, Run:ai assumes that an Identity Provider has been defined at the OpenShift level and that the Run:ai Cluster installation has set the <code>OpenshiftIdp</code> flag to true. For more information see the Run:ai OpenShift control-plane setup.</p> <p>Edit Rancher <code>cluster.yml</code> (with Rancher UI, follow this). Add the following:</p> cluster.yml<pre><code>kube-api:\n    always_pull_images: false\n    extra_args:\n        oidc-client-id: runai  # (1)\n        ...\n</code></pre> <ol> <li>These are example parameters. Copy the actual parameters from <code>Settings | General | Researcher Authentication</code> as described above.</li> </ol> <p>You can verify that the flags have been incorporated into the RKE cluster by following the instructions here and running <code>docker inspect &lt;kube-api-server-container-id&gt;</code>, where <code>&lt;kube-api-server-container-id&gt;</code> is the container ID of api-server via obtained in the Rancher document. </p> <p>If working via the RKE2 Quickstart, edit <code>/etc/rancher/rke2/config.yaml</code>. Add the parameters provided in the server configuration section as described above in the following fashion:</p> /etc/rancher/rke2/config.yaml<pre><code>kube-apiserver-arg:\n- \"oidc-client-id=runai\" # (1)\n...\n</code></pre> <ol> <li>These are example parameters. Copy the actual parameters from <code>Settings | General | Researcher Authentication</code> as described above.</li> </ol> <p>If working via Rancher UI, need to add the flag as part of the cluster provisioning. </p> <p>Under <code>Cluster Management | Create</code>, turn on RKE2 and select a platform. Under <code>Cluster Configuration | Advanced | Additional API Server Args</code>. Add the Run:ai flags as <code>&lt;key&gt;=&lt;value&gt;</code> (e.g. <code>oidc-username-prefix=-</code>).</p> <p>Install Anthos identity service by running:</p> <pre><code>gcloud container clusters update &lt;gke-cluster-name&gt; \\\n    --enable-identity-service --project=&lt;gcp-project-name&gt; --zone=&lt;gcp-zone-name&gt;\n</code></pre> <p>Install the yq utility and run:</p> <p>For username-password authentication, run:</p> <pre><code>kubectl get clientconfig default -n kube-public -o yaml &gt; login-config.yaml\nyq -i e \".spec +={\\\"authentication\\\":[{\\\"name\\\":\\\"oidc\\\",\\\"oidc\\\":{\\\"clientID\\\":\\\"runai\\\",\\\"issuerURI\\\":\\\"$OIDC_ISSUER_URL\\\",\\\"kubectlRedirectURI\\\":\\\"http://localhost:8000/callback\\\",\\\"userClaim\\\":\\\"sub\\\",\\\"userPrefix\\\":\\\"-\\\"}}]}\" login-config.yaml\nkubectl apply -f login-config.yaml\n</code></pre> <p>For single-sign-on, run:</p> <pre><code>kubectl get clientconfig default -n kube-public -o yaml &gt; login-config.yaml\nyq -i e \".spec +={\\\"authentication\\\":[{\\\"name\\\":\\\"oidc\\\",\\\"oidc\\\":{\\\"clientID\\\":\\\"runai\\\",\\\"issuerURI\\\":\\\"$OIDC_ISSUER_URL\\\",\\\"groupsClaim\\\":\\\"groups\\\",\\\"kubectlRedirectURI\\\":\\\"http://localhost:8000/callback\\\",\\\"userClaim\\\":\\\"email\\\",\\\"userPrefix\\\":\\\"-\\\"}}]}\" login-config.yaml\nkubectl apply -f login-config.yaml\n</code></pre> <p>Where the <code>OIDC</code> flags are provided in the Run:ai server configuration section as described above. </p> <p>Then update runaiconfig with  the Anthos endpoint - gke-oidc-envoy. Get the externel IP of the service in the Anthos namespace.</p> <pre><code>kubectl get svc -n anthos-identity-service\nNAME               TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)              AGE\ngke-oidc-envoy     LoadBalancer   10.37.3.111   39.201.319.10   443:31545/TCP        12h\n</code></pre> <p>Add the IP to runaiconfig </p> <pre><code>kubectl -n runai patch runaiconfig runai -p '{\"spec\": {\"researcher-service\": {\"args\": {\"gkeOidcEnvoyHost\": \"35.236.229.19\"}}}}'  --type=\"merge\"\n</code></pre> <p>To create a kubeconfig profile for Researchers run:</p> <pre><code>kubectl oidc login --cluster=CLUSTER_NAME --login-config=login-config.yaml \\\n    --kubeconfig=developer-kubeconfig\n</code></pre> <p>(this will require installing the kubectl oidc plug-in as described in the Anthos document above <code>gcloud components install kubectl-oidc</code>)</p> <p>Then modify the <code>developer-kubeconfig</code> file as described in the Command-line Inteface Access section below.</p> <ul> <li>In the AWS Console, under EKS, find your cluster.</li> <li>Go to <code>Configuration</code> and then to <code>Authentication</code>.</li> <li>Associate a new <code>identity provider</code>. Use the parameters provided in the server configuration section as described above. The process can take up to 30 minutes.</li> </ul> <p>Please follow the \"Vanilla Kubernetes\" instructions</p> <p>Please contact Run:ai customer support.</p> <p>See specific instructions in the documentation of the Kubernetes distribution.  </p>"},{"location":"admin/runai-setup/authentication/researcher-authentication/#command-line-interface-access","title":"Command-line Interface Access","text":"<p>To control access to Run:ai (and Kubernetes) resources, you must modify the Kubernetes configuration file. The file is distributed to users as part of the Command-line interface installation.</p> <p>When making changes to the file, keep a copy of the original file to be used for cluster administration. After making the modifications, distribute the modified file to Researchers.</p> <ul> <li>Under the <code>~/.kube</code> directory edit the <code>config</code> file, remove the administrative user, and replace it with text from <code>Settings | General | Researcher Authentication</code> | <code>Client Configuration</code>.</li> <li>Under <code>contexts | context | user</code> change the user to <code>runai-authenticated-user</code>.</li> </ul>"},{"location":"admin/runai-setup/authentication/researcher-authentication/#test-via-command-line-interface","title":"Test via Command-line interface","text":"<ul> <li>Run: <code>runai login</code> (in OpenShift environments use <code>oc login</code> rather than <code>runai login</code>).</li> <li>You will be prompted for a username and password. In a single sign-on flow, you will be asked to copy a link to a browser, log in and return a code.</li> <li>Once login is successful, submit a Job.</li> <li>If the Job was submitted with a Project to which you have no access, your access will be denied.</li> <li>If the Job was submitted with a Project to which you have access, your access will be granted.</li> </ul> <p>You can also submit a Job from the Run:ai User interface and verify that the new job shows on the job list with your user name.</p>"},{"location":"admin/runai-setup/authentication/researcher-authentication/#test-via-user-interface","title":"Test via User Interface","text":"<ul> <li>Open the Run:ai user interface, go to <code>Jobs</code>.</li> <li>On the top-right, select <code>Submit Job</code>.</li> </ul> <p>Tip</p> <p>If you do not see the button or it is disabled, then you either do not have <code>Researcher</code> access or the cluster has not been set up correctly. For more information, refer to user interface overview.</p>"},{"location":"admin/runai-setup/authentication/sso/","title":"Single Sign-On","text":"<p>Single Sign-On (SSO) is an authentication scheme that allows a user to log in with a single ID to other, independent, software systems. SSO solves security issues involving multiple user/password data entries, multiple compliance schemes, etc.</p> <p>Run:ai supports SSO using the SAML 2.0 protocol and Open ID Connect or OIDC.</p> <p>Caution</p> <p>Single sign-on is only available with SaaS installations where the tenant has been created post-January 2022 or any Self-hosted installation of release 2.0.58 or later. If you are using single sign-on with older versions of Run:ai, please contact Run:ai customer support</p>"},{"location":"admin/runai-setup/authentication/sso/#terminology","title":"Terminology","text":"<p>Identity Provider (Idp)\u2014 a system that creates, maintains, and manages identity information. Example IdPs: Google, Keycloak, Salesforce, Auth0.</p>"},{"location":"admin/runai-setup/authentication/sso/#saml-prerequisites","title":"SAML Prerequisites","text":"<p>XML Metadata\u2014you must have an XML Metadata file retrieved from your IdP. Upload the file to a web server such that you will have a URL to the file. The URL must have the XML file extension. For example, to connect using Google, you must create a custom SAML App here, download the Metadata file, and upload it to a web server.</p>"},{"location":"admin/runai-setup/authentication/sso/#oidc-prerequisites","title":"OIDC Prerequisites","text":"<ul> <li>Discovery URL\u2014the OpenID server where the content discovery information is published.</li> <li>ClientID\u2014the ID used to identify the client with the Authorization Server.</li> <li>Client Secret\u2014a secret password that only the Client and Authorization Server know.</li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#additional-attribute-mappings","title":"Additional attribute mappings","text":"<p>You can configure your IdP to map several IdP attributes:</p> IdP attribute Default Run:ai name Description User email email (cannot be changed) (Mandatory) <code>e-mail</code> is the user identifier with Run:ai. User role groups GROUPS (Optional) If exists, allows assigning Run:ai role groups via the IdP. The IdP attribute must be of a type of list of strings. See more below Linux User ID UID (Optional) If exists in IdP, allows Researcher containers to start with the Linux User <code>UID</code>. Used to map access to network resources such as file systems to users. The IdP attribute must be of integer type. Linux Group ID GID (Optional) If exists in IdP, allows Researcher containers to start with the Linux Group <code>GID</code>. The IdP attribute must be of integer type. Linux Supplementary Groups SUPPLEMENTARYGROUPS (Optional) If exists in IdP, allows Researcher containers to start with the relevant Linux supplementary groups. The IdP attribute must be of a type of list of integers. User first name firstName (Optional) Used as the first name showing in the Run:ai user interface. User last name lastName (Optional) Used as the last name showing in the Run:ai user interface"},{"location":"admin/runai-setup/authentication/sso/#example-attribute-mapping-for-google-suite","title":"Example attribute mapping for Google Suite","text":"<p>If you are using Google Suite as your Identity provider, to map custom attributes follow the Google support article. Use the Whole Number attribute type. For Supplementary Groups use the Multi-value designation.</p>"},{"location":"admin/runai-setup/authentication/sso/#step-1-ui-configuration","title":"Step 1: UI Configuration","text":"<ol> <li>Press the <code>Tools &amp; Settings</code> then press <code>General</code>.</li> <li>Open the <code>Security</code> pane and press <code>+Identity provider</code>.</li> <li> <p>Select the SSO protocol. Choose <code>SAML 2</code> or <code>Open ID Connect</code>.</p> SAML 2Open ID Connect <ol> <li> <p>Choose <code>From computer</code> or <code>From URL</code>.</p> <ol> <li>For <code>From computer</code>, press the <code>Metadata XML file</code> field, then select your file for upload. </li> <li>For <code>From URL</code>, in the <code>Metadata XML Url</code> field, enter the URL to the XML Metadata file.</li> </ol> </li> <li> <p>Copy the <code>Redirect URL</code> and <code>Entity ID</code> and use them in your identity provider.</p> </li> <li>In the <code>User attributes</code> field enter the attribute and the value in the identity provider. (optional)</li> <li>When complete, press <code>Save</code>.</li> </ol> <p>After you have configured the SAML 2 settings, you can download the XML file, and view the identity provider settings. </p> <p>Press <code>Download</code> to download the file.</p> <p>Pres <code>Edit</code> to both download the file, and view the:</p> <ul> <li>Identity provider URL.</li> <li>Identity provider entity ID.</li> <li>Certificate expiration date.</li> </ul> <ol> <li>In the <code>Discovery URL</code> field, enter the discovery URL .</li> <li>In the <code>Client ID</code> field, enter the client ID.</li> <li>In the <code>Client Secret</code> field, enter the client secret.</li> <li>In the <code>User attributes</code> field enter the attribute and the value in the identity provider. (optional) 5.When complete, press <code>Save</code>.</li> </ol> </li> <li> <p>In the <code>Logout uri</code> field, enter the desired URL logout page. If left empty, you will be redirected to the Run:ai portal.</p> </li> <li>In the <code>Session timeout</code> field, enter the amount of idle time before users are automatically logged out. (Default is 60 minutes)</li> </ol> <p>Important</p> <p>When pressing <code>Save</code>, all existing users will be rendered non-functional. You can always revert by deleting the identity provider.</p>"},{"location":"admin/runai-setup/authentication/sso/#test","title":"Test","text":"<p>Test Connectivity to Administration User Interface:</p> <ul> <li>Using an incognito browser tab and open the Run:ai user interface.</li> <li>Select the <code>Login with SSO</code> button.</li> <li>You will be redirected to the IdP login page. Use the previously entered Administrator email* to log in.</li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#troubleshooting","title":"Troubleshooting","text":"<p>The SSO login can be separated into two parts:</p> <ol> <li>Run:ai redirects to the IdP (for example, Google) for login using a SAML Request.</li> <li>Upon successful login, IdP redirects back to Run:ai with a SAML Response.</li> </ol> <p>You can follow that by following the URL changes from app.run.ai to the IdP provider (for example, accounts.google.com) and back to app.run.ai:</p> <ul> <li>If there is an issue on the IdP site (for example, <code>app_is_not_configured</code> error in Google), the problem is likely to be in the SAML Request.</li> <li>If the user is redirected back to Run:ai and something goes wrong, the problem is most likely in the SAML Response.</li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#troubleshooting-saml-request","title":"Troubleshooting SAML Request","text":"<ul> <li>When logging in, have the Chrome network inspector open (Open by <code>Right-Click | Inspect</code> on the page, then open the network tab).</li> <li>After the IdP login screen shows, search in the network tab for an HTTP request showing the SAML Request. Depending on the IdP this would be a request to the IdP domain name. For example, accounts.google.com/idp?1234.</li> <li>When found, go to the \"Payload\" tab and copy the value of the SAML Request.</li> <li>Paste the value into a SAML decoder. A typical response should look like this:</li> </ul> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;samlp:AuthnRequest xmlns:samlp=\"urn:oasis:names:tc:SAML:2.0:protocol\" \n    xmlns=\"urn:oasis:names:tc:SAML:2.0:assertion\" \n    xmlns:saml=\"urn:oasis:names:tc:SAML:2.0:assertion\" \n        AssertionConsumerServiceURL=\"https://.../auth/realms/runai/broker/saml/endpoint\" \n        Destination=\"https://accounts.google.com/o/saml2/idp?idpid=....\" \n        ForceAuthn=\"false\" ID=\"ID_66da617d-b862-4cca-9ei5-b727a920f3cb\" \n        IssueInstant=\"2022-01-12T12:54:22.907Z\" \n        ProtocolBinding=\"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\" Version=\"2.0\"&gt;\n  &lt;saml:Issuer&gt;runai-jtqee5v8ob&lt;/saml:Issuer&gt;\n  &lt;samlp:NameIDPolicy AllowCreate=\"true\" Format=\"urn:oasis:names:tc:SAML:2.0:nameid-format:persistent\"/&gt;\n&lt;/samlp:AuthnRequest&gt;\n</code></pre> <p>Check in the above that:</p> <ul> <li>The content of the <code>&lt;saml:Issuer&gt;</code> tag is the same as <code>Entity ID</code> defined above.</li> <li><code>AssertionConsumerServiceURL</code> is the same as the <code>Redirect URI</code>.</li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#troubleshooting-saml-response","title":"Troubleshooting SAML Response","text":"<ul> <li>When logging in, have the Chrome network inspector open (Open by <code>Right-Click | Inspect</code> on the page, then open the network tab).</li> <li>Search for \"endpoint\".</li> <li>When found, go to the \"Payload\" tab and copy the value of the SAML Response.</li> <li>Paste the value into a SAML decoder. A typical response should look like this:</li> </ul> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\n&lt;saml2p:Response\n    xmlns:saml2p=\"urn:oasis:names:tc:SAML:2.0:protocol\" Destination=\"https://.../auth/realms/runai/broker/saml/endpoint\" ID=\"_2d085ed4f45a7ab221a49e6c02e30cac\" InResponseTo=\"ID_295f2723-79f5-4410-99b2-5f4acb2d4f8e\" IssueInstant=\"2022-01-12T12:06:31.175Z\" Version=\"2.0\"&gt;\n    &lt;saml2:Issuer\n        xmlns:saml2=\"urn:oasis:names:tc:SAML:2.0:assertion\"&gt;https://accounts.google.com/o/saml2?idpid=....\n    &lt;/saml2:Issuer&gt;\n    &lt;saml2p:Status&gt;\n        &lt;saml2p:StatusCode Value=\"urn:oasis:names:tc:SAML:2.0:status:Success\"/&gt;\n    &lt;/saml2p:Status&gt;\n    &lt;saml2:Assertion\n        xmlns:saml2=\"urn:oasis:names:tc:SAML:2.0:assertion\" ID=\"_befe8441fa06594b365c516558dc5636\" IssueInstant=\"2022-01-12T12:06:31.175Z\" Version=\"2.0\"&gt;\n        &lt;saml2:Issuer&gt;https://accounts.google.com/o/saml2?idpid=...&lt;/saml2:Issuer&gt;\n        &lt;ds:Signature\n            xmlns:ds=\"http://www.w3.org/2000/09/xmldsig#\"&gt;\n            &lt;ds:SignedInfo&gt;\n                &lt;ds:CanonicalizationMethod Algorithm=\"http://www.w3.org/2001/10/xml-exc-c14n#\"/&gt;\n                &lt;ds:SignatureMethod Algorithm=\"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\"/&gt;\n                &lt;ds:Reference URI=\"#_befe8441fa06594b365c516558dc5636\"&gt;\n                    &lt;ds:Transforms&gt;\n                        &lt;ds:Transform Algorithm=\"http://www.w3.org/2000/09/xmldsig#enveloped-signature\"/&gt;\n                        &lt;ds:Transform Algorithm=\"http://www.w3.org/2001/10/xml-exc-c14n#\"/&gt;\n                    &lt;/ds:Transforms&gt;\n                    &lt;ds:DigestMethod Algorithm=\"http://www.w3.org/2001/04/xmlenc#sha256\"/&gt;\n                    &lt;ds:DigestValue&gt;QxNCjtz9Gomv2qaz8Rb4X8cQJOSGkK+87CrHDkBPidM=&lt;/ds:DigestValue&gt;\n                &lt;/ds:Reference&gt;\n            &lt;/ds:SignedInfo&gt;\n            &lt;ds:SignatureValue&gt;...&lt;/ds:SignatureValue&gt;\n            &lt;ds:KeyInfo&gt;\n                &lt;ds:X509Data&gt;\n                    &lt;ds:X509SubjectName&gt;ST=California,C=US,OU=Google For Work,CN=Google,L=Mountain View,O=Google Inc.&lt;/ds:X509SubjectName&gt;\n                    &lt;ds:X509Certificate&gt;...&lt;/ds:X509Certificate&gt;\n                &lt;/ds:X509Data&gt;\n            &lt;/ds:KeyInfo&gt;\n        &lt;/ds:Signature&gt;\n        &lt;saml2:Subject&gt;\n            &lt;saml2:NameID Format=\"urn:oasis:names:tc:SAML:2.0:nameid-format:persistent\"&gt;john@example.com&lt;/saml2:NameID&gt;\n            &lt;saml2:SubjectConfirmation Method=\"urn:oasis:names:tc:SAML:2.0:cm:bearer\"&gt;\n                &lt;saml2:SubjectConfirmationData \n                    InResponseTo=\"ID_295f2723-79f5-4410-99b2-5f4acb2d4f8e\" \n                    NotOnOrAfter=\"2022-01-12T12:11:31.175Z\" \n                    Recipient=\"https://.../auth/realms/runai/broker/saml/endpoint\"/&gt;\n            &lt;/saml2:SubjectConfirmation&gt;\n        &lt;/saml2:Subject&gt;\n        &lt;saml2:Conditions NotBefore=\"2022-01-12T12:01:31.175Z\" NotOnOrAfter=\"2022-01-12T12:11:31.175Z\"&gt;\n            &lt;saml2:AudienceRestriction&gt;\n                &lt;saml2:Audience&gt;runai-jtqee5v8ob&lt;/saml2:Audience&gt;\n            &lt;/saml2:AudienceRestriction&gt;\n        &lt;/saml2:Conditions&gt;\n        &lt;saml2:AttributeStatement&gt;\n            &lt;saml2:Attribute Name=\"email\"&gt;\n                &lt;saml2:AttributeValue\n                    xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\n                    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;john@example.com\n                &lt;/saml2:AttributeValue&gt;\n            &lt;/saml2:Attribute&gt;\n            &lt;saml2:Attribute Name=\"GID\"&gt;\n                &lt;saml2:AttributeValue\n                    xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\n                    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;8765\n                &lt;/saml2:AttributeValue&gt;\n            &lt;/saml2:Attribute&gt;\n            &lt;saml2:Attribute Name=\"SUPPLEMENTARYGROUPS\"&gt;\n                &lt;saml2:AttributeValue\n                    xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\n                    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;200\n                &lt;/saml2:AttributeValue&gt;\n                &lt;saml2:AttributeValue\n                    xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\n                    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;300\n                &lt;/saml2:AttributeValue&gt;\n                &lt;saml2:AttributeValue\n                    xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\n                    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;400\n                &lt;/saml2:AttributeValue&gt;\n                &lt;saml2:AttributeValue\n                    xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\n                    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;100\n                &lt;/saml2:AttributeValue&gt;\n            &lt;/saml2:Attribute&gt;\n            &lt;saml2:Attribute Name=\"UID\"&gt;\n                &lt;saml2:AttributeValue\n                    xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\n                    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;4321\n                &lt;/saml2:AttributeValue&gt;\n            &lt;/saml2:Attribute&gt;\n        &lt;/saml2:AttributeStatement&gt;\n        &lt;saml2:AuthnStatement AuthnInstant=\"2022-01-12T12:06:30.000Z\" SessionIndex=\"_befe8441fa06594b365c516558dc5636\"&gt;\n            &lt;saml2:AuthnContext&gt;\n                &lt;saml2:AuthnContextClassRef&gt;urn:oasis:names:tc:SAML:2.0:ac:classes:unspecified&lt;/saml2:AuthnContextClassRef&gt;\n            &lt;/saml2:AuthnContext&gt;\n        &lt;/saml2:AuthnStatement&gt;\n    &lt;/saml2:Assertion&gt;\n&lt;/saml2p:Response&gt;\n</code></pre> <p>Check in the above that:</p> <ul> <li>The content of the <code>&lt;saml2:Audience&gt;</code> tag is the same as <code>Entity ID</code> defined above.</li> <li>The <code>Destination</code> at the top is the same as the <code>Redirect URI</code>.</li> <li>The user email under the <code>&lt;saml2:Subject&gt;</code> tag is the same as the logged-in user.</li> <li>Make sure that under the <code>&lt;saml2:AttributeStatement&gt;</code> tag, there is an Attribute named <code>email</code> (lowercase). This attribute is mandatory.</li> <li>If other, optional attributes (such as UID, GID) are mapped, make sure they exist under <code>&lt;saml2:AttributeStatement&gt;</code> along with their respective values.</li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#step-2-cluster-authentication","title":"Step 2: Cluster Authentication","text":"<p>Researchers should be authenticated when accessing the Run:ai GPU Cluster. To perform that, the Kubernetes cluster and the user's Kubernetes profile must be aware of the IdP. Follow the instructions here. If you have followed these instructions in the past, you must do so again and replace the client-side and server-side configuration values. To see the new values, press <code>Tools &amp; Settings</code> then <code>General</code>, and expand the   <code>Cluster Authentication</code> pane.</p>"},{"location":"admin/runai-setup/authentication/sso/#connectivity-test","title":"Connectivity test","text":"<p>Test connectivity to Run:ai command-line interface:</p> <ul> <li>In the command-line, run <code>runai login</code>.</li> <li>You will receive a link that you must copy and open in your browser. Post login you will receive a verification code which you must paste into the shell window.</li> <li>Verify successful login.</li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#step-3-uidgid-mapping","title":"Step 3: UID/GID Mapping","text":"<p>You can configure the IdP to add UID, GID, and Supplementary groups in the IdP. To configure, see UI Configuration.</p>"},{"location":"admin/runai-setup/authentication/sso/#mapping-test","title":"Mapping test","text":"<p>Test the mapping of UID/GID to within the container:</p> <p>Submit a job with the flag <code>--run-as-user</code>, for example:</p> <pre><code>runai submit -i ubuntu --interactive --run-as-user --attach -- bash\n</code></pre> <p>When a shell opens inside the container, run <code>id</code> and verify that UID, GID, and the supplementary groups are the same as in the user's profile in the organization's directory.</p>"},{"location":"admin/runai-setup/authentication/sso/#step-4-adding-users","title":"Step 4: Adding Users","text":"<p>You can add additional users, by either:</p> <ol> <li>Manually adding roles for each user.</li> <li>Mapping roles to IdP groups.</li> </ol> <p>The latter option is easier to maintain.</p>"},{"location":"admin/runai-setup/authentication/sso/#adding-roles-for-a-user","title":"Adding Roles for a User","text":"<ul> <li>Go to <code>Tools &amp; Settings</code>, then press <code>Users</code>.</li> <li>Select the <code>Users</code> button at the top.</li> <li>Map users as explained here.</li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#mapping-role-groups","title":"Mapping Role Groups","text":"<ul> <li>Go to Go to <code>Tools &amp; Settings</code>, then press <code>Users</code>.</li> <li>Select the <code>Groups</code> button.</li> <li>Assuming you have mapped the IdP <code>Groups</code> attribute as described in the prerequisites section above, add a name of a group that has been created in the directory and create an equivalent Run:ai Group.</li> <li>If the role group contains the <code>Researcher</code> role, you can assign this group to a Run:ai Project. All members of the group will have access to the cluster.</li> </ul> <p>Note</p> <p>This feature also works in OpenShift. If you create a group in Run:ai with the same name as an OpenShift Group, the associated permissions will be applied to all users in the group.</p>"},{"location":"admin/runai-setup/authentication/sso/#implementation-notes","title":"Implementation Notes","text":"<p>Run:ai SSO does not support single logout. As such, logging out from Run:ai will not log you out from other systems.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-delete/","title":"Deleting a Cluster Installation","text":"<p>To delete a Run:ai Cluster installation while retaining existing running jobs, run the following commands:</p> Version 2.9 or later <pre><code>helm uninstall runai-cluster -n runai\n</code></pre> Version 2.8 <pre><code>kubectl delete RunaiConfig runai -n runai\nhelm uninstall runai-cluster -n runai\n</code></pre> Version 2.7 or earlier <pre><code>kubectl patch RunaiConfig runai -n runai -p '{\"metadata\":{\"finalizers\":[]}}' --type=\"merge\"\nkubectl delete RunaiConfig runai -n runai\nhelm uninstall runai-cluster runai -n runai\n</code></pre> <p>The commands will not delete existing Jobs submitted by users.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/","title":"Cluster Install","text":"<p>Below are instructions on how to install a Run:ai cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#prerequisites","title":"Prerequisites","text":"<p>Before installing, please review the installation prerequisites listed in Run:ai GPU Cluster Prerequisites.</p> <p>Important</p> <p>We strongly recommend running the Run:ai pre-install script to verify that all prerequisites are met. </p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#install-runai","title":"Install Run:ai","text":"<p>Log in to Run:ai user interface at <code>&lt;company-name&gt;.run.ai</code>. Use credentials provided by Run:ai Customer Support:</p> <ul> <li>If no clusters are currently configured, you will see a Cluster installation wizard.</li> <li>If a cluster has already been configured, use the menu on the top left and select <code>Clusters</code>. On the top left, click <code>New Cluster</code>.</li> </ul> <p>Using the cluster wizard:</p> <ul> <li>Choose a name for your cluster.</li> <li>Choose the Run:ai version for the cluster.</li> <li>Choose a target Kubernetes distribution (see table for supported distributions).</li> <li>(SaaS and remote self-hosted cluster only) Enter a URL for the Kubernetes cluster. The URL need only be accessible within the organization's network. For more informtaion see here.</li> <li>Press <code>Continue</code>.</li> </ul> <p>On the next page:</p> <ul> <li>(SaaS and remote self-hosted cluster only) Install a trusted certificate to the domain entered above.</li> <li>Run the Helm command provided in the wizard.</li> <li>In case of a failure, see the Installation troubleshooting guide.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#verify-your-clusters-health","title":"Verify your cluster's health","text":"<ul> <li>Verify that the cluster status in the Run:ai Control Plane's Clusters Table is <code>Connected</code>.</li> <li>Go to the Overview Dashboard and verify that the number of GPUs on the top right reflects your GPU resources on your cluster and the list of machines with GPU resources appears on the bottom line.</li> <li>In case of issues, see the Troubleshooting guide.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#researcher-authentication","title":"Researcher Authentication","text":"<p>If you will be using the Run:ai command-line interface or sending YAMLs directly to Kubernetes, you must now set up Researcher Access Control.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#cluster-table","title":"Cluster Table","text":"<p>After you have installed your cluster on the platform, you will see it appear in the Cluster Table. The Cluster Table provides a quick and easy way to see the status of your cluster.</p> <p>In the left menu, press Clusters to view the cluster table. Use Add filter to add one or more filter results based on the columns that are in the table. In the Contains pane, you can use partial or complete text. Filtered text is not case sensitive. To remove the filter, press X next to the filter.</p> <p>The table provides the following columns:</p> <ul> <li>Cluster\u2014the name of the cluster.</li> <li>Kubernetes distribution\u2014the flavor of Kubernetes distribution.</li> <li>Kubernetes version\u2014the version of Kubernetes installed.</li> <li>Status\u2014the status of the cluster. For more information see Cluster status. Hover over the information icon to see a short description and links to troubleshooting.</li> <li>Creation time\u2014the timestamp the cluster was created.</li> <li>URL\u2014the URL that was given to the cluster at the time of creation.</li> <li>Run:ai cluster version\u2014the Run:ai version installed on the cluster.</li> <li>Run:ai cluster UUI\u2014the unique ID of the cluster.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#cluster-status","title":"Cluster Status","text":"<p>The following table describes the different statuses that a cluster could be in.</p> Status Description Waiting to connect The cluster has never been connected. Disconnected There is no communication from the cluster to the Control Plane. This may be due to a network issue. Missing prerequisites At least one of the Mandatory Prerequisites has not been met. Service issues At least one of the Services is not working properly. You can view the list of nonfunctioning services for more information Connected All services are connected and up and running. <p>See the Troubleshooting guide to help troubleshoot issues in the cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#customize-your-installation","title":"Customize your installation","text":"<p>To customize specific aspects of the cluster installation see customize cluster installation.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#set-node-roles-optional","title":"Set Node Roles (Optional)","text":"<p>When installing a production cluster you may want to:</p> <ul> <li>Set one or more Run:ai system nodes. These are nodes dedicated to Run:ai software.</li> <li>Machine learning frequently requires jobs that require CPU but not GPU. You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines.</li> <li>Limit Run:ai to specific nodes in the cluster.</li> </ul> <p>To perform these tasks. See Set Node Roles.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#next-steps","title":"Next Steps","text":"<ul> <li>Set up Run:ai Users Working with Users.</li> <li>Set up Projects for Researchers Working with Projects.</li> <li>Set up Researchers to work with the Run:ai Command-line interface (CLI). See  Installing the Run:ai Command-line Interface on how to install the CLI for users.</li> <li>Review advanced setup and maintenance scenarios.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/","title":"Prerequisites in a nutshell","text":""},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#prerequisites-in-a-nutshell","title":"Prerequisites in a Nutshell","text":"<p>The following is a checklist of the Run:ai prerequisites:</p> Prerequisite Details Kubernetes Verify certified vendor and correct version. NVIDIA GPU Operator Different Kubernetes flavors have slightly different setup instructions.   Verify correct version. Ingress Controller Install and configure NGINX (some Kubernetes flavors have NGINX pre-installed). Prometheus Install Prometheus. Trusted domain name You must provide a trusted domain name. Accessible only inside the organization (Optional) Distributed Training Install Kubeflow Training Operator if required. (Optional) Inference Some third party software needs to be installed to use the Run:ai inference module. <p>There are also specific hardware, operating system and network access requirements. A pre-install script is available to test if the prerequisites are met before installation.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#software-requirements","title":"Software Requirements","text":""},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#operating-system","title":"Operating System","text":"<ul> <li>Run:ai will work on any Linux operating system that is supported by both Kubernetes and NVIDIA.</li> <li>An important highlight is that GKE (Google Kubernetes Engine) will only work with Ubuntu, as NVIDIA does not support the default Container-Optimized OS with Containerd image.</li> <li>Run:ai performs its internal tests on Ubuntu 20.04 and CoreOS for OpenShift.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#kubernetes","title":"Kubernetes","text":"<p>Run:ai requires Kubernetes. Run:ai is been certified with the following Kubernetes distributions:</p> Kubernetes Distribution Description Installation Notes Vanilla Kubernetes Using no specific distribution but rather k8s vanilla installation See instructions for a simple (non-production-ready) Kubernetes Installation script. OCP OpenShift Container Platform The Run:ai operator is certified for OpenShift by Red Hat. EKS Amazon Elastic Kubernetes Service AKS Azure Kubernetes Services GKE Google Kubernetes Engine RKE Rancher Kubernetes Engine When installing Run:ai, select On Premise BCM NVIDIA Base Command Manager In addition, NVIDIA DGX comes bundled with Run:ai <p>Run:ai has been tested with the following Kubernetes distributions. Please contact Run:ai Customer Support for up to date certification details:</p> Kubernetes Distribution Description Installation Notes Ezmeral HPE Ezmeral Container Platform See Run:ai at Ezmeral marketplace Tanzu VMWare Kubernetes Tanzu supports containerd rather than docker. See the NVIDIA prerequisites below as well as cluster customization for changes required for containerd <p>Following is a Kubernetes support matrix for the latest Run:ai releases:</p> Run:ai version Supported Kubernetes versions Supported OpenShift versions Run:ai 2.9 1.21 through 1.26 4.8 through 4.11 Run:ai 2.10 1.21 through 1.26 (see note below) 4.8 through 4.11 Run:ai 2.13 1.23 through 1.28 (see note below) 4.10 through 4.13 Run:ai 2.15 1.25 through 1.28 4.11 through 4.13 Run:ai 2.16 1.26 through 1.28 4.11 through 4.14 Run:ai 2.17 1.27 through 1.29 4.12 through 4.15 <p>For information on supported versions of managed Kubernetes, it's important to consult the release notes provided by your Kubernetes service provider. Within these notes, you can confirm the specific version of the underlying Kubernetes platform supported by the provider, ensuring compatibility with Run:ai.</p> <p>For an up-to-date end-of-life statement of Kubernetes see Kubernetes Release History.</p> <p>Note</p> <p>Run:ai allows scheduling of Jobs with PVCs. See for example the command-line interface flag --pvc-new. A Job scheduled with a PVC based on a specific type of storage class (a storage class with the property <code>volumeBindingMode</code> equals to <code>WaitForFirstConsumer</code>) will not work on Kubernetes 1.23 or lower.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#pod-security-admission","title":"Pod Security Admission","text":"<p>Run:ai version 2.15 and above supports <code>restricted</code> policy for Pod Security Admission (PSA) on OpenShift only. Other Kubernetes distributions are only supported with <code>Privileged</code> policy.</p> <p>For Run:ai on OpenShift to run with PSA <code>restricted</code> policy:</p> <ol> <li>The <code>runai</code> namespace should still be marked as <code>privileged</code> as described in Pod Security Admission. Specifically, label the namespace with the following labels:</li> </ol> <pre><code>pod-security.kubernetes.io/audit=privileged\npod-security.kubernetes.io/enforce=privileged\npod-security.kubernetes.io/warn=privileged\n</code></pre> <ol> <li>The workloads submitted through Run:ai should comply with the restrictions of PSA <code>restricted</code> policy, which are dropping all Linux capabilities and setting <code>runAsNonRoot</code> to <code>true</code>. This can be done and enforced using Policies.</li> </ol>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#nvidia","title":"NVIDIA","text":"<p>Run:ai has been certified on NVIDIA GPU Operator  22.9 to 24.6. Older versions (1.10 and 1.11) have a documented NVIDIA issue.</p> <p>Follow the Getting Started guide to install the NVIDIA GPU Operator, or see the distribution-specific instructions below:</p> EKSGKERKE2 <ul> <li>When setting up EKS, do not install the NVIDIA device plug-in  (as we want the NVIDIA GPU Operator to install it instead). When using the eksctl tool to create an AWS EKS cluster, use the flag <code>--install-nvidia-plugin=false</code> to disable this install.</li> <li>Follow the Getting Started guide to install the NVIDIA GPU Operator. For GPU nodes, EKS uses an AMI which already contains the NVIDIA drivers. As such, you must use the GPU Operator flags: <code>--set driver.enabled=false</code>.</li> </ul> <p>Create the <code>gpu-operator</code> namespace by running</p> <pre><code>kubectl create ns gpu-operator\n</code></pre> <p>Before installing the GPU Operator you must create the following file:</p> resourcequota.yaml<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: gcp-critical-pods\n  namespace: gpu-operator\nspec:\n  scopeSelector:\n    matchExpressions:\n    - operator: In\n      scopeName: PriorityClass\n      values:\n      - system-node-critical\n      - system-cluster-critical\n</code></pre> <p>Then run: <code>kubectl apply -f resourcequota.yaml</code></p> <p>Important</p> <ul> <li>Run:ai on GKE has only been tested with GPU Operator version 22.9 and up.</li> <li>The above only works for Run:ai 2.7.16 and above. </li> </ul> <ul> <li>Follow the Getting Started guide to install the NVIDIA GPU Operator.</li> <li>Make sure to specify the <code>CONTAINERD_CONFIG</code> option exactly with the value specified in the document <code>/var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl</code> even though the file may not exist in your system.</li> </ul> <p>Notes</p> <ul> <li>Use the default namespace <code>gpu-operator</code>. Otherwise, you must specify the target namespace using the flag <code>runai-operator.config.nvidiaDcgmExporter.namespace</code> as described in customized cluster installation.</li> <li>NVIDIA drivers may already be installed on the nodes. In such cases, use the NVIDIA GPU Operator flags <code>--set driver.enabled=false</code>. DGX OS is one such example as it comes bundled with NVIDIA Drivers. </li> <li>To use Dynamic MIG, the GPU Operator must be installed with the flag <code>mig.strategy=mixed</code>. If the GPU Operator is already installed, edit the clusterPolicy by running <code>kubectl patch clusterPolicy cluster-policy -n gpu-operator --type=merge -p '{\"spec\":{\"mig\":{\"strategy\": \"mixed\"}}}</code></li> <li>For troubleshooting information, see the NVIDIA GPU Operator Troubleshooting Guide.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#ingress-controller","title":"Ingress Controller","text":"<p>Run:ai requires an ingress controller as a prerequisite. The Run:ai cluster installation configures one or more ingress objects on top of the controller.</p> <p>There are many ways to install and configure an ingress controller and configuration is environment-dependent. A simple solution is to install &amp; configure *NGINX_:</p> On PremRKEManaged Kubernetes <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm upgrade -i nginx-ingress ingress-nginx/ingress-nginx   \\\n    --namespace nginx-ingress --create-namespace \\\n    --set controller.kind=DaemonSet \\\n    --set controller.service.externalIPs=\"{&lt;INTERNAL-IP&gt;,&lt;EXTERNAL-IP&gt;}\" # (1)\n</code></pre> <ol> <li>External and internal IP of one of the nodes</li> </ol> <p>RKE and RKE2 come pre-installed with NGINX. No further action needs to be taken.</p> <p>For managed Kubernetes such as EKS:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm install nginx-ingress ingress-nginx/ingress-nginx \\\n    --namespace nginx-ingress --create-namespace \n</code></pre> <p>For support of ingress controllers different than NGINX please contact Run:ai customer support.</p> <p>Note</p> <p>In a self-hosted installation, the typical scenario is to install the first Run:ai cluster on the same Kubernetes cluster as the control plane. In this case, there is no need to install an ingress controller as it is pre-installed by the control plane.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#cluster-url","title":"Cluster URL","text":""},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#cluster-ip","title":"Prerequisites in a nutshell","text":""},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#domain-name","title":"Prerequisites in a nutshell","text":"<p>The Run:ai cluster creation wizard requires a domain name (FQDN) to the Kubernetes cluster as well as a trusted certificate for that domain. The domain name needs to be accessible inside the organization only.</p> <p>Use an HTTPS-based domain (e.g. https://my-cluster.com) as the cluster URL. Make sure that the DNS is configured with the cluster IP.</p> <p>In addition, to configure HTTPS for your URL, you must create a TLS secret named <code>runai-cluster-domain-tls-secret</code> in the <code>runai</code> namespace. The secret should contain a trusted certificate for the domain:</p> <pre><code>kubectl create ns runai\nkubectl create secret tls runai-cluster-domain-tls-secret -n runai \\\n    --cert /path/to/fullchain.pem  \\ # (1)\n    --key /path/to/private.pem # (2)\n</code></pre> <ol> <li>The domain's cert (public key).</li> <li>The domain's private key.</li> </ol> <p>For more information on how to create a TLS secret see: https://kubernetes.io/docs/concepts/configuration/secret/#tls-secrets.</p> <p>Note</p> <p>In a self-hosted installation, the typical scenario is to install the first Run:ai cluster on the same Kubernetes cluster as the control plane. In this case, the cluster URL need not be provided as it will be the same as the control-plane URL.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#prometheus","title":"Prometheus","text":"<p>If not already installed on your cluster, install the full <code>kube-prometheus-stack</code> through the Prometheus community Operator.</p> <p>Note</p> <ul> <li>If Prometheus has been installed on the cluster in the past, even if it was uninstalled (such as when upgrading from Run:ai 2.8 or lower), you will need to update Prometheus CRDs as described here. For more information on the  Prometheus bug see here.</li> <li>If you are running Kubernetes 1.21, you must install a Prometheus stack version of 45.23.0 or lower. Use the <code>--version</code> flag below. Alternatively, use Helm version 3.12 or later. For more information on the related Prometheus bug see here</li> </ul> <p>Then install the Prometheus stack by running:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n    -n monitoring --create-namespace --set grafana.enabled=false # (1)\n</code></pre> <ol> <li>The Grafana component is not required for Run:ai.</li> </ol> <p>Notes</p> <ul> <li>In an air-gapped environment, if needed, configure the <code>global.imageRegistry</code> value to reference the local registry hosting the Prometheus images.</li> <li>For troubleshooting information, see the Prometheus Troubleshooting Guide.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#optional-software-requirements","title":"Optional Software Requirements","text":"<p>The following software enables specific features of Run:ai</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#distributed-training","title":"Distributed Training","text":"<p>Distributed training allows the Researcher to train models over multiple nodes. Run:ai supports the following distributed training frameworks:</p> <ul> <li>TensorFlow</li> <li>PyTorch</li> <li>XGBoost</li> <li>MPI</li> </ul> <p>All are part of the Kubeflow Training Operator. Run:ai supports Training Operator version 1.7 and up. To install run:</p> <pre><code>kubectl apply -k \"github.com/kubeflow/training-operator/manifests/overlays/standalone?ref=v1.7.0\"\n</code></pre> <p>The Kuberflow Training Operator is packaged with MPI version 1.0 which is not supported by Run:ai. You need to separately install MPI v2beta1:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubeflow/mpi-operator/v0.4.0/deploy/v2beta1/mpi-operator.yaml\n</code></pre> <p>Important</p> <p>If you need both MPI and one of the other frameworks, follow the following process:</p> <ul> <li>Install the training operator as above.</li> <li>Disable MPI in the Training operator by running: <pre><code>kubectl patch deployment training-operator -n kubeflow --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args\", \"value\": [\"--enable-scheme=tfjob\", \"--enable-scheme=pytorchjob\", \"--enable-scheme=xgboostjob\"]}]'\n</code></pre></li> <li>Run: <code>kubectl delete crd mpijobs.kubeflow.org</code>.</li> <li>Install MPI v2beta1 as above.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#inference","title":"Inference","text":"<p>To use the Run:ai inference module you must pre-install Knative Serving. Follow the instructions here to install. Run:ai is certified on Knative 1.4 to 1.12.</p> <p>Post-install, you must configure Knative to use the Run:ai scheduler and allow pod affinity, by running:</p> <pre><code>kubectl patch configmap/config-features \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"kubernetes.podspec-schedulername\":\"enabled\",\"kubernetes.podspec-affinity\":\"enabled\",\"kubernetes.podspec-tolerations\":\"enabled\",\"kubernetes.podspec-volumes-emptydir\":\"enabled\",\"kubernetes.podspec-securitycontext\":\"enabled\",\"kubernetes.podspec-persistent-volume-claim\":\"enabled\",\"kubernetes.podspec-persistent-volume-write\":\"enabled\",\"multi-container\":\"enabled\",\"kubernetes.podspec-init-containers\":\"enabled\"}}'\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#inference-autoscaling","title":"Inference Autoscaling","text":"<p>Run:ai allows to autoscale a deployment using the following metrics:</p> <ol> <li>Throughput (requests/second)</li> <li>Concurrency</li> </ol>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#accessing-inference-from-outside-the-cluster","title":"Accessing Inference from outside the Cluster","text":"<p>Inference workloads will typically be accessed by consumers residing outside the cluster. You will hence want to provide consumers with a URL to access the workload. The URL can be found in the Run:ai user interface under the deployment screen (alternatively, run <code>kubectl get ksvc -n &lt;project-namespace&gt;</code>).</p> <p>However, for the URL to be accessible outside the cluster you must configure your DNS as described here.</p> Alternative Configuration <p>When the above DNS configuration is not possible, you can manually add the <code>Host</code> header to the REST request as follows:</p> <ul> <li>Get an <code>&lt;external-ip&gt;</code> by running <code>kubectl get service -n kourier-system kourier</code>. If you have been using istio during Run:ai installation, run:  <code>kubectl -n istio-system get service istio-ingressgateway</code> instead. </li> <li>Send a request to your workload by using the external ip, and place the workload url as a <code>Host</code> header. For example</li> </ul> <pre><code>curl http://&lt;external-ip&gt;/&lt;container-specific-path&gt;\n    -H 'Host: &lt;host-name&gt;'\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#hardware-requirements","title":"Hardware Requirements","text":"<p>(see picture below)</p> <ul> <li> <p>(Production only) Run:ai System Nodes: To reduce downtime and save CPU cycles on expensive GPU Machines, we recommend that production deployments will contain two or more worker machines, designated for Run:ai Software. The nodes do not have to be dedicated to Run:ai, but for Run:ai purposes we would need:</p> </li> <li> <p>8 CPUs</p> </li> <li>16GB of RAM</li> <li> <p>50GB of Disk space  </p> </li> <li> <p>Shared data volume: Run:ai uses Kubernetes to abstract away the machine on which a container is running:</p> </li> <li> <p>Researcher containers: The Researcher's containers need to be able to access data from any machine in a uniform way, to access training data and code as well as save checkpoints, weights, and other machine-learning-related artifacts.</p> </li> <li> <p>The Run:ai system needs to save data on a storage device that is not dependent on a specific node.  </p> <p>Typically, this is achieved via Kubernetes Storage class  based on Network File Storage (NFS) or Network-attached storage (NAS).</p> </li> <li> <p>Docker Registry: With Run:ai, Workloads are based on Docker images. For container images to run on any machine, these images must be downloaded from a docker registry rather than reside on the local machine (though this also is possible. You can use a public registry such as docker hub or set up a local registry on-prem (preferably on a dedicated machine). Run:ai can assist with setting up the repository.</p> </li> <li> <p>Kubernetes: Production Kubernetes installation requires separate nodes for the Kubernetes master. For more details see your specific Kubernetes distribution documentation.</p> </li> </ul> <p></p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#user-requirements","title":"User requirements","text":"<p>Usage of containers and images: The individual Researcher's work must be based on container images.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#network-access-requirements","title":"Network Access Requirements","text":"<p>Internal networking: Kubernetes networking is an add-on rather than a core part of Kubernetes. Different add-ons have different network requirements. You should consult the documentation of the specific add-on on which ports to open. It is however important to note that unless special provisions are made, Kubernetes assumes all cluster nodes can interconnect using all ports.</p> <p>Outbound network: Run:ai user interface runs from the cloud. All container nodes must be able to connect to the Run:ai cloud. Inbound connectivity (connecting from the cloud into nodes) is not required. If outbound connectivity is limited, the following exceptions should be applied:</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#during-installation","title":"During Installation","text":"<p>Run:ai requires an installation over the Kubernetes cluster. The installation access the web to download various images and registries. Some organizations place limitations on what you can pull from the internet. The following list shows the various solution components and their origin:</p> Name Description URLs Ports Run:ai  Repository Run:ai Helm Package Repository runai.jfrog.io/ui/native/run-ai-charts 443 Docker Images Repository Run:ai images gcr.io/run-ai-prod 443 Docker Images Repository Third party Images hub.docker.com  and quay.io 443 Run:ai Run:ai   Cloud instance app.run.ai 443"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#post-installation","title":"Post Installation","text":"<p>In addition, once running, Run:ai requires an outbound network connection to the following targets:</p> Name Description URLs Ports Grafana Grafana Metrics Server prometheus-us-central1.grafana.net and runailabs.com 443 Run:ai Run:ai   Cloud instance app.run.ai 443"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#network-proxy","title":"Network Proxy","text":"<p>If you are using a Proxy for outbound communication please contact Run:ai customer support</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#pre-install-script","title":"Pre-install Script","text":"<p>Once you believe that the Run:ai prerequisites are met, we highly recommend installing and running the Run:ai pre-install diagnostics script. The tool:</p> <ul> <li>Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking.</li> <li>Looks at additional components installed and analyze their relevance to a successful Run:ai installation.</li> </ul> <p>To use the script download the latest version of the script and run:</p> <pre><code>chmod +x preinstall-diagnostics-&lt;platform&gt;\n./preinstall-diagnostics-&lt;platform&gt;\n</code></pre> <p>If the script shows warnings or errors, locate the file <code>runai-preinstall-diagnostics.txt</code> in the current directory and send it to Run:ai technical support.</p> <p>For more information on the script including additional command-line flags, see here.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/","title":"SaaS Cluster Setup Introduction","text":"<p>This section is a step-by-step guide for setting up a Run:ai cluster. </p> <ul> <li>A Run:ai cluster is installed on top of a Kubernetes cluster.</li> <li>A Run:ai cluster connects to the Run:ai control plane on the cloud. The control plane provides a control point as well as a monitoring and control user interface for Administrators and Researchers.</li> <li>A customer may have multiple Run:ai Clusters, all connecting to a single control plane.</li> </ul> <p>For additional details see the Run:ai system components</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#documents","title":"Documents","text":"<ul> <li>Review Run:ai cluster prerequisites.</li> <li>Step-by-step installation instructions.</li> <li>Look for troubleshooting tips if required.</li> <li>Upgrade cluster and delete cluster instructions. </li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#customization","title":"Customization","text":"<p>For a list of optional customizations see Customize Installation</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#additional-configuration","title":"Additional Configuration","text":"<p>For a list of advanced configuration scenarios such as configuring researcher authentication, Single sign-on limiting the installation to specific nodes, and more, see the Configuration Articles section.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#next-steps","title":"Next Steps","text":"<p>After setting up the cluster, you may want to start setting up Researchers. See: Researcher Setup.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/","title":"Upgrading a Cluster Installation","text":""},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#find-out-runai-cluster-version","title":"Find out Run:ai Cluster version","text":"<p>To find the Run:ai cluster version, run:</p> <pre><code>helm list -n runai -f runai-cluster\n</code></pre> <p>and record the chart version in the form of <code>runai-cluster-&lt;version-number&gt;</code></p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade-runai-cluster","title":"Upgrade Run:ai cluster","text":""},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade-to-version-215","title":"Upgrade to version 2.15","text":"<p>The cluster installation has changed in version 2.15 such that no values file ia needed and old customizations do not have to be copied. Hence, simply follow the instructions for Installing Run:ai to install Run:ai.</p> <p>All customizations done in <code>RunaiConfig</code> are saved during the upgrade. </p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade-from-version-29-210-211-or-212-to-version-213","title":"Upgrade from version 2.9, 2.10, 2.11 or 2.12 to version 2.13","text":"<p>Run:</p> <pre><code>helm get values runai-cluster -n runai &gt; old-values.yaml\n</code></pre> <ol> <li>Review the file <code>old-values.yaml</code> and see if there are any changes performed during the last installation.</li> <li>Follow the instructions for Installing Run:ai to download a new values file. </li> <li>Merge the changes from Step 1 into the new values file.</li> <li>Run <code>helm upgrade</code> as per the instructions in the link above. </li> </ol> <p>Note</p> <p>To upgrade to a specific version of the Run:ai cluster, add <code>--version &lt;version-number&gt;</code> to the <code>helm upgrade</code> command. You can find the relevant version with <code>helm search repo</code> as described above. </p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade-from-version-27-or-28","title":"Upgrade from version 2.7 or 2.8","text":"<p>The process of upgrading from 2.7 or 2.8 requires uninstalling and then installing again. No data is lost during the process. </p> <p>Note</p> <p>The reason for this process is that Run:ai 2.9 cluster installation no longer installs pre-requisites. As such ownership of dependencies such as Prometheus will be undefined if a <code>helm upgrade</code> is run.</p> <p>The process:</p> <ul> <li>Delete the Run:ai cluster installation according to these instructions (do not delete the Run:ai cluster object from the user interface).</li> <li>The following commands should be executed after running the helm uninstall command      <pre><code>kubectl -n runai delete all --all\nkubectl -n runai delete cm --all\nkubectl -n runai delete secret --all\nkubectl -n runai delete roles --all\nkubectl -n runai delete rolebindings --all\nkubectl -n runai delete ingress --all\nkubectl -n runai delete servicemonitors --all\nkubectl -n runai delete podmonitors --all\nkubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io -l app=runai\nkubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io -l app=runai\nkubectl delete svc -n kube-system runai-cluster-kube-prometh-kubelet\n</code></pre></li> <li> <p>Install the mandatory Run:ai prerequisites:</p> <ul> <li>If you have previously installed the SaaS version of Run:ai version 2.7 or below, you will need to install both Ingress Controller and Prometheus.</li> <li>If you have previously installed the SaaS version of Run:ai version 2.8 or any Self-hosted version of Run:ai, you will need to install Prometheus only.</li> </ul> </li> <li> <p>Install Run:ai cluster as described here</p> </li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#verify-successful-installation","title":"Verify Successful Installation","text":"<p>See Verify your installation on how to verify a Run:ai cluster installation</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/","title":"Customize Cluster Installation","text":"<p>This document explains how to customize the Run:ai cluster installation. Customizing the cluster installation is useful if you want to implement specific features.</p> <p>Important</p> <p>Using these instructions to customize your cluster is optional.</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#how-to-customize","title":"How to customize","text":"<p>After the cluster is installed, you can edit the <code>runaiconfig</code> object to add/change configuration. Use the command:</p> <pre><code>kubectl edit runaconfig runai -n runai\n</code></pre> <p>All customizations will be saved when upgrading the cluster to a future version.</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#configurations","title":"Configurations","text":"Key Default Description <code>spec.project-controller.createNamespaces</code> <code>true</code> Set to <code>false</code>if unwilling to provide Run:ai the ability to create namespaces. When set to false, will requires an additional manual step when creating new Run:ai Projects as described below <code>spec.project-controller.clusterWideSecret</code> <code>true</code> Set to <code>false</code> if unwilling to provide Run:ai the ability to create Kubernetes Secrets. When not enabled, automatic secret propagation will not be available <code>spec.mps-server.enabled</code> <code>false</code> Set to <code>true</code> to allow the use of NVIDIA MPS. MPS is useful with Inference workloads <code>spec.global.subdomainSupport</code> <code>false</code> Set to true to allow researcher tools with a sub domain to be spawned from the Run:ai user interface. For more information see External access to containers <code>spec.global.schedulingservices</code> <code>spec.global.syncServices</code> <code>spec.global.workloadServices</code> Set requests and limit configurations for CPU and memory for Run:ai containers. For more information see Large cluster configuration <code>spec.runai-container-toolkit.enabled</code> <code>true</code> Controls the usage of GPU fractions. <code>spec.researcherService.ingress.tlsSecret</code> On Kubernetes distributions other than OpenShift, set a dedicated certificate for the researcher service ingress in the cluster. When not set, the certificate inserted when installing the cluster will be used. The value should be a Kubernetes secret  in the runai namespace <code>spec.researcherService.route.tlsSecret</code> On OpenShift, set a dedicated certificate for the researcher service route. When not set, the OpenShift certificate will be used.  The value should be a Kubernetes secret  in the runai namespace <code>global.image.registry</code> In air-gapped environment, allow cluster images to be pulled from private docker registry. For more information see self-hosted cluster installation <code>global.additionalImagePullSecrets</code> [] Defines a list of secrets to be used to pull images from a private docker registry <code>global.nodeAffinity.restrictScheduling</code> false Restrict scheduling of workloads to specific nodes, based on node labels. For more information see node roles <code>global.nodeAffinity.restrictRunaiSystem</code> false Restrict scheduling of Run:ai system pods to Run:ai designated nodes For more information see system workers <code>spec.prometheus.spec.retention</code> 2h The interval of time where Prometheus will save Run:ai metrics. Promethues is only used as an intermediary to another metrics storage facility and metrics are typically moved within tens of seconds, so changing this setting is mostly for debugging purposes. <code>spec.prometheus.spec.retentionSize</code> Not set The amount of storage allocated for metrics by Prometheus. For more information see Prometheus Storage. <code>spec.prometheus.spec.imagePullSecrets</code> Not set An optional list of references to secrets in the runai namespace to use for pulling Prometheus images (relevant for air-gapped installations)."},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#understanding-custom-access-roles","title":"Understanding Custom Access Roles","text":"<p>To review the access roles created by the Run:ai Cluster installation, see Understanding Access Roles.</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#manual-creation-of-namespaces","title":"Manual Creation of Namespaces","text":"<p>Run:ai Projects are implemented as Kubernetes namespaces. By default, the administrator creates a new Project via the Administration user interface which then triggers the creation of a Kubernetes namespace named <code>runai-&lt;PROJECT-NAME&gt;</code>. There are a couple of use cases that customers will want to disable this feature:</p> <ul> <li>Some organizations prefer to use their internal naming convention for Kubernetes namespaces, rather than Run:ai's default <code>runai-&lt;PROJECT-NAME&gt;</code> convention.</li> <li>Some organizations will not allow Run:ai to automatically create Kubernetes namespaces.</li> </ul> <p>Follow these steps to achieve this:</p> <ol> <li>Disable the namespace creation functionality. See the  <code>runai-operator.config.project-controller.createNamespaces</code> flag above.</li> <li>Create a Project using the Run:ai User Interface.</li> <li>Create the namespace if needed by running: <code>kubectl create ns &lt;NAMESPACE&gt;</code>. The suggested Run:ai default is <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> <li>Label the namespace to connect it to the Run:ai Project by running <code>kubectl label ns &lt;NAMESPACE&gt;  runai/queue=&lt;PROJECT_NAME&gt;</code>, where <code>&lt;PROJECT_NAME&gt;</code> is the name of the project you have created in the Run:ai user interface above and <code>&lt;NAMESPACE&gt;</code> is the name you chose for your namespace.</li> </ol>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/","title":"NVIDIA DGX Bundle","text":""},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#runai-nvidia-dgx-bundle","title":"Run:ai &amp; NVIDIA DGX Bundle","text":"<p>NVIDIA DGX is a line of NVIDIA-produced servers and workstations which specialize in using GPUs to accelerate deep learning applications.</p> <p>NVIDIA DGX comes bundled out of the box with Run:ai. The purpose of this document is to guide you through the process of installing and configuring Run:ai in this scenario</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#nvidia-base-command-manager","title":"NVIDIA Base Command Manager","text":"<p>NVIDIA Base Command Manager allows the deployment of software on NVIDIA DGX servers. During the installation of the DGX you will select <code>Run:ai</code> as well as Run:ai prerequisites from the Base Command Manager installer.</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#prerequisites","title":"Prerequisites","text":""},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#software-prerequisites","title":"Software Prerequisites","text":"<p>Run:ai assumes the following components to be pre-installed:</p> <ul> <li><code>NVIDIA GPU Operator</code> - available for installation via the Base Command Manager installer</li> <li><code>Prometheus</code> - available for installation via the Base Command Manager installer</li> <li><code>Ingress controller</code> - NGINX is available for installation via the Base Command Manager installer. </li> </ul>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#runai-prerequisites","title":"Run:ai prerequisites","text":"<p>The Run:ai cluster installer will require the following:</p> <ul> <li><code>Run:ai tenant name</code> - provided by Run:ai customer support.</li> <li><code>Run:ai install secret</code> - provided by Run:ai customer support.</li> <li><code>Cluster URL</code> - your organization should provide you with a domain name.</li> <li><code>Private and public keys</code> -your organization should provide a trusted certificate for the above domain name. The Run:ai installer will require both private key and full-chain in PEM format. </li> <li>Post-installation - credentials for the Run:ai user interface. Provided by Run:ai customer support.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#installing-runai-installer","title":"Installing Run:ai installer","text":"<p>Select Run:ai via the Base Command Manager installer. Remember to select all of the above software prerequisites as well. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#using-the-runai-installer","title":"Using the Run:ai installer","text":"<p>Find out the cluster's IP address. Then browse to <code>http://&lt;CLUSTER-IP&gt;:30080/runai-installer</code>. Alternatively use the Base Command Manager landing page at <code>http://&lt;CLUSTER-IP&gt;/#runai</code>.  </p> <p>Note</p> <ul> <li>Use <code>http</code> rather than <code>https</code>.</li> <li>Use the IP and not a domain name.</li> </ul> <p>A wizard would open up containing 3 pages: Prerequisites, setup, and installation. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#prerequisites-page","title":"Prerequisites Page","text":"<p>The first, verification page, verifies that all of the above software prerequisites are met. Press the \"Verify\" button. You will not be able to continue unless all prerequisites are met. When all are met, press the <code>Continue</code> button. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#setup-page","title":"Setup Page","text":"<p>The setup page asks to provide all of the Run:ai prerequisites described above. The page will verify the Run:ai input (tenant name and install secret) but will not verify the validity of the cluster URL and certificate. If those are incorrect, the Run:ai installation will show as successful but certain aspects of Run:ai will not work. </p> <p>After filling up the form, press <code>Continue</code>. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#installation-page","title":"Installation page","text":"<p>The Run:ai installation will start. Depending on your download network speed the installation can take from 2 to 10 minutes. When the installation is successful you will see a <code>START USING RUN:AI</code> button. Press the button and enter your credentials to enter the Run:ai user interface. </p> <p>Save the URL for future use. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#post-installation","title":"Post-installation.","text":"<p>Post installation, you will want to:</p> <ul> <li>(Mandatory) Set up Researcher Access Control. Without this, the Job Submit form will not work.</li> <li>Set up Run:ai Users Working with Users.</li> <li>Set up Projects for Researchers Working with Projects.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#troubleshooting","title":"Troubleshooting","text":"<p>The cluster installer is a pod in Kubernetes. The pod is responsible for the installation preparation and prerequisite gathering phase. In case of an error during this pre-installation, you need to gather the pod's log. </p> <p>Once the Run:ai cluster installation has started, the behavior is identical to any Run:ai cluster installation flavor. See the troubleshooting page.</p>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/","title":"Vanilla Kubernetes Installation","text":"<p>Kubernetes is composed of master(s) and workers. The instructions and script below are for creating a bare-bones installation of a single master and several workers for testing purposes. For a more complex, production-grade, Kubernetes installation, use tools such as Rancher Kubernetes Engine or review Kubernetes documentation to learn how to customize the native installation.</p>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#prerequisites","title":"Prerequisites","text":"<ul> <li>The script below assumes all machines have Ubuntu 18.04 or later. For other Linux-based operating systems see Kubernetes documentation](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/){target=_blank}. </li> <li>The script must be run with ROOT privileges.</li> <li>Inbound ports 6443,443,8080 must be allowed. </li> <li>The script supports Kubernetes 1.24 or later.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#install-kubernetes","title":"Install Kubernetes","text":""},{"location":"admin/runai-setup/cluster-setup/install-k8s/#install-kubernetes-master","title":"Install Kubernetes Master","text":"<ul> <li>Get the script by running:</li> </ul> <pre><code>wget https://raw.githubusercontent.com/run-ai/docs/v2.17/install/kube-install.sh\n</code></pre> <ul> <li>Run the script with ROOT privileges: <code>sudo ./kube-install.sh</code></li> <li>When prompted, select the option to install Kubernetes master.</li> <li>Select the Kubernetes version you want or press <code>Enter</code> for the default script version. </li> <li>Select the CNI (networking) version or press <code>Enter</code> for the default.</li> </ul> <p>When the script finishes, it will prompt a join command_ to be run on all workers. Save the command for later use.</p> <p>Note</p> <p>The default token expires after 24 hours. If the token has expired, go to the master node and run <code>sudo kubeadm token create --print-join-command</code>. This will produce an up-to-date join command.</p> <p>Test that Kubernetes is up by running: <pre><code>kubectl get nodes\n</code></pre> Verify that the master node is ready</p>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#install-kubernetes-workers","title":"Install Kubernetes Workers","text":"<p>On each designated worker node:</p> <ul> <li>Get the script by running:</li> </ul> <pre><code>wget https://raw.githubusercontent.com/run-ai/docs/v2.17/install/kube-install.sh\n</code></pre> <ul> <li>Run the script with ROOT privileges: <code>sudo ./kube-install.sh</code></li> <li>When prompted, select the option to install Kubernetes worker.</li> <li>Select the Kubernetes version you want or press <code>Enter</code> for the default script version. The version should be the same as the one selected for the Kubernetes master. </li> </ul> <p>When the script finishes, run the join command saved above. </p> <p>To test that the worker has successfully joined, on the master node run: <pre><code>kubectl get nodes\n</code></pre> Verify that the new worker node is showing and ready (may take a couple of seconds).</p>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#avoiding-accidental-upgrades","title":"Avoiding Accidental Upgrades","text":"<p>To avoid an accidental upgrade of Kubernetes binaries during Linux upgrades, it is recommended to hold the version. Run the following on all nodes:</p> <pre><code>sudo apt-mark hold kubeadm kubelet kubectl\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#next-steps","title":"Next Steps","text":"<p>The administrative Kubernetes profile can be found in the master node under the <code>.kube</code> folder. </p>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#reset-nodes","title":"Reset Nodes","text":"<p>The same script also contains an option to completely remove Kubernetes from nodes (master or workers). To use, run: </p> <ul> <li>Get the script by running:</li> </ul> <pre><code>wget https://raw.githubusercontent.com/run-ai/docs/v2.17/install/kube-install.sh\n</code></pre> <ul> <li>Run the script with ROOT privileges: <code>sudo ./kube-install.sh</code></li> <li>When prompted, select the option to reset/delete kubernetes.</li> <li>Select yes when prompted to reset the cluster and remove Kubernetes packages.</li> </ul>"},{"location":"admin/runai-setup/config/access-roles/","title":"Understand the Kubernetes Cluster Access provided to Run:ai","text":"<p>Run:ai has configuration flags that control specific behavioral aspects of Run:ai. Specifically, those which require additional permissions. Such as automatic namespace/project creation, secret propagation, and more.</p> <p>The purpose of this document is to provide security officers with the ability to review what cluster-wide access Run:ai requires, and verify that it is in line with organizational policy, before installing the Run:ai cluster. </p>"},{"location":"admin/runai-setup/config/access-roles/#review-cluster-access-roles","title":"Review Cluster Access Roles","text":"<p>Run the folloinwg:</p> <pre><code>helm repo add runai https://run-ai-charts.storage.googleapis.com\nhelm repo update\nhelm install runai-cluster runai/runai-cluster -n runai -f runai-&lt;cluster-name&gt;.yaml \\\n        --dry-run &gt; cluster-all.yaml\n</code></pre> <p>The file <code>cluster-all.yaml</code> can be then be reviewed. You can use the internal filenames (provided in comments within the file) to gain more understanding according to the table below:</p> Folder File Purpose <code>clusterroles</code> <code>base.yaml</code> Mandatory Kubernetes Cluster Roles and Cluster Role Bindings <code>clusterroles</code> <code>project-controller-ns-creation.yaml</code> Automatic Project Creation and Maintenance. Provides Run:ai with the ability to create Kubernetes namespaces when the Run:ai administrator creates new Projects. Can be turned on/off via flag <code>clusterroles</code> <code>project-controller-rb-creation.yaml</code> Automatically assign Users to Projects. Can be turned on/off via flag <code>clusterroles</code> <code>project-controller-cluster-wide-secrets.yaml</code> Allow the propagation of Secrets. See Secrets in Jobs. Can be turned on/off via flag <code>clusterroles</code> <code>project-controller-limit-range.yaml</code> Disables the usage of the Kubernetes Limit Range feature. Can be turned on/off via flag <code>ocp</code> <code>scc.yaml</code> OpenShift-specific Security Contexts <code>priorityclasses</code> 4 files Folder contains a list of Priority Classes used by Run:ai"},{"location":"admin/runai-setup/config/allow-external-access-to-containers/","title":"External access to Containers","text":""},{"location":"admin/runai-setup/config/allow-external-access-to-containers/#introduction","title":"Introduction","text":"<p>Researchers working with containers may at times need to remotely access the container. Some examples:</p> <ul> <li>Using a Jupyter notebook that runs within the container</li> <li>Using PyCharm to run python commands remotely.</li> <li>Using TensorBoard to view machine learning visualizations</li> </ul> <p>This requires exposing container ports. When using docker, the way Researchers expose ports is by declaring them when starting the container. Run:ai has similar syntax.</p> <p>Run:ai is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers several options:</p> Method Description Prerequisites Port Forwarding Simple port forwarding allows access to the container via local and/or remote port. None NodePort Exposes the service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service from outside the cluster by requesting <code>&lt;NODE-IP&gt;:&lt;NODE-PORT&gt;</code> regardless of which node the container actually resides in. None LoadBalancer Exposes the service externally using a cloud provider\u2019s load balancer. Only available with cloud providers <p>See https://kubernetes.io/docs/concepts/services-networking/service for further details on these options.</p>"},{"location":"admin/runai-setup/config/allow-external-access-to-containers/#workspaces-configuration","title":"Workspaces configuration","text":"<p> Version 2.9 and up </p> <p>Version 2.9 introduces Workspaces which allow the Researcher to build AI models interactively. </p> <p>Workspaces allow the Researcher to launch tools such as Visual Studio code, TensorFlow, TensorBoard etc. These tools require access to the container. Access is provided via URLs. </p> <p>Run:ai uses the Cluster URL provided to dynamically create SSL-secured URLs for researchers\u2019 workspaces in the format of <code>https://&lt;CLUSTER_URL&gt;/project-name/workspace-name</code>.</p> <p>While this form of path-based routing conveniently works with applications like Jupyter Notebooks, it may often not be compatible with other applications. These applications assume running at the root file system, so hardcoded file paths and settings within the container may become invalid when running at a path other than the root. For instance, if the container is expecting to find a file at <code>/etc/config.json</code> but is running at <code>/project-name/workspace-name</code>, the file will not be found. This can cause the container to fail or not function as intended.</p> <p>To address this issue, Run:ai provides support for host-based routing. When enabled, Run:ai creates workspace URLs in a subdomain format (<code>https://project-name-workspace-name.&lt;CLUSTER_URL&gt;/</code>), which allows all workspaces to run at the root path and function properly. </p> <p>To enable host-based routing you must perform the following steps:</p> <ol> <li>Create a second DNS entry <code>*.&lt;CLUSTER_URL&gt;</code>, pointing to the same IP as the original Cluster URL DNS.</li> <li> <p>Obtain a star SSL certificate for this DNS.</p> </li> <li> <p>Add the certificate as a secret:</p> </li> </ol> <pre><code>kubectl create secret tls runai-cluster-domain-star-tls-secret -n runai \\ \n    --cert /path/to/fullchain.pem --key /path/to/private.pem\n</code></pre> <ol> <li>Create the following ingress rule:</li> </ol> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: runai-cluster-domain-star-ingress\n  namespace: runai\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: '*.&lt;CLUSTER_URL&gt;'\n  tls:\n  - hosts:\n    - '*.&lt;CLUSTER_URL&gt;'\n    secretName: runai-cluster-domain-star-tls-secret\n</code></pre> <p>Replace <code>&lt;CLUSTER_URL&gt;</code> as described above.</p> <ol> <li>Edit Runaiconfig to generate the URLs correctly:</li> </ol> <pre><code>kubectl patch RunaiConfig runai -n runai --type=\"merge\" \\\n    -p '{\"spec\":{\"global\":{\"subdomainSupport\": true}}}' \n</code></pre> <p>Once these requirements have been met, all workspaces will automatically be assigned a secured URL with a subdomain, ensuring full functionality for all researcher applications.</p>"},{"location":"admin/runai-setup/config/allow-external-access-to-containers/#see-also","title":"See Also","text":"<ul> <li>To learn how to use port forwarding see the Quickstart document:  Launch an Interactive Build Workload with Connected Ports.</li> <li>See CLI command runai submit.</li> </ul>"},{"location":"admin/runai-setup/config/cli-admin-install/","title":"Install the Run:ai Administrator Command-line Interface","text":"<p>The Run:ai Administrator Command-line Interface (Administrator CLI) allows performing administrative tasks on the Run:ai Cluster.  </p> <p>The instructions below will guide you through the process of installing the Administrator CLI.</p>"},{"location":"admin/runai-setup/config/cli-admin-install/#prerequisites","title":"Prerequisites","text":"<ul> <li>Run:ai Administrator CLI runs on Mac and Linux.   </li> <li>Kubectl (Kubernetes command-line interface) is installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/</li> <li>A Kubernetes configuration file obtained from a computer previously connected to the Kubernetes cluster</li> </ul>"},{"location":"admin/runai-setup/config/cli-admin-install/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<p>The Run:ai Administrator CLI requires a Kubernetes profile with cluster administrative rights. </p>"},{"location":"admin/runai-setup/config/cli-admin-install/#installation","title":"Installation","text":"<p>Download the Run:ai Administrator Command-line Interface by running:</p> MacLinux <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/darwin  # (1) \nchmod +x runai-adm\nsudo mv runai-adm /usr/local/bin/runai-adm\n</code></pre> <ol> <li>In self-hosted environment, use the control-plane URL instead of <code>app.run.ai</code> </li> </ol> <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/linux  # (1)\nchmod +x runai-adm\nsudo mv runai-adm /usr/local/bin/runai-adm\n</code></pre> <ol> <li>In self-hosted environment, use the control-plane URL instead of <code>app.run.ai</code> </li> </ol> <p>To verify the installation run:</p> <pre><code>runai-adm version\n</code></pre>"},{"location":"admin/runai-setup/config/cli-admin-install/#download-a-specific-version","title":"Download a specific version","text":"<p>To download a specific version of <code>runai-adm</code> add the version number to URL. For example:</p> <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/v2.7.22/darwin\n</code></pre>"},{"location":"admin/runai-setup/config/cli-admin-install/#updating-the-runai-administrator-cli","title":"Updating the Run:ai Administrator CLI","text":"<p>To update the CLI to the latest version perform the same install process again. The command <code>runai-adm update</code> is no longer supported.</p>"},{"location":"admin/runai-setup/config/dr/","title":"Backup &amp; Restore","text":""},{"location":"admin/runai-setup/config/dr/#runai-cluster","title":"Run:ai Cluster","text":"<p>The SaaS version of Run:ai does not store any data. The required data is moved into the control-plane. As such, backup of data is not an issue in such environments.</p>"},{"location":"admin/runai-setup/config/dr/#backing-up-cluster-configuration","title":"Backing up Cluster Configuration","text":"<p>The installation of Run:ai cluster can be customized. Changes to the defaults can be configured by editing the <code>runaiconfig</code> object as described here.  These changes will be preserved on upgrade, but will not be preserved on uninstall or damage to Kubernetes. Thus, it is best to back up these customizations. For a list of such changes run:</p> <pre><code>kubectl get runaiconfig runai -n runai -o yaml -o=jsonpath='{.spec}'\n</code></pre>"},{"location":"admin/runai-setup/config/dr/#runai-control-plane","title":"Run:ai Control Plane","text":"<p>The self-hosted variant of Run:ai also installs the control-plane at the customer site. As such, it becomes the responsibility of the IT organization to verify that the system is configured for proper backup and learn how to recover the data when needed.</p>"},{"location":"admin/runai-setup/config/dr/#database-storage","title":"Database Storage","text":"<p>Run:ai uses an internal PostgreSQL database. The database is stored on a Kubernetes Persistent Volume (PV). You must provide a backup solution for the database. Some options:</p> <ul> <li>Backing up of PostgreSQL itself. Example: <code>kubectl -n runai-backend exec -it runai-backend-postgresql-0 -- env  PGPASSWORD=password pg_dump -U postgres   backend   &gt; cluster_name_db_backup.sql</code></li> <li>Backing up the persistent volume holding the database storage.</li> <li>Using third-party backup solutions.</li> </ul> <p>Run:ai also supports an external PostgreSQL database. For details on how to configure an external database please contact Run:ai customer support.</p>"},{"location":"admin/runai-setup/config/dr/#metrics-storage","title":"Metrics Storage","text":"<p>Run:ai stores metric history using Thanos. Thanos is configured to store data on a persistent volume. The recommendation is to back up the PV.</p>"},{"location":"admin/runai-setup/config/dr/#backing-up-control-plane-configuration","title":"Backing up Control-Plane Configuration","text":"<p>The installation of the Run:ai control plane can be configured. The configuration is provided as <code>--set</code> command in the helm installation. These changes will be preserved on upgrade, but will not be preserved on uninstall or on damage to Kubernetes. Thus, it is best to back up these customizations. For a list of customizations used during the installation, run:</p> <p><code>helm get values runai-backend -n runai-backend</code></p>"},{"location":"admin/runai-setup/config/dr/#recovery","title":"Recovery","text":"<p>To recover Run:ai</p> <ul> <li>Re-create the Kubernetes/OpenShift cluster.</li> <li>Recover the persistent volumes for metrics and database.</li> <li>Re-install the Run:ai control plane. Use the additional configuration previously saved and connect to the restored PostgreSQL PV. Connect Prometheus to the stored metrics PV.</li> <li>Re-install the cluster. Add additional configuration post-install.  </li> <li>If the cluster is configured such that Projects do not create a namespace automatically, you will need to re-create namespaces and apply role bindings as discussed in Kubernetes or OpenShift.</li> </ul>"},{"location":"admin/runai-setup/config/ha/","title":"High Availability","text":"<p>The purpose of this document is to configure Run:ai such that it will continue to provide service even if parts of the system are down.</p> <p>A frequent fail scenario is a physical node in the system becoming non-responsive due to physical problems or lack of resources. In such a case, Kubernetes will attempt to relocate the running pods, but the process may take time, during which Run:ai will be down.</p> <p>A different scenario is a high transaction load, leading to system overload. To address such a scenario, please review the article: scaling the Run:ai system.</p>"},{"location":"admin/runai-setup/config/ha/#runai-control-plane","title":"Run:ai Control Plane","text":""},{"location":"admin/runai-setup/config/ha/#runai-system-workers","title":"Run:ai system workers","text":"<p>The Run:ai control plane allows the optional gathering of Run:ai pods into specific nodes. When this feature is used, it is important to set more than one node as a Run:ai system worker. Otherwise, the horizontal scaling described below will not span multiple nodes, and the system will remain with a single point of failure.  </p>"},{"location":"admin/runai-setup/config/ha/#horizontal-scalability-of-runai-services","title":"Horizontal Scalability of Run:ai services","text":"<p>Horizontal scalability is about instructing the system to create more pods to dynamically scale according to incoming load and downsize when the load subsides.</p> <p>The Run:ai control plane is running on a single Kubernetes namespace named <code>runai-backend</code>. The namespace contains various Kubernetes Deployments and StatefulSets. Each of these services can be scaled horizontally.</p>"},{"location":"admin/runai-setup/config/ha/#deployments","title":"Deployments","text":"<p>Each of the Run:ai deployments can be set to scale up, by adding a helm settings on install/upgrade. E.g. <code>--set frontend.autoscaling.enabled=true</code>. For a full list of settings, please contact Run:ai customer support.</p>"},{"location":"admin/runai-setup/config/ha/#statefulsets","title":"StatefulSets","text":"<p>Run:ai uses three third parties which are managed as Kubernetes StatefulSets:</p> <ul> <li>Keycloak\u2014Stores the Run:ai authentication configuration as well as user identities. To scale Keycloak, use the Run:ai control-plane helm flag <code>--set keycloakx.autoscaling.enabled=true</code>. By default, Keycloak sets a minimum of 3 pods and will scale to more on transaction load.</li> <li>PostgreSQL\u2014It is not possible to configure an internal PostgreSQL to scale horizontally. If this is of importance, please contact Customer Support to understand how to connect Run:ai to an external PostgreSQL service which can be configured for high availability.</li> <li>Thanos\u2014To enable Thanos autoscaling, use the following Run:ai control-plane helm flags:</li> </ul> <p>``` --set thanos.query.autoscaling.enabled=true --set thanos.query.autoscaling.maxReplicas=2  --set thanos.query.autoscaling.minReplicas=2 </p>"},{"location":"admin/runai-setup/config/ha/#runai-cluster","title":"Run:ai Cluster","text":""},{"location":"admin/runai-setup/config/ha/#runai-system-workers_1","title":"Run:ai system workers","text":"<p>The Run:ai cluster allows the mandatory gathering of Run:ai pods into specific nodes. When this feature is used, it is important to set more than one node as a Run:ai system worker. Otherwise, the horizontal scaling described below may not span multiple nodes, and the system will remain with a single point of failure.  </p>"},{"location":"admin/runai-setup/config/ha/#prometheus","title":"Prometheus","text":"<p>The default Prometheus installation uses a single pod replica. If the node running the pod is unresponsive, metrics will not be scraped from the cluster and will not be sent to the Run:ai control-plane.</p> <p>Prometheus supports high availability by allowing to run multiple instances. The tradeoff of this approach is that all instances will scrape and send the same data. The Run:ai control plane will identify duplicate metric series and ignore them. This approach will thus increase network, CPU and memory consumption.</p> <p>To change the number of Prometheus instances, edit the <code>runaiconfig</code> as described under customizing the Run:ai cluster. Under <code>prometheus.spec</code>, set <code>replicas</code> to 2.</p>"},{"location":"admin/runai-setup/config/large-clusters/","title":"Scaling the Run:ai system","text":"<p>The purpose of this document is to provide information on how to scale the Run:ai cluster and the Run:ai control-plane to withstand large transaction loads</p>"},{"location":"admin/runai-setup/config/large-clusters/#scaling-the-runai-control-plane","title":"Scaling the Run:ai Control Plane","text":"<p>The Control plane deployments which may encounter load are:</p> Name Kubernetes Deployment name Purpose Backend runai-backend-backend Main control-plane service Frontend runai-backend-frontend Serving of the Run:ai console Grafana runai-backend-grafana Serving of the Run:ai metrics inside the Run:ai console <p>To increase the number of replicas, run:</p> <p>To increase the number of replicas, use the following Run:ai control-plane helm flags</p> <pre><code>--set backend.autoscaling.enabled=true \n--set frontend.autoscaling.enabled=true\n--set grafana.autoscaling.enabled=true --set grafana.autoscaling.minReplicas=2\n</code></pre> <p>Important</p> <p>If you have chosen to mark some of the nodes as Run:ai System Workers, the new replicas will attempt to use these nodes first. Thus, for high availability purposes, you will want to mark more than one node as a Run:ai System Worker.  </p>"},{"location":"admin/runai-setup/config/large-clusters/#thanos","title":"Thanos","text":"<p>Thanos is the 3rd party used by Run:ai to store metrics Under a significant user load, we would also need to increase resources for the Thanos query function. Use the following Run:ai control-plane helm flags:</p> <pre><code>--set thanos.query.resources.limits.memory=3G\n--set thanos.query.resources.requests.memory=3G\n--set thanos.query.resources.limits.cpu=1\n--set thanos.query.resources.requests.cpu=1\n\n--set thanos.receive.resources.limits.memory=6G \n--set thanos.receive.resources.requests.memory=6G\n--set thanos.receive.resources.limits.cpu=1 \n--set thanos.receive.resources.requests.cpu=1\n</code></pre>"},{"location":"admin/runai-setup/config/large-clusters/#scaling-the-runai-cluster","title":"Scaling the Run:ai Cluster","text":""},{"location":"admin/runai-setup/config/large-clusters/#cpu-memory-resources","title":"CPU &amp; Memory Resources","text":"<p>Under Kubernetes, each of the Run:ai containers, has default resource requirements that reflect an average customer load. With significantly larger cluster loads, certain Run:ai services will require more CPU and memory resources. Run:ai now supports the ability to configure these resources and to do so for each Run:ai service group separately.</p>"},{"location":"admin/runai-setup/config/large-clusters/#service-groups","title":"Service Groups","text":"<p>Run:ai supports setting requests and limits configurations for CPU and memory for Run:ai containers. The configuration is set per service group. Each service group reflects a certain load type:</p> Service Group Description Run:ai containers SchedulingServices Containers associated with the Run:ai scheduler Scheduler, StatusUpdater, MetricsExporter, PodGrouper, PodGroupAssigner, Binder SyncServices Containers associated with syncing updates between the Run:ai cluster and the Run:ai control plane Agent, ClusterSync, AssetsSync WorkloadServices Containers associated with submitting Run:ai Workloads WorkloadController, JobController"},{"location":"admin/runai-setup/config/large-clusters/#configuration-steps","title":"Configuration Steps","text":"<p>To configure resource requirements for a group of services, update the RunaiConfig. Set the <code>spec.global.&lt;service-group&gt;.</code> resources section. The following example shows the configuration of scheduling services resource requirements:</p> <pre><code>apiVersion: run.ai/v1\nkind: RunaiConfig\nmetadata:\nspec:\n global:\n   schedulingServices:\n     resources:\n       limits:\n         cpu: 1000m\n         memory: 1Gi\n       requests:\n         cpu: 100m\n         memory: 512Mi\n</code></pre> <p>Use <code>syncServices</code> and <code>workloadServices</code> for the other two service groups.</p>"},{"location":"admin/runai-setup/config/large-clusters/#recommended-resource-specifications-for-large-clusters","title":"Recommended Resource Specifications For Large Clusters","text":"<p>In large clusters (100 nodes or 1500 GPUs or more), we recommend the following configuration for SchedulingServices and SyncServices groups:</p> <pre><code>resources:\n requests:\n   cpu: 1\n   memory: 1Gi\n limits:\n   cpu: 2\n   memory: 2Gi\n</code></pre>"},{"location":"admin/runai-setup/config/large-clusters/#sending-metrics","title":"Sending Metrics","text":"<p>Run:ai uses Prometheus to scrape metrics from the Run:ai cluster and to send them to the Run:ai control plane. The number of metrics is a function of the number of Nodes, Jobs and Projects which the system contains. When reaching hundreds of Nodes and Projects, the system will be sending large quantities of metrics which, in turn, will create a strain on the network as well as the receiving side in the control plane (SaaS or self-hosted).</p> <p>To reduce this strain, we suggest to configure Prometheus to send information in larger bulks and reduce the number of network connections:</p> <ul> <li>Edit the <code>runaiconfig</code> as described under customizing the cluster.</li> <li>Under <code>prometheus.remoteWrite</code> add the following:</li> </ul> <pre><code>queueConfig:\n  capacity: 5000\n  maxSamplesPerSend: 1000\n  maxShards: 100\n</code></pre> <p>This article provides additional details and insight.</p> <p>Also, note that this configuration enlarges the Prometheus queues and thus increases the required memory. It is hence suggested to reduce the metrics retention period as described here</p>"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/","title":"Node affinity with cloud node pools","text":"<p>Run:ai allows for node affinity. Node affinity is the ability to assign a Project to run on specific nodes. To use the node affinity feature, You will need to label the target nodes with the label  <code>run.ai/node-type</code>. Most cloud clusters allow configuring node labels for the node pools in the cluster. This guide shows how to apply this configuration to different cloud providers.</p> <p>To make the node affinity work with node pools on various cloud providers, we need to make sure the node pools are configured with the appropriate Kubernetes label (<code>run.ai/type=&lt;TYPE_VALUE&gt;</code>).</p>"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/#setting-node-labels-while-creating-a-new-cluster","title":"Setting node labels while creating a new cluster","text":"<p>You can configure node-pool labels at cluster creation time</p> GKEAKSEKS <ul> <li>At the first creation screen, you will see a menu on the left side named <code>node-pools</code>.</li> <li>Expand the node pool you want to label.</li> <li>Click on <code>Metadata</code>.</li> <li>Near the bottom, you will find the Kubernetes <code>label</code> section. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>When creating AKS cluster at the node-pools page click on create new node-pool.</li> <li>Go to the <code>labels</code> section and add key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>Create a regular EKS cluster.</li> <li>Click on <code>compute</code>.</li> <li>Click on <code>Add node group</code>.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul>"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/#setting-node-labels-for-a-new-node-pool","title":"Setting node labels for a new node pool","text":"GKEAKSEKS <ul> <li>At the node pool creation screen, go to the <code>metadata</code> section.</li> <li>Near the bottom, you will find the Kubernetes <code>label</code> section. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>Go to your AKS page at Azure.</li> <li>On the left menu click the <code>node-pools</code> button.</li> <li>Click on <code>Add Node Pool</code>.</li> <li>In the new Node Pool page go to <code>Optional settings</code>.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>Go to <code>Add node group</code> screen.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul>"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/#editing-node-labels-for-an-existing-node-pool","title":"Editing node labels for an existing node pool","text":"GKEAKSEKS <ul> <li>Go to the <code>Google Kubernetes Engine</code> page in the Google Cloud console.</li> <li>Go to <code>Google Kubernetes Engine</code>.</li> <li>In the cluster list, click the name of the cluster you want to modify.</li> <li>Click the <code>Nodes</code> tab</li> <li>Under <code>Node Pools</code>, click the name of the node pool you want to modify, then click <code>Edit</code>.</li> <li>Near the bottom, you will find the Kubernetes <code>label</code> section. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <p>To update an existing node pool label you must use the azure cli. Run the following command:</p> <pre><code>az aks nodepool update \\\n    --resource-group [RESOURCE GROUP] \\\n    --cluster-name [CLUSTER NAME] \\\n    --name labelnp \\\n    --labels run.ai/type=[TYPE_VALUE] \\\n    --no-wait\n</code></pre> <ul> <li>Go to the <code>node group</code> page and click on <code>Edit</code>.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul>"},{"location":"admin/runai-setup/config/node-roles/","title":"Designating Specific Role Nodes","text":"<p>When installing a production cluster you may want to:</p> <ul> <li>Set one or more Run:ai system nodes. These are nodes dedicated to Run:ai software. </li> <li>Machine learning frequently requires jobs that require CPU but not GPU. You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines. </li> <li>Limit Run:ai monitoring and scheduling to specific nodes in the cluster. </li> </ul> <p>To perform these tasks you will need the Run:ai Administrator CLI. See Installing the Run:ai Administrator Command-line Interface.</p>"},{"location":"admin/runai-setup/config/node-roles/#dedicated-runai-system-nodes","title":"Dedicated Run:ai System Nodes","text":"<p>Find out the names of the nodes designated for the Run:ai system by running <code>kubectl get nodes</code>. For each such node run:</p> <pre><code>runai-adm set node-role --runai-system-worker &lt;node-name&gt;\n</code></pre> <p>If you re-run <code>kubectl get nodes</code> you will see the node role of these nodes changed to <code>runai-system</code></p> <p>To remove the runai-system node role run:</p> <pre><code>runai-adm remove node-role --runai-system-worker &lt;node-name&gt;\n</code></pre> <p>Warning</p> <p>Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).</p>"},{"location":"admin/runai-setup/config/node-roles/#dedicated-gpu-cpu-nodes","title":"Dedicated GPU &amp; CPU Nodes","text":"<p>Important</p> <p>To enable this feature, you must set the cluster configuration flag <code>global.nodeAffinity.restrictScheduling</code> to <code>true</code>. For more information see customize cluster.</p> <p>Separate nodes into those that:</p> <ul> <li>Run GPU workloads</li> <li>Run CPU workloads</li> <li>Do not run Run:ai at all. these jobs will not be monitored using the Run:ai Administration User interface. </li> </ul> <p>Review nodes names using <code>kubectl get nodes</code>. For each such node run:</p> <pre><code>runai-adm set node-role --gpu-worker &lt;node-name&gt;\n</code></pre> <p>or </p> <pre><code>runai-adm set node-role --cpu-worker &lt;node-name&gt;\n</code></pre> <p>Nodes not marked as GPU worker or CPU worker will not run Run:ai at all.</p> <p>To set all workers not running runai-system as GPU workers run:</p> <pre><code>runai-adm set node-role --all &lt;node-name&gt;\n</code></pre> <p>To remove the CPU or GPU worker node role run:</p> <pre><code>runai-adm remove node-role --cpu-worker &lt;node-name&gt;\n</code></pre> <p>or </p> <pre><code>runai-adm remove node-role --gpu-worker &lt;node-name&gt;\n</code></pre>"},{"location":"admin/runai-setup/config/non-root-containers/","title":"User Identity in Container","text":"<p>The identity of the user in the container determines its access to resources. For example, network file storage solutions typically use this identity to determine the container's access to network volumes. This document explains multiple ways for propagating the user identity into the container.</p>"},{"location":"admin/runai-setup/config/non-root-containers/#the-default-root-access","title":"The Default: Root Access","text":"<p>In docker, as well as in Kubernetes, the default for running containers is running as root. The implication of running as root is that processes running within the container have enough permissions to change anything in the container, and if propagated to network resources - can have permissions outside the container as well.</p> <p>This gives a lot of power to the Researcher but does not sit well with modern security standards of enterprise security.</p> <p>By default, if you run:</p> <p><pre><code>runai submit -i ubuntu --attach --interactive -- bash\n</code></pre> then run <code>id</code>, you will see the root user.</p>"},{"location":"admin/runai-setup/config/non-root-containers/#use-runai-flags-to-limit-root-access","title":"Use Run:ai flags to limit root access","text":"<p>There are two runai submit flags which control user identity at the Researcher level:</p> <ul> <li>The flag <code>--run-as-user</code> starts the container with a specific user. The user is the current Linux user (see below for other behaviors if used in conjunction with Single sign-on).</li> <li>The flag <code>--prevent-privilege-escalation</code> prevents the container from elevating its own privileges into <code>root</code> (e.g. running <code>sudo</code> or changing system files.).</li> </ul> <p>Equivalent flags exist in the Researcher User Interface.</p>"},{"location":"admin/runai-setup/config/non-root-containers/#run-as-current-user","title":"Run as Current User","text":"<p>From a Linux/Mac box, run:</p> <pre><code>runai submit -i ubuntu --attach --interactive --run-as-user -- bash\n</code></pre> <p>then run <code>id</code>, you will see the users and groups of the box you have been using to launch the Job.</p>"},{"location":"admin/runai-setup/config/non-root-containers/#prevent-escalation","title":"Prevent Escalation","text":"<p>From a Linux/Mac box, run:</p> <pre><code>runai submit -i ubuntu --attach --interactive --run-as-user \\\n  --prevent-privilege-escalation  -- bash\n</code></pre> <p>then verify that you cannot run <code>su</code> to become root within the container.</p>"},{"location":"admin/runai-setup/config/non-root-containers/#setting-a-cluster-wide-default","title":"Setting a Cluster-Wide Default","text":"<p>The two flags are voluntary. They are not enforced by the system. It is however possible to enforce them using Policies. Polices allow an Administrator to force compliance on both the User Interface and Command-line interface.</p>"},{"location":"admin/runai-setup/config/non-root-containers/#passing-user-identity","title":"Passing user identity","text":""},{"location":"admin/runai-setup/config/non-root-containers/#passing-user-identity-from-identity-provider","title":"Passing user identity from Identity Provider","text":"<p>A best practice is to store the user identifier (UID) and the group identifier (GID) in the organization's directory. Run:ai allows you to pass these values to the container and use them as the container identity.</p> <p>To perform this, you must:</p> <ul> <li>Set up single sign-on. Perform the steps for UID/GID integration.</li> <li>Run: <code>runai login</code> and enter your credentials</li> <li>Use the flag --run-as-user</li> </ul> <p>Running <code>id</code> should show the identifier from the directory.</p>"},{"location":"admin/runai-setup/config/non-root-containers/#passing-user-identity-explicitly-via-the-researcher-ui","title":"Passing user identity explicitly via the Researcher UI","text":"<p>Via the Researcher User Interface, it is possible to explicitly provide the user id and group id:</p> <p></p>"},{"location":"admin/runai-setup/config/non-root-containers/#using-openshift-or-gatekeeper-to-provide-cluster-level-controls","title":"Using OpenShift or Gatekeeper to provide Cluster Level Controls","text":"<p>Run:ai supports OpenShift as a Kubernetes platform. In OpenShift the system will provide a random UID to containers. The flags <code>--run-as-user</code> and <code>--prevent-privilege-escalation</code> are disabled on OpenShift. It is possible to achieve a similar effect on Kubernetes systems that are not OpenShift. A leading tool is Gatekeeper. Gatekeeper similarly enforces non-root on containers at the system level.</p>"},{"location":"admin/runai-setup/config/non-root-containers/#creating-a-temporary-home-directory","title":"Creating a Temporary Home Directory","text":"<p>When containers run as a specific user, the user needs to have a pre-created home directory within the image. Otherwise, when running a shell, you will not have a home directory:</p> <pre><code>runai submit -i ubuntu --attach --interactive --run-as-user -- bash\nThe job 'job-0' has been submitted successfully\nYou can run `runai describe job job-0 -p team-a` to check the job status\nWaiting for pod to start running...\nINFO[0007] Job started\nConnecting to pod job-0-0-0\nIf you don't see a command prompt, try pressing enter.\nI have no name!@job-0-0-0:/$ \n</code></pre> <p>Adding home directories to an image per user is not a viable solution. To overcome this, Run:ai provides an additional flag <code>--create-home-dir</code>. Adding this flag creates a temporary home directory for the user within the container.  </p> <p>Notes</p> <ul> <li>Data saved in this directory will not be saved when the container exits.</li> <li>This flag is set by default to true when the <code>--run-as-user</code> flag is used, and false if not.</li> </ul>"},{"location":"admin/runai-setup/config/org-cert/","title":"None","text":"<p>In the context of Run:ai, the cluster and control-plane need to be aware of this certificate for consumers to be able to connect to the system.</p>"},{"location":"admin/runai-setup/config/org-cert/#preparation","title":"Preparation","text":"<p>You will need to have the public key of the local certificate authority. </p>"},{"location":"admin/runai-setup/config/org-cert/#control-plane-installation","title":"Control-Plane Installation","text":"<ul> <li>Create the <code>runai-backend</code> namespace if it does not exist. </li> <li> <p>Add the public key to the <code>runai-backend</code> namespace: <pre><code>kubectl -n runai-backend create secret generic runai-ca-cert \\ \n    --from-file=runai-ca.pem=&lt;ca_bundle_path&gt;\n</code></pre></p> </li> <li> <p>As part of the installation instructions you need to create a secret for runai-backend-tls. Use the local certificate authority instead.</p> </li> <li>Install the control plane, add the following flag to the helm command <code>--set global.customCA.enabled=true</code></li> </ul>"},{"location":"admin/runai-setup/config/org-cert/#cluster-installation","title":"Cluster Installation","text":"<ul> <li>Create the <code>runai</code> namespace if it does not exist. </li> <li>Add the public key to the <code>runai</code> namespace: <pre><code>kubectl -n runai create secret generic runai-ca-cert \\\n    --from-file=runai-ca.pem=&lt;ca_bundle_path&gt;\n</code></pre></li> <li>In case you're using Openshift, add the public key to the <code>openshift-monitoring</code> namespace: <pre><code>kubectl -n openshift-monitoring create secret generic runai-ca-cert \\\n    --from-file=runai-ca.pem=&lt;ca_bundle_path&gt;\n</code></pre></li> <li>Install the Run:ai operator, add the following flag to the helm command <code>--set global.customCA.enabled=true</code></li> </ul>"},{"location":"admin/runai-setup/config/overview/","title":"Run:ai Configuration Articles","text":"<p>This section provides a list of installation-related articles dealing with a wide range of subjects:</p> Article Purpose Designating Specific Role Nodes Set one or more designated Run:ai system nodes or limit Run:ai monitoring and scheduling to specific nodes in the cluster. Setup Project-based Researcher Access Control Enable  Run:ai access control is at the Project level. Single sign-on Integrate with the organization's Identity Provider to provide single sign-on for Run:ai Review Kubernetes Access provided to Run:ai In Restrictive Kubernetes environments such as when using OpenShift, understand and control what Kubernetes roles are provided to Run:ai External access to Containers Understand the available options for Researchers to access containers from the outside User Identity in Container The identity of the user in the container determines its access to cluster resources. The document explains multiple way on how to propagate the user identity into the container. Install the Run:ai Administrator Command-line Interface The Administrator command-line is useful in a variety of flows such as cluster upgrade, node setup etc."},{"location":"admin/runai-setup/maintenance/alert-monitoring/","title":"Setting up Alert Monitoring for Run:ai Using Alertmanager in Prometheus","text":""},{"location":"admin/runai-setup/maintenance/alert-monitoring/#introduction","title":"Introduction","text":"<p>This documentation outlines the steps required to set up Alertmanager within the Prometheus Operator ecosystem. It also provides guidance on configuring Prometheus to send alerts to Alertmanager and customizing Alertmanager to trigger alerts based on specific Run.ai conditions.</p>"},{"location":"admin/runai-setup/maintenance/alert-monitoring/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster with the necessary permissions and manage resources.</li> <li><code>kubectl</code> command-line tool installed and configured to interact with the cluster.</li> <li>Basic knowledge of Kubernetes resources and manifests.</li> <li>up and running Prometheus Operator</li> <li>Up and running Run.ai environment</li> </ul>"},{"location":"admin/runai-setup/maintenance/alert-monitoring/#validate-prometheus-operator-installed","title":"Validate Prometheus Operator Installed","text":"<ol> <li> <p>Verify that the Prometheus Operator deployment is running:</p> <p><code>kubectl get deployment prometheus-operator -n runai</code></p> <p>You should see output indicating the deployment's status, including the number of replicas and their current state.</p> </li> <li> <p>Check if Prometheus instances are running:</p> <p><code>kubectl get prometheus -n runai</code></p> <p>You should see the Prometheus instance(s) listed along with their status.</p> </li> </ol>"},{"location":"admin/runai-setup/maintenance/alert-monitoring/#enabling-alertmanager","title":"Enabling Alertmanager","text":"<ol> <li> <p>Create an <code>AlertmanagerConfig</code> file that triggers alerts on Run.ai events:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f \napiVersion: monitoring.coreos.com/v1alpha1\nkind: AlertmanagerConfig\nmetadata:\n   name: runai\n   namespace: runai\n   labels:\n      alertmanagerConfig: runai\nEOF\n</code></pre> </li> <li> <p>Create the Alertmanager CustomResource to enable Alertmanager:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f - \napiVersion: monitoring.coreos.com/v1\nkind: Alertmanager\nmetadata:\n   name: runai\n   namespace: runai\nspec:\n   replicas: 1\n   alertmanagerConfigSelector:\n      matchLabels:\n         alertmanagerConfig: runai\nEOF\n</code></pre> </li> <li> <p>Exposing the Alertmanager Service</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f - \napiVersion: v1\nkind: Service\nmetadata:\n   name: alertmanager-runai\n   namespace: runai\nspec:\n   type: NodePort\n   ports:\n   - name: web\n      nodePort: 30903\n      port: 9093\n      protocol: TCP\n      targetPort: web\n   selector:\n      alertmanager: runai\nEOF\n</code></pre> </li> </ol>"},{"location":"admin/runai-setup/maintenance/alert-monitoring/#configuring-prometheus-to-send-alerts","title":"Configuring Prometheus to Send Alerts","text":"<ol> <li> <p>Edit the Prometheus configuration:</p> <p><code>kubectl edit prometheus runai -n runai</code></p> </li> <li> <p>Add the following to the <code>alerting</code> section:</p> <pre><code>alerting:\n   alertmanagers:\n      - namespace: runai\n        name: alertmanager-runai\n        port: web\n</code></pre> </li> <li> <p>Save and exit the editor. The configuration will be automatically reloaded.</p> </li> </ol>"},{"location":"admin/runai-setup/maintenance/alert-monitoring/#configuring-alertmanager-for-custom-email-alerts","title":"Configuring Alertmanager for Custom Email Alerts","text":"<ol> <li> <p>Add your smtp password as a secret:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f \napiVersion: v1\nkind: Secret\nmetadata:\n   name: smtp-password\n   namespace: runai\nstringData:\n   password: \"your_smtp_password\"\nEOF\n</code></pre> </li> <li> <p>Edit the Alertmanager configuration:</p> <p><code>kubectl edit alertmanagerconfig -n runai</code></p> </li> <li> <p>Add to the <code>spec</code> section, a new receiver configuration to send alerts via email:</p> <pre><code>receivers:\n- name: 'email'\n   emailConfigs:\n   - to: 'your_email@example.com'\n      from: 'alertmanager@example.com'\n      smarthost: 'smtp.yourmailprovider.com:587'\n      authUsername: 'your_username'\n      authPassword:\n      name: smtp-password\n      key: password\n</code></pre> <p>Note</p> <p>Different receivers can be configured using Alertmanager receiver-integration-settings.</p> </li> <li> <p>Add to the <code>spec</code> section, a new route that forwards Run.ai alerts to the mail receiver:</p> <pre><code>route:\n   continue: true\n   groupBy: \n   - alertname\n     groupWait: 30s\n     groupInterval: 5m\n     repeatInterval: 1h\n\n   matchers:\n   - matchType: =~\n     name: alertname\n     value: Runai.*\n\n   receiver: email\n</code></pre> </li> <li> <p>Save and exit the editor. The configuration will be automatically reloaded.</p> </li> </ol>"},{"location":"admin/runai-setup/maintenance/alert-monitoring/#alert-messages","title":"Alert Messages","text":"<p>Alerts help you troubleshoot your system and give you a better understanding of currently occurring issues that affect performance. For more insight into the meaning of the alert messages, see Prometheus Alerts.</p>"},{"location":"admin/runai-setup/maintenance/audit-log/","title":"Audit Log","text":""},{"location":"admin/runai-setup/maintenance/audit-log/#introduction","title":"Introduction","text":"<p>The Run:ai control plane provides audit log API and audit log user interface table. Both reflect the same information:</p> <ul> <li>All changes to business objects</li> <li>All logins to the control plane.</li> </ul>"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-audit-log-user-interface","title":"Event History - Audit Log User Interface","text":"<p>The Administrators of the system can view the audit log using the user interface. The audit log screen is under the 'Event History' section:</p> <p></p>"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-audit-log-information-fields","title":"Event History (audit log) information fields","text":"<p>The Administrator can choose what information fields to view within the audit log table, this is done by clicking the 'Columns' button and checking the required fields to be presented:</p> <p></p> <p></p> <p>Here's the list of available information fields in the Event History (audit log) table:</p> Field Type Description User/App user id The identity of the User or Application that executed this operation. Data &amp; Time date The exact timestamp at which the event occured.  Format <code>dd/mm/yyyy</code> for date and <code>hh:mm am/pm</code> for time. Event event type The type of the logged operation. Possible values: <code>Create</code>, <code>Update</code>, <code>Delete</code>, <code>Login</code>. Event ID integer Sequanicialy incrmental number of the logged operation, lower number means older event, higher means newer event. Status string The outcome of the logged operation. Possible values: <code>Succeeded</code>, <code>Failed</code>. Entity type string The type of the logged business object. Possible values: <code>Project</code>, <code>Department</code>, <code>User</code>, <code>Group</code>, <code>Login</code>, <code>Settings</code>, <code>Applications</code>, <code>Node Pool</code>. Entity name string The name of logged business object. Entity ID string The system's internal id of the logged business object. Cluster Name string The name of the cluster that the loged operation relates to. If the operation is not cluster specific - cluster name remains empty. Cluster ID string The system internal identifier of the cluster that the loged operation relates to. If the operation is not cluster specific - cluster id remains empty."},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-date-selector","title":"Event History - Date Selector","text":"<p>The Event History table saves logged operations for the last 90 days. However, the table itself presents up to the last 30 days of information due to the potentially very high number of operations that might be logged during this period. To view older logged operations, or if you wish to refine your search and get more specific results or fewer results, you should use the time selector and change the period you search for. You can also refine your search by using filters as explained below.  </p> <p></p>"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-filters","title":"Event History - Filters","text":"<p>The administrator can choose to filter the table using a list of predefined filters. The filter's value is a free text keyword entered by the administrator and must be fully matched to the requested field's actual value, otherwise, the filter will not find the requested keyword. Multiple filters can be set in parallel.</p> <p></p> <p></p>"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-download-the-audit-log-file","title":"Event History - Download the Audit Log file","text":"<p>The event history table allows you to download the logged information in text form formatted as CSV or JSON files. The scope of the downloaded information is set by the scope of the table filters, i.e. if no filters or date selectors are used, the downloaded file includes the full scope of the information that the table holds - i.e. up to 30 days of logged information. To view older logged information (up to 90 days older, but no more than 30 days at a time), shorter periods, or narrower (filtered) scopes - use the date selector and filters.</p> <p></p>"},{"location":"admin/runai-setup/maintenance/audit-log/#audit-log-api","title":"Audit log API","text":"<p>Since the amount of data is not trivial, the API is based on paging in the sense that it will retrieve a specified number of items for each API call. You can get more data by using subsequent calls. </p>"},{"location":"admin/runai-setup/maintenance/audit-log/#retrieve-audit-log-data-via-api","title":"Retrieve Audit Log data via API","text":"<p>To retrieve the Audit log you need to call an API. You can do this via code or by using the Audit function via a user interface for calling APIs.</p>"},{"location":"admin/runai-setup/maintenance/audit-log/#retrieve-via-code","title":"Retrieve via Code","text":"<p>Create an Application and generate a bearer token by following the API Authentication document.  </p> <p>To get the first 40 records of the audit log starting January 1st, 2022, run:</p> <pre><code>curl -X 'GET' \\\n  'https://&lt;COMPANY-URL&gt;/v1/k8s/audit?start=2022-1-1' \\  # (1)\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer &lt;ACCESS-TOKEN&gt;' # (2)\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is <code>app.run.ai</code> for SaaS installations (not <code>&lt;company&gt;.run.ai</code>) or the Run:ai user interface URL for Self-hosted installations.</li> <li>To obtain a Bearer token see API authentication.</li> </ol> <p>Sample result:</p> <pre><code>[\n    {\n        \"id\": 3,\n        \"tenantId\": 1,\n        \"happenedAt\": \"2022-07-07T09:45:32.069Z\",\n        \"action\": \"Update\",\n        \"version\": \"1.0\",\n        \"entityId\": \"1\",\n        \"entityType\": \"Project\",\n        \"entityName\": \"team-a\",\n        \"sourceType\": \"User\",\n        \"sourceId\": \"a79500fb-c452-471f-adc0-b65c972bd5c2\",\n        \"sourceName\": \"test@run.ai\",\n        \"context\": {\n            \"user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\",\n            \"ip_address\": \"10.244.0.0\"\n        }\n    },\n    {\n        \"id\": 2,\n        \"tenantId\": 1,\n        \"happenedAt\": \"2022-07-07T08:27:39.649Z\",\n        \"action\": \"Create\",\n        \"version\": \"1.0\",\n        \"entityId\": \"fdc90aab-b183-4856-8337-14039063b876\",\n        \"entityType\": \"App\",\n        \"entityName\": \"admin\",\n        \"sourceType\": \"User\",\n        \"sourceId\": \"a79500fb-c452-471f-adc0-b65c972bd5c2\",\n        \"sourceName\": \"test@run.ai\",\n        \"context\": {\n            \"user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\",\n            \"ip_address\": \"10.244.0.0\"\n        }\n    },\n...\n]\n</code></pre>"},{"location":"admin/runai-setup/maintenance/audit-log/#paging","title":"Paging","text":"<p>Use the <code>limit</code> and <code>offset</code> properties to retrieve all audit log entries.</p>"},{"location":"admin/runai-setup/maintenance/audit-log/#additional-filter","title":"Additional filter","text":"<p>You can add additional filters to the query as follows:</p> Field Type Description start date Start date for audit logs retrieval.  Format <code>yyyy-MM-dd</code> for date or <code>yyyy-MM-ddThh:mm:ss</code> for date-time. end date End date for audit logs retrieval.  Format <code>yyyy-MM-dd</code> for date or <code>yyyy-MM-ddThh:mm:ss</code> for date-time. action string The action of the logged operation. Possible values: <code>Create</code>, <code>Update</code>, <code>Delete</code>, <code>Login</code> source_type string The initiator of the action (user or machine to machine key). Possible values: <code>User</code>, <code>Application</code> source_id string The id of the source of the action. For <code>User</code>, this is the internal user id. For an <code>Application</code>, this is the internal id of the Application source_name string The name of the source of the action. For a <code>User</code>, this is the user's email, for an <code>Application</code>, this is the Application name. entity_type string The type of business object. Possible values: <code>Project</code>, <code>Department</code>, <code>User</code>, <code>Group</code>, <code>Login</code>, <code>Settings</code>, <code>Applications</code> entity_id string The id of the business object limit integer Paging: the number of records to fetch at once (default is 40 record) offset integer Paging: The offset from which to start fetching records. success string enter true for successful audit log records and false for failures (default is all records) download string enter true to download the logs into a file <p></p>"},{"location":"admin/runai-setup/maintenance/node-downtime/","title":"Planned and Unplanned Node Downtime","text":""},{"location":"admin/runai-setup/maintenance/node-downtime/#introduction","title":"Introduction","text":"<p>Nodes (Machines) that are part of the cluster are susceptible to occasional downtime. This can be either as part of planned maintenance where we bring down the node for a specified time in an orderly fashion or an unplanned downtime where the machine abruptly stops due to a software or hardware issue.</p> <p>The purpose of this document is to provide a process for retaining the Run:ai service and Researcher workloads during and after the downtime. </p>"},{"location":"admin/runai-setup/maintenance/node-downtime/#self-hosted-installation","title":"Self-hosted installation","text":"<p>The self-hosted installation differs from the Classic (SaaS) installation of Run:ai in that it includes the Run:ai control-plane. The control plane contains data that must be preserved during downtime. As such, you must first follow the disaster recovery planning process. </p>"},{"location":"admin/runai-setup/maintenance/node-downtime/#node-types","title":"Node Types","text":"<p>The document differentiates between Run:ai System Worker Nodes and GPU Worker Nodes:</p> <ul> <li>Worker Nodes - are where Machine Learning workloads run. </li> <li>Run:ai System Nodes - In a production installation Run:ai software runs on one or more Run:ai System Nodes on which the Run:ai software runs. </li> </ul>"},{"location":"admin/runai-setup/maintenance/node-downtime/#worker-nodes","title":"Worker Nodes","text":"<p>Worker Nodes are where machine learning workloads run. Ideally, when a node is down, whether for planned maintenance or due to an abrupt downtime, these workloads should migrate to other available nodes or wait in the queue to be started when possible. </p>"},{"location":"admin/runai-setup/maintenance/node-downtime/#training-vs-interactive","title":"Training vs. Interactive","text":"<p>Run:ai differentiates between Training and Interactive workloads. The key difference at node downtime is that Training workloads will automatically move to a new node while Interactive workloads require a manual process. The manual process is recommended for Training workloads as well, as it hastens the process -- it takes time for Kubernetes to identify that a node is down.</p>"},{"location":"admin/runai-setup/maintenance/node-downtime/#planned-maintenance","title":"Planned Maintenance","text":"<p>Before stopping a Worker node, perform the following: </p> <ul> <li>Stop the Kubernetes scheduler from starting new workloads on the node and drain node from all existing workloads. Workloads will move to other nodes or await on queue for renewed execution:</li> </ul> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute\n</code></pre> <ul> <li> <p>Shut down the node and perform the required maintenance. </p> </li> <li> <p>When done, start the node and then run:</p> </li> </ul> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute-\n</code></pre>"},{"location":"admin/runai-setup/maintenance/node-downtime/#unplanned-downtime","title":"Unplanned Downtime","text":"<ul> <li> <p>If a node has failed and has immediately restarted, all services will automatically start. </p> </li> <li> <p>If a node is to remain down for some time, you will want to drain the node so that workloads will migrate to another node:</p> </li> </ul> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute\n</code></pre> <p>When the node is up again, run: </p> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute-\n</code></pre> <ul> <li>If the node is to be permanently shut down, you can remove it completely from Kubernetes. Run:</li> </ul> <pre><code>kubectl delete node &lt;node-name&gt;\n</code></pre> <p>However, if you plan to bring back the node, you will need to rejoin the node into the cluster. See Rejoin.</p>"},{"location":"admin/runai-setup/maintenance/node-downtime/#runai-system-nodes","title":"Run:ai System Nodes","text":"<p>In a production installation, Run:ai software runs on one or more Run:ai system nodes. As a best practice, it's best to have more than one such node so that during planned maintenance or unplanned downtime of a single node, the other node will take over. If a second node does not exist, you will have to designate an arbitrary node on the cluster as a Run:ai system node to complete the process below.</p> <p>Protocols for planned maintenance and unplanned downtime are identical to Worker Nodes. See the section above. </p>"},{"location":"admin/runai-setup/maintenance/node-downtime/#rejoin-a-node-into-the-kubernetes-cluster","title":"Rejoin a Node into the Kubernetes Cluster","text":"<p>To rejoin a node to the cluster follow the following steps:</p> <ul> <li>On the master node, run:</li> </ul> <p><pre><code>kubeadm token create --print-join-command\n</code></pre> * This would output a <code>kubeadm join</code> command. Run the command on the worker node for it to re-join the Kubernetes cluster.  * Verify that the node is joined by running:</p> <pre><code>kubectl get nodes\n</code></pre> <ul> <li>When the machine is up you will need to re-label nodes according to their role</li> </ul>"},{"location":"admin/runai-setup/self-hosted/overview/","title":"Self Hosted Run:ai Installation","text":"<p>The self-hosted option is for organizations that cannot use a SaaS solution due to data leakage concerns.</p> <p>Run:ai self-hosting comes with two variants:</p> Self-hosting Type Description Connected The organization can freely download from the internet (though upload is not allowed) Air-gapped The organization has no connection to the internet  <p>The self-hosted installation is priced differently. For further information please talk to Run:ai sales. </p>"},{"location":"admin/runai-setup/self-hosted/overview/#self-hosting-with-kubernetes-vs-openshift","title":"Self-hosting with Kubernetes vs OpenShift","text":"<p>Run:ai has been certified with a specified set of Kubernetes distributions. The OpenShift installation is different from the rest. As such, the Run:ai self-hosted installation instructions are divided into two separate sections:</p> <ul> <li>OpenShift-based installation. See Run:ai OpenShift installation. The Run:ai operator for OpenShift is certified by Red Hat.</li> <li>Kubernetes-based installation. See Run:ai Kubernetes installation.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/additional-clusters/","title":"Installing additional Clusters","text":"<p>The first Run:ai cluster is typically installed on the same Kubernetes cluster as the Run:ai control plane. Run:ai supports multiple clusters per single control plane. This document is about installing additional clusters on different Kubernetes clusters.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/additional-clusters/#installation","title":"Installation","text":"<p>Follow the Run:ai SaaS installation network instructions as described here.  Specifically:</p> <ol> <li>Install Run:ai prerequisites. Including ingress controller and Prometheus. </li> <li>The Cluster should have a dedicated URL with a trusted certificate.</li> <li>Create a secret in the Run:ai namespace containing the details of a trusted certificate. </li> <li>Run the <code>helm</code> command as instructed.  </li> </ol>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/","title":"Install the Run:ai Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/k8s/backend/#prerequisites-and-preperations","title":"Prerequisites and preperations","text":"<p>Make sure you have followed the Control Plane prerequisites and preperations.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#install-the-control-plane","title":"Install the Control Plane","text":"<p>Run the helm command below:</p> ConnectedAirgapped <pre><code>helm repo add runai-backend https://runai.jfrog.io/artifactory/cp-charts-prod\nhelm repo update\nhelm upgrade -i runai-backend -n runai-backend runai-backend/control-plane --version \"~2.17.0\" \\\n    --set global.domain=&lt;DOMAIN&gt;  # (1)\n</code></pre> <ol> <li>Domain name described here. </li> </ol> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-backend</code>.</p> <pre><code>helm upgrade -i runai-backend control-plane-&lt;VERSION&gt;.tgz  \\ # (1)\n    --set global.domain=&lt;DOMAIN&gt;  \\ # (2)\n    --set global.customCA.enabled=true \\  # (3)\n    -n runai-backend -f custom-env.yaml  # (4)\n</code></pre> <ol> <li>Replace <code>&lt;VERSION&gt;</code> with the Run:ai control plane version.</li> <li>Domain name described here. </li> <li>See the Local Certificate Authority instructions below</li> <li><code>custom-env.yaml</code> should have been created by the prepare installation script in the previous section. </li> </ol> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#additional-configurations-optional","title":"Additional configurations (optional)","text":"<p>There may be cases where you need to set additional properties as follows:</p> Key Change Description <code>redis.auth.password</code> Redis (Runai internal cache mechanism) applicative password Override the default password <code>keycloakx.adminUser</code> KeyCloak (Run:ai internal identity provider) administrator username Override the default user name of the Keycloak administrator user <code>keycloakx.adminPassword</code> KeyCloak (Run:ai internal identity provider) administrator password Override the default password of the Keycloak administrator user <code>global.keycloakx.host</code> KeyCloak (Run:ai internal identity provider) host path Override the DNS for Keycloak. This can be used to access Keycloak from outside the Run:ai Control Plane cluster via ingress <code>global.ingress.ingressClass</code> Ingress class Run:ai default is using NGINX. If your cluster has a different ingress controller, you can configure the ingress class to be created by Run:ai <code>global.ingress.tlsSecretName</code> TLS secret name Run:ai requires the creation of a secret with domain certificate. See above. If the <code>runai-backend</code> namespace already had such a secret, you can set the secret name here <code>global.postgresql.auth.port</code> PostgreSQL port Override the default PostgreSQL port for the Run:ai database <code>global.postgresql.auth.username</code> PostgreSQL username Override the Run:ai default user name for accessing the Run:ai database <code>global.postgresql.auth.password</code> PostgreSQL password Override the Run:ai default password for accessing the Run:ai database <code>global.postgresql.auth.postgresPassword</code> PostgreSQL default admin password Set the password of the admin user created by default by PostgreSQL <code>postgresql.primary.initdb.password</code> PostgreSQL default admin password Set the same password as in <code>global.postgresql.auth.postgresPassword</code> (if changed) <code>grafana.adminUser</code> Grafana username Override the Run:ai default user name for accessing Grafana <code>grafana.adminPassword</code> Grafana password Override the Run:ai default password for accessing Grafana <code>grafana.dbUser</code> Grafana's username for PostgreSQL Override the Run:ai default user name for Grafana to access Run:ai database (PostgreSQL) <code>grafana.dbPassword</code> Grafana's password for PostgreSQL Override the Run:ai default password for Grafana to access Run:ai database (PostgreSQL) <code>grafana.grafana.ini.database.user</code> Reference to Grafana's username for PostgreSQL Don't override this value <code>grafana.grafana.ini.database.password</code> Reference to Grafana's password for PostgreSQL Don't override this value <code>tenantsManager.config.adminUsername</code> Run:ai first admin username Override the default user name of the first admin user created with Run:ai <code>tenantsManager.config.adminPassword</code> Run:ai first admin user's password Override the default password of the first admin user created with Run:ai <code>thanos.receive.persistence.storageClass</code> and <code>postgresql.primary.persistence.storageClass</code> Storage class The installation to work with a specific storage class rather than the default one <code>&lt;component&gt;</code> <code>resources:</code> <code>limits:</code> <code>cpu: 500m</code> <code>memory: 512Mi</code> <code>requests:</code> <code>cpu: 250m</code> <code>memory: 256Mi</code> Pod request and limits <code>&lt;component&gt;</code> may be anyone of the following: <code>backend</code>, <code>frontend</code>, <code>assetsService</code>, <code>identityManager</code>, <code>tenantsManager</code>, <code>keycloakx</code>, <code>grafana</code>, <code>authorization</code>, <code>orgUnitService</code>,<code>policyService</code> <p>Use the <code>--set</code> syntax in the helm command above.  </p> <p>Note</p> <p>If you modify one of the usernames or passwords (KeyCloak, PostgreSQL, Grafana) after Run:ai is already installed, perform the following steps to apply the change:</p> <ol> <li>Modify the username/password within the relevant component as well (KeyCloak, PostgreSQL, Grafana).</li> <li>Run <code>helm upgrade</code> for Run:ai with the right values, and restart the relevant Run:ai pods so they can fetch the new username/password.</li> </ol>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#next-steps","title":"Next Steps","text":""},{"location":"admin/runai-setup/self-hosted/k8s/backend/#connect-to-runai-user-interface","title":"Connect to Run:ai User interface","text":"<p>Go to: <code>runai.&lt;domain&gt;</code>. Log in using the default credentials: User: <code>test@run.ai</code>, Password: <code>Abcd!234</code>. Go to the Users area and change the password.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#enable-forgot-password-optional","title":"Enable Forgot Password (optional)","text":"<p>To support the Forgot password functionality, follow the steps below.</p> <ul> <li>Go to <code>runai.&lt;domain&gt;/auth</code> and Log in.</li> <li>Under <code>Realm settings</code>, select the <code>Login</code> tab and enable the <code>Forgot password</code> feature.</li> <li>Under the <code>Email</code> tab, define an SMTP server, as explained here</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#install-runai-cluster","title":"Install Run:ai Cluster","text":"<p>Continue with installing a Run:ai Cluster.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/","title":"Self Hosted installation over Kubernetes - Cluster Setup","text":""},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#prerequisites","title":"Prerequisites","text":"<p>Install prerequisites as per cluster prerequisites document.  </p>"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#install-cluster","title":"Install Cluster","text":"ConnectedAirgapped <p>Perform the cluster installation instructions explained here.</p> <p>Perform the cluster installation instructions explained here.</p> <p>On the second tab of the cluster wizard, when copying the helm command for installation, you will need to use the pre-provided installation file instead of using helm repositories. As such:</p> <ul> <li>Do not add the helm repository and do not run <code>helm repo update</code>.</li> <li>Instead, edit the <code>helm upgrade</code> command. <ul> <li>Replace <code>runai/runai-cluster</code> with <code>runai-cluster-&lt;version&gt;.tgz</code>. </li> <li>Add  <code>--set global.image.registry=&lt;Docker Registry address&gt;</code> where the registry address is as entered in the preparation section</li> </ul> </li> </ul> <p>The command should look like the following:</p> <pre><code>helm upgrade -i runai-cluster runai-cluster-&lt;version&gt;.tgz \\\n    --set controlPlane.url=... \\\n    --set controlPlane.clientSecret=... \\\n    --set cluster.uid=... \\\n    --set cluster.url=... --create-namespace \\\n    --set global.image.registry=registry.mycompany.local \\\n</code></pre> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation. For more details see Understanding cluster access roles.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#optional-customize-installation","title":"(Optional) Customize Installation","text":"<p>To customize specific aspects of the cluster installation see customize cluster installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/next-steps/","title":"Next Steps","text":"<ul> <li>Create additional I Users.</li> <li>Set up Project-based Researcher Access Control.</li> <li>Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users.</li> <li>Review advanced setup and maintenace scenarios.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/","title":"Preparing for a Run:ai Kubernetes installation","text":"<p>The following section provides IT with the information needed to prepare for a Run:ai installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#prerequisites","title":"Prerequisites","text":"<p>Follow the prerequisites as explained in Self-Hosted installation over Kubernetes.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#software-artifacts","title":"Software artifacts","text":"ConnectedAirgapped <p>You should receive a file: <code>runai-gcr-secret.yaml</code> from Run:ai Customer Support. The file provides access to the Run:ai Container registry.</p> <p>SSH into a node with <code>kubectl</code> access to the cluster and <code>Docker</code> installed. Run the following to enable image download from the Run:ai Container Registry on Google cloud:</p> <pre><code>kubectl create namespace runai-backend\nkubectl apply -f runai-reg-creds.yaml\n</code></pre> <p>You should receive a single file <code>runai-air-gapped-&lt;VERSION&gt;.tar.gz</code> from Run:ai customer support</p> <p>SSH into a node with <code>kubectl</code> access to the cluster and <code>Docker</code> installed.</p> <p>Run:ai assumes the existence of a Docker registry for images. Most likely installed within the organization. The installation requires the network address and port for the registry (referenced below as <code>&lt;REGISTRY_URL&gt;</code>). </p> <p>To extract Run:ai files, replace <code>&lt;VERSION&gt;</code> in the command below and run: </p> <pre><code>tar xvf runai-airgapped-package-&lt;VERSION&gt;.tar.gz\n\nkubectl create namespace runai-backend\n</code></pre> <p>Upload images</p> <p>Upload images to a local Docker Registry. Set the Docker Registry address in the form of <code>NAME:PORT</code> (do not add <code>https</code>):</p> <pre><code>export REGISTRY_URL=&lt;Docker Registry address&gt;\n</code></pre> <p>Run the following script (you must dockerd installed and at least 20GB of free disk space to run): </p> <pre><code>sudo -E ./prepare_installation.sh\n</code></pre> <p>If Docker is configured to run as non-root then <code>sudo</code> is not required.</p> <p>The script should create a file named <code>custom-env.yaml</code> which will be used by the control-plane installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#private-docker-registry-optional","title":"Private Docker Registry (optional)","text":"<p>To access the organization's docker registry it is required to set the registry's credentials (imagePullSecret)</p> <p>Create the secret named <code>runai-reg-creds</code> based on your existing credentials. For more information, see Pull an Image from a Private Registry.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#configure-your-environment","title":"Configure your environment","text":""},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#domain-certificate","title":"Domain Certificate","text":"<p>The Run:ai control plane requires a domain name (FQDN). You must supply a domain name as well as a trusted certificate for that domain.</p> <ul> <li>When installing the first Run:ai cluster on the same Kubernetes cluster as the control plane, the Run:ai cluster URL will be the same as the control-plane URL.</li> <li>When installing the Run:ai cluster on a separate Kubernetes cluster, follow the Run:ai domain name requirements.</li> <li>If your network is air-gapped, you will need to provide the Run:ai control-plane and cluster with information about the local certificate authority.</li> </ul> <p>You must provide the domain's private key and crt as a Kubernetes secret in the <code>runai-backend</code> namespace. Run:</p> <pre><code>kubectl create secret tls runai-backend-tls -n runai-backend \\\n    --cert /path/to/fullchain.pem --key /path/to/private.pem\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#local-certificate-authority-air-gapped-only","title":"Local Certificate Authority (air-gapped only)","text":"<p>In air-gapped environments, you must prepare the public key of your local certificate authority as described here. It will need to be installed in Kubernetes for the installation to succeed.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#mark-runai-system-workers-optional","title":"Mark Run:ai system workers (optional)","text":"<p>You can optionally set the Run:ai control plane to run on specific nodes. Kubernetes will attempt to schedule Run:ai pods to these nodes. If lacking resources, the Run:ai nodes will move to another, non-labeled node.  </p> <p>To set system worker nodes run:</p> <pre><code>kubectl label node &lt;NODE-NAME&gt; node-role.kubernetes.io/runai-system=true\n</code></pre> <p>Warning</p> <p>Do not select the Kubernetes master as a <code>runai-system</code> node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#additional-permissions","title":"Additional permissions","text":"<p>As part of the installation, you will be required to install the Run:ai Control Plane and Cluster Helm Charts. The Helm Charts require Kubernetes administrator permissions. You can review the exact permissions provided by using the <code>--dry-run</code> on both helm charts.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#validate-prerequisites","title":"Validate Prerequisites","text":"<p>Once you believe that the Run:ai prerequisites and preperations are met, we highly recommend installing and running the Run:ai pre-install diagnostics script. The tool:</p> <ul> <li>Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking.</li> <li>Looks at additional components installed and analyze their relevance to a successful Run:ai installation.</li> </ul> <p>To use the script download the latest version of the script and run:</p> <pre><code>chmod +x preinstall-diagnostics-&lt;platform&gt;\n./preinstall-diagnostics-&lt;platform&gt; --domain &lt;dns-entry&gt;\n</code></pre> <p>If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file <code>runai-preinstall-diagnostics.txt</code> in the current directory and send it to Run:ai technical support.</p> <p>For more information on the script including additional command-line flags, see here.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#next-steps","title":"Next steps","text":"<p>Continue with installing the Run:ai Control Plane.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/","title":"Self-Hosted installation over Kubernetes - Prerequisites","text":"<p>Before proceeding with this document, please review the installation types documentation to understand the difference between air-gapped and connected installations. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#runai-components","title":"Run:ai Components","text":"<p>As part of the installation process you will install:</p> <ul> <li>A control-plane managing cluster</li> <li>One or more clusters</li> </ul> <p>Both the control plane and clusters require Kubernetes. Typically the control plane and first cluster are installed on the same Kubernetes cluster but this is not a must. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#installer-machine","title":"Installer machine","text":"<p>The machine running the installation script (typically the Kubernetes master) must have:</p> <ul> <li>At least 50GB of free space.</li> <li>Docker installed.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#helm","title":"Helm","text":"<p>Run:ai requires Helm 3.14 or later. To install Helm, see Installing Helm. If you are installing an air-gapped version of Run:ai, The Run:ai tar file contains the helm binary. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#cluster-hardware-requirements","title":"Cluster hardware requirements","text":"<p>See Cluster prerequisites hardware requirements.</p> <p>In addition, the control plane installation of Run:ai requires the configuration of Kubernetes Persistent Volumes of a total size of 110GB. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#runai-software-requirements","title":"Run:ai software requirements","text":""},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#operating-system","title":"Operating system","text":"<p>See Run:ai Cluster prerequisites operating system requirements.</p> <p>The Run:ai control plane operating system prerequisites are identical.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#kubernetes","title":"Kubernetes","text":"<p>See Run:ai Cluster prerequisites Kubernetes requirements.</p> <p>The Run:ai control plane operating system prerequisites are identical.</p> <p>The Run:ai control-plane requires a default storage class to create persistent volume claims for Run:ai storage. The storage class, as per Kubernetes standards, controls the reclaim behavior: whether the Run:ai persistent data is saved or deleted when the Run:ai control plane is deleted. </p> <p>Note</p> <p>For a simple (nonproduction) storage class example see Kubernetes Local Storage Class. The storage class will set the directory <code>/opt/local-path-provisioner</code> to be used across all nodes as the path for provisioning persistent volumes.</p> <p>Then set the new storage class as default:</p> <pre><code>kubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#install-prerequisites","title":"Install prerequisites","text":""},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#ingress-controller","title":"Ingress Controller","text":"<p>The Run:ai control plane installation assumes an existing installation of NGINX as the ingress controller. You can follow the Run:ai Cluster prerequisites ingress controller installation.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#nvidia-gpu-operator","title":"NVIDIA GPU Operator","text":"<p>See Run:ai Cluster prerequisites NVIDIA requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the NVIDIA prerequisites.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#prometheus","title":"Prometheus","text":"<p>See Run:ai Cluster prerequisites Prometheus requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the Prometheus prerequisites. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#inference-optional","title":"Inference (optional)","text":"<p>See Run:ai Cluster prerequisites Inference requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the Inference prerequisites. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#next-steps","title":"Next steps","text":"<p>Continue to Preparing for a Run:ai Kubernetes Installation .</p>"},{"location":"admin/runai-setup/self-hosted/k8s/project-management/","title":"Self Hosted installation over Kubernetes - Create Projects","text":""},{"location":"admin/runai-setup/self-hosted/k8s/project-management/#introduction","title":"Introduction","text":"<p>The Administrator creates Run:ai Projects via the Run:ai user interface. When enabling Researcher Authentication you also assign users to Projects.</p> <p>Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically:</p> <ol> <li>Creates a namespace by the name of <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> <li>Labels the namespace as managed by Run:ai.</li> <li>Provides access to the namespace for Run:ai services.</li> <li>Associates users with the namespace.</li> </ol> <p>This process may need to be altered if,</p> <ul> <li>Researchers already have existing Kubernetes namespaces</li> <li>The organization's Kubernetes namespace naming convention does not allow the <code>runai-</code> prefix.</li> <li>The organization's policy does not allow the automatic creation of namespaces.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/project-management/#process","title":"Process","text":"<p>Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace:</p> <ul> <li>When setting up a Run:ai cluster, Disable namespace creation by setting the cluster flag <code>createNamespaces</code> to <code>false</code>.</li> <li>Using the Run:ai User Interface, create a new Project <code>&lt;PROJECT-NAME&gt;</code>. A namespace will not be created.</li> <li>Associate and existing namepace <code>&lt;NAMESPACE&gt;</code> with the Run:ai project by running:</li> </ul> <pre><code>kubectl label ns &lt;NAMESPACE&gt;  runai/queue=&lt;PROJECT_NAME&gt;\n</code></pre> <p>Caution</p> <p>Setting the <code>createNamespaces</code> flag to <code>false</code> moves the responsibility of creating namespaces to match Run:ai Projects to the administrator.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/","title":"Uninstall Run:ai","text":""},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/#uninstall-a-runai-cluster","title":"Uninstall a Run:ai Cluster","text":"<p>To uninstall the cluster see: cluster delete </p>"},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/#uninstall-the-runai-control-plane","title":"Uninstall the Run:ai Control Plane","text":"<p>To delete the control plane, run:</p> <pre><code>helm uninstall runai-backend -n runai-backend\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/","title":"Upgrade Run:ai","text":"<p>Important</p> <p>Run:ai data is stored in Kubernetes persistent volumes (PVs). Prior to Run:ai 2.12, PVs are owned by the Run:ai installation. Thus, uninstalling the <code>runai-backend</code> helm chart may delete all of your data. </p> <p>From version 2.12 forward, PVs are owned the customer and are independent of the Run:ai installation. As such, they are subject to storage class reclaim policy.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#preparations","title":"Preparations","text":""},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#helm","title":"Helm","text":"<p>Run:ai requires Helm 3.14 or later. Before you continue, validate your installed helm client version. To install or upgrade Helm, see Installing Helm. If you are installing an air-gapped version of Run:ai, The Run:ai tar file contains the helm binary. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#software-files","title":"Software files","text":"ConnectedAirgapped <p>Run the helm command below:</p> <pre><code>helm repo add runai-backend https://runai.jfrog.io/artifactory/cp-charts-prod\nhelm repo update\n</code></pre> <ul> <li>Ask for a tar file <code>runai-air-gapped-&lt;NEW-VERSION&gt;.tar.gz</code> from Run:ai customer support. The file contains the new version you want to upgrade to. <code>&lt;NEW-VERSION&gt;</code> is the updated version of the Run:ai control plane.</li> <li>Upload the images as described here.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#before-upgrade","title":"Before upgrade","text":"<p>Before proceeding with the upgrade, it's crucial to apply the specific prerequisites associated with your current version of Run:ai and every version in between up to the version you are upgrading to.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-from-version-27-or-28","title":"Upgrade from version 2.7 or 2.8","text":"<p>Before upgrading the control plane, run: </p> <pre><code>POSTGRES_PV=$(kubectl get pvc pvc-postgresql -n runai-backend -o jsonpath='{.spec.volumeName}')\nTHANOS_PV=$(kubectl get pvc pvc-thanos-receive -n runai-backend -o jsonpath='{.spec.volumeName}')\nkubectl patch pv $POSTGRES_PV $THANOS_PV -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}'\n\nkubectl delete secret -n runai-backend runai-backend-postgresql\nkubectl delete sts -n runai-backend keycloak runai-backend-postgresql\n</code></pre> <p>Before version 2.9, the Run:ai installation, by default, included NGINX. It was possible to disable this installation. If NGINX is enabled in your current installation, as per the default, run the following 2 lines:</p> <p><pre><code>kubectl delete ValidatingWebhookConfiguration runai-backend-nginx-ingress-admission\nkubectl delete ingressclass nginx \n</code></pre> (If Run:ai configuration has previously disabled NGINX installation then these lines should not be run).</p> <p>Next, install NGINX as described here</p> <p>Then create a TLS secret and upgrade the control plane as described in the control plane installation. Before upgrading, find customizations and merge them as discussed below. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-from-version-29-210-or-211","title":"Upgrade from version 2.9, 2.10 , or 2.11","text":"<p>Two significant changes to the control-plane installation have happened with version 2.12: PVC ownership and installation customization. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#pvc-ownership","title":"PVC ownership","text":"<p>Run:ai will no longer directly create the PVCs that store Run:ai data (metrics and database). Instead, going forward, </p> <ul> <li>Run:ai requires a Kubernetes storage class to be installed.</li> <li>The PVCs are created by the Kubernetes StatefulSets. </li> </ul> <p>The storage class, as per Kubernetes standards, controls the reclaim behavior: whether the data is saved or deleted when the Run:ai control plane is deleted.  </p> <p>To remove the ownership in an older installation, run:</p> <pre><code>kubectl patch pvc -n runai-backend pvc-thanos-receive  -p '{\"metadata\": {\"annotations\":{\"helm.sh/resource-policy\": \"keep\"}}}'\nkubectl patch pvc -n runai-backend pvc-postgresql  -p '{\"metadata\": {\"annotations\":{\"helm.sh/resource-policy\": \"keep\"}}}'\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#ingress","title":"Ingress","text":"<p>Delete the ingress object which will be recreated by the control plane upgrade</p> <pre><code>kubectl delete ing -n runai-backend runai-backend-ingress\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#installation-customization","title":"Installation customization","text":"<p>The Run:ai control-plane installation has been rewritten and is no longer using a backend values file. Instead, to customize the installation use standard <code>--set</code> flags. If you have previously customized the installation, you must now extract these customizations and add them as <code>--set</code> flag to the helm installation:</p> <ul> <li>Find previous customizations to the control plane if such exist. Run:ai provides a utility for that here <code>https://raw.githubusercontent.com/run-ai/docs/v2.13/install/backend/cp-helm-vals-diff.sh</code>. For information on how to use this utility please contact Run:ai customer support. </li> <li>Search for the customizations you found in the optional configurations table and add them in the new format. </li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-control-plane","title":"Upgrade Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-from-version-213-or-later","title":"Upgrade from version 2.13, or later","text":"ConnectedAirgapped <pre><code>helm get values runai-backend -n runai-backend &gt; runai_control_plane_values.yaml\nhelm upgrade runai-backend -n runai-backend runai-backend/control-plane --version \"~2.17.0\" -f runai_control_plane_values.yaml --reset-then-reuse-values\n</code></pre> <pre><code>helm get values runai-backend -n runai-backend &gt; runai_control_plane_values.yaml\nhelm upgrade runai-backend control-plane-&lt;NEW-VERSION&gt;.tgz -n runai-backend  -f runai_control_plane_values.yaml --reset-then-reuse-values\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-from-version-27-28-29-or-211","title":"Upgrade from version 2.7, 2.8, 2.9, or 2.11","text":"<ul> <li>Create a <code>tls secret</code> as described in the control plane installation. </li> <li>Upgrade the control plane as described in the control plane installation. During the upgrade, you must tell the installation not to create the two PVCs:</li> </ul> ConnectedAirgapped <pre><code>helm upgrade -i runai-backend -n runai-backend runai-backend/control-plane --version \"~2.17.0\" \\\n--set global.domain=&lt;DOMAIN&gt; \\\n--set postgresql.primary.persistence.existingClaim=pvc-postgresql \\ \n--set thanos.receive.persistence.existingClaim=pvc-thanos-receive \n</code></pre> <p>Note</p> <p>The helm repository name has changed from <code>runai-backend/runai-backend</code> to <code>runai-backend/control-plane</code>.</p> <pre><code>helm upgrade -i runai-backend control-plane-&lt;NEW-VERSION&gt;.tgz -n runai-backend \\\n--set global.domain=&lt;DOMAIN&gt; \\\n--set postgresql.primary.persistence.existingClaim=pvc-postgresql \\ \n--set thanos.receive.persistence.existingClaim=pvc-thanos-receive \n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-cluster","title":"Upgrade Cluster","text":"<p>To upgrade the cluster follow the instructions here.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/additional-clusters/","title":"Installing additional Clusters","text":"<p>The first Run:ai cluster is typically installed on the same OpenShift cluster as the Run:ai control plane. Run:ai supports multiple clusters per single control plane. This document is about installing additional clusters on different OpenShift clusters.</p> <p>The instructions are for Run:ai version 2.13 and up.</p> <p>Limitation</p> <p>When you log in, you do so in the context of a specific cluster. When you switch to a different cluster, you will be prompted to log in to that cluster. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/additional-clusters/#configuration","title":"Configuration","text":"<p>The exact configuration details must be worked together with Run:ai customer support. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/additional-clusters/#additional-cluster-installation","title":"Additional Cluster Installation","text":"<p>Create a new cluster, then:</p> <ul> <li>Select a target platform <code>OpenShift</code> </li> <li>Select a Cluster location <code>Remote to Control Plane</code>.</li> <li>You must enter a specific cluster URL with the format <code>https://runai.apps.&lt;BASE_DOMAIN&gt;</code>. To get the base Domain run <code>oc get dns cluster -oyaml | grep baseDomain</code></li> <li>Ignore the instructions for creating a secret.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/additional-clusters/#login","title":"Login","text":"<p>When configured, you will see an option to choose a cluster at the bottom of the login screen:</p> <p></p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/","title":"Install the Run:ai Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/ocp/backend/#prerequisites-and-preperations","title":"Prerequisites and preperations","text":"<p>Make sure you have followed the Control Plane prerequisites and preperations.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#install-the-control-plane","title":"Install the Control Plane","text":"<p>Run the helm command below:</p> ConnectedAirgapped <pre><code>helm repo add runai-backend https://runai.jfrog.io/artifactory/cp-charts-prod\nhelm repo update\nhelm upgrade -i runai-backend -n runai-backend runai-backend/control-plane --version \"~2.17.0\" \\\n    --set global.domain=runai.apps.&lt;OPENSHIFT-CLUSTER-DOMAIN&gt; \\ # (1)\n    --set global.config.kubernetesDistribution=openshift\n</code></pre> <ol> <li>The subdomain configured for the OpenShift cluster.</li> </ol> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-backend</code>.</p> <pre><code>helm upgrade -i runai-backend  ./control-plane-&lt;version&gt;.tgz -n runai-backend \\\n    --set global.domain=runai.apps.&lt;OPENSHIFT-CLUSTER-DOMAIN&gt; \\ # (1)\n    --set global.config.kubernetesDistribution=openshift \\\n    --set global.customCA.enabled=true \\ # (2)\n    -f custom-env.yaml  # (3)\n</code></pre> <ol> <li>The domain configured for the OpenShift cluster. To find out the OpenShift cluster domain, run <code>oc get routes -A</code></li> <li>See the Local Certificate Authority instructions below</li> <li><code>custom-env.yaml</code> should have been created by the prepare installation script in the previous section. </li> </ol> <p>(replace <code>&lt;version&gt;</code> with the control plane version)</p> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#additional-configurations-optional","title":"Additional configurations (optional)","text":"<p>There may be cases where you need to set additional properties as follows:</p> Key Change Description <code>keycloakx.adminUser</code> KeyCloak (Run:ai internal identity provider) administrator username Override the default user name of the Keycloak administrator user <code>keycloakx.adminPassword</code> KeyCloak (Run:ai internal identity provider) administrator password Override the default password of the Keycloak administrator user <code>global.keycloakx.host</code> KeyCloak (Run:ai internal identity provider) host path Override the DNS for Keycloak. This can be used to access Keycloak from outside the Run:ai Control Plane cluster via ingress <code>global.postgresql.auth.port</code> PostgreSQL port Override the default PostgreSQL port for the Run:ai database <code>global.postgresql.auth.username</code> PostgreSQL username Override the Run:ai default user name for accessing the Run:ai database <code>global.postgresql.auth.password</code> PostgreSQL password Override the Run:ai default password for accessing the Run:ai database <code>global.postgresql.auth.postgresPassword</code> PostgreSQL default admin password Set the password of the admin user created by default by PostgreSQL <code>postgresql.primary.initdb.password</code> PostgreSQL default admin password Set the same password as in <code>global.postgresql.auth.postgresPassword</code> (if changed) <code>grafana.adminUser</code> Grafana username Override the Run:ai default user name for accessing Grafana <code>grafana.adminPassword</code> Grafana password Override the Run:ai default password for accessing Grafana <code>grafana.dbUser</code> Grafana's username for PostgreSQL Override the Run:ai default user name for Grafana to access Run:ai database (PostgreSQL) <code>grafana.dbPassword</code> Grafana's password for PostgreSQL Override the Run:ai default password for Grafana to access Run:ai database (PostgreSQL) <code>grafana.grafana.ini.database.user</code> Reference to Grafana's username for PostgreSQL Don't override this value <code>grafana.grafana.ini.database.password</code> Reference to Grafana's password for PostgreSQL Don't override this value <code>tenantsManager.config.adminUsername</code> Run:ai first admin username Override the default user name of the first admin user created with Run:ai <code>tenantsManager.config.adminPassword</code> Run:ai first admin user's password Override the default password of the first admin user created with Run:ai <code>thanos.receive.persistence.storageClass</code> and <code>postgresql.primary.persistence.storageClass</code> Storage class The installation to work with a specific storage class rather than the default one <code>&lt;component&gt;</code> <code>resources:</code> <code>limits:</code> <code>cpu: 500m</code> <code>memory: 512Mi</code> <code>requests:</code> <code>cpu: 250m</code> <code>memory: 256Mi</code> Pod request and limits <code>&lt;component&gt;</code> may be anyone of the following: <code>backend</code>, <code>frontend</code>, <code>assetsService</code>, <code>identityManager</code>, <code>tenantsManager</code>, <code>keycloakx</code>, <code>grafana</code>, <code>authorization</code>, <code>orgUnitService</code>,<code>policyService</code> <p>Use the <code>--set</code> syntax in the helm command above.</p> <p>Note</p> <p>If you modify one of the usernames or passwords (KeyCloak, PostgreSQL, Grafana) after Run:ai is already installed, perform the following steps to apply the change:</p> <ol> <li>Modify the username/password within the relevant component as well (KeyCloak, PostgreSQL, Grafana).</li> <li>Run <code>helm upgrade</code> for Run:ai with the right values, and restart the relevant Run:ai pods so they can fetch the new username/password.</li> </ol>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#next-steps","title":"Next steps","text":""},{"location":"admin/runai-setup/self-hosted/ocp/backend/#connect-to-runai-user-interface","title":"Connect to Run:ai user interface","text":"<ul> <li>Run: <code>oc get routes -n runai-backend</code> to find the Run:ai Administration User Interface URL.</li> <li>Log in using the default credentials: User: <code>test@run.ai</code>, Password: <code>Abcd!234</code>.</li> <li>Go to the Users area and change the password.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#enable-forgot-password-optional","title":"Enable Forgot Password (optional)","text":"<p>To support the Forgot password functionality, follow the steps below.</p> <ul> <li>Go to <code>runai.&lt;openshift-cluster-domain&gt;/auth</code> and Log in.</li> <li>Under <code>Realm settings</code>, select the <code>Login</code> tab and enable the <code>Forgot password</code> feature.</li> <li>Under the <code>Email</code> tab, define an SMTP server, as explained here</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#install-runai-cluster","title":"Install Run:ai Cluster","text":"<p>Continue with installing a Run:ai Cluster.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/","title":"Self-Hosted installation over OpenShift - Cluster Setup","text":""},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#prerequisites","title":"Prerequisites","text":"<p>Before installing Run:ai, you must install NVIDIA software on your OpenShift cluster to enable GPUs. NVIDIA has provided detailed documentation. Follow the instructions to install the two operators <code>Node Feature Discovery</code> and <code>NVIDIA GPU Operator</code> from the OpenShift web console.</p> <p>When done, verify that the GPU Operator is installed by running:</p> <pre><code>oc get pods -n nvidia-gpu-operator\n</code></pre> <p>(the GPU Operator namespace may differ in different operator versions).</p> <p>Note</p> <p>You must have Cluster Administrator rights to install these dependencies.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#create-openshift-projects","title":"Create OpenShift Projects","text":"<p>Run:ai cluster installation uses several namespaces (or projects in OpenShift terminology). Run the following:</p> <pre><code>oc new-project runai\noc new-project runai-reservation\noc new-project runai-scale-adjust\n</code></pre> <p>The last namespace (<code>runai-scale-adjust</code>) is only required if the cluster is a cloud cluster and is configured for auto-scaling.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#cluster-installation","title":"Cluster Installation","text":"ConnectedAirgapped <p>Perform the cluster installation instructions explained here. When creating a new cluster, select the OpenShift  target platform.</p> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-cluster</code>.</p> <p>Perform the cluster installation instructions explained here. When creating a new cluster, select the OpenShift  target platform.</p> <p>On the second tab of the cluster wizard, when copying the helm command for installation, you will need to use the pre-provided installation file instead of using helm repositories. As such:</p> <ul> <li>Do not add the helm repository and do not run <code>helm repo update</code>.</li> <li>Instead, edit the <code>helm upgrade</code> command. <ul> <li>Replace <code>runai/runai-cluster</code> with <code>runai-cluster-&lt;version&gt;.tgz</code>. </li> <li>Add  <code>--set global.image.registry=&lt;Docker Registry address&gt;</code> where the registry address is as entered in the preparation section</li> <li>Add <code>--set global.customCA.enabled=true</code> and perform the instructions for local certificate authority.</li> </ul> </li> </ul> <p>The command should look like the following: <pre><code>helm upgrade -i runai-cluster runai-cluster-&lt;version&gt;.tgz \\\n    --set controlPlane.url=... \\\n    --set controlPlane.clientSecret=... \\\n    --set cluster.uid=... \\\n    --set cluster.url=... --create-namespace \\\n    --set global.image.registry=registry.mycompany.local \\\n    --set global.customCA.enabled=true\n</code></pre></p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#optional-customize-installation","title":"(Optional) Customize Installation","text":"<p>To customize specific aspects of the cluster installation see customize cluster installation.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#next-steps","title":"Next Steps","text":"<p>Continue to create Run:ai Projects.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/next-steps/","title":"Next Steps","text":"<ul> <li>Create additional Run:ai Users.</li> <li>Set up Project-based Researcher Access Control.</li> <li>Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users.</li> <li>Review advanced setup and maintenace scenarios.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/","title":"Preparing for a Run:ai OpenShift installation","text":"<p>The following section provides IT with the information needed to prepare for a Run:ai installation.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#prerequisites","title":"Prerequisites","text":"<p>See the Prerequisites section above.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#software-artifacts","title":"Software artifacts","text":"ConnectedAirgapped <p>You should receive a file: <code>runai-reg-creds.yaml</code> from Run:ai Customer Support. The file provides access to the Run:ai Container registry.</p> <p>SSH into a node with <code>oc</code> access (<code>oc</code> is the OpenShift command line) to the cluster and <code>Docker</code> installed.</p> <p>Run the following to enable image download from the Run:ai Container Registry on Google cloud:</p> <pre><code>oc apply -f runai-gcr-secret.yaml -n runai-backend\n</code></pre> <p>You should receive a single file <code>runai-&lt;version&gt;.tar</code> from Run:ai customer support</p> <p>Run:ai assumes the existence of a Docker registry for images. Most likely installed within the organization. The installation requires the network address and port for the registry (referenced below as <code>&lt;REGISTRY_URL&gt;</code>). </p> <p>SSH into a node with <code>oc</code> access (<code>oc</code> is the OpenShift command line) to the cluster and <code>Docker</code> installed.</p> <p>To extract Run:ai files, replace <code>&lt;VERSION&gt;</code> in the command below and run: </p> <p><pre><code>tar xvf runai-airgapped-package-&lt;VERSION&gt;.tar.gz\n</code></pre> Upload images</p> <p>Upload images to a local Docker Registry. Set the Docker Registry address in the form of <code>NAME:PORT</code> (do not add <code>https</code>):</p> <pre><code>export REGISTRY_URL=&lt;Docker Registry address&gt;\n</code></pre> <p>Run the following script (you must have at least 20GB of free disk space to run): </p> <pre><code>./setup.sh\n</code></pre> <p>(If docker is configured to run as non-root then <code>sudo</code> is not required).</p> <p>The script should create a file named custom-env.yaml which will be used by the control-plane installation.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#private-docker-registry-optional","title":"Private Docker Registry (optional)","text":"<p>To access the organization's docker registry it is required to set the registry's credentials (imagePullSecret)</p> <p>Create the secret named <code>runai-reg-creds</code> in the <code>runai-backend</code> namespace based on your existing credentials. The configuration will be copied over to the <code>runai</code> namespace at cluster install. For more information, see Allowing pods to reference images from other secured registries.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#configure-your-environment","title":"Configure your environment","text":""},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#create-openshift-project","title":"Create OpenShift project","text":"<p>The Run:ai control plane uses a namespace (or project in OpenShift terminology) name <code>runai-backend</code>. You must create it before installing:</p> <pre><code>oc new-project runai-backend\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#local-certificate-authority-air-gapped-only","title":"Local Certificate Authority (air-gapped only)","text":"<p>In Air-gapped environments, you must prepare the public key of your local certificate authority as described here. It will need to be installed in Kubernetes for the installation to succeed.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#mark-runai-system-workers-optional","title":"Mark Run:ai system workers (optional)","text":"<p>You can optionally set the Run:ai control plane to run on specific nodes. Kubernetes will attempt to schedule Run:ai pods to these nodes. If lacking resources, the Run:ai nodes will move to another, non-labeled node.  </p> <p>To set system worker nodes run:</p> <pre><code>kubectl label node &lt;NODE-NAME&gt; node-role.kubernetes.io/runai-system=true\n</code></pre> <p>Warning</p> <p>Do not select the Kubernetes master as a <code>runai-system</code> node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#additional-permissions","title":"Additional permissions","text":"<p>As part of the installation, you will be required to install the Control plane and Cluster Helm Charts. The Helm Charts require Kubernetes administrator permissions. You can review the exact permissions provided by using the <code>--dry-run</code> on both helm charts.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#validate-prerequisites","title":"Validate prerequisites","text":"<p>Once you believe that the Run:ai prerequisites and preperations are met, we highly recommend installing and running the Run:ai pre-install diagnostics script. The tool:</p> <ul> <li>Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking.</li> <li>Looks at additional components installed and analyzes their relevancy to a successful Run:ai installation.</li> </ul> <p>To use the script download the latest version of the script and run:</p> <pre><code>chmod +x preinstall-diagnostics-&lt;platform&gt;\n./preinstall-diagnostics-&lt;platform&gt; \n</code></pre> <p>If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file <code>runai-preinstall-diagnostics.txt</code> in the current directory and send it to Run:ai technical support.</p> <p>For more information on the script including additional command-line flags, see here.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#next-steps","title":"Next steps","text":"<p>Continue with installing the Run:ai Control Plane.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/","title":"Self Hosted installation over OpenShift - prerequisites","text":"<p>Before proceeding with this document, please review the installation types documentation to understand the difference between air-gapped and connected installations. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#runai-components","title":"Run:ai components","text":"<p>As part of the installation process you will install:</p> <ul> <li>A control-plane managing cluster</li> <li>One or more clusters</li> </ul> <p>Both the control plane and clusters require Kubernetes. Typically the control plane and first cluster are installed on the same Kubernetes cluster but this is not a must. </p> <p>Important</p> <p>In OpenShift environments, adding a cluster connecting to a remote control plane currently requires the assistance of customer support. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#installer-machine","title":"Installer machine","text":"<p>The machine running the installation script (typically the Kubernetes master) must have:</p> <ul> <li>At least 50GB of free space.</li> <li>Docker installed. </li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#helm","title":"Helm","text":"<p>Run:ai requires Helm 3.14 or later. To install Helm, see Installing Helm. If you are installing an air-gapped version of Run:ai, The Run:ai tar file contains the helm binary. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#cluster-hardware-requirements","title":"Cluster hardware requirements","text":"<p>See Cluster prerequisites hardware requirements.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#runai-software-requirements","title":"Run:ai software requirements","text":""},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#operating-system","title":"Operating System","text":"<p>OpenShift has specific operating system requirements that can be found in the RedHat documentation. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#openshift","title":"OpenShift","text":"<p>Run:ai supports OpenShift. OpenShift Versions supported are detailed here.</p> <ul> <li>OpenShift must be configured with a trusted certificate. Run:ai installation relies on OpenShift to create certificates for subdomains. </li> <li>OpenShift must have a configured identity provider (Idp). </li> <li>If your network is air-gapped, you will need to provide the Run:ai control-plane and cluster with information about the local certificate authority.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#install-prerequisites","title":"Install prerequisites","text":""},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#nvidia-gpu-operator","title":"NVIDIA GPU Operator","text":"<p>See Run:ai Cluster prerequisites installing NVIDIA dependencies in OpenShift.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the NVIDIA prerequisites.</p> <p>Information on how to download the GPU Operator for air-gapped installation can be found in the NVIDIA GPU Operator pre-requisites. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#inference-optional","title":"Inference (optional)","text":"<p>See Run:ai Cluster prerequisites Inference requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the Inference prerequisites. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#next-steps","title":"Next steps","text":"<p>Continue to Preparing for a Run:ai OpenShift Installation .</p>"},{"location":"admin/runai-setup/self-hosted/ocp/project-management/","title":"Self Hosted installation over OpenShift - Create Projects","text":""},{"location":"admin/runai-setup/self-hosted/ocp/project-management/#introduction","title":"Introduction","text":"<p>The Administrator creates Run:ai Projects via the Run:ai User Interface. When enabling Researcher Authentication you also assign users to Projects.</p> <p>Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically:</p> <ol> <li>Creates a namespace by the name of <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> <li>Labels the namespace as managed by Run:ai.</li> <li>Provides access to the namespace for Run:ai services.</li> <li>Associates users with the namespace.</li> </ol> <p>This process may need to be altered if,</p> <ul> <li>Researchers already have existing Kubernetes namespaces</li> <li>The organization's Kubernetes namespace naming convention does not allow the <code>runai-</code> prefix.</li> <li>The organization's policy does not allow the automatic creation of namespaces</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/project-management/#process","title":"Process","text":"<p>Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace:</p> <ul> <li>When setting up a Run:ai cluster, Disable namespace creation by setting the cluster flag <code>createNamespaces</code> to <code>false</code>.</li> <li>Using the Run:ai User Interface, create a new Project <code>&lt;PROJECT-NAME&gt;</code>. A namespace will not be created.</li> <li>Associate and existing namepace <code>&lt;NAMESPACE&gt;</code> with the Run:ai project by running:</li> </ul> <pre><code>oc label ns &lt;NAMESPACE&gt;  runai/queue=&lt;PROJECT_NAME&gt;\n</code></pre> <p>Caution</p> <p>Setting the <code>createNamespaces</code> flag to <code>false</code> moves the responsibility of creating namespaces to match Run:ai Projects to the administrator.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/uninstall/","title":"Uninstall Run:ai","text":"<p>See uninstall section here</p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/","title":"Upgrade Run:ai","text":"<p>Important</p> <p>Run:ai data is stored in Kubernetes persistent volumes (PVs). Prior to Run:ai 2.12, PVs are owned by the Run:ai installation. Thus, uninstalling the <code>runai-backend</code> helm chart may delete all of your data. </p> <p>From version 2.12 forward, PVs are owned the customer and are independent of the Run:ai installation. As such, they are subject to storage class reclaim policy.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#preparations","title":"Preparations","text":""},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#helm","title":"Helm","text":"<p>Run:ai requires Helm 3.14 or later. Before you continue, validate your installed helm client version. To install or upgrade Helm, see Installing Helm. If you are installing an air-gapped version of Run:ai, The Run:ai tar file contains the helm binary. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#software-files","title":"Software files","text":"ConnectedAirgapped <p>Run the helm command below:</p> <pre><code>helm repo add runai-backend https://runai.jfrog.io/artifactory/cp-charts-prod\nhelm repo update\n</code></pre> <ul> <li>Ask for a tar file <code>runai-air-gapped-&lt;NEW-VERSION&gt;.tar.gz</code> from Run:ai customer support. The file contains the new version you want to upgrade to. <code>&lt;NEW-VERSION&gt;</code> is the updated version of the Run:ai control plane.</li> <li>Upload the images as described here.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#before-upgrade","title":"Before upgrade","text":"<p>Before proceeding with the upgrade, it's crucial to apply the specific prerequisites associated with your current version of Run:ai and every version in between up to the version you are upgrading to.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-from-version-27-or-28","title":"Upgrade from version 2.7 or 2.8","text":"<p>Before upgrading the control plane, run: </p> <pre><code>POSTGRES_PV=$(kubectl get pvc pvc-postgresql -n runai-backend -o jsonpath='{.spec.volumeName}')\nkubectl patch pv $POSTGRES_PV -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}'\n\nkubectl delete secret -n runai-backend runai-backend-postgresql\nkubectl delete sts -n runai-backend keycloak runai-backend-postgresql\n</code></pre> <p>Then upgrade the control plane as described below. Before upgrading, find customizations and merge them as discussed below. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-from-version-29-210-or-211","title":"Upgrade from version 2.9, 2.10 or 2.11","text":"<p>Two significant changes to the control-plane installation have happened with version 2.12: PVC ownership and installation customization. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#pvc-ownership","title":"PVC ownership","text":"<p>Run:ai will no longer directly create the PVCs that store Run:ai data (metrics and database). Instead, going forward, </p> <ul> <li>Run:ai requires a Kubernetes storage class to be installed.</li> <li>The PVCs are created by the Kubernetes StatefulSets. </li> </ul> <p>The storage class, as per Kubernetes standards, controls the reclaim behavior: whether the data is saved or deleted when the Run:ai control plane is deleted.  </p> <p>To remove the ownership in an older installation, run:</p> <pre><code>kubectl patch pvc -n runai-backend pvc-postgresql  -p '{\"metadata\": {\"annotations\":{\"helm.sh/resource-policy\": \"keep\"}}}'\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#installation-customization","title":"Installation customization","text":"<p>The Run:ai control-plane installation has been rewritten and is no longer using a backend values file. Instead, to customize the installation use standard <code>--set</code> flags. If you have previously customized the installation, you must now extract these customizations and add them as <code>--set</code> flag to the helm installation:</p> <ul> <li>Find previous customizations to the control plane if such exist. Run:ai provides a utility for that here <code>https://raw.githubusercontent.com/run-ai/docs/v2.13/install/backend/cp-helm-vals-diff.sh</code>. For information on how to use this utility please contact Run:ai customer support. </li> <li>Search for the customizations you found in the optional configurations table and add them in the new format.  </li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-control-plane","title":"Upgrade Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-from-version-213-or-later","title":"Upgrade from version 2.13, or later","text":"ConnectedAirgapped <pre><code>helm get values runai-backend -n runai-backend &gt; runai_control_plane_values.yaml\nhelm upgrade runai-backend -n runai-backend runai-backend/control-plane -f runai_control_plane_values.yaml --reset-then-reuse-values\n</code></pre> <pre><code>helm get values runai-backend -n runai-backend &gt; runai_control_plane_values.yaml\nhelm upgrade runai-backend control-plane-&lt;NEW-VERSION&gt;.tgz -n runai-backend  -f runai_control_plane_values.yaml --reset-then-reuse-values\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-from-version-27-28-29-or-211","title":"Upgrade from version 2.7, 2.8, 2.9, or 2.11","text":"ConnectedAirgapped <pre><code>helm upgrade -i runai-backend -n runai-backend runai-backend/control-plane  \\\n--set global.domain=runai.apps.&lt;OPENSHIFT-CLUSTER-DOMAIN&gt; \\ #(1)\n--set global.config.kubernetesDistribution=openshift \\\n--set thanos.query.stores={thanos-grpc-port-forwarder:10901} \\\n--set postgresql.primary.persistence.existingClaim=pvc-postgresql\n</code></pre> <ol> <li>The subdomain configured for the OpenShift cluster.</li> </ol> <p>Note</p> <p>The helm repository name has changed from <code>runai-backend/runai-backend</code> to <code>runai-backend/control-plane</code>.</p> <pre><code>helm upgrade -i runai-backend  ./control-plane-&lt;NEW-VERSION&gt;.tgz -n runai-backend \\\n--set global.domain=runai.apps.&lt;OPENSHIFT-CLUSTER-DOMAIN&gt; \\ #(1)\n--set global.config.kubernetesDistribution=openshift \\\n--set thanos.query.stores={thanos-grpc-port-forwarder:10901} \\\n--set postgresql.primary.persistence.existingClaim=pvc-postgresql\n</code></pre> <ol> <li>The subdomain configured for the OpenShift cluster.</li> </ol>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-cluster","title":"Upgrade Cluster","text":"<p>To upgrade the cluster follow the instructions here.</p>"},{"location":"admin/troubleshooting/cluster-health-check/","title":"Cluster Health and Troubleshooting","text":"<p>This troubleshooting guide helps you diagnose and resolve issues you may find in your cluster. The cluster status is displayed in the Run:ai Contol Plane. See Cluster Status a list of possible statuses.</p>"},{"location":"admin/troubleshooting/cluster-health-check/#cluster-is-disconnected","title":"Cluster is disconnected","text":"<p>When a cluster's status shows Disconnected, this means that no communication from the Run:ai cluster services reaches the Run:ai Control Plane.</p> <p>This may reflect a networking issue from or to your Kubernetes cluster regardless of Run:ai components. In some cases, it may indicate an issue with one or more Run:ai services that communicate with the Control Plane. These are:</p> <ul> <li>Cluster sync (<code>cluster-sync</code>)</li> <li>Agent (<code>runai-agent</code>)</li> <li>Assets sync (<code>assets-sync</code>)</li> </ul>"},{"location":"admin/troubleshooting/cluster-health-check/#troubleshooting-actions","title":"Troubleshooting actions","text":"<p>Use the following steps to troubleshoot the issue:</p> <ol> <li> <p>Check that the Run:ai services that communicate with the Control Plane are up and running. Run the following command:</p> <p><code>kubectl get pods -n runai | grep -E 'runai-agent|cluster-sync|assets-sync'</code></p> <p>If one or more of the services are not running, see Cluster has Service issues for futher guidelines.</p> </li> <li> <p>Check the network connection from the <code>runai</code> namespace in your cluster to the Control Plane.</p> <p>You can do that by running a connectivity check pod. This pod can be a simple container with basic network troubleshooting tools, such as <code>curl</code> or <code>get</code>. Use the following command to create the pod and determine if it can establish connections to the necessary Control Plane endpoints:</p> <pre><code>kubectl run control-plane-connectivity-check -n runai --image=wbitt/network-multitool --command -- /bin/sh -c 'curl -sSf &lt;control-plane-endpoint&gt; &gt; /dev/null &amp;&amp; echo \"Connection Successful\" || echo \"Failed connecting to the Control Plane\"'\n</code></pre> <p>Replace <code>control-plane-endpoint</code> with the URL of the Control Plane in your environment.</p> <p>If the pod has failed to connect to the Control Plane, check for potential network issues. Use the following guidelines:</p> <ul> <li>Ensure that the network policies in your Kubernetes cluster allow communication between the Control Plane and the Run:ai services that communicate with the Control Plane.</li> <li>Check both Kubernetes Network Policies and any network-related configurations at the infrastructure level.</li> <li>Verify that the required ports and protocols are not blocked.</li> </ul> </li> <li> <p>Verify Run:ai services logs. Inspecting the logs of the Run:ai services that communicate with the CP is essential to identify any error messages.</p> <p>Run the following command on each one of the services:</p> <pre><code>kubectl logs deployment/runai-agent -n runai\nkubectl logs deployment/cluster-sync -n runai\nkubectl logs deployment/assets-sync -n runai\n</code></pre> <p>Try to identify the problem from the logs. If you cannot resolve the issue, continue to the next step.</p> </li> <li> <p>If the issue persists, contact Run:ai support for assistance.</p> <p>Note</p> <p>The previous steps can be used if you installed the cluster and the status is stuck in Waiting to connect for a long time.</p> </li> </ol>"},{"location":"admin/troubleshooting/cluster-health-check/#cluster-has-service-issues","title":"Cluster has service issues","text":"<p>When a cluster's status shows Service issues, this means that one or more Run:ai services that are running in the cluster are not available.</p> <ol> <li> <p>Run the following command to verify which are the non-functioning services, and to get more details about deployment issues and resources required by these services that may not be ready (for example, ingress was not created or is unhealthy):</p> <pre><code>kubectl get runaiconfig -n runai runai -ojson | jq -r '.status.conditions | map(select(.type == \"Available\"))'\n</code></pre> <p>The list of non-functioning services is also available on the UI Clusters page.</p> </li> <li> <p>After determining the non-functioning services, use the following guidelines to further investigate the issue.</p> <p>Run the following to get all the Kubernetes events and look for recent failures:</p> <pre><code>Kubectl get events  -A\n</code></pre> <p>If a required resource was created but not available or unhealthy, check the details by running:</p> <pre><code>Kubectl describe &lt;resource_type&gt; &lt;name&gt;\n</code></pre> </li> <li> <p>If the issue persists, contact Run:ai support for assistance.</p> </li> </ol>"},{"location":"admin/troubleshooting/cluster-health-check/#cluster-has-missing-prerequisites","title":"Cluster has missing prerequisites","text":"<p>When a cluster's status displays Missing prerequisites, it indicates that at least one of the Mandatory Prerequisites has not been fulfilled. In such cases, Run:ai services may not function properly.</p> <p>If you have ensured that all prerequisites are installed and the status still shows Missing prerequisites, follow these steps:</p> <ol> <li>Check the message in the Control Plane for further details regarding the missing prerequisites.</li> <li>Inspect the runai-public ConfigMap and look for the <code>dependencies.required</code> field to obtain detailed information about the missing resources.</li> <li>If the issue persists, contact Run:ai support for assistance.</li> </ol>"},{"location":"admin/troubleshooting/cluster-health-check/#general-tests-to-verify-the-runai-cluster-health","title":"General tests to verify the Run:ai cluster health","text":"<p>Use the following tests regularly to determine the health of the Run:ai cluster, regardless of the cluster status and the troubleshooting options previously described.</p>"},{"location":"admin/troubleshooting/cluster-health-check/#verify-that-data-is-sent-to-the-cloud","title":"Verify that data is sent to the cloud","text":"<p>Log in to <code>&lt;company-name&gt;.run.ai/dashboards/now</code>.</p> <ul> <li>Verify that all metrics in the overview dashboard are showing.</li> <li>Verify that all metrics are showing in the Nodes view.</li> <li>Go to Projects and create a new Project. Find the new Project using the CLI command: <code>runai list projects</code></li> </ul>"},{"location":"admin/troubleshooting/cluster-health-check/#verify-that-the-runai-services-are-running","title":"Verify that the Run:ai services are running","text":"<p>Run:</p> <pre><code>kubectl get runaiconfig -n runai runai -ojson | jq -r '.status.conditions | map(select(.type == \"Available\"))'\n</code></pre> <p>Verify that all the Run:ai services are available and have all their required resources available.</p> <p>Run:</p> <pre><code>kubectl get pods -n runai\nkubectl get pods -n monitoring\n</code></pre> <p>Verify that all pods are in <code>Running</code> status and a ready state (1/1 or similar).</p> <p>Run:</p> <pre><code>kubectl get deployments -n runai\n</code></pre> <p>Check that all deployments are in a ready state (1/1).</p> <p>Run:</p> <pre><code>kubectl get daemonset -n runai\n</code></pre> <p>A Daemonset runs on every node. Some of the Run:ai daemon-sets run on all nodes. Others run only on nodes that contain GPUs. Verify that for all daemonsets the desired number is equal to current and to ready.</p>"},{"location":"admin/troubleshooting/cluster-health-check/#submit-a-job-using-the-command-line-interface","title":"Submit a job using the command-line interface","text":"<p>Submitting a Job allows you to verify that the Run:ai scheduling service is running properly.</p> <ol> <li>Make sure that the Project you have created has a quota of at least 1 GPU.</li> <li> <p>Run:</p> <pre><code>runai config project &lt;project-name&gt;\nrunai submit -i gcr.io/run-ai-demo/quickstart -g 1\n</code></pre> </li> <li> <p>Verify that the Job is in a Running state by running:</p> <pre><code>runai list jobs\n</code></pre> </li> <li> <p>Verify that the Job is showing in the Jobs area at <code>&lt;company-name&gt;.run.ai/jobs</code>.</p> </li> </ol>"},{"location":"admin/troubleshooting/cluster-health-check/#submit-a-job-using-the-user-interface","title":"Submit a job using the user interface","text":"<p>Log into the Run:ai user interface, and verify that you have a <code>Researcher</code> or <code>Research Manager</code> role. Go to the <code>Jobs</code> area. On the top right, press the button to create a Job. Once the form opens, you can submit a Job.</p>"},{"location":"admin/troubleshooting/cluster-health-check/#advanced-troubleshooting","title":"Advanced troubleshooting","text":""},{"location":"admin/troubleshooting/cluster-health-check/#runai-public-configmap","title":"Run:ai public ConfigMap","text":"<p>Run:ai services use the <code>runai-public</code> ConfigMap to store information about the cluster status. This ConfigMap can be helpful in troubleshooting issues with Run:ai services. Inspect the ConfigMap by running:</p> <pre><code>kubectl get cm runai-public -oyaml\n</code></pre>"},{"location":"admin/troubleshooting/cluster-health-check/#resources-not-deployed-system-unavailable-reconciliation-failed","title":"Resources not deployed / System unavailable / Reconciliation failed","text":"<ol> <li>Run the Preinstall diagnostic script and check for issues.</li> <li>Run</li> </ol> <pre><code>   kubectl get pods -n runai\n   kubectl get pods -n monitoring\n</code></pre> <p>Look for any failing pods and check their logs for more information by running <code>kubectl describe pod -n &lt;pod_namespace&gt; &lt;pod_name&gt;</code>.</p>"},{"location":"admin/troubleshooting/diagnostics/","title":"Diagnostic Tools","text":""},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-the-database-container","title":"Add Verbosity to the Database container","text":"<p>Run:ai Self-hosted installation contains an internal database. To diagnose database issues, you can run the database in debug mode.</p> <p>In the runai-backend-values, search for <code>postgresql</code>. Add: </p> <pre><code>postgresql:\n  image:\n    debug: true\n</code></pre> <p>Re-install the Run:ai control-plane and then review the database logs by running: </p> <pre><code>kubectl logs -n runai-backend runai-postgresql-0\n</code></pre>"},{"location":"admin/troubleshooting/diagnostics/#internal-networking-issues","title":"Internal Networking Issues","text":"<p>Run:ai is based on Kubernetes. Kubernetes runs its own internal subnet with a separate DNS service. If you see in the logs that services have trouble connecting, the problem may reside there.  You can find further information on how to debug Kubernetes DNS here. Specifically, it is useful to start a pod with networking utilities and use it for network resolution:</p> <pre><code>kubectl run -i --tty netutils --image=dersimn/netutils -- bash\n</code></pre>"},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-prometheus","title":"Add Verbosity to Prometheus","text":"<p>Add verbosity to Prometheus by editing RunaiConfig:</p> <pre><code>kubectl edit runaiconfig runai -n runai\n</code></pre> <p>Add a <code>debug</code> log level:</p> <pre><code>prometheus-operator:\n  prometheus:\n    prometheusSpec:\n      logLevel: debug\n</code></pre> <p>To view logs, run: <pre><code>kubectl logs prometheus-runai-prometheus-operator-prometheus-0 prometheus \\\n      -n monitoring -f --tail 100\n</code></pre></p>"},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-scheduler","title":"Add Verbosity to Scheduler","text":"<p>To view extended logs run:</p> <pre><code>kubectl edit ruaiconfig runai -n runai\n</code></pre> <p>Then under the <code>scheduler</code> section add:</p> <pre><code>runai-scheduler:\n   args:\n     verbosity: 6\n</code></pre> <p>Warning</p> <p>Verbose scheduler logs consume a significant amount of disk space.</p>"},{"location":"admin/troubleshooting/troubleshooting/","title":"Troubleshooting Run:ai","text":""},{"location":"admin/troubleshooting/troubleshooting/#installation","title":"Installation","text":"Upgrade fails with \"Ingress already exists\" <p>Symptom:  The installation fails with error: <code>Error: rendered manifests contain a resource that already exists. Unable to continue with install: IngressClass \"nginx\" in namespace \"\" exists</code></p> <p>Root cause: Run:ai installs <code>NGINX</code>, but there is an existing NGINX on the cluster. </p> <p>Resolution: In the Run:ai cluster YAML file, disable the installation of NGINX by setting:</p> <pre><code>ingress-nginx:\n    enabled: false\n</code></pre> How to get installation logs <p>Symptom: Installation fails and you need to troubleshoot the issue.</p> <p>Resolution: Run the following script to obtain any relevant installation logs in case of an error.</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/run-ai/public/main/installation/get-installation-logs.sh | bash\n</code></pre> Upgrade fails with \"rendered manifests contain a resource that already exists\" error <p>Symptom: The installation fails with error: <code>Error: rendered manifests contain a resource that already exists. Unable to continue with install:...</code></p> <p>Root cause: The Run:ai installation is trying to create a resource that already exists, which may be due to a previous installation that was not properly removed.</p> <p>Resolution: Run the following script to remove all Run:ai resources and reinstall:</p> <pre><code>helm template &lt;release-name&gt; &lt;chart-name&gt; --namespace &lt;namespace&gt; | kubectl delete -f -\n</code></pre> <p>Then reinstall Run:ai.</p> Pods are failing due to certificate issues <p>Symptom: Pods are failing with certificate issues.</p> <p>Root cause: The certificate provided during the Control Plane's installation is not valid.</p> <p>Resolution: Verify that the certificate is valid and trusted. If the certificate is valid, but is signed by a local CA, make sure you have followed the procedure for a local certificate authority.</p>"},{"location":"admin/troubleshooting/troubleshooting/#dashboard-issues","title":"Dashboard Issues","text":"No Metrics are showing on Dashboard <p>Symptom: No metrics are showing on dashboards at <code>https://&lt;company-name&gt;.run.ai/dashboards/now</code></p> <p>Typical root causes:</p> <ul> <li>Firewall-related issues.</li> <li>Internal clock is not synced.</li> <li>Prometheus pods are not running.</li> </ul> <p>Firewall issues</p> <p>Add verbosity to Prometheus as describe here.Verify that there are no errors. If there are connectivity-related errors you may need to:</p> <ul> <li>Check your firewall for outbound connections. See the required permitted URL list in Network requirements.</li> <li>If you need to set up an internet proxy or certificate, please contact Run:ai customer support. </li> </ul> <p>Machine Clocks are not synced</p> <p>Run: <code>date</code> on cluster nodes and verify that date/time is correct.  If not:</p> <ul> <li>Set the Linux time service (NTP).</li> <li>Restart Run:ai services. Depending on the previous time gap between servers, you may need to reinstall the Run:ai cluster</li> </ul> <p>Prometheus pods are not running</p> <p>Run: <code>kubectl get pods -n monitoring -o wide</code></p> <ul> <li>Verify that all pods are running.</li> <li>The default Prometheus installation is not built for high availability. If a node is down, the Prometheus pod may not recover by itself unless manually deleted. Delete the pod to see it start on a different node and consider adding a second replica to Prometheus.</li> </ul> GPU Related metrics not showing <p>Symptom: GPU-related metrics such as <code>GPU Nodes</code> and <code>Total GPUs</code> are showing zero but other metrics, such as <code>Cluster load</code> are shown.</p> <p>Root cause: An installation issue related to the NVIDIA stack.</p> <p>Resolution: </p> <p>Need to run through the NVIDIA stack and find the issue. The current NVIDIA stack looks as follows:</p> <ol> <li>NVIDIA Drivers (at the OS level, on every node)</li> <li>NVIDIA Docker (extension to Docker, on every node)</li> <li>Kubernetes Node feature discovery (mark node properties)</li> <li>NVIDIA GPU Feature discovery (mark nodes as \u201chaving GPUs\u201d)</li> <li>NVIDIA Device plug-in (Exposes GPUs to Kubernetes)</li> <li>NVIDIA DCGM Exporter (Exposes metrics from GPUs in Kubernetes)</li> </ol> <p>Run:ai requires the installation of the NVIDIA GPU Operator which installs the entire stack above. However, there are two alternative methods for using the operator:</p> <ul> <li>Use the default operator values to install 1 through 6.</li> <li>If  NVIDIA Drivers (#1 above) are already installed on all nodes, use the operator with a flag that disables drivers install. </li> </ul> <p>For more information see Cluster prerequisites.</p> <p>NVIDIA GPU Operator</p> <p>Run: <code>kubectl get pods -n gpu-operator | grep nvidia</code> and verify that all pods are running.</p> <p>Node and GPU feature discovery</p> <p>Kubernetes Node feature discovery identifies and annotates nodes. NVIDIA GPU Feature Discovery identifies and annotates nodes with GPU properties. See that: </p> <ul> <li>All such pods are up.</li> <li>The GPU feature discovery pod is available for every node with a GPU.</li> <li>And finally, when describing nodes, they show an active <code>gpu/nvidia</code> resource.</li> </ul> <p>NVIDIA Drivers</p> <ul> <li>If NVIDIA drivers have been installed on the nodes themselves, ssh into each node and run <code>nvidia-smi</code>. Run <code>sudo systemctl status docker</code> and verify that docker is running. Run <code>nvidia-docker</code> and verify that it is installed and working.  Linux software upgrades may require a node restart.</li> <li>If NVIDIA drivers are installed by the Operator, verify that the NVIDIA driver daemonset has created a pod for each node and that all nodes are running. Review the logs of all such pods. A typical problem may be the driver version which is too advanced for the GPU hardware. You can set the driver version via operator flags. </li> </ul> <p>NVIDIA DCGM Exporter</p> <ul> <li>View the logs of the DCGM exporter pod and verify that no errors are prohibiting the sending of metrics. </li> <li>To validate that the dcgm-exporter exposes metrics, find one of the DCGM Exporter pods and run:</li> </ul> <pre><code>kubectl port-forward &lt;dcgm-exporter-pod-name&gt; 9400:9400\n</code></pre> <p>Then browse to http://localhost:9400/metrics and verify that the metrics have reached the DCGM exporter.</p> <ul> <li>The next step after the DCGM Exporter is <code>Prometheus</code>. To validate that metrics from the DCGM Exporter reach Prometheus, run:</li> </ul> <pre><code>kubectl port-forward svc/runai-cluster-kube-prometh-prometheus -n monitoring 9090:9090\n</code></pre> <p>Then browse to localhost:9090. In the UI, type <code>DCGM_FI_DEV_GPU_UTIL</code> as the metric name, and verify that the metric has reached Prometheus. </p> <p>If the DCGM Exporter is running correctly and exposing metrics, but this metric does not appear in Prometheus, there may be a connectivity issue between these components.</p> Allocation-related metrics not showing <p>Symptom: GPU Allocation-related metrics such as <code>Allocated GPUs</code> are showing zero but other metrics, such as <code>Cluster load</code> are shown.</p> <p>Root cause: The origin of such metrics is the scheduler. </p> <p>Resolution:</p> <ul> <li>Run: <code>kubectl get pods -n runai | grep scheduler</code>. Verify that the pod is running.</li> <li>Review the scheduler logs and look for errors. If such errors exist, contact Run:ai customer support. </li> </ul> All metrics are showing \"No Data\" <p>Symptom: All data on all dashboards is showing the text \"No Data\".</p> <p>Root cause: Internal issue with metrics infrastructure.</p> <p>Resolution: Please contact Run:ai customer support.</p>"},{"location":"admin/troubleshooting/troubleshooting/#authentication-issues","title":"Authentication Issues","text":"After a successful login, you are redirected to the same login page <p>For a self-hosted installation, check Linux clock synchronization as described above. Use the Run:ai pre-install script to test this automatically. </p> Single-sign-on issues <p>For single-sign-on issues, see the troubleshooting section in the single-sign-on configuration document. </p>"},{"location":"admin/troubleshooting/troubleshooting/#user-interface-submit-job-issues","title":"User Interface Submit Job Issues","text":"New Job button is grayed out <p>Symptom: The <code>New Job</code> button on the top right of the Job list is grayed out.</p> <p>Root Cause: This can happen due to multiple configuration issues: </p> <ul> <li>Open Chrome developer tools and refresh the screen.</li> <li>Under <code>Network</code> locate a network call error. Search for the HTTP error code.</li> </ul> <p>Resolution for 401 HTTP Error</p> <ul> <li>The Cluster certificate provided as part of the installation is valid and trusted (not self-signed).</li> <li>Researcher Authentication has not been properly configured. Try running <code>runai login</code> from the Command-line interface. Alternatively, run: <code>kubectl get pods -n kube-system</code>, identify the api-server pod and review its logs. </li> </ul> <p>Resolution for 403 HTTP Error</p> <p>Run: <code>kubectl get pods -n runai</code>, identify the <code>agent</code> pod, see that it's running, and review its logs.</p> New Job button is not showing <p>Symptom: The <code>New Job</code> button on the top right of the Job list does not show.</p> <p>Root Causes: (multiple)</p> <ul> <li>You do not have <code>Researcher</code> or <code>Research Manager</code> permissions.</li> <li>Under <code>Settings | General</code>, verify that <code>Unified UI</code> is on.</li> </ul> Submit form is distorted <p>Symptom: Submit form is showing vertical lines.</p> <p>Root Cause: The control plane does not know the cluster URL.</p> <p>Using the Run:ai user interface, go to the Clusters list. See that there is no cluster URL next to your cluster.</p> <p>Resolution: Cluster must be re-installed. </p> Submit form does not show the list of Projects <p>Symptom: When connected with Single-sign-on, in the Submit form, the list of Projects is empty.</p> <p>Root Cause:  SSO is on and researcher authentication is not properly configured as such.</p> <p>Resolution: Verify API Server settings as described in Researcher Authentication configuration.</p> Job form is not opening on OpenShift <p>Symptom: When clicking on \"New Job\" the Job forms does not load. Network shows 405</p> <p>Root Cause: An installation step has been missed. </p> <p>Resolution: Open the Cluster list and open the cluster installation wizard again. After selecting OpenShift, you will see a <code>patch</code> command at the end of the instruction set. Run it. </p>"},{"location":"admin/troubleshooting/troubleshooting/#networking-issues","title":"Networking Issues","text":"'admission controller' connectivity issue <p>Symptoms:</p> <ul> <li>Pods are failing with 'admission controller' connectivity errors.</li> <li>The command-line <code>runai submit</code> fails with an 'admission controller' connectivity error.</li> <li>Agent or cluster sync pods are crashing in self-hosted installation.</li> </ul> <p>Root cause: Connectivity issues between different nodes in the cluster.</p> <p>Resolution:</p> <ul> <li>Run the preinstall script and search for networking errors.</li> <li>Run: <code>kubectl get pods -n kube-system -o wide</code>. Verify that all networking pods are running. </li> <li>Run: <code>kubectl get nodes</code>. Check that all nodes are ready and connected.</li> <li>Run: <code>kubectl get pods -o wide -A</code> to see which pods are Pending or in Error and which nodes they belong to. </li> <li>See if pods from different nodes have trouble communicating with each other.</li> <li>Advanced, run: <code>kubectl exec &lt;pod-name&gt; -it /bin/sh</code> from a pod in one node and ping a pod from another. </li> </ul> Projects are not syncing <p>Symptom: Create a Project on the Run:ai user interface, then run: <code>runai list projects</code>. The new Project does not appear.</p> <p>Root cause: The Run:ai agent is not syncing properly. This may be due to firewall issues. </p> <p>Resolution</p> <ul> <li>Run: <code>runai pods -n runai | grep agent</code>. See that the agent is in Running state. Select the agent's full name and run: <code>kubectl logs -n runai runai-agent-&lt;id&gt;</code>.</li> <li>Verify that there are no errors. If there are connectivity-related errors you may need to check your firewall for outbound connections. See the required permitted URL list in Network requirements. </li> <li>If you need to set up an internet proxy or certificate, please contact Run:ai customer support. </li> </ul> Jobs are not syncing <p>Symptom: A Job on the cluster (<code>runai list jobs</code>) does not show in the Run:ai user interface Job list. </p> <p>Root cause: The Run:ai cluster-sync pod is not syncing properly.  </p> <p>Resolution: Search the cluster-sync pod for errors.</p>"},{"location":"admin/troubleshooting/troubleshooting/#job-related-issues","title":"Job-related Issues","text":"Jobs fail with ContainerCannotRun status  <p>Symptom: When running <code>runai list jobs</code>, your Job has a status of <code>ContainerCannotRun</code>.</p> <p>Root Cause: The issue may be caused due to an unattended upgrade of the NVIDIA driver.</p> <p>To verify, run: <code>runai describe job &lt;job-name&gt;</code>, and search for an error <code>driver/library version mismatch</code>.</p> <p>Resolution: Reboot the node on which the Job attempted to run.</p> <p>Going forward, we recommend blacklisting NVIDIA driver from unattended-upgrades. You can do that by editing <code>/etc/apt/apt.conf.d/50unattended-upgrades</code>, and adding <code>nvidia-driver-</code> to the <code>Unattended-Upgrade::Package-Blacklist</code> section. It should look something like that:</p> <pre><code>Unattended-Upgrade::Package-Blacklist {\n    // The following matches all packages starting with linux-\n    //  \"linux-\";\n    \"nvidia-driver-\";\n</code></pre>"},{"location":"admin/troubleshooting/troubleshooting/#inference-issues","title":"Inference Issues","text":"New Deployment button is grayed out <p>Symptoms: </p> <ul> <li>The <code>New workload type</code> -&gt; <code>Inference</code> button is grayed out.</li> <li>Cannot create a deployment via Inference API.</li> </ul> <p>Root Cause: Run:ai Inference prerequisites have not been met.</p> <p>Resolution: Review inference prerequisites and install accordingly.</p> Submitted workload type of inference remains in Pending state <p>Symptom: A submitted inference is not running.</p> <p>Root Cause: The patch statement to add the runai-scheduler has not been performed. </p> Workload of type inference status is \"Failed\" <p>Symptom: Inference status is always <code>Failed</code>.</p> <p>Root Cause: (multiple)</p> <ul> <li>Not enough resources in the cluster.</li> <li>Server model command is misconfigured (i.e sleep infinity).</li> <li>Server port is misconfigured. </li> </ul> Worload of type inference does not scale up from zero <p>Symptom: In the Inference form, when \"Auto-scaling\" is enabled, and \"Minimum Replicas\" is set to zero, the inference cannot scale up from zero.</p> <p>Root Cause: </p> <ul> <li>Clients are not sending requests.</li> <li>Clients are not using the same port/protocol as the server model.</li> <li>Server model command is misconfigured (i.e sleep infinity).</li> </ul>"},{"location":"admin/troubleshooting/troubleshooting/#command-line-interface-issues","title":"Command-line interface Issues","text":"Unable to install CLI due to certificate errors <p>Symptom: The curl command and download button to download the CLI is not working.</p> <p>Root Cause: The cluster is not accessible from the download location</p> <p>Resolution: </p> <p>Use an alternate method for downloading the CLI. Run:</p> <pre><code>kubectl port-forward -n runai svc/researcher-service 4180\n</code></pre> <p>In another shell, run: <pre><code>wget --content-disposition http://localhost:4180/cli/linux\n</code></pre></p> When running the CLI you get an error: open .../.kube/config.lock: permission denied <p>Symptom: When running any CLI command you get a permission denied error.</p> <p>Root Cause: The user running the CLI does not have read permissions to the <code>.kube</code> directory.</p> <p>Resolution: Change permissions for the directory.</p> When running 'runai logs', the logs are delayed <p>Symptom: Printout from the container is not immediately shown in the log. </p> <p>Root Cause: By default, Python buffers stdout, and stderr, which are not flushed in real-time. This may cause logs to appear sometimes minutes after being buffered.</p> <p>Resolution: Set the env var PYTHONUNBUFFERED to any non-empty string or pass -u to Python. e.g. <code>python -u main.py</code>.</p> CLI does not download properly on OpenShift <p>Symptom: When trying to download the CLI on OpenShift, the <code>wget</code> statement downloads a text file named <code>darwin</code> or <code>linux</code> rather than the binary <code>runai</code>.</p> <p>Root Cause: An installation step has been missed. </p> <p>Resolution: Open the Cluster list and open the cluster installation wizard again. After selecting OpenShift, you will see a <code>patch</code> command at the end of the instruction set. Run it. </p>"},{"location":"admin/troubleshooting/alertmanager/","title":"Prometheus Alerts","text":"<p>The Prometheus Alertmanager handles alerts created and sent as by client applications using alerting rules. It takes care of deduplicating, grouping, and sending out notifications via methods such as email, on-call notification systems, and chat platforms.</p> <p>To configure Prometheus to send alerts, see Setting up Alert Monitoring for Run:ai Using Alertmanager in Prometheus.</p>"},{"location":"admin/troubleshooting/alertmanager/#list-of-alerts","title":"List of Alerts","text":"<p>The following is a list of Run:ai alerts that you will receive once Prometheus is configured.</p> Alert Name RunaiAgentClusterInfoPushRateLow RunaiAgentPullRateLow RunaiContainerMemoryUsageCritical RunaiContainerMemoryUsageWarning RunaiContainerRestarting RunaiCpuUsageWarning RunaiCriticalProblem RunaiDaemonSetRolloutStuck RunaiDaemonSetUnavailableOnNodes RunaiDeploymentInsufficientReplicas RunaiDeploymentNoAvailableReplicas RunaiDeploymentUnavailableReplicas RunaiProjectControllerReconcileFailure RunaiStatefulSetInsufficientReplicas RunaiStatefulSetNoAvailableReplicas"},{"location":"admin/troubleshooting/alertmanager/RunaiAgentClusterInfoPushRateLow/","title":"RunaiAgentClusterInfoPushRateLow","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiAgentClusterInfoPushRateLow/#meaning","title":"Meaning","text":"<p>The <code>cluster-sync</code> pod in the <code>runai</code> namespace might not be functioning properly.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiAgentClusterInfoPushRateLow/#impact","title":"Impact","text":"<p>No information or partial information from the cluster is being synced back to the Run:ai Control Plane.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiAgentClusterInfoPushRateLow/#severity","title":"Severity","text":"<p>Critical</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiAgentClusterInfoPushRateLow/#diagnosis","title":"Diagnosis","text":"<p>Run <code>kubectl get pod -n runai</code> to see if the <code>cluster-sync</code> pod is running.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiAgentClusterInfoPushRateLow/#mitigation","title":"Mitigation","text":"<p>Run  <pre><code>kubectl describe deployment cluster-sync -n runai \nkubectl logs deployment/cluster-sync -n runai\n</code></pre></p> <p>From the logs and pod details, try and figure out why the <code>cluster-sync</code> pod is not functioning properly.</p> <p>It is possible that there is a connectivity issue from the cluster to the Run:ai Control Plane.</p> <p>If the network is from the cluster to the Control Plane works as expected, and you cannot correct the issue, contact Run:ai support. </p>"},{"location":"admin/troubleshooting/alertmanager/RunaiAgentPullRateLow/","title":"RunaiAgentPullRateLow","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiAgentPullRateLow/#meaning","title":"Meaning","text":"<p>The <code>runai-agent</code> pod may be too loaded, is slow in processing data (possible in very big clusters), or the <code>runai-agent</code> pod itself in the <code>runai</code> namespace may not be functioning properly.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiAgentPullRateLow/#impact","title":"Impact","text":"<p>No information or partial information from the Run:ai Control Plane is being synced to the cluster.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiAgentPullRateLow/#severity","title":"Severity","text":"<p>Critical</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiAgentPullRateLow/#diagnosis","title":"Diagnosis","text":"<p>Run <code>kubectl get pod -n runai</code> to see if the <code>runai-agent</code> pod is running.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiAgentPullRateLow/#mitigation","title":"Mitigation","text":"<p>Run:  <pre><code>kubectl describe deployment runai-agent -n runai\nkubectl logs deployment/runai-agent -n runai\n</code></pre></p> <p>From the logs and pod details, try and figure out why the <code>runai-agent</code> pod is not functioning properly.</p> <p>It is possible that there is a connectivity issue from the cluster to the Run:ai Control Plane.</p> <p>If it seems that the <code>runai-agent</code> pod is functioning properly, but the cluster is very big and loaded, it is possible that the agent is taking time to process the data coming from the Run:ai Control Plane. If this is the case, and you want the alert to stop firing, you can try to edit the value under which the alert starts firing.</p> <p>Run <code>kubectl edit runaiconfig -n runai</code>.</p> <p>In the  <code>spec</code>:<code>prometheus</code> verify that the  <code>agentPullPushRateMinForAlert</code> (if the property does not exist, add it). If the property exists, the default value is 0.05. You can change it to less than that (for example, 0.045 or 0.04).</p> <p>If the above instructions did not correct the issue, contact Run:ai support. </p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerMemoryUsageCritical/","title":"RunaiContainerMemoryUsageCritical","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiContainerMemoryUsageCritical/#meaning","title":"Meaning","text":"<p>A Run:ai container is using more than 90% of its Memory limit.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerMemoryUsageCritical/#impact","title":"Impact","text":"<p>The container might go out of memory (OOM) and crash.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerMemoryUsageCritical/#severity","title":"Severity","text":"<p>Critical</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerMemoryUsageCritical/#diagnosis","title":"Diagnosis","text":"<p>Use the command <code>kubectl top</code> on the relevant pod specified in the alert.  </p> <p>If this tool is unavailable, you can calculate the memory usage by running <code>container_memory_usage_bytes{namespace=~\"runai|runai-backend\"}</code>.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerMemoryUsageCritical/#mitigation","title":"Mitigation","text":"<p>Add memory resources to the container.  If the issue is not resolved, contact Run:ai support.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerMemoryUsageWarning/","title":"RunaiContainerMemoryUsageWarning","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiContainerMemoryUsageWarning/#meaning","title":"Meaning","text":"<p>A Run:ai container is using more than 80% of its Memory limit.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerMemoryUsageWarning/#impact","title":"Impact","text":"<p>The container might go out of memory (OOM) and crash.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerMemoryUsageWarning/#severity","title":"Severity","text":"<p>Warning</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerMemoryUsageWarning/#diagnosis","title":"Diagnosis","text":"<p>Use the command <code>kubectl top</code> on the relevant pod. </p> <p>If this tool is unavailable, you can calculate the memory usage by running <code>container_memory_usage_bytes{namespace=~\"runai|runai-backend\"}</code>.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerMemoryUsageWarning/#mitigation","title":"Mitigation","text":"<p>Add memory resources to the container.  If the issue is not resolved, contact Run:ai support.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerRestarting/","title":"RunaiContainerRestarting","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiContainerRestarting/#meaning","title":"Meaning","text":"<p>A Run:ai container has restarted more than twice in the last 10 minutes.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerRestarting/#impact","title":"Impact","text":"<p>The container may be unavailable and affect Run:ai system feature functionalities.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerRestarting/#severity","title":"Severity","text":"<p>Warning</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerRestarting/#diagnosis","title":"Diagnosis","text":"<p>To diagnose the issue, and the pods with issues, run: <pre><code>kubectl get pods -n runai\nkubectl get pods -n runai-backend\n</code></pre></p> <p>The expected result should be one or more pods where the restart count &gt;= 2.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiContainerRestarting/#mitigation","title":"Mitigation","text":"<p>Run <code>kubectl logs -n NAMESPACE POD_NAME</code> on the relevant pod. Check to see if there is something in the logs that stands out. Then, check that the container has enough resources.</p> <p>Contact Run:ai for more assistance.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiCpuUsageWarning/","title":"RunaiCpuUsageWarning","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiCpuUsageWarning/#meaning","title":"Meaning","text":"<p>A Run:ai container is using more than 80% of its CPU limit.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiCpuUsageWarning/#impact","title":"Impact","text":"<p>This may cause slowness in the operation of certain Run:ai features.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiCpuUsageWarning/#severity","title":"Severity","text":"<p>Warning</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiCpuUsageWarning/#diagnosis","title":"Diagnosis","text":"<p>Use the command <code>kubectl top</code> on the relevant pod. </p> <p>If this tool is unavailable, you can calculate the CPU usage by running <code>rate(container_cpu_usage_seconds_total{namespace=~\"runai|runai-backend\"}[2m])</code>.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiCpuUsageWarning/#mitigation","title":"Mitigation","text":"<p>Add CPU resources to the container.  If the issue is not resolved, contact Run:ai support.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiCriticalProblem/","title":"RunaiCriticalProblem","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiCriticalProblem/#meaning","title":"Meaning","text":"<p>One of the critical Run:ai alerts is active.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiCriticalProblem/#impact","title":"Impact","text":"<p>The impact is based on the active alert.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiCriticalProblem/#severity","title":"Severity","text":"<p>Critical</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiCriticalProblem/#diagnosis","title":"Diagnosis","text":"<p>Check Run:ai alerts in Prometheus to find the active critical alert. </p>"},{"location":"admin/troubleshooting/alertmanager/RunaiCriticalProblem/#mitigation","title":"Mitigation","text":"<p>Go to the mitigation section of the relevant alert. If you cannot resolve the issue, contact Run:ai support.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDaemonSetRolloutStuck/","title":"RunaiDaemonSetRolloutStuck","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiDaemonSetRolloutStuck/#meaning","title":"Meaning","text":"<p>Runai daemonset has 0 available pods on a relevant node.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDaemonSetRolloutStuck/#impact","title":"Impact","text":"<p>No fractional gpu workloads support.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDaemonSetRolloutStuck/#severity","title":"Severity","text":"<p>Critical</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDaemonSetRolloutStuck/#diagnosis","title":"Diagnosis","text":"<p>Run</p> <p><code>kubectl get daemonset -n runai-backend</code></p> <p>Identify the one or more daemonsets that have no running pods on some of the nodes.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDaemonSetRolloutStuck/#mitigation","title":"Mitigation","text":"<p>Run <code>kubectl describe daemonset X -n runai</code> on the relevant deamonset(s) to try and figure out why it cannot create pods.</p> <p>If you cannot correct the issue, contact Run:ai support.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDaemonSetUnavailableOnNodes/","title":"RunaiDaemonSetUnavailableOnNodes","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiDaemonSetUnavailableOnNodes/#meaning","title":"Meaning","text":"<p>One or more of the Run:ai daemonsets has zero available pods on a relevant node.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDaemonSetUnavailableOnNodes/#impact","title":"Impact","text":"<p>Fractional GPU workloads may not be working as expected.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDaemonSetUnavailableOnNodes/#severity","title":"Severity","text":"<p>Critical</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDaemonSetUnavailableOnNodes/#diagnosis","title":"Diagnosis","text":"<p>Run  <code>kubectl get daemonset -n runai-backend</code> </p> <p>Identify the one or more daemonsets that have no running pods on some of the nodes.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDaemonSetUnavailableOnNodes/#mitigation","title":"Mitigation","text":"<p>Run <code>kubectl describe daemonset X -n runai</code> on the relevant deamonset(s) to try and figure out why it cannot create pods. </p> <p>If you cannot correct the issue, contact Run:ai support. </p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentInsufficientReplicas/","title":"RunaiDeploymentInsufficientReplicas","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentInsufficientReplicas/#meaning","title":"Meaning","text":"<p>A Run:ai deployment has one or more unavailable pods.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentInsufficientReplicas/#impact","title":"Impact","text":"<p>This alert may indicate scaling issues. If a new version of the deployment can not be deployed, this may cause new features to be missing.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentInsufficientReplicas/#severity","title":"Severity","text":"<p>Critical</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentInsufficientReplicas/#diagnosis","title":"Diagnosis","text":"<p>Run: <pre><code>kubectl get deployment -n runai\nkubectl get deployment -n runai-backend\n</code></pre></p> <p>Identify one or more deployments that have missing pods.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentInsufficientReplicas/#mitigation","title":"Mitigation","text":"<p>Run: <pre><code>kubectl describe deployment X -n runai/runai-backend\nkubectl describe replicaset X -n runai/runai-backend\nkubectl logs deployment/X -n runai/runai-backend\n</code></pre></p> <p>From the logs and deployment details, try and figure out why the deployment cannot create pods. </p> <p>If you cannot correct the issue, contact Run:ai support.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentNoAvailableReplicas/","title":"RunaiDeploymentNoAvailableReplicas","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentNoAvailableReplicas/#meaning","title":"Meaning","text":"<p>A Run:ai deployment has zero available pods.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentNoAvailableReplicas/#impact","title":"Impact","text":"<p>Run:ai may not be able to function, depending on the unavailable deployment.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentNoAvailableReplicas/#severity","title":"Severity","text":"<p>Critical</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentNoAvailableReplicas/#diagnosis","title":"Diagnosis","text":"<p>Run: <pre><code>kubectl get deployment -n runai\nkubectl get deployment -n runai-backend\n</code></pre></p> <p>Identify one or more deployments that have no running pods.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentNoAvailableReplicas/#mitigation","title":"Mitigation","text":"<p>Run: <pre><code>kubectl describe deployment X -n runai/runai-backend\nkubectl describe replicaset X -n runai/runai-backend\nkubectl logs deployment/X -n runai/runai-backend\n</code></pre></p> <p>From the logs and deployment details, try and figure out why the deployment cannot create pods. </p> <p>If you cannot correct the issue, contact Run:ai support.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentUnavailableReplicas/","title":"RunaiDeploymentUnavailableReplicas","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentUnavailableReplicas/#meaning","title":"Meaning","text":"<p>A Run:ai deployment has one or more unavailable pods.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentUnavailableReplicas/#impact","title":"Impact","text":"<p>This alert may indicate scaling issues. If a new version of the deployment can not be deployed, this may cause new features to be missing.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentUnavailableReplicas/#severity","title":"Severity","text":"<p>Warning</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentUnavailableReplicas/#diagnosis","title":"Diagnosis","text":"<p>Run: <pre><code>kubectl get deployment -n runai\nkubectl get deployment -n runai-backend\n</code></pre></p> <p>Identify one or more deployments that have missing pods.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiDeploymentUnavailableReplicas/#mitigation","title":"Mitigation","text":"<p>Run: <pre><code>kubectl describe deployment X -n runai/runai-backend \nkubectl describe replicaset X -n runai/runai-backend \nkubectl logs deployment/X -n runai/runai-backend\n</code></pre></p> <p>From the logs and deployment details, try and figure out why the deployment cannot create pods. </p> <p>If you cannot correct the issue, contact Run:ai support.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiProjectControllerReconcileFailure/","title":"RunaiProjectControllerReconcileFailure","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiProjectControllerReconcileFailure/#meaning","title":"Meaning","text":"<p>The <code>project-controller</code> deployment in the \u2018runai\u2019 namespace had errors while reconciling projects.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiProjectControllerReconcileFailure/#impact","title":"Impact","text":"<p>Some of the projects may not be in a Ready state.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiProjectControllerReconcileFailure/#severity","title":"Severity","text":"<p>Critical</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiProjectControllerReconcileFailure/#diagnosis","title":"Diagnosis","text":"<p>Run <code>kubectl logs deployment/project-controller -n runai</code> to try and find errors in the logs.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiProjectControllerReconcileFailure/#mitigation","title":"Mitigation","text":"<p>The error in the logs should be descriptive and help to understand what is wrong.</p> <p>If you see which of the projects failed to reconcile, you can check their status by running <code>kubectl get project &lt;PROJECT_NAME&gt; -oyaml</code>. The status describes what the issue may be. </p> <p>If you cannot correct the issue, contact Run:ai support.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiStatefulSetInsufficientReplicas/","title":"RunaiStatefulSetInsufficientReplicas","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiStatefulSetInsufficientReplicas/#meaning","title":"Meaning","text":"<p>A Run:ai <code>statefulset</code> has no available pods.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiStatefulSetInsufficientReplicas/#impact","title":"Impact","text":"<p>There are no metrics avaliable.</p> <p>There is no database.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiStatefulSetInsufficientReplicas/#severity","title":"Severity","text":"<p>Critical</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiStatefulSetInsufficientReplicas/#diagnosis","title":"Diagnosis","text":"<p>Run <code>kubectl get statefulset -n runai-backend</code> to identify the one or more stateful sets with no running pods.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiStatefulSetInsufficientReplicas/#mitigation","title":"Mitigation","text":"<p>Run <code>kubectl describe statefulset X -n runai-backend</code> </p> <p>From the stateful set details, try to figure out why the it cannot create pods. </p> <p>If you cannot correct the issue, contact Run:ai support.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiStatefulSetNoAvailableReplicas/","title":"RunaiStatefulSetNoAvailableReplicas","text":""},{"location":"admin/troubleshooting/alertmanager/RunaiStatefulSetNoAvailableReplicas/#meaning","title":"Meaning","text":"<p>A Run:ai <code>statefulset</code> has no available pods.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiStatefulSetNoAvailableReplicas/#impact","title":"Impact","text":"<p>There are no metrics avaliable.</p> <p>There is no database.</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiStatefulSetNoAvailableReplicas/#severity","title":"Severity","text":"<p>Critical</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiStatefulSetNoAvailableReplicas/#diagnosis","title":"Diagnosis","text":"<p>Run <code>kubectl get statefulset -n runai-backend</code> to identify the one or more stateful sets with no running pods..</p>"},{"location":"admin/troubleshooting/alertmanager/RunaiStatefulSetNoAvailableReplicas/#mitigation","title":"Mitigation","text":"<p>Run  <code>kubectl describe statefulset X -n runai-backend</code> </p> <p>From the stateful set details, try to figure out why the it cannot create pods. </p> <p>If you cannot correct the issue, contact Run:ai support.</p>"},{"location":"admin/workloads/","title":"Workloads Overview","text":"<p>Runai is an open platform and supports three types of workloads each with a different set of features:</p> <ul> <li>Run:ai native workloads.</li> <li>Third party integrations.</li> <li>Typical Kubernetes workloads.</li> </ul>"},{"location":"admin/workloads/#runai-native-workloads","title":"Run:ai native workloads","text":"<p>Run:ai native workloads are workloads (trainings, workspaces, deployments) that are fully controlled by Run:ai. Run: workloads are the most comprehensive and include Third party integrations and Typical Kubernetes workload types. Specific characteristics of Run: ai native workloads include:</p> <ol> <li>Submitting of workloads via UI/CLI.</li> <li>Workload control (delete/stop/connect).</li> <li>Workload policies (default rules for all policies, specific workload policies, and enforcing of those rules).</li> <li>Scheduling rules.</li> <li>Role based access control.</li> </ol>"},{"location":"admin/workloads/#third-party-integrations","title":"Third party integrations","text":"<p>Third party integrations are tools that Run:ai supports and manages. These are tools that are typically used to build workloads for specific purposes. Third party integrations also include Typical Kubernetes workloads. Specific characteristics of third party tool support include:</p> <ol> <li>Smart gang scheduling (workload aware).</li> <li>Specific workload aware visibility so that different kinds of pods are identified as a single workload (for example, GPU Utilization, workload view, dashboards).</li> </ol> <p>For more information, see Supported integrations.</p>"},{"location":"admin/workloads/#typical-kubernetes-workloads","title":"Typical Kubernetes workloads","text":"<p>Typical Kubernetes workloads are any kind of workload built for Kubernetes. The Run:ai platform allows you to submit standard Kubernetes CRDs. Specific characteristics of Typical Kubernetes workloads that Run:ai can manage include:</p> <ol> <li>Fairness</li> <li>Nodepools</li> <li>Bin packing/spread</li> <li>Fractions</li> <li>Overprovisioning</li> </ol>"},{"location":"admin/workloads/#workloads-view","title":"Workloads View","text":"<p>Run:ai makes it easy to run machine learning workloads effectively on Kubernetes. Run:ai provides both a UI and API interface that introduces a simple and more efficient way to manage machine learning workloads, which will appeal to data scientists and engineers alike.</p> <p>The Workloads table provides:</p> <ul> <li>Changing of the layout of the Workloads table by pressing Columns to add or remove columns from the table.</li> <li>Download the table to a CSV file by pressing More, then pressing Download as CSV.</li> <li>Search for a workload by pressing Search and entering the name of the workload.</li> <li>Advanced workload management.</li> <li>Added workload statuses for better tracking of workload flow.</li> </ul> <p>To create new workloads, press New Workload.</p>"},{"location":"admin/workloads/#api-documentation","title":"API Documentation","text":"<p>Access the platform API documentation for more information on using the API to manage workloads.</p>"},{"location":"admin/workloads/#managing-workloads","title":"Managing Workloads","text":"<p>You can manage a workload by selecting one from the view. Once selected, you can:</p> <ul> <li>Run a workload.</li> <li>Stop a workload.</li> <li>Connect to a workload\u2014provides a connection to the selected workload's designated tool. Press the item in the column to show the connection URL.</li> <li>Delete a workload.</li> <li> <p>Copy and edit a workload\u2014use this function to run another workload based on the selected workload.</p> <ul> <li>If the workload was submitted using the UI, then a copy of the original workload form will open allowing you to make changes to the workload properties.</li> <li>If the workload was submitted using the CLI, then a window shows with the original CLI command. Copy the command and make changes to the submission.</li> </ul> </li> <li> <p>Show details\u2014provides in-depth information about the selected workload including:</p> <ul> <li>Event history\u2014workload status over time. Use the filter to search through the history for specific events.</li> <li> <p>Metrics\u2014use the drop down to filter metrics per pod. Select a category from the list below:</p> <ul> <li>GPU compute utilization\u2014hover over for individual GPU details</li> <li>GPU memory usage\u2014hover over for individual GPU details</li> <li>CPU usage\u2014hover over for usage details</li> <li>CPU memory usage\u2014hover over for usage details</li> </ul> </li> <li> <p>Logs\u2014logs of the selected workload. Use the drop down to filter metrics per pod. Use the Download button to download the logs.</p> </li> </ul> </li> </ul>"},{"location":"admin/workloads/#workloads-status","title":"Workloads Status","text":"<p>The Status column shows the current status of the workload. The following table describes the statuses presented:</p> Phase Name Description Entry Condition Exit Condition Creating Workload setup is initiated in the cluster. Resources and pods are now provisioning. A workload is submitted. A multi-pod group is created. Pending Workload is queued and awaiting resource allocation. A pod group exists. All pods are scheduled. Initializing Workload is retrieving images, starting containers, and preparing pods. All pods are scheduled\u2014handling of multi-pod groups TBD. All pods are initialized or a failure to initialize is detected. Running Workload is currently in progress with all pods operational. All pods initialized (all containers in pods are ready). Job completion or failure. Degraded Pods may not align with specifications, network services might be incomplete, or persistent volumes may be detached. Check your logs for specific details. Pending\u2014All pods are running but with issues.  Running\u2014All pods are running with no issues. Running\u2014All resources are OK. Completed\u2014 Job finished with fewer resources.Failed\u2014Job failure or user-defined rules. Deleting Workload and its associated resources are being decommissioned from the cluster. Deleting of the workload. Resources are fully deleted. Stopped Workload is on hold and resources are intact but inactive. Stopping the workload without deleting resources. Transitioning back to the initializing phase or proceeded to deleting the workload. Failed Image retrieval failed or containers experienced a crash. Check your logs for specific details. An error occurs preventing the successful completion of the job. Terminal state. Completed Workload has successfully finished its execution. The job has finished processing without errors. Terminal state."},{"location":"admin/workloads/#successful-flow","title":"Successful flow","text":"<p>A successful flow will follow the following flow chart:</p> <pre><code>flowchart LR\n A(Creating) --&gt; B(Pending)\n B--&gt;C(Initializing)\n C--&gt;D(Running)\n D--&gt;E(Completed)</code></pre> <p>To get the full experience of Run:ai\u2019s environment and platform use the following types of workloads.</p> <ul> <li>Workspaces</li> <li>Trainings (Only available when using the Jobs view)</li> <li>Distributed trainings</li> <li>Deployment</li> </ul>"},{"location":"admin/workloads/#supported-integrations","title":"Supported integrations","text":"<p>To assist you with other platforms, and other types of workloads use the integrations listed below.</p> <ol> <li>Airflow</li> <li>MLflow</li> <li>Kubeflow</li> <li>Seldon Core</li> <li>Spark</li> <li>Ray</li> <li>KubeVirt (VM)</li> </ol>"},{"location":"admin/workloads/inference-overview/","title":"Inference overview","text":""},{"location":"admin/workloads/inference-overview/#what-is-inference","title":"What is Inference","text":"<p>Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output.</p> <p>With Inference workloads, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time.</p>"},{"location":"admin/workloads/inference-overview/#inference-and-gpus","title":"Inference and GPUs","text":"<p>The Inference process is a subset of the original Training algorithm on a single datum (e.g. one sentence or one image), or a small batch. As such, GPU memory requirements are typically smaller than a full-blown Training process.</p> <p>Given that, Inference lends itself nicely to the usage of Run:ai Fractions. You can, for example, run 4 instances of an Inference server on a single GPU, each employing a fourth of the memory.</p>"},{"location":"admin/workloads/inference-overview/#inference-runai","title":"Inference @Run:ai","text":"<p>Run:ai provides Inference services as an equal part together with the other two Workload types: Train and Build.</p> <ul> <li> <p>Inference is considered a high-priority workload as it is customer-facing. Running an Inference workload (within the Project's quota) will preempt any Run:ai Workload marked as Training.</p> </li> <li> <p>Inference workloads will receive priority over Train and Build workloads during scheduling.</p> </li> <li> <p>Inference is implemented as a Kubernetes Deployment object with a defined number of replicas. The replicas are load-balanced by Kubernetes so adding more replicas will improve the overall throughput of the system.</p> </li> <li> <p>Multiple replicas will appear in Run:ai as a single Inference workload. The workload will appear in all Run:ai dashboards and views as well as the Command-line interface.</p> </li> <li> <p>Inference workloads can be submitted via Run:ai user interface as well as Run:ai API. Internally, spawning an Inference workload also creates a Kubernetes Service. The service is an end-point to which clients can connect.</p> </li> </ul>"},{"location":"admin/workloads/inference-overview/#autoscaling","title":"Autoscaling","text":"<p>To withstand SLA, Inference workloads are typically set with auto scaling. Auto-scaling is the ability to add more computing power (Kubernetes pods) when the load increases and shrink allocated resources when the system is idle.</p> <p>There are a number of ways to trigger autoscaling. Run:ai supports the following:</p> Metric Units Run:ai name Throughput requests/second throughput Concurrency concurrency <p>The Minimum and Maximum number of replicas can be configured as part of the autoscaling configuration.</p> <p>Autoscaling also supports a scale to zero policy with Throughput and Concurrency metrics, meaning that given enough time under the target threshold, the number of replicas will be scaled down to 0.</p> <p>This has the benefit of conserving resources at the risk of a delay from \"cold starting\" the model when traffic resumes.</p>"},{"location":"admin/workloads/inference-overview/#see-also","title":"See Also","text":"<ul> <li>To set up Inference, see Cluster installation prerequisites.</li> <li>For running Inference see Inference quick-start.</li> <li>To run Inference using API see Workload overview.</li> </ul>"},{"location":"admin/workloads/secrets/","title":"Secrets in Workloads","text":""},{"location":"admin/workloads/secrets/#kubernetes-secrets","title":"Kubernetes Secrets","text":"<p>Sometimes you want to use sensitive information within your code. For example passwords, OAuth tokens, or ssh keys. The best practice for saving such information in Kubernetes is via Kubernetes Secrets. Kubernetes Secrets let you store and manage sensitive information. Access to secrets is limited via configuration.</p> <p>A Kubernetes secret may hold multiple key - value pairs.</p>"},{"location":"admin/workloads/secrets/#using-secrets-in-runai-workloads","title":"Using Secrets in Run:ai Workloads","text":"<p>Our goal is to provide Run:ai Workloads with secrets as input in a secure way. Using the Run:ai command line, you will be able to pass a reference to a secret that already exists in Kubernetes.</p>"},{"location":"admin/workloads/secrets/#creating-a-secret","title":"Creating a secret","text":"<p>For details on how to create a Kubernetes secret see: https://kubernetes.io/docs/concepts/configuration/secret/. Here is an example:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\n  namespace: runai-&lt;project-name&gt;\ndata:\n  username: am9obgo=\n  password: bXktcGFzc3dvcmQK\n</code></pre> <p>Then run: <pre><code>kubectl apply -f &lt;file-name&gt;\n</code></pre></p> <p>Notes</p> <ul> <li>Secrets are base64 encoded</li> <li>Secrets are stored in the scope of a namespace and will not be accessible from other namespaces. Hence the reference to the Run:ai Project name above. Run:ai provides the ability to propagate secrets throughout all Run:ai Projects. See below.</li> </ul>"},{"location":"admin/workloads/secrets/#attaching-a-secret-to-a-workload-on-submit","title":"Attaching a secret to a Workload on Submit","text":"<p>When you submit a new Workload, you will want to connect the secret to the new Workload. To do that, run:</p> <pre><code>runai submit -e &lt;ENV-VARIABLE&gt;=SECRET:&lt;secret-name&gt;,&lt;secret-key&gt; ....\n</code></pre> <p>For example:</p> <pre><code>runai submit -i ubuntu -e MYUSERNAME=SECRET:my-secret,username\n</code></pre>"},{"location":"admin/workloads/secrets/#secrets-and-projects","title":"Secrets and Projects","text":"<p>As per the note above, secrets are namespace-specific. If your secret relates to all Run:ai Projects, do the following to propagate the secret to all Projects:</p> <ul> <li>Create a secret within the <code>runai</code> namespace.</li> <li>Run the following once to allow Run:ai to propagate the secret to all Run:ai Projects:</li> </ul> <pre><code>runai-adm set secret &lt;secret name&gt; --cluster-wide\n</code></pre> <p>Reminder</p> <p>The Run:ai Administrator CLI can be obtained here.</p> <p>To delete a secret from all Run:ai Projects, run:</p> <pre><code>runai-adm remove secret &lt;secret name&gt; --cluster-wide\n</code></pre>"},{"location":"admin/workloads/secrets/#secrets-and-policies","title":"Secrets and Policies","text":"<p>A Secret can be set at the policy level. For additional information see policies guide.</p>"},{"location":"admin/workloads/submitting-workloads/","title":"Submitting Workloads","text":""},{"location":"admin/workloads/submitting-workloads/#how-to-submit-a-workload","title":"How to Submit a Workload","text":"<p>To submit a workload using the UI:</p> <ol> <li>In the left menu press Workloads.</li> <li>Press New Workload, and select Workspace, Training, or Inference.</li> </ol> WorkspaceTrainingInference <ol> <li>In the Projects pane, select a project. Use the search box to find projects that are not listed. If you can't find the project, see your system administrator.</li> <li>In the Templates pane, select a template from the list. Use the search box to find templates that are not listed. If you can't find the specific template you need, create a new one, or see your system administrator.</li> <li>Enter a <code>Workspace</code> name, and press continue.</li> <li>In the Environment pane select or create a new environment. Use the search box to find environments that are not listed.</li> <li>In the Compute resource pane, select resources for your trainings or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> <li> <p>Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a *Volume persistency.</li> </ol> </li> <li> <p>In the Data sources pane, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> <p>Note</p> <p>Data sources that have private credentials that have the status of issues found will be greyed out.</p> </li> <li> <p>In the General pane, add special settings for your training (optional):</p> <ol> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minuets, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails.</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>When complete, press *Create workspace.</p> </li> </ol> <ol> <li>In the Projects pane, select the destination project. Use the search box to find projects that are not listed. If you can't find the project, you can create your own, or see your system administrator.</li> <li>In the Multi-node pane, choose <code>Single node</code> for a single node training, or <code>Multi-node (distributed)</code> for distributed training. When you choose <code>Multi-node</code>, select a framework that is listed, then select the <code>multi-node</code> training configuration by selecting either <code>Workers &amp; master</code> or <code>Workers only</code>.</li> <li>In the Templates pane, select a template from the list. Use the search box to find templates that are not listed. If you can't find the specific template you need, see your system administrator.</li> <li>In the Training name pane, enter a name for the Training, then press continue.</li> <li>In the Environment pane select or create a new environment. Use the search box to find environments that are not listed. Press More settings to add an <code>Environment variable</code> or to edit the Command and Arguments field for the environment you selected.</li> <li> <p>In the Compute resource pane:</p> <ol> <li>Select the number of workers for your training.</li> <li>Select Compute resources for your training or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> </ol> <p>Note</p> <p>The number of compute resources for the workers is based on the number of workers selected.</p> </li> <li> <p>(Optional) Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a *Volume persistency.</li> </ol> </li> <li> <p>(Optional) In the Data sources pane, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> <p>Note</p> <p>Data sources that have private credentials that have the status of issues found will be greyed out.</p> </li> <li> <p>(Optional) In the General pane, add special settings for your training (optional):</p> <ol> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minuets, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails.</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>If you if selected  <code>Workers &amp; master</code> Press Continue to <code>Configure the master</code> and go to the next step. If not, then press Create training.</p> </li> <li> <p>If you do not want a different setup for the master, press Create training. If you would like to have a different setup for the master, toggle the switch to enable to enable a different setup.</p> <ol> <li>In the Environment pane select or create a new environment. Use the search box to find environments that are not listed. Press More settings to add an <code>Environment variable</code> or to edit the Command and Arguments field for the environment you selected.</li> <li>In the Compute resource pane, select a Compute resources for your training or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> <li> <p>(Optional) Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a *Volume persistency.</li> </ol> </li> <li> <p>(Optional) In the Data sources pane, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> </li> </ol> <p>!!! Note       Data sources that have private credentials that have the status of issues found will be greyed out.</p> <ol> <li> <p>(Optional) In the General pane, add special settings for your training (optional):</p> <ol> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minuets, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails.</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> </ol> </li> <li> <p>When your training configuration is complete. press Create training.</p> </li> </ol> <ol> <li>In the Projects pane, select a project. Use the search box to find projects that are not listed. If you can't find the project, see your system administrator.</li> <li> <p>In the Inference by type pane select Custom or model.</p> <p>When you select Model:</p> <ol> <li>Select a model from the tiles. Use the search box to find a model that is not listed. If you can't find the model, see your system administrator.</li> <li>In the Inference name field, enter a name for the workload.</li> <li>In the Compute resource field, select a compute resource from the tiles.<ol> <li>In the Replica autoscaling section, set the minimum and maximum replicas for your inference. Then select Never or After one minute of inactivity  to set when the replicas should be automatically scaled down to zero.</li> <li>In the Nodes field, change the order of priority of the node pools, or add a new node pool to the list.</li> </ol> </li> <li>When complete, press Create inference.</li> </ol> <p>When you select Custom:</p> <ol> <li>In the Inference name field, enter a name for the workload.</li> <li>In the Environment field, select an environment. Use the search box to find an environment that is not listed. If you can't find an environment, press New environment or see your system administrator. <ol> <li>In the Set the connection for your tool(s) pane, choose a tool for your environment (if available).</li> <li>In the Runtime settings field, Set commands and arguments for the container running in the pod. (optional)</li> <li>In the Environment variable field, you can set one or more environment variables. (optional)</li> </ol> </li> <li>In the Compute resource field, select a compute resource from the tiles. Use the search box to find a compute resource that is not listed. If you can't find an environment, press New compute resource or see your system administrator.<ol> <li>In the Replica autoscaling section, set the minimum and maximum replicas for your inference. Then select Never or After one minute of inactivity  to set when the replicas should be automatically scaled down to zero.</li> <li>In the Nodes field, change the order of priority of the node pools, or add a new node pool to the list.</li> </ol> </li> <li> <p>In the Data sources field, add a New data source. (optional)</p> <p>Note</p> <ul> <li>Data sources that are not available will be greyed out.</li> <li>Assets that are cluster syncing will be greyed out.</li> <li>Only PVC, Git, and ConfigMap resources are supported.</li> </ul> </li> <li> <p>In the General field you can:</p> <ol> <li>Add an Auto-deletion time. This sets the timeframe between inference completion/failure and auto-deletion. (optional)</li> <li>Add one or more Annotation. (optional)</li> <li>Add one or more Labels. (optional)</li> </ol> </li> <li>When complete, press Create inference.</li> </ol> </li> </ol>"},{"location":"admin/workloads/submitting-workloads/#workload-policies","title":"Workload Policies","text":"<p>As an administrator, you can set Policies on Workloads.  Policies allow administrators to impose restrictions and set default values for Researcher Workloads. For more information see Workload Policies.</p>"},{"location":"admin/workloads/workload-overview-admin/","title":"Workloads Overview","text":""},{"location":"admin/workloads/workload-overview-admin/#workloads","title":"Workloads","text":"<p>Run:ai schedules Workloads. Run:ai workloads are comprised of:</p> <ul> <li>The Kubernetes object (Job, Inference, etc) which is used to launch the container, inside which the data science code runs.</li> <li>A set of additional resources that are required to run the Workload. Examples: a service entry point that allows access to the Job, a persistent volume claim to access data on the network, and more.</li> </ul> <p>All of these components are created together and deleted together when the Workload ends.</p> <p>Run:ai currently supports the following Workloads types:</p> Workload Type Kubernetes Name Description Interactive <code>InteractiveWorkload</code> Submit an interactive workload Training <code>TrainingWorkload</code> Submit a training workload Distributed Training <code>DistributedWorkload</code> Submit a distributed training workload using TensorFlow, PyTorch or MPI Inference <code>InferenceWorkload</code> Submit an inference workload"},{"location":"admin/workloads/workload-overview-admin/#workloads-view","title":"Workloads View","text":"<p>The Workloads view provides a more advanced UI than the previous Jobs UI. The new table format provides:</p> <ul> <li>Improved views of the data</li> <li>Improved filters and search</li> <li>More information</li> </ul> <p>To enable the Workloads view, go to the Jobs table and toggle the switch from <code>Jobs</code> to <code>Workloads</code>. To return, switch the toggle from <code>Workloads</code> to <code>Jobs</code>.</p> <p>Use the search feature to find a specific workload in the list.</p> <p>Use the columns button to change the columns that are displayed in the table.</p> <p>Use the More button to download the table and the displayed columns to a CSV file.</p> <p>Manage Workloads by selecting a workload from the table. Once selected, you can:</p> <ul> <li>Delete a workload</li> <li>Connect</li> <li>Stop a workload</li> <li>Activate a workload</li> <li>Show details</li> </ul> <p>The Show details button provides in-depth information about the selected workload including:</p> <ul> <li>Event history\u2014workload status over time. Use the filter to search through the history for specific events.</li> <li>Metrics\u2014metric for GPU utilization, CPU usage, GPU memory usage, and CPU memory usage. Use the date selector to choose the time period for the metrics.</li> <li>Logs\u2014logs of the current status. Use the Download button to download the logs.</li> </ul>"},{"location":"admin/workloads/workload-overview-admin/#values","title":"Values","text":"<p>A Workload will typically have a list of values (sometimes called flags), such as name, image, and resources. A full list of values is available in the runai-submit Command-line reference.</p>"},{"location":"admin/workloads/workload-overview-admin/#how-to-submit","title":"How to Submit","text":"<p>A Workload can be submitted via in the following ways:</p> <ul> <li>The Run:ai user interface.</li> <li>The Run:ai command-line interface, via the runai submit command.</li> <li>The Run:ai Cluster API.</li> <li>The Run:ai Workloads page.</li> </ul>"},{"location":"admin/workloads/workload-overview-admin/#submit-a-workload-using-the-ui","title":"Submit a Workload Using the UI","text":"<p>Important</p> <p>Make sure you have the <code>Workloads</code> view enabled. To enable this view, see Workloads toggle</p> <p>To submit a workload using the UI:</p> <ol> <li>In the left menu press Workloads.</li> <li>Press New Workload, and select <code>Workspace</code> or <code>Training</code>.</li> </ol>"},{"location":"admin/workloads/workload-overview-admin/#for-workspace","title":"For <code>Workspace</code>","text":"<ol> <li>In the Projects pane, select a project. Use the search box to find projects that are not listed. If you can't find the project, see your system administrator.</li> <li>In the Templates pane, select a template from the list. Use the search box to find templates that are not listed. If you can't find the specific template you need, create a new one, or see your system administrator.</li> <li>Enter a <code>Workspace</code> name, and press continue.</li> <li>In the Environment pane select or create a new environment. Use the search box to find environments that are not listed.</li> <li>In the Compute resource pane, select resources for your tranings or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> <li> <p>Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a *Volume persistency.</li> </ol> </li> <li> <p>In the Data sources pane, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> </li> <li> <p>In the General pane, add special settings for your training (optional):</p> <ol> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minuets, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails.</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>When complete, press *Create workspace.</p> </li> </ol>"},{"location":"admin/workloads/workload-overview-admin/#for-training","title":"For <code>Training</code>","text":"<ol> <li>In the Projects pane, select the destination project. Use the search box to find projects that are not listed. If you can't find the project, you can create your own, or see your system administrator.</li> <li>In the Multi-node pane, choose <code>Single node</code> for a single node training, or <code>Multi-node (distributed)</code> for distributed training. When you choose <code>Multi-node</code>, select a framework that is listed, then select the <code>multi-node</code> training configuration by selecting either <code>Workers &amp; master</code> or <code>Workers only</code>.</li> <li>In the Templates pane, select a template from the list. Use the search box to find templates that are not listed. If you can't find the specific template you need, see your system administrator.</li> <li>In the Training name pane, enter a name for the Traninng, then press continue.</li> <li>In the Environment pane select or create a new environment. Use the search box to find environments that are not listed. Press More settings to add an <code>Environment variable</code> or to edit the Command and Arguments field for the environment you selected.</li> <li> <p>In the Compute resource pane:</p> <ol> <li>Select the number of workers for your training.</li> <li>Select Compute resources for your training or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> </ol> <p>Note</p> <p>The number of compute resources for the workers is based on the number of workers selected.</p> </li> <li> <p>(Optional) Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a *Volume persistency.</li> </ol> </li> <li> <p>(Optional) In the Data sources pane, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> </li> <li> <p>(Optional) In the General pane, add special settings for your training (optional):</p> <ol> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minuets, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails.</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> <li> <p>If you if selected  <code>Workers &amp; master</code> Press Continue to <code>Configure the master</code> and go to the next step. If not, then press Create training.</p> </li> <li> <p>If you do not want a different setup for the master, press Create training. If you would like to have a different setup for the master, toggle the switch to enable to enable a different setup.</p> <ol> <li>In the Environment pane select or create a new environment. Use the search box to find environments that are not listed. Press More settings to add an <code>Environment variable</code> or to edit the Command and Arguments field for the environment you selected.</li> <li>In the Compute resource pane, select a Compute resources for your training or create a new compute resource. Use the search box to find resources that are not listed. Press More settings to use Node Affinity to limit the resources to a specific node.</li> <li> <p>(Optional) Open the Volume pane, and press Volume to add a volume to your training.</p> <ol> <li>Select the Storage class from the dropdown.</li> <li>Select the Access mode from the dropdown.</li> <li>Enter a claim size, and select the units.</li> <li>Select a Volume system, mode from the dropdown.</li> <li>Enter the Container path for volume target location.</li> <li>Select a *Volume persistency.</li> </ol> </li> <li> <p>(Optional) In the Data sources pane, press add a new data source. For more information, see Creating a new data source When complete press, Create Data Source.</p> </li> <li> <p>(Optional) In the General pane, add special settings for your training (optional):</p> <ol> <li>Press Auto-deletion to delete the training automatically when it either completes or fails. You can configure the timeframe in days, hours, minuets, and seconds. If the timeframe is set to 0, the training will be deleted immediately after it completes or fails.</li> <li>Press Annotation to a name and value to annotate the training. Repeat this step to add multiple annotations.</li> <li>Press Label to a name and value to label the training. Repeat this step to add multiple labels.</li> </ol> </li> </ol> </li> <li> <p>When your training configuration is complete. press Create training.</p> </li> </ol>"},{"location":"admin/workloads/workload-overview-admin/#workload-policies","title":"Workload Policies","text":"<p>As an administrator, you can set Policies on Workloads.  Policies allow administrators to impose restrictions and set default values for Researcher Workloads. For more information see Workload Policies.</p>"},{"location":"admin/workloads/policies/","title":"Policies","text":""},{"location":"admin/workloads/policies/#introduction","title":"Introduction","text":"<p>Policies allow administrators to impose restrictions and set default values for researcher workloads. Restrictions and default values can be placed on CPUs, GPUs, and other resources or entities. Enabling the New Policy Manager provides information about resources that are non-compliant to applied policies. Resources that are non-compliant will appear greyed out. To see how a resource is not compliant, press on the clipboard icon in the upper right hand corner of the resource.</p> <p>Note</p> <p>Policies from Run:ai versions 2.15 or lower will still work after enabling the New Policy Manager. However, showing non-compliant policy rules will not be available. For more information about policies for version 2.15 or lower, see What are Policies.</p> <p>For example, an administrator can create and apply a policy that will restrict researchers from requesting more than 2 GPUs, or less than 1GB of memory per type of workload.</p> <p>Another example is an administrator who wants to set different amounts of CPU, GPUs and memory for different kinds of workloads. A training workload can have a default of 1 GB of memory, or an interactive workload can have a default amount of GPUs.</p> <p>Policies are created for each Run:ai project (Kubernetes namespace). When a policy is created in the <code>runai</code> namespace, it will take effect when there is no project-specific policy for the workloads of the same kind.</p> <p>In interactive workloads or workspaces, applied policies will only allow researchers access to resources that are permitted in the policy. This can include compute resources as well as node pools and node pool priority.</p> <p>To enable the new Policy Manager:</p> <ol> <li>Enable the Policy Manager feature in the clusters <code>runaiconfig</code>: <pre><code>kubectl patch runaiconfigs runai -n runai --type=merge -p='{\"spec\": {\"global\": {\"controlPlanePoliciesEnabled\": true}}}'\n</code></pre></li> <li>In the UI, Press the Tools and Settings icon, then press General.</li> <li>Toggle the New Policy Manager switch to on.</li> </ol> <p>To return to the previous Policy Manager toggle the switch off.</p>"},{"location":"admin/workloads/policies/#runai-policies-vs-kyverno-policies","title":"Run:ai Policies vs. Kyverno Policies","text":"<p>Kyverno runs as a dynamic admission controller in a Kubernetes cluster. Kyverno receives validating and mutating admission webhook HTTP callbacks from the Kubernetes API server and applies matching policies to return results that enforce admission policies or reject requests. Kyverno policies can match resources using the resource kind, name, label selectors, and much more. For more information, see How Kyverno Works.</p>"},{"location":"admin/workloads/policies/#policy-types","title":"Policy Types","text":"<p>When you configure a policy, you need to specify the workload type. The following workload types are available:</p> <ul> <li>Training\u2014places policy restrictions on trainings.</li> <li>Workspace\u2014places policy restrictions on Workspaces.</li> </ul>"},{"location":"admin/workloads/policies/#policy-inheritance","title":"Policy Inheritance","text":"<p>A policy configured to a specific scope, is applied to all elements in that scope. You can add more policy restrictions to individual elements in the scope in order to override the base policy or add more restrictions.</p>"},{"location":"admin/workloads/policies/#policy-format","title":"Policy Format","text":""},{"location":"admin/workloads/policies/#policy-editor-ui","title":"Policy Editor UI","text":"<p>Policies are added to the system using the policy editor and are written in YAML format. YAML\u2122 is a human-friendly, cross language, Unicode based data serialization language designed around the common native data types of dynamic programming languages. It is useful for programming needs ranging from configuration files to internet messaging to object persistence to data auditing and visualization. For more information, see YAML.org.</p>"},{"location":"admin/workloads/policies/#policy-api","title":"Policy API","text":"<p>Access the Policy API reference to see how to apply Policies in the Run:ai platform.</p>"},{"location":"admin/workloads/policies/#example-policy","title":"Example Policy","text":"<p>The following is an example of a workspace policy you can apply in your platform. Copy the values and paste them into the policy editor in the UI.</p> <pre><code>defaults:\n    environment:\n      allowPrivilegeEscalation: false\n      createHomeDir: true\n      environmentVariables:\n        - name: MY_ENV\n          value: my_value\n    workspace:\n      allowOverQuota: true\nrules:\n    compute:\n      cpuCoreLimit:\n        min: 0\n        max: 9\n        required: true\n      gpuPortionRequest:\n        min: 0\n        max: 10\n    s3:\n      url:\n        options:\n          - displayed: \"https://www.google.com\"\n            value: \"https://www.google.com\"\n          - displayed: \"https://www.yahoo.com\"\n            value: \"https://www.yahoo.com\"\n    environment:\n      imagePullPolicy:\n        options:\n          - displayed: \"Always\"\n            value: \"Always\"\n          - displayed: \"Never\"\n            value: \"Never\"\n        required: true\n      runAsUid:\n        min: 1\n        max: 32700\n      createHomeDir:\n        canEdit: false\n      allowPrivilegeEscalation:\n        canEdit: false\n    workspace:\n      allowOverQuota:\n        canEdit: false\n    imposedAssets:\n      dataSources:\n        nfs:\n          canAdd: false\n</code></pre>"},{"location":"admin/workloads/policies/policies/","title":"Policies","text":""},{"location":"admin/workloads/policies/policies/#what-are-policies","title":"What are Policies?","text":"<p>Policies allow administrators to impose restrictions and set default values for Researcher Workloads. For example:</p> <ol> <li>Restrict researchers from requesting more than 2 GPUs, or less than 1GB of memory for an interactive workload.</li> <li>Set the default memory of each training job to 1GB, or mount a default volume to be used by any submitted Workload.</li> </ol> <p>Policies are stored as Kubernetes custom resources.</p> <p>Policies are specific to Workload type as such there are several kinds of Policies:</p> Workload Type Kubernetes Workload Name Kubernetes Policy Name Interactive <code>InteractiveWorkload</code> <code>InteractivePolicy</code> Training <code>TrainingWorkload</code> <code>TrainingPolicy</code> Distributed Training <code>DistributedWorkload</code> <code>DistributedPolicy</code> Inference <code>InferenceWorkload</code> <code>InferencePolicy</code> <p>A Policy can be created per Run:ai Project (Kubernetes namespace). Additionally, a Policy resource can be created in the <code>runai</code> namespace. This special Policy will take effect when there is no project-specific Policy for the relevant workload kind.</p> <p>When researchers create a new interactive workload or workspace, they see list of available node pools and their priority. Priority is set by dragging and dropping the node pools in the desired order of priority. When the node pool priority list is locked by an administrator policy, the node pool list isn't editable by the Researcher even if the workspace is created from a template or copied from another workspace.</p> <p>Note</p> <p>Policies on this page cannot be added to platform 2.16 or higher that have the New Policy Manager enabled.</p>"},{"location":"admin/workloads/policies/policies/#creating-a-policy","title":"Creating a Policy","text":""},{"location":"admin/workloads/policies/policies/#creating-your-first-policy","title":"Creating your First Policy","text":"<p>To create a sample <code>InteractivePolicy</code>, prepare a file (e.g. <code>policy.yaml</code>) containing the following YAML:</p> gpupolicy.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: InteractivePolicy\nmetadata:\n  name: interactive-policy1\n  namespace: runai-team-a # (1)\nspec:\n  gpu:\n    rules:\n      required: true\n      min: \"1\"  # (2)\n      max: \"4\"  \n    value: \"1\"\n</code></pre> <ol> <li>Set the Project namespace here.</li> <li>GPU values are quoted as they can contain non-integer values.</li> </ol> <p>The policy places a default and limit on the available values for GPU allocation. To apply this policy, run:</p> <pre><code>kubectl apply -f gpupolicy.yaml \n</code></pre> <p>Now, try the following command:</p> <pre><code>runai submit --gpu 5 --interactive -p team-a\n</code></pre> <p>The following message will appear:</p> <pre><code>gpu: must be no greater than 4\n</code></pre> <p>A similar message will appear in the New Job form of the Run:ai user interface, when attempting to enter the number of GPUs, which is out of range for a training job.</p>"},{"location":"admin/workloads/policies/policies/#gpu-and-cpu-memory-limits","title":"GPU and CPU memory limits","text":"<p>The following policy places a default and limit on the available values for CPU and GPU memory allocation.</p> gpumemorypolicy.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: training-policy\n  namespace: runai\nspec:\n  gpuMemory:\n    rules:\n      min: 100M\n      max: 2G\nmemory:\n    rules:\n      min: 100M\n      max: 2G\n</code></pre>"},{"location":"admin/workloads/policies/policies/#read-only-values","title":"Read-only values","text":"<p>When you do not want the user to be able to change a value, you can force the corresponding user interface control to become read-only by using the <code>canEdit</code> key. For example,</p> runasuserpolicy.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: train-policy1\n  namespace: runai-team-a # (1) \n\nspec:\n  runAsUser:\n    rules:\n      required: true  # (2)\n      canEdit: false  # (3)\n    value: true # (4)\n</code></pre> <ol> <li>Set the Project namespace here.</li> <li>The field is required.</li> <li>The field will be shown as read-only in the user interface.</li> <li>The field value is true.  </li> </ol>"},{"location":"admin/workloads/policies/policies/#complex-values","title":"Complex Values","text":"<p>The example above illustrated rules for parameters of \"primitive\" types, such as GPU allocation, CPU memory, working directory, etc. These parameters contain a single value.</p> <p>Other workload parameters, such as ports or volumes, are \"complex\", in the sense that they may contain multiple values: a workload may contain multiple ports and multiple volumes.</p> <p>The following is an example of a policy containing the value <code>ports</code>, which is complex: The <code>ports</code> flag typically contains two values: The <code>external</code> port that is mapped to an internal <code>container</code> port. One can have multiple port tuples defined for a single Workload:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: InteractivePolicy\nmetadata:\n  name: interactive-policy\n  namespace: runai\nspec:\n  ports:\n    rules:\n      canAdd: true\n    itemRules:\n      container:\n        min: 30000\n        max: 32767\n      external:\n        max: 32767\n    items:\n      admin-port-a:\n        rules:\n          canRemove: false\n          canEdit: false\n        value:\n          container: 30100\n          external: 8080\n      admin-port-b:\n        value:\n          container: 30101\n          external: 8081\n</code></pre> <p>A policy for a complex field is composed of three parts:</p> <ul> <li>Rules: Rules apply to the <code>ports</code> parameter as a whole. In this example, the administrator specifies <code>canAdd</code> rule with <code>true</code> value, indicating that a researcher submitting an interactive job can add additional ports to the ports listed by the policy (true is the default for <code>canAdd</code>, so it actually could have been omitted from the policy above). When <code>canAdd</code> is set to <code>false</code>, the researcher will not be able to add any additional port except those already specified by the policy.</li> <li>itemRules: itemRules impose restrictions on the data members of each item, in this case - <code>container</code> and <code>external</code>. In the above example, the administrator has limited the value of <code>container</code> to 30000-32767, and the value of <code>external</code> to a maximum of 32767.</li> <li>Items: Specifies a list of default ports. Each port is an item in the ports list and given a label (e.g. <code>admin-port-b</code>). The administrator can also specify whether a researcher can change/delete ports from the submitted workload. In the above example, <code>admin-port-a</code> is hardwired and cannot be changed or deleted, while <code>admin-port-b</code> can be changed or deleted by the researcher when submitting the Workload. It is possible to specify a label using the reserved name of <code>DEFAULTS</code>. This item provides the defaults for all other items.</li> </ul> <p>The following is an example of a complex policy for PVCs which contains <code>DEFAULTS</code>.</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: tp # use your name.\n  namespace: runai-team-a # use your namespace\nspec:\n  pvcs:\n    itemRules:\n      existingPvc:\n        canEdit: false\n      claimName:\n        required: true\n    items:\n      DEFAULTS:\n        value:\n          existingPvc: true\n          path: nil\n</code></pre>"},{"location":"admin/workloads/policies/policies/#syntax","title":"Syntax","text":"<p>The complete syntax of the policy YAML can be obtained using the <code>explain</code> command of kubectl. For example:</p> <p><pre><code>kubectl explain trainingpolicy.spec\n</code></pre> Should provide the list of all possible fields in the spec of training policies:</p> <pre><code>KIND:     TrainingPolicy\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: spec &lt;Object&gt;\n\nDESCRIPTION:\nThe specifications of this TrainingPolicy\n\nFIELDS:\nannotations &lt;Object&gt;\nSpecifies annotations to be set in the container running the created\nworkload.\n\narguments   &lt;Object&gt;\nIf set, the arguments are sent along with the command which overrides the\nimage's entry point of the created workload.\n\ncommand &lt;Object&gt;\nIf set, overrides the image's entry point with the supplied command.\n...\n</code></pre> <p>You can further drill down to get the syntax for <code>ports</code> by running:</p> <pre><code>kubectl explain trainingpolicy.spec.ports\n</code></pre> <pre><code>KIND:     TrainingPolicy\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: ports &lt;Object&gt;\n\nDESCRIPTION:\n     Specify the set of ports exposed from the container running the created\n     workload. Used together with --service-type.\n\nFIELDS:\n   itemRules    &lt;Object&gt;\n\n   items    &lt;map[string]Object&gt;\n\n   rules    &lt;Object&gt;\n     these rules apply to a value of type map (=non primitive) as a whole\n     additionally there are rules which apply for specific items of the map\n</code></pre> <p>Drill down into the <code>ports.rules</code> object by running:</p> <pre><code>kubectl explain trainingpolicy.spec.ports.rules\n</code></pre> <pre><code>KIND:     TrainingPolicy\nVERSION:  run.ai/\n\nRESOURCE: rules &lt;Object&gt;\n\nDESCRIPTION:\n     these rules apply to a value of type map (=non primitive) as a whole\n     additionally there are rules which apply for specific items of the map\n\nFIELDS:\n   canAdd   &lt;boolean&gt;\n     is it allowed for a workload to add items to this map\n\n   required &lt;boolean&gt;\n     if the map as a whole is required\n</code></pre> <p>Note that each kind of policy has a slightly different set of parameters. For example, an <code>InteractivePolicy</code> has a <code>jupyter</code> parameter that is not available under <code>TrainingPolicy</code>.</p>"},{"location":"admin/workloads/policies/policies/#using-secrets-for-environment-variables","title":"Using Secrets for Environment Variables","text":"<p>It is possible to add values from Kubernetes secrets as the value of environment variables included in the policy. The secret will be extracted from the secret object when the Job is created. For example:</p> <pre><code>  environment:\n    items:\n      MYPASSWORD:\n        value: \"SECRET:my-secret,password\"\n</code></pre> <p>When submitting a workload that is affected by this policy, the created container will have an environment variable called <code>MYPASSWORD</code> whose value is the key <code>password</code> residing in Kubernetes secret <code>my-secret</code> which has been pre-created in the namespace where the workload runs.</p> <p>Note</p> <p>Run:ai provides a secret propagation mechanism from the <code>runai</code> namespace to all project namespaces. For further information see secret propagation.</p>"},{"location":"admin/workloads/policies/policies/#prevent-data-storage-on-the-node","title":"Prevent Data Storage on the Node","text":"<p>You can configure policies to prevent the submission of workloads that use data sources that consist of a host path. This setting prevents data from being stored on the node so that in the event when a node is deleted, all data stored on that node is lost.</p> <p>Example for rejecting workloads requesting host path:</p> <pre><code>spec:\n  volumes:\n    itemRules:\n      nfsServer:\n        required: true\n</code></pre>"},{"location":"admin/workloads/policies/policies/#terminate-runai-training-jobs-after-preemption-policy","title":"Terminate Run:ai training Jobs after preemption policy","text":"<p>Administrators can set a \u2018termination after preemption\u2019 policy to Run:ai training jobs. After applying this policy, a training job will be terminated once it has been preempted from any reason. For example, a training job that is using over-quota resources (e.g. GPUs) and the owner of those GPUs wants to reclaim them back, the Training job is preempted and typically goes back to the pending queue. However, if the termination policy is applied, the job is terminated instead of reinstated as pending. The Termination after Preemption Policy can be set as a cluster-wide policy (applicable to all namespaces/projects) or per project/namespace.</p> <p>To use this feature the administrator should configure either a cluster wide or namespace policy.</p> <p>For cluster wide (all namespaces/projects) use this YAML based policy:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: training-policy\n  namespace: runai\nspec:\n  terminateAfterPreemption:\n    value: true\n</code></pre> <p>For per namespace (project) use this YAML based policy:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\n  name: training-policy\n  namespace: runai-&lt;PROJECT_NAME&gt;\nspec:\n  terminateAfterPreemption:\n    value: false\n</code></pre>"},{"location":"admin/workloads/policies/policies/#modifyingdeleting-policies","title":"Modifying/Deleting Policies","text":"<p>Use the standard kubectl get/apply/delete commands to modify and delete policies.</p> <p>For example, to view the global interactive policy:</p> <pre><code>kubectl get interactivepolicies -n runai\n</code></pre> <p>Should return the following:</p> <pre><code>NAME                 AGE\ninteractive-policy   2d3h\n</code></pre> <p>To delete this policy:</p> <pre><code>kubectl delete InteractivePolicy interactive-policy -n runai\n</code></pre> <p>To access project-specific policies, replace the <code>-n runai</code> parameter with the namespace of the relevant project.</p>"},{"location":"admin/workloads/policies/policies/#see-also","title":"See Also","text":"<ul> <li>For creating workloads based on policies, see the Run:ai submitting workloads</li> </ul>"},{"location":"admin/workloads/policies/training-policy/","title":"Training Policy","text":"<p>A Training policy places resource restrictions and defaults on training worloads in the Run:ai platform. Restrictions and default values can be placed on CPUs, GPUs, and other resources or entities.</p>"},{"location":"admin/workloads/policies/training-policy/#example","title":"Example","text":"<p>Below is an example policy you can use in your platform.</p> <p>Note</p> <ul> <li>Not all the configurable fields available are listed in the example below. </li> <li>Replace the values listed in the example below with values that match your platform requirements.</li> </ul> <pre><code># insert example here\n</code></pre>"},{"location":"admin/workloads/policies/training-policy/#viewing-or-edit-a-policy","title":"Viewing or Edit a Policy","text":"<p>To view or edit a policy:</p> <ol> <li>Press Tools and Settings.</li> <li>Press Policies. The policy grid is displayed.</li> <li>Select a policy from the list. If there are no policies, then create a new policy.</li> <li>Pres Edit to view the policy details, then press Edit Policy to edit the YAML file.</li> <li>When done, press Apply.</li> </ol>"},{"location":"admin/workloads/policies/training-policy/#creating-a-new-policy","title":"Creating a New Policy","text":"<p>To create a policy:</p> <ol> <li>Press Tools and Settings.</li> <li>Press Policies. The policy grid is displayed.</li> <li>Press New Policy.</li> <li>Select a scope for the policy.</li> <li>Select a workload type using the dropdown.</li> <li>In the Policy YAML pane, press + POLICY YAML to open the policy editor.</li> <li>Enter your policy in the policy editor. Add policy properties and variables in YAML format. When complete, press APPLY.</li> <li>When done, press SAVE POLICY.</li> </ol> <p>Note</p> <p>After saving, the form will wait for the policy to sync with the cluster.</p>"},{"location":"admin/workloads/policies/training-policy/#configurable-fields","title":"Configurable Fields","text":"<p>The following parameters can be configured in the policy manager.</p> <p>Note</p> <p>In the tables below, when a Type has <code>null</code> as an option, you can choose to either not use the Field or use the value <code>null</code> in the policy YAML.</p>"},{"location":"admin/workloads/policies/training-policy/#defaults","title":"Defaults","text":"<p>The <code>defaults</code> section of the policy file is...</p> Field Type Description <code>environment</code> <code>object</code> or <code>null</code> Environment fields that can be overridden when creating a workload. <code>compute</code> <code>object</code> or <code>null</code> Compute resources requested. <code>hostPath</code> <code>object</code> or <code>null</code> Volumes resource definitions. <code>nfs</code> <code>object</code> or <code>null</code> NFS volume definitions. <code>pvc</code> <code>object</code> or <code>null</code> PVC definitions. <code>git</code> <code>object</code> or <code>null</code> Git repository definitions. <code>s3</code> <code>object</code> or <code>null</code> S3 resource definitions. <code>configmap</code> <code>object</code> or <code>null</code> ConfigMap definitions. <code>imposedAssets</code> <code>object</code> or <code>null</code> A list of asset to be imposed on the workloads created in org units affected by this policy."},{"location":"admin/workloads/policies/training-policy/#environment-fields","title":"Environment Fields","text":"Field Type Description <code>command</code> <code>string</code> or <code>null</code> (non-empty) A command sent to the server used as the entry point of the container running the workspace. <code>args</code> <code>string</code> or <code>null</code> (non-empty) Arguments applied to the command that the container running the workspace executes. <code>environmentVariables</code> <code>array of objects</code> or <code>null</code> or <code>null</code> An array of environment variables to populate into the container running the workspace. <code>runAsUid</code> <code>integer</code>  or <code>null</code> The userid to run the entrypoint of the container. Default to the (optional) value specified in the environment asset <code>runAsUid</code> field. Can be provided only when the source uid/gid of the environment asset is not <code>fromTheImage</code>, and <code>overrideUidGidInWorkspace</code> is enabled. <code>runAsGid</code> <code>integer</code>  or <code>null</code> \u00a0The group id to run the entrypoint of the container. Default to the (optional) value specified in the environment asset runAsGid field. Can be provided only when the source uid/gid of the environment asset is not <code>fromTheImage</code>, and <code>overrideUidGidInWorkspace</code> is enabled. <code>supplementalGroups</code> <code>string</code> or <code>null</code> Comma seperated list of groups that the user running the container belongs to, in addition to the group indicated by <code>runAsGid</code>. Can be provided only when the source uid/gid of the environment asset is not <code>fromTheImage</code>, and <code>overrideUidGidInWorkspace</code> is enabled. Empty string implies reverting to the supplementary groups of the image. <code>image</code> <code>string</code> or <code>null</code> (non-empty) Docker image name.\u00a0Image name is mandatory for creating a workspace. See Images <code>imagePullPolicy</code> <code>string</code> or <code>null</code>\u00a0(non-empty) Image pull policy.  Select from:\u00a0<code>Always</code>,\u00a0<code>Never</code>, or\u00a0<code>IfNotPresent</code>. Defaults to Always if <code>latest tag</code> is specified, or <code>IfNotPresent</code> otherwise. <code>workingDir</code> <code>string</code> or <code>null</code> (non-empty) The container's working directory. If not specified, the container runtime default will be used, which might be configured in the container image. <code>hostIpc</code> <code>boolean</code> or <code>null</code> Enable host IPC. Defaults to <code>false</code>. <code>hostNetwork</code> <code>boolean</code> or <code>null</code> Enable host networking. Default to <code>false</code>. <code>connections</code> <code>array of\u00a0objects</code> List of connections that either expose ports from the container (each port is associated with a tool that the container runs), or URL's to be used for connecting to an external tool that is related to the action of the container (such as Weights &amp; Biases). <code>createHomeDir</code> <code>boolean</code> or <code>null</code> Create a home directory for the container. <code>allowPrivilegeEscalation</code> <code>boolean</code> or <code>null</code> Allow the container running the workload and all launched processes to gain additional privileges after the workload starts. For more information, see User Identity in Container. <code>uidGidSource</code> <code>string</code> or <code>null</code> Indicate the way to determine the user and group ids of the container. Choose from:  <code>fromTheImage</code>\u2014user and group ids are determined by the docker image that the container runs (Default). <code>custom</code>\u2014user and group ids can be specified in the environment asset and/or the workspace creation request.  <code>idpToken</code>\u2014user and group ids are determined according to the identity provider (idp) access token. This option is intended for internal use of the environment UI form. For more information see User Identity guide. <code>overrideUidGidInWorkspace</code> <code>boolean</code> Allow specifying uid/gid as part of create workspace. This is relevant only for custom uigGidSource. Default:\u00a0false <code>capabilities</code> <code>array of\u00a0strings</code> or <code>null</code> The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime. Choose from: <code>AUDIT CONTROL</code>, <code>AUDIT READ</code>, <code>AUDIT WRITE</code>, <code>BLOCK SUSPEND</code>, <code>CHOWN</code>, <code>DAC OVERRIDE</code>, <code>DAC READ SEARCH</code>, <code>FOWNER</code>, <code>FSETID</code>, <code>IPC LOCK</code>, <code>IPC OWNER</code>, <code>KILL</code>, <code>LEASE</code>, <code>LINUX IMMUTABLE</code>, <code>MAC ADMIN</code>, <code>MAC OVERRIDE</code>, <code>MKNOD</code>, <code>NET ADMIN</code>, <code>NET BIND SERVICE</code>, <code>NET BROADCAST</code>, <code>NET RAW</code>, <code>SETGID</code>, <code>SETFCAP</code>, <code>SETPCAP</code>, <code>SETUID</code>, <code>SYS ADMIN</code>, <code>SYS BOOT</code>, <code>SYS CHROOT</code>, <code>SYS MODULE</code>, <code>SYS NICE</code>, <code>SYS PACCT</code>, <code>SYS PTRACE</code>, <code>SYS RAWIO</code>, <code>SYS RESOURCE</code>, <code>SYS TIME</code>, <code>SYS TTY CONFIG</code>, <code>SYSLOG</code>, <code>WAKE ALARM</code>. <code>seccompProfileType</code> <code>string</code> or <code>null</code> Indicates which kind of seccomp profile will be applied to the container. Choose from: <code>Runtime</code> (default)\u2014the container runtime default profile should be used.  <code>Unconfined</code>&amp;mdashno profile should be applied.  <code>Localhost</code> is not yet supported by Run:ai. <code>runAsNonRoot</code> <code>boolean</code> or <code>null</code> Indicates that the container must run as a non-root user."},{"location":"admin/workloads/policies/training-policy/#environment-variables","title":"Environment Variables","text":"Field Type Description <code>name</code> (required) <code>string</code> (non-empty) The name of the environment variable. <code>value</code> (required) <code>string</code> The value to set the environment variable to. <code>deleted</code> <code>boolean</code> Exclude this environment variable from the workload. This is necessary in case the variable definition is inherited from a policy."},{"location":"admin/workloads/policies/training-policy/#connections-variables","title":"Connections Variables","text":"Field Type Description <code>namerequired</code> <code>string</code> (non-empty) A unique name of this connection. This name correlates between the connection information specified at the environment asset, to the information about the connection as specified in <code>SpecificEnv</code> for a specific workspace. <code>isExternal</code> <code>boolean</code> Internal tools (<code>isExternal=false</code>) are tools that run as part of the container. External tools (<code>isExternal=true</code>) run outside the container, typically in the cloud. Default:\u00a0false. <code>internalToolInfo</code> <code>object</code> or <code>null</code> Information about the internal tool. <code>externalToolInfo</code> <code>object</code> or <code>null</code> Information about the external tool."},{"location":"admin/workloads/policies/training-policy/#internal-tool-variables","title":"Internal Tool Variables","text":"Field Type Description <code>toolType</code> (required) <code>string</code>\u00a0(non-empty) The type of the internal tool. This runs within the container and exposes ports associated with the tool using <code>NodePort</code>, <code>LoadBalancer</code> or <code>ExternalUrl</code>. Choose from: <code>jupyter-notebook</code>, <code>pycharm</code>, <code>visual-studio-code</code>, <code>tensorboard</code>,\u00a0<code>rstudio</code>,\u00a0<code>mlflow</code>,\u00a0<code>custom</code>, or\u00a0<code>matlab</code>. <code>connectionType</code> (required) <code>string</code>\u00a0(non-empty) The type of connection that exposes the container port. Choose from: <code>LoadBalancer</code>,\u00a0<code>NodePort</code>, or <code>ExternalUrl</code>. <code>containerPort</code> (required) <code>integer</code> The port within the container that the connection exposes. <code>nodePortInfo</code> <code>object</code> or <code>null</code> Use the <code>isCustomPort</code> variable (<code>boolean</code>) to ensute that the node port is provided in the specific env of the workspace. Use the default <code>false</code> to ensure the node port is auto generated by the system. <code>externalUrlInfo</code> <code>object</code> or <code>null</code> Use the <code>isCustomUrl</code> variable (boolean) to indicate whether the external url is provided in the specific env of the workspace. Use the default <code>false</code>to ensure the external url is auto generated by the system.  Use the <code>externalUrl</code> variable (<code>string</code> or <code>null</code> - non-empty) to decalre the default value for the external url. You can override it in the specific env of the workspace."},{"location":"admin/workloads/policies/training-policy/#external-tool-variables","title":"External Tool Variables","text":"Field Type Description <code>toolType</code> (required) <code>string</code>\u00a0(non-empty) The type of external tool that is associated with the connection. External tools typically run in the cloud and require an external url to connect to it. Choose from <code>wandb</code> or <code>comet</code>. <code>externalUrl</code> (required) <code>string</code>\u00a0(non-empty) The external url for connecting to the external tool. The url can include environment variables that will be replaced with the values provided when the workspace is created."},{"location":"admin/workloads/policies/training-policy/#compute-resource-fields","title":"Compute Resource Fields","text":"Field Type Description <code>gpuDevicesRequest</code> <code>integer</code> or <code>null</code> Requested number of GPU devices. Currently if more than one device is requested, it is not possible to provide values for gpuMemory/migProfile/gpuPortion. <code>gpuRequestType</code> <code>string</code> or <code>null</code> (GpuRequestType)\u00a0non-empty Enum:\u00a0\"portion\"\u00a0\"memory\"\u00a0\"migProfile\"Whether the request for GPU resources is stated in terms of portion, memory or mig profile. If gpuDevicesRequest &gt; 1, only portion with gpuPortionRequest 1 is supported. If gpuDeviceRequest = 1, request type can be stated as portion, memory or migProfile. <code>gpuPortionRequest</code> <code>number</code> or <code>null</code> Required if and only if gpuRequestType is portion. States the portion of the GPU to allocate for the created workload, per GPU device, between 0 and 1. The default is no allocated GPUs. <code>gpuPortionLimit</code> <code>number</code> or <code>null</code> Limitations on the portion consumed by the workload, per GPU device. The system guarantees The puPotionLimit must be no less than the gpuPortionRequest. <code>gpuMemoryRequest</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ Required if and only if gpuRequestType is memory. States the GPU memory to allocate for the created workload, per GPU device. Note that the workload will not be scheduled unless the system can guarantee this amount of GPU memory to the workload. <code>gpuMemoryLimit</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ Limitation on the memory consumed by the workload, per GPU device. The system guarantees The gpuMemoryLimit must be no less than gpuMemoryRequest. <code>migProfile</code> <code>string</code> or <code>null</code>\u00a0(MigProfile)\u00a0non-empty Enum:\u00a0\"1g.5gb\"\u00a0\"1g.10gb\"\u00a0\"2g.10gb\"\u00a0\"2g.20gb\"\u00a0\"3g.20gb\"\u00a0\"3g.40gb\"\u00a0\"4g.20gb\"\u00a0\"4g.40gb\"\u00a0\"7g.40gb\"\u00a0\"7g.80gb\"Required if and only if gpuRequestType is migProfile. States the memory profile to be used for workload running on NVIDIA Multi-Instance GPU (MIG) technology. <code>cpuCoreRequest</code> <code>number</code> or <code>null</code> CPU units to allocate for the created workload (0.5, 1, .etc). The workload will receive at least this amount of CPU. Note that the workload will not be scheduled unless the system can guarantee this amount of CPUs to the workload. <code>cpuCoreLimit</code> <code>number</code> or <code>null</code> Limitations on the number of CPUs consumed by the workload (0.5, 1, .etc). The system guarantees that this workload will not be able to consume more than this amount of CPUs. <code>cpuMemoryRequest</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The amount of CPU memory to allocate for this workload (1G, 20M, .etc). The workload will receive at least this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of memory to the workload <code>cpuMemoryLimit</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ Limitations on the CPU memory to allocate for this workload (1G, 20M, .etc). The system guarantees that this workload will not be able to consume more than this amount of memory. The workload will receive an error when trying to allocate more memory than this limit. <code>largeShmRequest</code> <code>boolean</code> or <code>null</code> A large /dev/shm device to mount into a container running the created workload. An shm is a shared file system mounted on RAM. <code>extendedResources</code> <code>Array</code> An array of\u00a0objects or null or null\u00a0(ExtendedResources) - Set of extended resources with their quantity."},{"location":"admin/workloads/policies/training-policy/#extended-resources-array","title":"Extended Resources Array","text":"Field Type Description <code>resource required</code> <code>string</code> non-empty The name of the extended resource. <code>quantity required</code> <code>string</code> non-empty The requested quantity for the given resource. <code>deleted</code> <code>boolean</code> Whether to exclude this extended resource from the workload. This is necessary in case the extended resource definition is inherited from a policy."},{"location":"admin/workloads/policies/training-policy/#hostpath-resource-fields","title":"Hostpath Resource Fields","text":"Field Type Description <code>pathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty Local path within the controller to which the host volume will be mapped. Path is mandatory for creating a workspace. <code>readOnly</code> <code>boolean</code> or <code>null</code> Default:\u00a0true Whether to force the volume to be mounted with read-only permissions. Defaults to false. <code>mountPathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty The path that the host volume will be mounted to when in use. MountPath is mandatory for creating a workspace."},{"location":"admin/workloads/policies/training-policy/#nfs-description-fields","title":"NFS Description Fields","text":"Field Type Description <code>pathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty Path that is exported by the NFS server. More info at\u00a0https://kubernetes.io/docs/concepts/storage/volumes#nfs. Path is mandatory for creating a workspace. <code>readOnly</code> <code>boolean</code> or <code>null</code> Default:\u00a0true Whether to force the NFS export to be mounted with read-only permissions. <code>serverrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty The hostname or IP address of the NFS server. Server is mandatory for creating a workspace. <code>mountPathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty The path that the NFS volume will be mounted to when in use. MountPath is mandatory for creating a workspace."},{"location":"admin/workloads/policies/training-policy/#pvc-description-fields","title":"PVC Description Fields","text":"Field Type Description <code>existingPvc</code> <code>boolean</code> or <code>null</code> Default:\u00a0false Whether to assume that the PVC exists. If set to true, PVC is assumed to exist. If set to false, the PVC will be create if it does not exist. <code>claimNamerequired</code> <code>string</code> or <code>null</code>\u00a0non-empty A given name for the PVC. Allowed referencing it across workspaces. ClaimName is mandatory for creating a workspace. <code>pathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty Local path within the workspace to which the PVC bucket will be mapped. Path is mandatory for creating a workspace. <code>readOnly</code> <code>boolean</code> or <code>null</code> Default:\u00a0true Whether the path to the PVC permits only read access. <code>ephemeral</code> <code>boolean</code> or <code>null</code> Default:\u00a0false Whether the PVC is ephemeral. If set to true, the PVC will be deleted when the workspace is stopped. <code>claimInfo</code> <code>object</code> or <code>null</code> ClaimInfo Claim information for the newly created PVC. The information should not be provided when attempting to use existing PVC."},{"location":"admin/workloads/policies/training-policy/#claim-info","title":"Claim Info","text":"Field Type Description <code>sizerequired</code> <code>string</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ Requested size for the PVC. Mandatory when existingPvc is false. <code>storageClass</code> <code>string</code> or <code>null</code>\u00a0non-empty Storage class name to associate with the PVC. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. For more information see Storage classes. <code>accessModes</code> <code>object</code> or <code>null</code> AccessModes Requested access mode(s) for the newly created PVC. <code>volumeMode</code> <code>string</code> or <code>null</code> Enum:\u00a0\"Filesystem\"\u00a0\"Block\"The volume mode required by the claim, either Filesystem (default) or Block."},{"location":"admin/workloads/policies/training-policy/#access-modes","title":"Access Modes","text":"Field Type Description <code>readWriteOnce</code> <code>boolean</code> or <code>null</code> Default:\u00a0true Requesting claim that can be mounted in read/write mode to exactly one host. This is the default access mode. <code>readOnlyMany</code> <code>boolean</code> or <code>null</code> Default:\u00a0false Requesting claim that can be mounted in read-only mode to many hosts. <code>readWriteMany</code> <code>boolean</code> or <code>null</code> Default:\u00a0false Requesting claim that can be mounted in read/write mode to many hosts."},{"location":"admin/workloads/policies/training-policy/#git-repository-description-fields","title":"Git Repository Description Fields","text":"Field Type Description <code>repositoryrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty URL to a remote git repository. The content of this repository will be mapped to the container running the workload. Repository name is mandatory for creating a workspace. <code>branch</code> <code>string</code> or <code>null</code>\u00a0non-empty Specific branch to synchronize the repository from. <code>revision</code> <code>string</code> or <code>null</code>\u00a0non-empty Specific revision to synchronize the repository from. <code>pathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty Local path within the workspace to which the S3 bucket will be mapped. Path is mandatory for creating a workspace. <code>passwordAssetId</code> <code>string</code> or <code>null</code>\u00a0non-empty ID of credentials asset of type password. Needed for non public repository which requires authentication."},{"location":"admin/workloads/policies/training-policy/#s3-resource-description-fields","title":"S3 Resource Description Fields","text":"Field Type Description <code>bucketrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty The name of the bucket Bucket name is mandatory for creating a workspace. <code>pathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty Local path within the workspace to which the S3 bucket will be mapped. Path is mandatory for creating a workspace. <code>accessKeyAssetId</code> <code>string</code> or <code>null</code>\u00a0non-empty ID of credentials asset of type access-key, for private S3 buckets. <code>url</code> <code>string</code> or <code>null</code>\u00a0non-empty The url of the S3 service provider. The default is the URL of the Amazon AWS S3 service."},{"location":"admin/workloads/policies/training-policy/#training","title":"Training","text":""},{"location":"admin/workloads/policies/training-policy/#configmap-resource-description-fields","title":"ConfigMap Resource Description Fields","text":"Field Type Description <code>json:\"configMap\"</code> <code>string</code> The name of the ConfigMap. ConfigMap is mandatory for creating a workspace. <code>json:\"mountPath\"</code> <code>string</code> Local path within the workspace to which the ConfigMap will be mapped. ClaimName is mandatory for creating a workspace."},{"location":"admin/workloads/policies/training-policy/#workspace","title":"Workspace","text":"Field Type Description <code>nodeType</code> <code>string</code> or <code>null</code>\u00a0non-empty Nodes or a group of nodes on which the workload will run. To use this feature, your Administrator will need to label nodes as explained in Group Nodes. This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information, see Projects. <code>nodePools</code> <code>Array of\u00a0strings</code> or <code>null</code> A prioritize list of node pools for the scheduler to run the workspace on. The scheduler will always try to use the first node pool before moving to the next one when the fist is not available. <code>allowOverQuota</code> <code>boolean</code> or <code>null</code> Whether to allow the workspace to exceed the quota of the project. <code>annotations</code> <code>Array of\u00a0objects</code> or <code>null</code> Annotations Set of annotations to populate into the container running the workspace. <code>labels</code> <code>Array of\u00a0objects</code> or <code>null</code> Labels Set of labels to populate into the container running the workspace. <code>autoDeletionTimeAfterCompletionSeconds</code> <code>integer</code> or <code>null</code> \u00a0Specifies the duration after which a finished workload (Completed or Failed) will be automatically deleted. <code>terminateAfterPreemption</code> <code>boolean</code> or <code>null</code> Indicates whether the job should be terminated, by the system, after it has been preempted."},{"location":"admin/workloads/policies/training-policy/#annotations","title":"Annotations","text":"Field Type Description <code>namerequired</code> <code>string</code>\u00a0non-empty The name of the annotation. <code>valuerequired</code> <code>string</code> The value to set the annotation to. <code>deleted</code> <code>boolean</code> Whether to exclude this annotation from the workload. This is necessary in case the annotation definition is inherited from a policy."},{"location":"admin/workloads/policies/training-policy/#labels","title":"Labels","text":"Field Type Description <code>namerequired</code> <code>string</code>\u00a0non-empty The name of the annotation. <code>valuerequired</code> <code>string</code> The value to set the annotation to. <code>deleted</code> <code>boolean</code> Whether to exclude this annotation from the workload. This is necessary in case the annotation definition is inherited from a policy."},{"location":"admin/workloads/policies/training-policy/#imposed-assets","title":"Imposed Assets","text":"Field Type Description <code>datasources</code> <code>Array of strings</code> or <code>null</code> --"},{"location":"admin/workloads/policies/training-policy/#rules","title":"Rules","text":"<p>The <code>rules</code> section of the policy file is...</p> Field Type Description <code>environment</code> <code>object</code> or <code>null</code> Rules Environment fields fields that can be overridden when creating a workload. <code>compute</code> <code>object</code> or <code>null</code> Compute resources requested. <code>hostPath</code> <code>object</code> or <code>null</code> Volumes resource definitions. <code>nfs</code> <code>object</code> or <code>null</code> NFS volume definitions. <code>pvc</code> <code>object</code> or <code>null</code> PVC definitions. <code>git</code> <code>object</code> or <code>null</code> Git repository definitions. <code>s3</code> <code>object</code> or <code>null</code> S3 resource definitions. <code>imposedAssets</code> <code>object</code> or <code>null</code> A list of asset to be imposed on the workloads created in org units affected by this policy."},{"location":"admin/workloads/policies/training-policy/#rules-environment-fields","title":"Rules Environment fields","text":"Field Type Description <code>allowPrivilegeEscalation</code> <code>object</code> or <code>null</code> Allow Privilege Escalation -- <code>args</code> <code>object</code> or <code>null</code> (StringRulesOptional) -- <code>capabilities</code> <code>object</code> or <code>null</code> (ArrayRules) -- <code>command</code> <code>object</code> or <code>null</code> (StringRulesOptional) -- <code>createHomeDir</code> <code>object</code> or <code>null</code> (BooleanRules) -- <code>environmentVariables</code> <code>object</code> or <code>null</code> (EnvironmentVariablesRules) -- <code>hostIpc</code> <code>object</code> or <code>null</code> (BooleanRules) -- <code>hostNetwork</code> <code>object</code> or <code>null</code> (BooleanRules) -- <code>image</code> <code>object</code> or <code>null</code> (StringRules) -- <code>imagePullPolicy</code> <code>object</code> or <code>null</code> (ImagePullPolicyRules) -- <code>overrideUidGidInWorkspace</code> <code>object</code> or <code>null</code> (BooleanRulesOptional) -- <code>runAsUid</code> <code>object</code> or <code>null</code> (IntegerRulesOptional) -- <code>runAsGid</code> <code>object</code> or <code>null</code> (IntegerRulesOptional) -- <code>supplementalGroups</code> <code>object</code> or <code>null</code> (StringRulesOptional) -- <code>uidGidSource</code> <code>object</code> or <code>null</code> (StringRules) -- <code>workingDir</code> <code>object</code> or <code>null</code> (StringRules) -- <code>runAsNonRoot</code> <code>object</code> or <code>null</code> (BooleanRules) -- <code>seccompProfileType</code> <code>object</code> or <code>null</code> (StringRules) --"},{"location":"admin/workloads/policies/training-policy/#allow-privilege-escalation","title":"Allow Privilege Escalation","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable."},{"location":"admin/workloads/policies/training-policy/#source-of-rule","title":"Source of Rule","text":"Field Type Description <code>scoperequired</code> <code>string</code>\u00a0(Scope) Enum:\u00a0\"tenant\"\u00a0\"department\"\u00a0\"project\"The scope that the policy relates to. <code>projectId</code> <code>integer</code> or <code>null</code>\u00a0(ProjectId) The id of the project. Must be specified for project scoped assets. <code>departmentId</code> <code>string</code> or <code>null</code> \u00a0(DepartmentId)\u00a0non-emptyThe id of the department. Must be specified for department scoped policies."},{"location":"admin/workloads/policies/training-policy/#args","title":"Args","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#capabilities","title":"Capabilities","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable."},{"location":"admin/workloads/policies/training-policy/#command","title":"Command","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#create-home-dir","title":"Create Home Dir","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable."},{"location":"admin/workloads/policies/training-policy/#rules-environment-variables","title":"Rules Environment Variables","text":"Field Type Description <code>itemRules</code> <code>object</code> or <code>null</code> <code>sourceOfRule</code><code>object</code> or <code>null</code> Source Of RuleThis field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests.<code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is permitted to add items. Default to true.<code>locked</code><code>Array of\u00a0strings</code>Set of keys for items that are \"locked\", i.e. cannot be removed or deleted."},{"location":"admin/workloads/policies/training-policy/#host-ipc","title":"Host IPC","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#host-network","title":"Host Network","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#image","title":"Image","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#image-pull-policy","title":"Image Pull Policy","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#override-uidgid-in-workspace","title":"Override Uid/Gid In Workspace","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"admin/workloads/policies/training-policy/#run-as-uid","title":"Run as Uid","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable. <code>min</code> <code>integer</code> or <code>null</code> \u00a0The minimum value that the field can be assigned to. <code>max</code> <code>integer</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>integer</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=10, step=2 implies that the values the field can hold are 2, 4, 6, 8 and 10."},{"location":"admin/workloads/policies/training-policy/#run-as-gid","title":"Run as Gid","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable. <code>min</code> <code>integer</code> or <code>null</code> \u00a0The minimum value that the field can be assigned to. <code>max</code> <code>integer</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>integer</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=10, step=2 implies that the values the field can hold are 2, 4, 6, 8 and 10."},{"location":"admin/workloads/policies/training-policy/#supplemental-groups","title":"Supplemental Groups","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#uidgid-source","title":"Uid/Gid Source","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#working-dir","title":"Working Dir","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#run-as-non-root","title":"Run as Non-root","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"admin/workloads/policies/training-policy/#seccomp-profile-type","title":"Seccomp Profile Type","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#rules-compute-fields","title":"Rules Compute Fields","text":""},{"location":"admin/workloads/policies/training-policy/#cpu-core-request","title":"CPU Core Request","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>number</code> or <code>null</code> The minimum value that the field can be assigned to. <code>max</code> <code>number</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>number</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=3, step=0.25 implies that the values the field can hold are 2, 2.25, 2.5 and 3."},{"location":"admin/workloads/policies/training-policy/#cpu-core-limit","title":"CPU Core Limit","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>number</code> or <code>null</code> The minimum value that the field can be assigned to. <code>max</code> <code>number</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>number</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=3, step=0.25 implies that the values the field can hold are 2, 2.25, 2.5 and 3."},{"location":"admin/workloads/policies/training-policy/#cpu-memory-request","title":"CPU Memory Request","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The minimum value that the field can be assigned to. <code>max</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The maximum value that the field can be assigned to."},{"location":"admin/workloads/policies/training-policy/#cpu-memory-limit","title":"CPU Memory Limit","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The minimum value that the field can be assigned to. <code>max</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The maximum value that the field can be assigned to."},{"location":"admin/workloads/policies/training-policy/#extended-resources","title":"Extended Resources","text":"Field Type Description <code>itemRules</code> <code>object</code> or <code>null</code>\u00a0(ItemRules) <code>members</code> <code>object</code> or <code>null</code> the Quantity of rules. Use the following table for the fields and values."},{"location":"admin/workloads/policies/training-policy/#quantity","title":"Quantity","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#large-shm-request","title":"Large Shm Request","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"admin/workloads/policies/training-policy/#gpu-request-type","title":"GPU Request Type","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code> Set of options that the value of the field must be chosen from."},{"location":"admin/workloads/policies/training-policy/#mig-profile","title":"Mig Profile","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of Objects</code> or <code>null</code> Set of options that the value of the field must be chosen from. Required if and only if gpuRequestType is migProfile. States the memory profile to be used for workload running on NVIDIA Multi-Instance GPU (MIG) technology.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value."},{"location":"admin/workloads/policies/training-policy/#gpu-devices-request","title":"GPU Devices Request","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable. <code>min</code> <code>integer</code> or <code>null</code> \u00a0The minimum value that the field can be assigned to. <code>max</code> <code>integer</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>integer</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=10, step=2 implies that the values the field can hold are 2, 4, 6, 8 and 10."},{"location":"admin/workloads/policies/training-policy/#gpu-portion-request","title":"GPU Portion Request","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>number</code> or <code>null</code> The minimum value that the field can be assigned to. <code>max</code> <code>number</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>number</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=3, step=0.25 implies that the values the field can hold are 2, 2.25, 2.5 and 3."},{"location":"admin/workloads/policies/training-policy/#gpu-portion-limit","title":"GPU Portion Limit","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>number</code> or <code>null</code> The minimum value that the field can be assigned to. <code>max</code> <code>number</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>number</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=3, step=0.25 implies that the values the field can hold are 2, 2.25, 2.5 and 3."},{"location":"admin/workloads/policies/training-policy/#gpu-memory-request","title":"GPU Memory Request","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The minimum value that the field can be assigned to. <code>max</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The maximum value that the field can be assigned to."},{"location":"admin/workloads/policies/training-policy/#gpu-memory-limit","title":"GPU Memory Limit","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The minimum value that the field can be assigned to. <code>max</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The maximum value that the field can be assigned to."},{"location":"admin/workloads/policies/training-policy/#host-path","title":"Host Path","text":"Field Type Description <code>path</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>readOnly</code> <code>object</code> or <code>null</code>(BooleanRules) -- <code>mountPath</code> <code>object</code> or <code>null</code> (StringRules) --"},{"location":"admin/workloads/policies/training-policy/#path","title":"Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#read-only","title":"Read Only","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"admin/workloads/policies/training-policy/#mount-path","title":"Mount Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#nfs","title":"NFS","text":"Field Type Description <code>path</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>readOnly</code> <code>object</code> or <code>null</code>\u00a0(BooleanRules) -- <code>server</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>mountPath</code> <code>object</code> or <code>null</code>\u00a0(StringRules) --"},{"location":"admin/workloads/policies/training-policy/#path_1","title":"Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#read-only_1","title":"Read Only","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"admin/workloads/policies/training-policy/#server","title":"Server","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#mount-path_1","title":"Mount Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#pvc","title":"PVC","text":"Field Type Description <code>claimName</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>path</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>readOnly</code> <code>object</code> or <code>null</code>\u00a0(BooleanRules) -- <code>claimInfo</code> <code>object</code> or <code>null</code>\u00a0(ClaimInfoRules) --"},{"location":"admin/workloads/policies/training-policy/#claim-name","title":"Claim Name","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#path_2","title":"Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#read-only_2","title":"Read Only","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"admin/workloads/policies/training-policy/#claim-info_1","title":"Claim Info","text":"Field Type Description <code>size</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>storageClass</code> <code>object</code> or <code>null</code>\u00a0(StringRules) --"},{"location":"admin/workloads/policies/training-policy/#size","title":"Size","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#storage-class","title":"Storage Class","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#git","title":"Git","text":"Field Type Description <code>repository</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>branch</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>revision</code> <code>object</code> or <code>null</code> (StringRules) <code>path</code> <code>object</code> or <code>null</code> (StringRules) --"},{"location":"admin/workloads/policies/training-policy/#repository","title":"Repository","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#branch","title":"Branch","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#revision","title":"Revision","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#path_3","title":"Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#s3","title":"S3","text":"Field Type Description <code>bucket</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>path</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>url</code> <code>object</code> or <code>null</code>\u00a0(StringRules) --"},{"location":"admin/workloads/policies/training-policy/#bucket","title":"Bucket","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#path_4","title":"Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#url","title":"URL","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#imposed-assets_1","title":"Imposed Assets","text":"Field Type Description <code>datasources</code> <code>object</code> or <code>null</code> Use any of the fields in the following table:"},{"location":"admin/workloads/policies/training-policy/#data-sources","title":"Data Sources","text":""},{"location":"admin/workloads/policies/training-policy/#training_1","title":"Training","text":"<p>| -- | -- | --| |<code>hostPath</code> | <code>object</code> or <code>null</code> | <code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is possible to add non-imposed assets in the workload | |<code>nfs</code> | <code>object</code> or <code>null</code> | <code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is possible to add non-imposed assets in the workload | |<code>pvc</code> | <code>object</code> or <code>null</code> | <code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is possible to add non-imposed assets in the workload | |<code>git</code> | <code>object</code> or <code>null</code> | <code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is possible to add non-imposed assets in the workload | |<code>s3</code> | <code>object</code> or <code>null</code> | <code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is possible to add non-imposed assets in the workload |</p>"},{"location":"admin/workloads/policies/training-policy/#workspace_1","title":"Workspace","text":"Field Type Description <code>nodeType</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>nodePools</code> <code>object</code> or <code>null</code> (ArrayRules) -- <code>allowOverQuota</code> <code>object</code> or <code>null</code>\u00a0(BooleanRules) -- <code>annotations</code> <code>object</code> or <code>null</code>\u00a0(AnnotationsRules) -- <code>labels</code> <code>object</code> or <code>null</code>\u00a0(LabelsRules) -- <code>autoDeletionTimeAfterCompletionSeconds</code> <code>object</code> or <code>null</code>\u00a0(IntegerRules) -- <code>terminateAfterPreemption</code> <code>object</code> or <code>null</code>\u00a0(BooleanRules) --"},{"location":"admin/workloads/policies/training-policy/#node-type","title":"Node Type","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"admin/workloads/policies/training-policy/#node-pools","title":"Node Pools","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"admin/workloads/policies/training-policy/#allowed-over-quota","title":"Allowed Over-quota","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"admin/workloads/policies/training-policy/#annotations_1","title":"Annotations","text":"Field Type Description <code>itemRules</code> <code>object</code> or <code>null</code> <code>sourceOfRule</code><code>object</code> or <code>null</code> Source Of RuleThis field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests.<code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is permitted to add items. Default to true.<code>locked</code><code>Array of\u00a0strings</code>Set of keys for items that are \"locked\", i.e. cannot be removed or deleted."},{"location":"admin/workloads/policies/training-policy/#labels_1","title":"Labels","text":"Field Type Description <code>itemRules</code> <code>object</code> or <code>null</code> <code>sourceOfRule</code><code>object</code> or <code>null</code> Source Of RuleThis field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests.<code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is permitted to add items. Default to true.<code>locked</code><code>Array of\u00a0strings</code>Set of keys for items that are \"locked\", i.e. cannot be removed or deleted."},{"location":"admin/workloads/policies/training-policy/#auto-deletion-time-after-completion-seconds","title":"Auto Deletion Time After Completion Seconds","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable. <code>min</code> <code>integer</code> or <code>null</code> \u00a0The minimum value that the field can be assigned to. <code>max</code> <code>integer</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>integer</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=10, step=2 implies that the values the field can hold are 2, 4, 6, 8 and 10."},{"location":"admin/workloads/policies/workspaces-policy/","title":"Workspaces Policy","text":"<p>A Workspaces policy places resource restrictions and defaults on workspaces in the Run:ai platform. Restrictions and default values can be placed on CPUs, GPUs, and other resources or entities.</p>"},{"location":"admin/workloads/policies/workspaces-policy/#example","title":"Example","text":"<p>Below is an example policy you can use in your platform.</p> <p>Note</p> <ul> <li>Not all the configurable fields available are listed in the example below. </li> <li>Replace the values listed in the example below with values that match your platform requirements.</li> </ul> <pre><code>defaults:\n    environment:\n      allowPrivilegeEscalation: false\n      createHomeDir: true\n      environmentVariables:\n        - name: MY_ENV\n          value: my_value\n    workspace:\n      allowOverQuota: true\nrules:\n    compute:\n      cpuCoreLimit:\n        min: 0\n        max: 9\n        required: true\n      gpuPortionRequest:\n        min: 0\n        max: 10\n    s3:\n      url:\n        options:\n          - displayed: \"Google\"\n            value: \"https://www.google.com\"\n          - displayed: \"Yahoo\"\n            value: \"https://www.yahoo.com\"\n    environment:\n      imagePullPolicy:\n        options:\n          - displayed: \"Always\"\n            value: \"Always\"\n          - displayed: \"Never\"\n            value: \"Never\"\n        required: true\n      runAsUid:\n        min: 1\n        max: 32700\n      createHomeDir:\n        canEdit: false\n      allowPrivilegeEscalation:\n        canEdit: false\n    workspace:\n      allowOverQuota:\n        canEdit: false\n    imposedAssets:\n      dataSources:\n        nfs:\n          canAdd: false\n</code></pre>"},{"location":"admin/workloads/policies/workspaces-policy/#viewing-or-edit-a-policy","title":"Viewing or Edit a Policy","text":"<p>To view or edit a policy:</p> <ol> <li>Press Tools and Settings.</li> <li>Press Policies. The policy grid is displayed.</li> <li>Select a policy from the list. If there are no policies, then create a new policy.</li> <li>Pres Edit to view the policy details, then press Edit Policy to edit the YAML file.</li> <li>When done, press Apply.</li> </ol>"},{"location":"admin/workloads/policies/workspaces-policy/#creating-a-new-policy","title":"Creating a New Policy","text":"<p>To create a policy:</p> <ol> <li>Press Tools and Settings.</li> <li>Press Policies. The policy grid is displayed.</li> <li>Press New Policy.</li> <li>Select a scope for the policy.</li> <li>Select a workload type using the dropdown.</li> <li>In the Policy YAML pane, press + POLICY YAML to open the policy editor.</li> <li>Enter your policy in the policy editor. Add policy properties and variables in YAML format. When complete, press APPLY.</li> <li>When done, press SAVE POLICY.</li> </ol> <p>Note</p> <p>After saving, the form will wait for the policy to sync with the cluster.</p>"},{"location":"developer/overview-developer/","title":"Developer Documentation Overview","text":"<p>Developers can access Run:ai through various programmatic interfaces.</p>"},{"location":"developer/overview-developer/#api-support","title":"API Support","text":"<p>The endpoints and fields specified in the API reference are the ones that are officially supported by Run:ai. Endpoints and fields that are not listed in the API reference are not supported.</p> <p>Run:ai does not recommend using API endpoints and fields marked as <code>deprecated</code> and will not add functionality to them. Once an API endpoint or field is marked as <code>deprecated</code>, Run:ai will stop supporting it after 2 major releases for self-hosted deployments, and after 6 months for SaaS deployments.</p> <p>For details, see the Deprecation notifications.</p>"},{"location":"developer/overview-developer/#api-architecture","title":"API Architecture","text":"<p>Run:ai is composed of a single, multi-tenant control plane. Each tenant can be connected to one or more GPU clusters. See Run:ai system components for detailed information.</p> <p>Below is a diagram of the Run:ai API Architecture. A developer may:</p> <ol> <li>Access the control plane via the Administrator API.</li> <li>Access any one of the GPU clusters via Cluster API.</li> <li>Access cluster metrics via the Metrics API.  </li> </ol> <p></p>"},{"location":"developer/overview-developer/#administrator-api","title":"Administrator API","text":"<p>Add, delete, modify and list Run:ai meta-data objects such as Projects, Departments, Users, and more.</p> <p>The API is provided as REST and is accessible via the control plane endpoint.  </p> <p>For more information see Administrator REST API.</p>"},{"location":"developer/overview-developer/#cluster-api","title":"Cluster API","text":"<p>Submit and delete Workloads.</p> <p>The API is provided as Kubernetes API.</p> <p>Cluster API is accessible via the GPU cluster itself. As such, multiple clusters may have multiple endpoints.</p> <p>Note</p> <p>The same functionality is also available via the Run:ai Command-line interface. The CLI provides an alternative for automating with shell scripts.</p>"},{"location":"developer/overview-developer/#metrics-api","title":"Metrics API","text":"<p>Retrieve metrics from multiple GPU clusters.</p> <p>See the Metrics API document.</p>"},{"location":"developer/overview-developer/#api-authentication","title":"API Authentication","text":"<p>See API Authentication for information on how to gain authenticated access to Run:ai APIs.</p>"},{"location":"developer/rest-auth/","title":"API Authentication","text":"<p>The following document explains how to authenticate with Run:ai APIs.</p> <p>Run:ai APIs are accessed using bearer tokens. A token can be obtained in several ways:</p> <ul> <li>When logging into the Run:ai user interface, you enter an email and password (or authenticated via single sign-on) which are used to obtain a token.</li> <li>When using the Run:ai command-line, you use a Kubernetes profile and are authenticated by pre-running <code>runai login</code> (or oc login with OpenShift). The command attaches a token to the profile and allows you access to Run:ai functionality.</li> <li>When using Run:ai APIs, you need to create an Application through the Run:ai user interface. The Application is created with specific roles and contains a secret. Using the secret you can obtain a token and use it within subsequent API calls.</li> </ul>"},{"location":"developer/rest-auth/#create-a-client-application","title":"Create a Client Application","text":"<ul> <li>Open the Run:ai Run:ai User Interface.</li> <li>Go to <code>Settings &amp; Tools</code>, <code>Application</code> and create a new Application.</li> <li>Copy the <code>&lt;APPLICATION&gt;</code> and <code>&lt;SECRET KEY&gt;</code> to be used below</li> </ul>"},{"location":"developer/rest-auth/#access-rules-for-the-application","title":"Access rules for the Application","text":"<p>In order for you API requests to be accepted, you will need to set access rules for the application. To assign roles to an application, see Create or Delete rules.</p> <p>Use the Roles table to assign the correct roles to the application.</p>"},{"location":"developer/rest-auth/#request-an-api-token","title":"Request an API Token","text":"<p>Use the above parameters to get a temporary token to access Run:ai as follows.</p>"},{"location":"developer/rest-auth/#example-command-to-get-an-api-token","title":"Example command to get an API token","text":"<p>Replace <code>&lt;COMPANY-URL&gt;</code> below with:</p> <ul> <li> <p>For SaaS installations use <code>&lt;company&gt;.run.ai</code></p> </li> <li> <p>For self-hosted use the Run:ai user interface URL.</p> </li> </ul> cURLPython <pre><code>    curl  -X POST \\\n      'https://&lt;runai_url&gt;/api/v1/token' \\\n      --header 'Accept: */*' \\\n      --header 'Content-Type: application/json' \\\n      --data-raw '{\n      \"grantType\":\"app_token\",\n      \"AppId\":\"&lt;APPLICATION NAME&gt;\",\n      \"AppSecret\" : \"&lt;SECRET KEY&gt;\"\n    }'\n</code></pre> <pre><code>    import requests\n    import json\n    reqUrl = \"https://cp-590d-run-13764-kc-upgrade.runailabs.com/api/v1/token\"\n    headersList = {\n     \"Accept\": \"*/*\",\n     \"Content-Type\": \"application/json\"\n    }\n    payload = json.dumps({\n      \"grantType\":\"app_token\",\n      \"AppId\":\"&lt;APPLICATION NAME&gt;\",\n      \"AppSecret\" : \"&lt;SECRET KEY&gt;\"\n    })\n    response = requests.request(\"POST\", reqUrl, data=payload,  headers=headersList)\n    print(response.text)\n</code></pre>"},{"location":"developer/rest-auth/#response","title":"Response","text":"<p>The API response will look as follows:</p> API Response<pre><code>{\n  \"accessToken\": \"&lt;TOKEN&gt;\", \n}\n</code></pre> <p>To call APIs, the application must pass the retrieved <code>accessToken</code> as a Bearer token in the Authorization header of your HTTP request.</p> <ul> <li>To retrieve and manipulate Workloads, use the Cluster API. Researcher API works at the cluster level and you will have different endpoints for different clusters.</li> <li>To retrieve and manipulate other metadata objects, use the Administrator REST API. Administrator API works at the control-plane level and you have a single endpoint for all clusters.</li> </ul>"},{"location":"developer/admin-rest-api/overview/","title":"Control Plane REST API","text":"<p>The purpose of the Administrator REST API is to provide an easy-to-use programming interface for administrative tasks.</p>"},{"location":"developer/admin-rest-api/overview/#endpoint-url-for-api","title":"Endpoint URL for API","text":"<p>The domain used for Administrator REST APIs is the same domain used to browse for the Run:ai User Interface. Either <code>&lt;company&gt;.run.ai</code>, or <code>app.run.ai</code> for older tenants or a custom URL used for Self-hosted installations.</p>"},{"location":"developer/admin-rest-api/overview/#authentication","title":"Authentication","text":"<ul> <li>Create a Client Application to make API requests. Use the client application and secret, to obtain a time-bound bearer token (<code>&lt;ACCESS-TOKEN&gt;</code>). For details, see Calling REST APIs.</li> <li>Use the token for subsequent API calls.</li> </ul>"},{"location":"developer/admin-rest-api/overview/#example-usage","title":"Example Usage","text":"<p>For example, if you have an Administrator role, you can get a list of clusters by running:</p> cURLPython <pre><code>curl 'https://&lt;COMPANY-URL&gt;/v1/k8s/clusters' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;ACCESS-TOKEN&gt;'\n</code></pre> <pre><code>import http.client\n\nconn = http.client.HTTPSConnection(\"https://&lt;COMPANY-URL&gt;\")\nheaders = {\n    'content-type': \"application/json\",\n    'authorization': \"Bearer &lt;ACCESS-TOKEN&gt;\"\n    }\nconn.request(\"GET\", \"/v1/k8s/clusters\", headers=headers)\n\nres = conn.getresponse()\ndata = res.read()\n\nprint(data.decode(\"utf-8\"))\n</code></pre> <p>(replace <code>&lt;ACCESS-TOKEN&gt;</code> with the bearer token from above).</p> <p>For an additional example, see the following code. It is an example of how to use the Run:ai Administrator REST API to create a User and a Project and set the User to the Project.  </p>"},{"location":"developer/admin-rest-api/overview/#administrator-api-documentation","title":"Administrator API Documentation","text":"<p>The Administrator API provides the developer interfaces for getting and manipulating the Run:ai metadata objects such as Projects, Departments, Clusters, and Users.</p> <p>Detailed API documentation can be found at https://app.run.ai/api/docs. This represents the latest control-plane documentation. If you are running a self-hosted version, see <code>https://&lt;runai-company-url&gt;/api/docs</code>.</p> <p>Administrator API Documentation</p>"},{"location":"developer/cluster-api/other-resources/","title":"Support for other Kubernetes Applications","text":""},{"location":"developer/cluster-api/other-resources/#introduction","title":"Introduction","text":"<p>Kubernetes has several built-in resources that encapsulate running Pods. These are called Kubernetes Workloads and should not be confused with Run:ai Workloads.</p> <p>Examples of such resources are a Deployment that manages a stateless application, or a Job that runs tasks to completion.</p> <p>Run:ai natively runs Run:ai Workloads. A Run:ai workload encapsulates all the resources needed to run, creates them, and deletes them together. However, Run:ai, being an open platform allows the scheduling of any Kubernetes Workflow.</p>"},{"location":"developer/cluster-api/other-resources/#how-to","title":"How To","text":"<p>To run Kubernetes Workloads with Run:ai you must add the following to the YAML:</p> <ul> <li>A namespace that is associated with a Run:ai Project.</li> <li>A scheduler name: <code>runai-scheduler</code>.</li> <li>When using Fractions, use a specific syntax for the <code>nvidia/gpu</code> limit.</li> </ul>"},{"location":"developer/cluster-api/other-resources/#example-job","title":"Example: Job","text":"job1.yaml<pre><code>apiVersion: batch/v1\nkind: Job # (1)\nmetadata:\n  name: job1\n  namespace: runai-team-a # (2)\nspec:\n  template:\n    spec:\n      containers:\n      - name: job1-container\n        image: gcr.io/run-ai-demo/quickstart\n        resources:\n          limits:\n            nvidia.com/gpu: 1 # (4)\n      restartPolicy: Never\n      schedulerName: runai-scheduler # (3)\n</code></pre> <ol> <li>This is a Kubernetes Job.</li> <li>Namespace: Replace <code>runai-team-a</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).</li> <li>The job to be scheduled with the Run:ai scheduler.</li> <li>To run with half a GPU replace 1 with \"0.5\" (with apostrophes).</li> </ol> <p>To submit the Job run:</p> <pre><code>kubectl apply -f job1.yaml\n</code></pre> <p>You will be able to see the Job in the Run:ai User interface, including all metrics and lists</p>"},{"location":"developer/cluster-api/other-resources/#example-deployment","title":"Example: Deployment","text":"deployment1.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment # (1)\nmetadata:\n  name: inference-1\n  namespace: runai-team-a # (2)\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: inference-1\n  template:\n    metadata:\n      labels:\n        app: inference-1\n    spec:\n      containers:\n        - resources:\n            limits:\n              nvidia.com/gpu: 1 # (4)\n          image: runai/example-marian-server\n          imagePullPolicy: Always\n          name: inference-1\n          ports:\n            - containerPort: 8888\n      schedulerName: runai-scheduler # (3)\n\n---\napiVersion: v1\nkind: Service # (5)\nmetadata:\n  labels:\n    app: inference-1\n  name: inference-1\nspec:\n  type: ClusterIP\n  ports:\n    - port: 8888\n      targetPort: 8888\n  selector:\n    app: inference-1\n</code></pre> <ol> <li>This is a Kubernetes Deployment.</li> <li>Namespace: Replace <code>runai-team-a</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).</li> <li>The job to be scheduled with the Run:ai scheduler.</li> <li>To run with half a GPU replace 1 with \"0.5\" (with apostrophes).</li> <li>This example also contains the creation of a service to connect to the deployment. It is not mandatory.</li> </ol> <p>To submit the Deployment run:</p> <pre><code>kubectl apply -f deployment1.yaml\n</code></pre>"},{"location":"developer/cluster-api/other-resources/#limitations","title":"Limitations","text":"<p>The Run:ai command line interface provides limited support for Kubernetes Workloads.</p>"},{"location":"developer/cluster-api/other-resources/#see-also","title":"See Also","text":"<p>Run:ai has specific integrations with additional third-party tools such as KubeFlow, MLFlow, and more. These integrations use the same instructions as described above.</p>"},{"location":"developer/cluster-api/submit-cron-yaml/","title":"Submit a Cron job via YAML","text":"<p> Version 2.10 and later.</p> <p>The cron command-line utility is a job scheduler typically used to set up and maintain software environments at scheduled intervals. Run:ai now supports submitting jobs with cron using a YAML file. </p> <p>To submit a job using cron, run the following command:</p> <pre><code>kubectl apply -f &lt;file_name&gt;.yaml\n</code></pre> <p>The following is an example YAML file:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"* * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n          - (Mandatory) runai/queue: team-a\n        spec:\n          (Mandatory) schedulerName: runai-scheduler\n          containers:\n          - name: hello\n            image: busybox:1.28\n            imagePullPolicy: IfNotPresent\n            command:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n          (Optional) priorityClassName: build / train / inference / interactivePreemptible\n</code></pre>"},{"location":"developer/cluster-api/submit-rest/","title":"Submitting Workloads via HTTP/REST","text":"<p>You can submit Workloads via HTTP calls, using the Kubernetes REST API.</p>"},{"location":"developer/cluster-api/submit-rest/#submit-workload-example","title":"Submit Workload Example","text":"<p>To submit a workload via HTTP, run the following:</p> <pre><code>curl -X POST \\ # (1) \n'https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/trainingworkloads' \\ \n    --header 'Content-Type: application/yaml' \\\n    --header 'Authorization: Bearer &lt;BEARER&gt;' \\  # (2) \n    --data-raw 'apiVersion: run.ai/v2alpha1\nkind: TrainingWorkload  # (3)\nmetadata:\n  name: job-1    \nspec:\n  gpu:\n    value: \"1\"\n  image:\n    value: gcr.io/run-ai-demo/quickstart\n  name:\n    value: job-1  \n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code>, <code>distributedworkloads</code> or <code>inferenceworkloads</code> according to type.</li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> <li>See Submitting a Workload via YAML for an explanation of the YAML-based workload.</li> </ol> <p>Run: <code>runai list jobs</code> to see the new Workload.</p>"},{"location":"developer/cluster-api/submit-rest/#delete-workload-example","title":"Delete Workload Example","text":"<p>To delete a workload run:</p> <pre><code>curl -X DELETE \\ # (1) \n'https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/trainingworkloads/&lt;JOB-NAME&gt;' \\ \n    --header 'Content-Type: application/yaml' \\\n    --header 'Authorization: Bearer &lt;BEARER&gt;'   # (2)\n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code>, <code>distributedworkloads</code> or <code>inferenceworkloads</code> according to type.  Replace <code>&lt;JOB-NAME&gt;</code> with the name of the Job. </li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> </ol>"},{"location":"developer/cluster-api/submit-rest/#suspendstop-workload-example","title":"Suspend/Stop workload example","text":"<p>To suspend or stop a workload run:</p> <pre><code>curl -X PATCH \\ # (1) \n'https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/interactiveworkload/&lt;JOB-NAME&gt;' \\\n    --header 'Content-Type: application/json' \n    --header 'Authorization: Bearer &lt;TOKEN&gt;'# (2) \n    --data '{\"spec\":{\"active\": {\"value\": \"false\"}}}'\n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code>, <code>distributedworkloads</code> or <code>inferenceworkloads</code> according to type.  Replace <code>&lt;JOB-NAME&gt;</code> with the name of the Job. </li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> </ol>"},{"location":"developer/cluster-api/submit-rest/#using-other-programming-languages","title":"Using other Programming Languages","text":"<p>You can use any Kubernetes client library together with the YAML documentation above to submit workloads via other programming languages. For more information see Kubernetes client libraries.</p>"},{"location":"developer/cluster-api/submit-rest/#python-example","title":"Python example","text":"<p>Create the following file and run it via python:</p> create-train.py<pre><code>import json\nimport requests\n\n# (1)\nurl = \"https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/trainingworkloads\"\n\npayload = json.dumps({\n  \"apiVersion\": \"run.ai/v2alpha1\",\n  \"kind\": \"TrainingWorkload\",\n  \"metadata\": {\n    \"name\": \"train1\",\n    \"namespace\": \"runai-team-a\"\n  },\n  \"spec\": {\n    \"image\": {\n      \"value\": \"gcr.io/run-ai-demo/quickstart\"\n    },\n    \"name\": {\n      \"value\": \"train1\"\n    },\n    \"gpu\": {\n      \"value\": \"1\"\n    }\n  }\n})\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;TOKEN&gt;' #(2)\n}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload) # (3)\n\nprint(json.dumps(json.loads(response.text), indent=4))\n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code>, <code>distributedworkloads</code>or <code>inferenceworkloads</code> according to type.</li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> <li>if you do not have a valid certificate, you can add the flag <code>verify=False</code>.</li> </ol>"},{"location":"developer/cluster-api/submit-yaml/","title":"Submitting Workloads via YAML","text":"<p>You can use YAML to submit Workloads directly to Run:ai. Below are examples of how to create training, interactive and inference workloads via YAML.</p>"},{"location":"developer/cluster-api/submit-yaml/#submit-workload-example","title":"Submit Workload Example","text":"<p>Create a file named <code>training1.yaml</code> with the following text:</p> training1.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingWorkload # (1)\nmetadata:\n  name: job-1  # (2) \n  namespace: runai-team-a # (3)\nspec:\n  gpu:\n    value: \"1\"\n  image:\n    value: gcr.io/run-ai-demo/quickstart\n  name:\n    value: job-1 # (4)\n</code></pre> <ol> <li>This is a Training workload.</li> <li>Kubernetes object name. Mandatory, but has no functional significance.</li> <li>Namespace: Replace <code>runai-team-a</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).</li> <li>Job name as appears in Run:ai. Can provide name, or create automatically if name prefix is configured. </li> </ol> <p>Change the namespace and run: <code>kubectl apply -f training1.yaml</code></p> <p>Run: <code>runai list jobs</code> to see the new Workload.</p>"},{"location":"developer/cluster-api/submit-yaml/#delete-workload-example","title":"Delete Workload Example","text":"<p>Run: <code>kubectl delete -f training1.yaml</code> to delete the Workload. </p>"},{"location":"developer/cluster-api/submit-yaml/#creating-a-yaml-syntax-from-a-cli-command","title":"Creating a YAML syntax from a CLI command","text":"<p>An easy way to create a YAML for a workload is to generate it from the <code>runai submit</code> command by using the <code>--dry-run</code> flag. For example, run:</p> <pre><code>runai submit build1 -i ubuntu -g 1 --interactive --dry-run \\\n     -- sleep infinity \n</code></pre> <p>The result will be the following Kubernetes object declaration:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: InteractiveWorkload  # (1)\nmetadata:\n  creationTimestamp: null\n  labels:\n    PreviousJob: \"true\"\n  name: job-0-2022-05-02t08-50-57\n  namespace: runai-team-a\nspec:\n  command:\n    value: sleep infinity\n  gpu:\n    value: \"1\"\n  image:\n    value: ubuntu\n  imagePullPolicy:\n    value: Always\n  name:\n    value: job-0\n\n... Additional internal and status properties...\n</code></pre> <ol> <li>This is an Interactive workload.</li> </ol>"},{"location":"developer/cluster-api/submit-yaml/#inference-workload-example","title":"Inference Workload Example","text":"<p>Creating an inference workload is similar to the above two examples.</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: InferenceWorkload\nmetadata:\n  name: inference1\n  namespace: runai-team-a\nspec:\n  name:\n    value: inference1\n  gpu:\n    value: \"0.5\"\n  image:\n    value: \"gcr.io/run-ai-demo/example-triton-server\"\n  minScale:\n    value: 1\n  maxScale:\n    value: 2\n  metric:\n    value: concurrency # (1)\n  target:\n    value: 80  # (2)\n  ports:\n      items:\n        port1:\n          value:\n            container: 8000\n            protocol: http\n            serviceType: ServingPort\n</code></pre> <ol> <li>Possible metrics can be <code>cpu-utilization</code>, <code>latency</code>, <code>throughput</code>, <code>concurrency</code>, <code>gpu-utilization</code>, <code>custom</code>. Different metrics may require additional installations at the cluster level.</li> <li>Inference requires a port to receive requests.</li> </ol>"},{"location":"developer/cluster-api/submit-yaml/#suspendresume-interactivetraining-workload","title":"Suspend/Resume Interactive/Training Workload","text":"<p>To suspend training:</p> <p><pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingWorkload # \nmetadata:\n  name: job-1  #  \n  namespace: runai-team-a # \nspec:\n  gpu:\n    value: \"1\"\n  active:\n    value: false\n  image:\n    value: gcr.io/run-ai-demo/quickstart\n  name:\n    value: job-1 # \n</code></pre> In order to suspend the workload, set <code>active</code> to <code>false</code>. To resume the workload, either set <code>active</code> to <code>true</code> or remove it entirely.</p>"},{"location":"developer/cluster-api/submit-yaml/#see-also","title":"See Also","text":"<ul> <li>To understand how to connect to the inference workload, see Inference Quickstart.</li> <li>To learn more about Inference and Run:ai see Inference overview.</li> </ul>"},{"location":"developer/cluster-api/workload-overview-dev/","title":"Workloads Overview","text":""},{"location":"developer/cluster-api/workload-overview-dev/#workloads","title":"Workloads","text":"<p>Run:ai schedules Workloads. Run:ai workloads contain:</p> <ul> <li>The Kubernetes resource (Job, Deployment, etc) that is used to launch the container inside which the data science code runs.</li> <li>A set of additional resources that is required to run the Workload. Examples: a service entry point that allows access to the Job, a persistent volume claim to access data on the network and more.</li> </ul> <p>Run:ai supports the following Workloads types:</p> Workload Type Kubernetes Name Description Interactive <code>InteractiveWorkload</code> Submit an interactive workload Training <code>TrainingWorkload</code> Submit a training workload Distributed Training <code>DistributedWorkload</code> Submit a distributed training workload using TensorFlow, PyTorch or MPI Inference <code>InferenceWorkload</code> Submit an inference workload"},{"location":"developer/cluster-api/workload-overview-dev/#values","title":"Values","text":"<p>A Workload will typically have a list of values, such as name, image, and resources. A full list of values is available in the runai-submit Command-line reference.  </p> <p>You can also find the exact YAML syntax run:</p> <pre><code>kubectl explain TrainingWorkload.spec\n</code></pre> <p>(and similarly for other Workload types).</p> <p>To get information on a specific value (e.g. <code>node type</code>), you can also run:</p> <pre><code>kubectl explain TrainingWorkload.spec.nodeType\n</code></pre> <p>Result:</p> <pre><code>KIND:     TrainingWorkload\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: nodeType &lt;Object&gt;\n\nDESCRIPTION:\n     Specifies nodes (machines) or a group of nodes on which the workload will\n     run. To use this feature, your Administrator will need to label nodes as\n     explained in the Group Nodes guide at\n     https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag\n     can be used in conjunction with Project-based affinity. In this case, the\n     flag is used to refine the list of allowable node groups set in the\n     Project. For more information consult the Projects guide at\n     https://docs.run.ai/admin/admin-ui-setup/project-setup.\n\nFIELDS:\n   value    &lt;string&gt;\n</code></pre>"},{"location":"developer/cluster-api/workload-overview-dev/#how-to-submit","title":"How to Submit","text":"<p>A Workload can be submitted via various channels:</p> <ul> <li>The Run:ai user interface.</li> <li>The Run:ai command-line interface, via the runai submit command.</li> <li>The Run:ai Cluster API.</li> </ul>"},{"location":"developer/cluster-api/workload-overview-dev/#policies","title":"Policies","text":"<p>An Administrator can set Policies for Workload submission. Policies serve two purposes:</p> <ol> <li>To constrain the values a researcher can specify.</li> <li>To provide default values.</li> </ol> <p>For example, an administrator can,</p> <ul> <li>Set a maximum of 5 GPUs per Workload.</li> <li>Provide a default value of 1 GPU for each container.</li> </ul> <p>Each workload type has a matching kind of workload policy. For example, an <code>InteractiveWorkload</code> has a matching <code>InteractivePolicy</code></p> <p>A Policy of each type can be defined per-project. There is also a global policy that applies to any project that does not have a per-project policy.</p> <p>For further details on policies, see Policies.</p>"},{"location":"developer/cluster-api/reference/distributed/","title":"Distributed Training Workload Parameters","text":"<p>The following is a full list of all distributed workload parameters. The text below is equivalent to running <code>kubectl explain distributedworkload.spec</code>. You can also run <code>kubectl explain distributedworkload.spec.&lt;parameter-name&gt;</code> to see the description of a specific parameter.</p> <pre><code>KIND:     DistributedWorkload\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: spec &lt;Object&gt;\n\nDESCRIPTION:\n     The specifications of this DistributedWorkload\n\nFIELDS:\n   allowPrivilegeEscalation &lt;Object&gt;\n     Allow the container running the workload and all launched processes to gain\n     additional privileges after the workload starts. For more information see\n     the \"User Identity in Container\" guide at\n     https://docs.run.ai/admin/runai-setup/config/non-root-containers/\n\n   annotations  &lt;Object&gt;\n     Specifies annotations to be set in the container that is running the\n     created workload.\n\n   arguments    &lt;Object&gt;\n     When set,contains the arguments sent along with the command. These override\n     the entry point of the image in the created workload.\n\n   autoDeletionTimeAfterCompletionSeconds   &lt;Object&gt;\n     Specifies the duration after which it is possible for a finished workload\n     to be automatically deleted. When the workload is being deleted, its\n     lifecycle guarantees (e.g. finalizers) will be honored. If this field is\n     unset, the workload won't be automatically deleted. If this field is set to\n     zero, the workload becomes eligible to be deleted immediately after it\n     finishes.\n\n   baseWorkload &lt;string&gt;\n     Reference to another workload. When set, this workload inherits its values\n     from the base workload. Base workload can either reside on the same\n     namespace of this workload (referred to as \"user\" template) or can reside\n     in the runai namespace (referred to as a \"global\" template)\n\n   capabilities &lt;Object&gt;\n     The capabilities field allows adding a set of unix capabilities to the\n     container running the workload. Capabilities are Linux distinct privileges\n     traditionally associated with superuser which can be independently enabled\n     and disabled. For more information see\n     https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container\n\n   command  &lt;Object&gt;\n     If set, overrides the image's entry point with the supplied command.\n\n   cpu  &lt;Object&gt;\n     Specifies CPU units to allocate for the created workload (0.5, 1, .etc).\n     The workload will receive at least this amount of CPU. Note that the\n     workload will not be scheduled unless the system can guarantee this amount\n     of CPUs to the workload.\n\n   cpuLimit &lt;Object&gt;\n     Specifies a limit on the number of CPUs consumed by the workload (0.5, 1,\n     .etc). The system guarantees that this workload will not be able to consume\n     more than this amount of CPUs.\n\n   createHomeDir    &lt;Object&gt;\n     Instructs the system to create a temporary home directory for the user\n     within the container. Data stored in this directory will not be saved when\n     the container exits. When the runAsUser flag is set to true, this flag will\n     default to true as well.\n\n   environment  &lt;Object&gt;\n     Specifies environment variables to be set in the container running the\n     created workload.\n\n   exposedUrls  &lt;Object&gt;\n     Specifies a set of exported url (e.g. ingress) from the container running\n     the created workload.\n\n   extendedResources    &lt;Object&gt;\n     Specifies values for extended resources. Extended resources are third-party\n     devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that\n     you want to allocate to your Job. For more information see:\n     https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/\n\n   gitSync  &lt;Object&gt;\n     Specifies git repositories to mount into the container running the\n     workload.\n\n   gpu  &lt;Object&gt;\n     Specifies the number on the number of GPUs to allocate for the created\n     workload. The default is no allocated GPUs. The GPU value can be an integer\n     or a fraction between 0 and 1.\n\n   gpuLimit &lt;Object&gt;\n     Specifies a limit on the GPUs to allocate for this workload (1G, 20M,\n     .etc). Intended to use for Opportunistic jobs (with the smart\n     node-scheduler).\n\n   gpuMemory    &lt;Object&gt;\n     Specifies GPU memory to allocate for the created workload. The workload\n     will receive this amount of memory. Note that the workload will not be\n     scheduled unless the system can guarantee this amount of GPU memory to the\n     workload.\n\n   hostIpc  &lt;Object&gt;\n     Specifies that the created workload will use the host's ipc namespace.\n\n   hostNetwork  &lt;Object&gt;\n     Specifies that the created workload will use the host's network stack\n     inside its container. For more information see the Docker Run Reference at\n     https://docs.docker.com/engine/reference/run/\n\n   image    &lt;Object&gt;\n     Specifies the image to use when creating the container running the\n     workload.\n\n   imagePullPolicy  &lt;Object&gt;\n     Specifies the pull policy of the image when starting a container running\n     the created workload. Options are: always, ifNotPresent, or never. For more\n     information see: https://kubernetes.io/docs/concepts/containers/images\n\n   ingressUrl   &lt;Object&gt;\n     This field is for internal use only.\n\n   jobType  &lt;string&gt;\n     The type of distributed job.\n\n   labels   &lt;Object&gt;\n     Specifies labels to be set in the container running the created workload.\n\n   largeShm &lt;Object&gt;\n     Specifies a large /dev/shm device to mount into a container running the\n     created workload. SHM is a shared file system mounted on RAM.\n\n   memory   &lt;Object&gt;\n     Specifies the amount of CPU memory to allocate for this workload (1G, 20M,\n     .etc). The workload will receive at least this amount of memory. Note that\n     the workload will not be scheduled unless the system can guarantee this\n     amount of memory to the workload\n\n   memoryLimit  &lt;Object&gt;\n     Specifies a limit on the CPU memory to allocate for this workload (1G, 20M,\n     .etc). The system guarantees that this workload will not be able to consume\n     more than this amount of memory. The workload will receive an error when\n     trying to allocate more memory than this limit.\n\n   migProfile   &lt;Object&gt;\n     Specifies the memory profile to be used for workload running on NVIDIA\n     Multi-Instance GPU (MIG) technology.\n\n   mountPropagation &lt;Object&gt;\n     Allows for sharing volumes mounted by a container to other containers in\n     the same pod, or even to other pods on the same node. The volume mount will\n     receive all subsequent mounts that are mounted to this volume or any of its\n     subdirectories.\n\n   mpiJob   &lt;Object&gt;\n     Specific fields for distributed MPI Job.\n\n   name &lt;Object&gt;\n     The specific name of the created resource. Either name of namePrefix should\n     be provided, but not both.\n\n   namePrefix   &lt;Object&gt;\n     A prefix used for assigning a name to the created resource. Either name of\n     namePrefix should be provided, but not both.\n\n   nfs  &lt;Object&gt;\n     Specifies nfs volumes to mount into a container running the created\n     workload.\n\n   noMaster &lt;Object&gt;\n     Request the job not to have a master pod.\n\n   nodePool &lt;Object&gt;\n     Specifies the NodePool name to be used to schedule this job on - DEPRECATED\n     use NodePools instead\n\n   nodePools    &lt;Object&gt;\n     Specifies the list of node pools to use for scheduling the job, ordered by\n     preference.\n\n   nodeType &lt;Object&gt;\n     Specifies nodes (machines) or a group of nodes on which the workload will\n     run. To use this feature, your Administrator will need to label nodes as\n     explained in the Group Nodes guide at\n     https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag\n     can be used in conjunction with Project-based affinity. In this case, the\n     flag is used to refine the list of allowable node groups set in the\n     Project. For more information see the Projects setup guide at\n     https://docs.run.ai/admin/admin-ui-setup/project-setup.\n\n   nonPreemptible   &lt;Object&gt;\n     Request the job to be non-preemptible.\n\n   podAffinity  &lt;Object&gt;\n     Indicates whether pod affinity scheduling rules apply.\n\n   podAffinitySchedulingRule    &lt;Object&gt;\n     Indicates if we want to use the Pod affinity rule as : the \"hard\"\n     (required) or the \"soft\" (preferred) This field can be specified only if\n     PodAffinity is set to true\n\n   podAffinityTopology  &lt;Object&gt;\n     Specifies the Pod Affinity Topology to be used for scheduling the job This\n     field can be specified only if PodAffinity is set to true\n\n   ports    &lt;Object&gt;\n     Specifies a set of ports exposed from the container running the created\n     workload. Used together with --service-type.\n\n   preemptionLimit  &lt;Object&gt;\n     indicates the number of times the job can be preempted\n\n   pvcs &lt;Object&gt;\n     Specifies persistent volume claims to mount into a container running the\n     created workload.\n\n   pyTorchJob   &lt;Object&gt;\n     Specific fields for distributed PyTorch Job.\n\n   runAsGid &lt;Object&gt;\n     Specifies the Unix group id with which the container should run. Will be\n     used only if runAsUser is set to true.\n\n   runAsNonRoot &lt;Object&gt;\n     Indicates that the container must run as a non-root user. If true, the\n     Kubelet will validate the image at runtime to ensure that it does not run\n     as UID 0 (root) and fail to start the container if it does. If unset or\n     false, no such validation will be performed.\n\n   runAsUid &lt;Object&gt;\n     Specifies the Unix user id with which the container running the created\n     workload should run. Will be used only if runAsUser is set to true.\n\n   runAsUser    &lt;Object&gt;\n     Limits the container running the created workload to run in the context of\n     a specific non-root user. The user id is provided by the runAsUid field.\n     This would manifest itself in access to operating system resources, in the\n     ownership of new folders created under shared directories, etc.\n     Alternatively, if your cluster is connected to Run:ai via SAML, you can map\n     the container to use the Linux UID/GID which is stored in the\n     organization's directory. For more information see the User Identity guide\n     at https://docs.run.ai/admin/runai-setup/config/non-root-containers/\n\n   runPolicy    &lt;Object&gt;\n     RunPolicy is shared between all distributed jobs.\n\n   s3   &lt;Object&gt;\n     Specifies S3 buckets to mount into the container running the workload\n\n   seccompProfileType   &lt;Object&gt;\n     Indicates which kind of seccomp profile will be applied to the container.\n     Valid options are: RuntimeDefault - the container runtime default profile\n     should be used. Unconfined - no profile should be applied. Localhost is not\n     yet supported by Run:ai.\n\n   serviceType  &lt;Object&gt;\n     Specifies the default service exposure method for ports. The default shall\n     be used for ports which do not specify service type. Options are:\n     LoadBalancer, NodePort or ClusterIP. For more information see the External\n     Access to Containers guide on\n     https://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/\n\n   stdin    &lt;Object&gt;\n     Instructs the system to keep stdin open for the container(s) running the\n     created workload, even if nothing is attached.\n\n   supplementalGroups   &lt;Object&gt;\n     ';' separated list of supplemental group IDs. Will be added to the security\n     context of the container running the created workload.\n\n   tolerations  &lt;Object&gt;\n     Toleration rules which apply to the pods running the workload. Toleration\n     rules guide (but do not require) the system to which node each pod can be\n     scheduled to or evicted from, based on matching between those rules and the\n     set of taints defined for each Kubernetes node.\n\n   tty  &lt;Object&gt;\n     Instructs the system to allocate a pseudo-TTY for the created workload.\n\n   usage    &lt;string&gt;\n     The intended usage of this workload. possible values are \"Template\": this\n     workload is used as the base for other workloads. \"Submit\": this workload\n     is used for submitting a job and/or other Kubernetes resources.\n\n   userId   &lt;Object&gt;\n     The user ID (\"Subject\" in the jwt-token) of the authenticated user who owns\n     the workload. The data might be used for authentication or authorization\n     purposes.\n\n   username &lt;Object&gt;\n     Display-only field describing the user who owns the workload. The data is\n     not used for authentication or authorization purposes.\n\n   volumes  &lt;Object&gt;\n     Specifies volumes to mount into a container running the created workload.\n\n   workers  &lt;Object&gt;\n     The desired number of worker pods.\n\n   workingDir   &lt;Object&gt;\n     Specifies a directory that will be used as the current directory when the\n     container running the created workload starts.\n</code></pre>"},{"location":"developer/cluster-api/reference/inference/","title":"Inference Workload Parameters","text":"<p>The following is a full list of all inference workload parameters. The text below is equivalent to running <code>kubectl explain inferenceworkload.spec</code>. You can also run <code>kubectl explain inferenceworkload.spec.&lt;parameter-name&gt;</code> to see the description of a specific parameter.</p> <pre><code>KIND:     InferenceWorkload\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: spec &lt;Object&gt;\n\nDESCRIPTION:\n     The specifications of this workload\n\nFIELDS:\n   annotations  &lt;Object&gt;\n     Specifies annotations to be set in the container that is running the\n     created workload.\n\n   arguments    &lt;Object&gt;\n     When set,contains the arguments sent along with the command. These override\n     the entry point of the image in the created workload.\n\n   autoDeletionTimeAfterCompletionSeconds   &lt;Object&gt;\n     Specifies the duration after which it is possible for a finished workload\n     to be automatically deleted. When the workload is being deleted, its\n     lifecycle guarantees (e.g. finalizers) will be honored. If this field is\n     unset, the workload won't be automatically deleted. If this field is set to\n     zero, the workload becomes eligible to be deleted immediately after it\n     finishes.\n\n   baseWorkload &lt;string&gt;\n     Reference to another workload. When set, this workload inherits its values\n     from the base workload. Base workload can either reside on the same\n     namespace of this workload (referred to as \"user\" template) or can reside\n     in the runai namespace (referred to as a \"global\" template)\n\n   capabilities &lt;Object&gt;\n     The capabilities field allows adding a set of unix capabilities to the\n     container running the workload. Capabilities are Linux distinct privileges\n     traditionally associated with superuser which can be independently enabled\n     and disabled. For more information see\n     https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container\n\n   class    &lt;Object&gt;\n     The autoscaler class for knative to use\n\n   command  &lt;Object&gt;\n     If set, overrides the image's entry point with the supplied command.\n\n   cpu  &lt;Object&gt;\n     Specifies CPU units to allocate for the created workload (0.5, 1, .etc).\n     The workload will receive at least this amount of CPU. Note that the\n     workload will not be scheduled unless the system can guarantee this amount\n     of CPUs to the workload.\n\n   cpuLimit &lt;Object&gt;\n     Specifies a limit on the number of CPUs consumed by the workload (0.5, 1,\n     .etc). The system guarantees that this workload will not be able to consume\n     more than this amount of CPUs.\n\n   createHomeDir    &lt;Object&gt;\n     Instructs the system to create a temporary home directory for the user\n     within the container. Data stored in this directory will not be saved when\n     the container exits. When the runAsUser flag is set to true, this flag will\n     default to true as well.\n\n   environment  &lt;Object&gt;\n     Specifies environment variables to be set in the container running the\n     created workload.\n\n   exposedUrls  &lt;Object&gt;\n     Specifies a set of exported url (e.g. ingress) from the container running\n     the created workload.\n\n   extendedResources    &lt;Object&gt;\n     Specifies values for extended resources. Extended resources are third-party\n     devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that\n     you want to allocate to your Job. For more information see:\n     https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/\n\n   gitSync  &lt;Object&gt;\n     Specifies git repositories to mount into the container running the\n     workload.\n\n   gpu  &lt;Object&gt;\n     Specifies the number on the number of GPUs to allocate for the created\n     workload. The default is no allocated GPUs. The GPU value can be an integer\n     or a fraction between 0 and 1.\n\n   gpuLimit &lt;Object&gt;\n     Specifies a limit on the GPUs to allocate for this workload (1G, 20M,\n     .etc). Intended to use for Opportunistic jobs (with the smart\n     node-scheduler).\n\n   gpuMemory    &lt;Object&gt;\n     Specifies GPU memory to allocate for the created workload. The workload\n     will receive this amount of memory. Note that the workload will not be\n     scheduled unless the system can guarantee this amount of GPU memory to the\n     workload.\n\n   hostIpc  &lt;Object&gt;\n     Specifies that the created workload will use the host's ipc namespace.\n\n   hostNetwork  &lt;Object&gt;\n     Specifies that the created workload will use the host's network stack\n     inside its container. For more information see the Docker Run Reference at\n     https://docs.docker.com/engine/reference/run/\n\n   image    &lt;Object&gt;\n     Specifies the image to use when creating the container running the\n     workload.\n\n   imagePullPolicy  &lt;Object&gt;\n     Specifies the pull policy of the image when starting a container running\n     the created workload. Options are: always, ifNotPresent, or never. For more\n     information see: https://kubernetes.io/docs/concepts/containers/images\n\n   ingressUrl   &lt;Object&gt;\n     This field is for internal use only.\n\n   isPrivateServiceUrl  &lt;Object&gt;\n     Configure the inference service to be available only on the cluster-local\n     network, and not on the public internet\n\n   labels   &lt;Object&gt;\n     Specifies labels to be set in the container running the created workload.\n\n   largeShm &lt;Object&gt;\n     Specifies a large /dev/shm device to mount into a container running the\n     created workload. SHM is a shared file system mounted on RAM.\n\n   maxScale &lt;Object&gt;\n     The maximum number of replicas to run\n\n   memory   &lt;Object&gt;\n     Specifies the amount of CPU memory to allocate for this workload (1G, 20M,\n     .etc). The workload will receive at least this amount of memory. Note that\n     the workload will not be scheduled unless the system can guarantee this\n     amount of memory to the workload\n\n   memoryLimit  &lt;Object&gt;\n     Specifies a limit on the CPU memory to allocate for this workload (1G, 20M,\n     .etc). The system guarantees that this workload will not be able to consume\n     more than this amount of memory. The workload will receive an error when\n     trying to allocate more memory than this limit.\n\n   metric   &lt;Object&gt;\n     The predefined metric to use for autoscaling. Possible values are:\n     cpu-utilization, latency, throughput, concurrency, gpu-utilization, custom.\n\n   metricName   &lt;Object&gt;\n     The exact metric name to use for autoscaling (overrides Metric field)\n\n   migProfile   &lt;Object&gt;\n     Specifies the memory profile to be used for workload running on NVIDIA\n     Multi-Instance GPU (MIG) technology.\n\n   minScale &lt;Object&gt;\n     The minimum number of replicas to run\n\n   mountPropagation &lt;Object&gt;\n     Allows for sharing volumes mounted by a container to other containers in\n     the same pod, or even to other pods on the same node. The volume mount will\n     receive all subsequent mounts that are mounted to this volume or any of its\n     subdirectories.\n\n   mps  &lt;Object&gt;\n\n   name &lt;Object&gt;\n     The specific name of the created resource. Either name of namePrefix should\n     be provided, but not both.\n\n   namePrefix   &lt;Object&gt;\n     A prefix used for assigning a name to the created resource. Either name of\n     namePrefix should be provided, but not both.\n\n   nfs  &lt;Object&gt;\n     Specifies nfs volumes to mount into a container running the created\n     workload.\n\n   nodePool &lt;Object&gt;\n     Specifies the NodePool name to be used to schedule this job on - DEPRECATED\n     use NodePools instead\n\n   nodePools    &lt;Object&gt;\n     Specifies the list of node pools to use for scheduling the job, ordered by\n     preference.\n\n   nodeType &lt;Object&gt;\n     Specifies nodes (machines) or a group of nodes on which the workload will\n     run. To use this feature, your Administrator will need to label nodes as\n     explained in the Group Nodes guide at\n     https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag\n     can be used in conjunction with Project-based affinity. In this case, the\n     flag is used to refine the list of allowable node groups set in the\n     Project. For more information see the Projects setup guide at\n     https://docs.run.ai/admin/admin-ui-setup/project-setup.\n\n   podAffinity  &lt;Object&gt;\n     Indicates whether pod affinity scheduling rules apply.\n\n   podAffinitySchedulingRule    &lt;Object&gt;\n     Indicates if we want to use the Pod affinity rule as : the \"hard\"\n     (required) or the \"soft\" (preferred) This field can be specified only if\n     PodAffinity is set to true\n\n   podAffinityTopology  &lt;Object&gt;\n     Specifies the Pod Affinity Topology to be used for scheduling the job This\n     field can be specified only if PodAffinity is set to true\n\n   ports    &lt;Object&gt;\n     Specifies a set of ports exposed from the container running the created\n     workload. Used together with --service-type.\n\n   preemptionLimit  &lt;Object&gt;\n     indicates the number of times the job can be preempted\n\n   pvcs &lt;Object&gt;\n     Specifies persistent volume claims to mount into a container running the\n     created workload.\n\n   runAsGid &lt;Object&gt;\n     Specifies the Unix group id with which the container should run. Will be\n     used only if runAsUser is set to true.\n\n   runAsNonRoot &lt;Object&gt;\n     Indicates that the container must run as a non-root user. If true, the\n     Kubelet will validate the image at runtime to ensure that it does not run\n     as UID 0 (root) and fail to start the container if it does. If unset or\n     false, no such validation will be performed.\n\n   runAsUid &lt;Object&gt;\n     Specifies the Unix user id with which the container running the created\n     workload should run. Will be used only if runAsUser is set to true.\n\n   runAsUser    &lt;Object&gt;\n     Limits the container running the created workload to run in the context of\n     a specific non-root user. The user id is provided by the runAsUid field.\n     This would manifest itself in access to operating system resources, in the\n     ownership of new folders created under shared directories, etc.\n     Alternatively, if your cluster is connected to Run:ai via SAML, you can map\n     the container to use the Linux UID/GID which is stored in the\n     organization's directory. For more information see the User Identity guide\n     at https://docs.run.ai/admin/runai-setup/config/non-root-containers/\n\n   s3   &lt;Object&gt;\n     Specifies S3 buckets to mount into the container running the workload\n\n   seccompProfileType   &lt;Object&gt;\n     Indicates which kind of seccomp profile will be applied to the container.\n     Valid options are: RuntimeDefault - the container runtime default profile\n     should be used. Unconfined - no profile should be applied. Localhost is not\n     yet supported by Run:ai.\n\n   serviceType  &lt;Object&gt;\n     Specifies the default service exposure method for ports. The default shall\n     be used for ports which do not specify service type. Options are:\n     LoadBalancer, NodePort or ClusterIP. For more information see the External\n     Access to Containers guide on\n     https://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/\n\n   stdin    &lt;Object&gt;\n     Instructs the system to keep stdin open for the container(s) running the\n     created workload, even if nothing is attached.\n\n   supplementalGroups   &lt;Object&gt;\n     ';' separated list of supplemental group IDs. Will be added to the security\n     context of the container running the created workload.\n\n   target   &lt;Object&gt;\n     The target value for the autoscaling metric\n\n   tolerations  &lt;Object&gt;\n     Toleration rules which apply to the pods running the workload. Toleration\n     rules guide (but do not require) the system to which node each pod can be\n     scheduled to or evicted from, based on matching between those rules and the\n     set of taints defined for each Kubernetes node.\n\n   tty  &lt;Object&gt;\n     Instructs the system to allocate a pseudo-TTY for the created workload.\n\n   usage    &lt;string&gt;\n     The intended usage of this workload. possible values are \"Template\": this\n     workload is used as the base for other workloads. \"Submit\": this workload\n     is used for submitting a job and/or other Kubernetes resources.\n\n   userId   &lt;Object&gt;\n     The user ID (\"Subject\" in the jwt-token) of the authenticated user who owns\n     the workload. The data might be used for authentication or authorization\n     purposes.\n\n   username &lt;Object&gt;\n     Display-only field describing the user who owns the workload. The data is\n     not used for authentication or authorization purposes.\n\n   volumes  &lt;Object&gt;\n     Specifies volumes to mount into a container running the created workload.\n\n   workingDir   &lt;Object&gt;\n     Specifies a directory that will be used as the current directory when the\n     container running the created workload starts.\n</code></pre>"},{"location":"developer/cluster-api/reference/interactive/","title":"Interactive Workload Parameters","text":"<p>The following is a full list of all interactive workload parameters. The text below is equivalent to running <code>kubectl explain interactiveworkload.spec</code>. You can also run <code>kubectl explain interactiveworkload.spec.&lt;parameter-name&gt;</code> to see the description of a specific parameter.</p> <pre><code>KIND:     InteractiveWorkload\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: spec &lt;Object&gt;\n\nDESCRIPTION:\n     The specifications of this InteractiveWorkload\n\nFIELDS:\n   active   &lt;Object&gt;\n     kubebuilder:default:={value: true} Specifies whether the workload should be\n     active or suspended.\n\n   allowPrivilegeEscalation &lt;Object&gt;\n     Allow the container running the workload and all launched processes to gain\n     additional privileges after the workload starts. For more information see\n     the \"User Identity in Container\" guide at\n     https://docs.run.ai/admin/runai-setup/config/non-root-containers/\n\n   annotations  &lt;Object&gt;\n     Specifies annotations to be set in the container that is running the\n     created workload.\n\n   arguments    &lt;Object&gt;\n     When set,contains the arguments sent along with the command. These override\n     the entry point of the image in the created workload.\n\n   autoDeletionTimeAfterCompletionSeconds   &lt;Object&gt;\n     Specifies the duration after which it is possible for a finished workload\n     to be automatically deleted. When the workload is being deleted, its\n     lifecycle guarantees (e.g. finalizers) will be honored. If this field is\n     unset, the workload won't be automatically deleted. If this field is set to\n     zero, the workload becomes eligible to be deleted immediately after it\n     finishes.\n\n   baseWorkload &lt;string&gt;\n     Reference to another workload. When set, this workload inherits its values\n     from the base workload. Base workload can either reside on the same\n     namespace of this workload (referred to as \"user\" template) or can reside\n     in the runai namespace (referred to as a \"global\" template)\n\n   capabilities &lt;Object&gt;\n     The capabilities field allows adding a set of unix capabilities to the\n     container running the workload. Capabilities are Linux distinct privileges\n     traditionally associated with superuser which can be independently enabled\n     and disabled. For more information see\n     https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container\n\n   command  &lt;Object&gt;\n     If set, overrides the image's entry point with the supplied command.\n\n   cpu  &lt;Object&gt;\n     Specifies CPU units to allocate for the created workload (0.5, 1, .etc).\n     The workload will receive at least this amount of CPU. Note that the\n     workload will not be scheduled unless the system can guarantee this amount\n     of CPUs to the workload.\n\n   cpuLimit &lt;Object&gt;\n     Specifies a limit on the number of CPUs consumed by the workload (0.5, 1,\n     .etc). The system guarantees that this workload will not be able to consume\n     more than this amount of CPUs.\n\n   createHomeDir    &lt;Object&gt;\n     Instructs the system to create a temporary home directory for the user\n     within the container. Data stored in this directory will not be saved when\n     the container exits. When the runAsUser flag is set to true, this flag will\n     default to true as well.\n\n   environment  &lt;Object&gt;\n     Specifies environment variables to be set in the container running the\n     created workload.\n\n   exposedUrls  &lt;Object&gt;\n     Specifies a set of exported url (e.g. ingress) from the container running\n     the created workload.\n\n   extendedResources    &lt;Object&gt;\n     Specifies values for extended resources. Extended resources are third-party\n     devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that\n     you want to allocate to your Job. For more information see:\n     https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/\n\n   gitSync  &lt;Object&gt;\n     Specifies git repositories to mount into the container running the\n     workload.\n\n   gpu  &lt;Object&gt;\n     Specifies the number on the number of GPUs to allocate for the created\n     workload. The default is no allocated GPUs. The GPU value can be an integer\n     or a fraction between 0 and 1.\n\n   gpuLimit &lt;Object&gt;\n     Specifies a limit on the GPUs to allocate for this workload (1G, 20M,\n     .etc). Intended to use for Opportunistic jobs (with the smart\n     node-scheduler).\n\n   gpuMemory    &lt;Object&gt;\n     Specifies GPU memory to allocate for the created workload. The workload\n     will receive this amount of memory. Note that the workload will not be\n     scheduled unless the system can guarantee this amount of GPU memory to the\n     workload.\n\n   hostIpc  &lt;Object&gt;\n     Specifies that the created workload will use the host's ipc namespace.\n\n   hostNetwork  &lt;Object&gt;\n     Specifies that the created workload will use the host's network stack\n     inside its container. For more information see the Docker Run Reference at\n     https://docs.docker.com/engine/reference/run/\n\n   image    &lt;Object&gt;\n     Specifies the image to use when creating the container running the\n     workload.\n\n   imagePullPolicy  &lt;Object&gt;\n     Specifies the pull policy of the image when starting a container running\n     the created workload. Options are: always, ifNotPresent, or never. For more\n     information see: https://kubernetes.io/docs/concepts/containers/images\n\n   ingressUrl   &lt;Object&gt;\n     This field is for internal use only.\n\n   jupyter  &lt;Object&gt;\n     Indication if an interactive workload should run jupyter notebook\n\n   labels   &lt;Object&gt;\n     Specifies labels to be set in the container running the created workload.\n\n   largeShm &lt;Object&gt;\n     Specifies a large /dev/shm device to mount into a container running the\n     created workload. SHM is a shared file system mounted on RAM.\n\n   memory   &lt;Object&gt;\n     Specifies the amount of CPU memory to allocate for this workload (1G, 20M,\n     .etc). The workload will receive at least this amount of memory. Note that\n     the workload will not be scheduled unless the system can guarantee this\n     amount of memory to the workload\n\n   memoryLimit  &lt;Object&gt;\n     Specifies a limit on the CPU memory to allocate for this workload (1G, 20M,\n     .etc). The system guarantees that this workload will not be able to consume\n     more than this amount of memory. The workload will receive an error when\n     trying to allocate more memory than this limit.\n\n   migProfile   &lt;Object&gt;\n     Specifies the memory profile to be used for workload running on NVIDIA\n     Multi-Instance GPU (MIG) technology.\n\n   mountPropagation &lt;Object&gt;\n     Allows for sharing volumes mounted by a container to other containers in\n     the same pod, or even to other pods on the same node. The volume mount will\n     receive all subsequent mounts that are mounted to this volume or any of its\n     subdirectories.\n\n   mpi  &lt;Object&gt;\n     This workload produces mpijob\n\n   name &lt;Object&gt;\n     The specific name of the created resource. Either name of namePrefix should\n     be provided, but not both.\n\n   namePrefix   &lt;Object&gt;\n     A prefix used for assigning a name to the created resource. Either name of\n     namePrefix should be provided, but not both.\n\n   nfs  &lt;Object&gt;\n     Specifies nfs volumes to mount into a container running the created\n     workload.\n\n   nodePool &lt;Object&gt;\n     Specifies the NodePool name to be used to schedule this job on - DEPRECATED\n     use NodePools instead\n\n   nodePools    &lt;Object&gt;\n     Specifies the list of node pools to use for scheduling the job, ordered by\n     preference.\n\n   nodeType &lt;Object&gt;\n     Specifies nodes (machines) or a group of nodes on which the workload will\n     run. To use this feature, your Administrator will need to label nodes as\n     explained in the Group Nodes guide at\n     https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag\n     can be used in conjunction with Project-based affinity. In this case, the\n     flag is used to refine the list of allowable node groups set in the\n     Project. For more information see the Projects setup guide at\n     https://docs.run.ai/admin/admin-ui-setup/project-setup.\n\n   notebookToken    &lt;Object&gt;\n     A token for connecting to a Jupyter Notebook created for workloads of type\n     Jupyter. When token authentication is enabled, the notebook uses this token\n     to authenticate requests. For more information see:\n     https://jupyter-notebook.readthedocs.io/en/stable/security.html\n\n   podAffinity  &lt;Object&gt;\n     Indicates whether pod affinity scheduling rules apply.\n\n   podAffinitySchedulingRule    &lt;Object&gt;\n     Indicates if we want to use the Pod affinity rule as : the \"hard\"\n     (required) or the \"soft\" (preferred) This field can be specified only if\n     PodAffinity is set to true\n\n   podAffinityTopology  &lt;Object&gt;\n     Specifies the Pod Affinity Topology to be used for scheduling the job This\n     field can be specified only if PodAffinity is set to true\n\n   ports    &lt;Object&gt;\n     Specifies a set of ports exposed from the container running the created\n     workload. Used together with --service-type.\n\n   preemptible  &lt;Object&gt;\n     Specifies that the created workload will be preemptible. Interactive\n     preemptible workloads can be scheduled above the guaranteed quota but may\n     be reclaimed at any time.\n\n   preemptionLimit  &lt;Object&gt;\n     indicates the number of times the job can be preempted\n\n   processes    &lt;Object&gt;\n     Number of distributed training processes that will be allocated for the\n     created mpijob.\n\n   pvcs &lt;Object&gt;\n     Specifies persistent volume claims to mount into a container running the\n     created workload.\n\n   runAsGid &lt;Object&gt;\n     Specifies the Unix group id with which the container should run. Will be\n     used only if runAsUser is set to true.\n\n   runAsNonRoot &lt;Object&gt;\n     Indicates that the container must run as a non-root user. If true, the\n     Kubelet will validate the image at runtime to ensure that it does not run\n     as UID 0 (root) and fail to start the container if it does. If unset or\n     false, no such validation will be performed.\n\n   runAsUid &lt;Object&gt;\n     Specifies the Unix user id with which the container running the created\n     workload should run. Will be used only if runAsUser is set to true.\n\n   runAsUser    &lt;Object&gt;\n     Limits the container running the created workload to run in the context of\n     a specific non-root user. The user id is provided by the runAsUid field.\n     This would manifest itself in access to operating system resources, in the\n     ownership of new folders created under shared directories, etc.\n     Alternatively, if your cluster is connected to Run:ai via SAML, you can map\n     the container to use the Linux UID/GID which is stored in the\n     organization's directory. For more information see the User Identity guide\n     at https://docs.run.ai/admin/runai-setup/config/non-root-containers/\n\n   s3   &lt;Object&gt;\n     Specifies S3 buckets to mount into the container running the workload\n\n   seccompProfileType   &lt;Object&gt;\n     Indicates which kind of seccomp profile will be applied to the container.\n     Valid options are: RuntimeDefault - the container runtime default profile\n     should be used. Unconfined - no profile should be applied. Localhost is not\n     yet supported by Run:ai.\n\n   serviceType  &lt;Object&gt;\n     Specifies the default service exposure method for ports. The default shall\n     be used for ports which do not specify service type. Options are:\n     LoadBalancer, NodePort or ClusterIP. For more information see the External\n     Access to Containers guide on\n     https://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/\n\n   slotsPerWorker   &lt;Object&gt;\n     Number of slots to allocate per worker in the created mpijob.\n\n   stdin    &lt;Object&gt;\n     Instructs the system to keep stdin open for the container(s) running the\n     created workload, even if nothing is attached.\n\n   supplementalGroups   &lt;Object&gt;\n     ';' separated list of supplemental group IDs. Will be added to the security\n     context of the container running the created workload.\n\n   tensorboard  &lt;Object&gt;\n     Indicates that this interactive workload should also run a TensorBoard\n     dashboard\n\n   tensorboardLogdir    &lt;Object&gt;\n     The TensorBoard Logs directory\n\n   terminateAfterPreemption &lt;Object&gt;\n     Indicates whether the job should be terminated, by the system, after it has\n     been preempted. Default to false.\n\n   tolerations  &lt;Object&gt;\n     Toleration rules which apply to the pods running the workload. Toleration\n     rules guide (but do not require) the system to which node each pod can be\n     scheduled to or evicted from, based on matching between those rules and the\n     set of taints defined for each Kubernetes node.\n\n   tty  &lt;Object&gt;\n     Instructs the system to allocate a pseudo-TTY for the created workload.\n\n   usage    &lt;string&gt;\n     The intended usage of this workload. possible values are \"Template\": this\n     workload is used as the base for other workloads. \"Submit\": this workload\n     is used for submitting a job and/or other Kubernetes resources.\n\n   userId   &lt;Object&gt;\n     The user ID (\"Subject\" in the jwt-token) of the authenticated user who owns\n     the workload. The data might be used for authentication or authorization\n     purposes.\n\n   username &lt;Object&gt;\n     Display-only field describing the user who owns the workload. The data is\n     not used for authentication or authorization purposes.\n\n   volumes  &lt;Object&gt;\n     Specifies volumes to mount into a container running the created workload.\n\n   workingDir   &lt;Object&gt;\n     Specifies a directory that will be used as the current directory when the\n     container running the created workload starts.\n</code></pre>"},{"location":"developer/cluster-api/reference/training/","title":"Training Workload Parameters","text":"<p>The following is a full list of all training workload parameters. The text below is equivalent to running <code>kubectl explain trainingworkload.spec</code>. You can also run <code>kubectl explain trainingworkload.spec.&lt;parameter-name&gt;</code> to see the description of a specific parameter.</p> <pre><code>KIND:     TrainingWorkload\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: spec &lt;Object&gt;\n\nDESCRIPTION:\n     The specifications of this TrainingWorkload\n\nFIELDS:\n   active   &lt;Object&gt;\n     kubebuilder:default:={value: true} Specifies whether the workload should be\n     active or suspended.\n\n   allowPrivilegeEscalation &lt;Object&gt;\n     Allow the container running the workload and all launched processes to gain\n     additional privileges after the workload starts. For more information see\n     the \"User Identity in Container\" guide at\n     https://docs.run.ai/admin/runai-setup/config/non-root-containers/\n\n   annotations  &lt;Object&gt;\n     Specifies annotations to be set in the container that is running the\n     created workload.\n\n   arguments    &lt;Object&gt;\n     When set,contains the arguments sent along with the command. These override\n     the entry point of the image in the created workload.\n\n   autoDeletionTimeAfterCompletionSeconds   &lt;Object&gt;\n     Specifies the duration after which it is possible for a finished workload\n     to be automatically deleted. When the workload is being deleted, its\n     lifecycle guarantees (e.g. finalizers) will be honored. If this field is\n     unset, the workload won't be automatically deleted. If this field is set to\n     zero, the workload becomes eligible to be deleted immediately after it\n     finishes.\n\n   backoffLimit &lt;Object&gt;\n     Specifies the number of retries before marking a workload as failed.\n     Defaults to 6\n\n   baseWorkload &lt;string&gt;\n     Reference to another workload. When set, this workload inherits its values\n     from the base workload. Base workload can either reside on the same\n     namespace of this workload (referred to as \"user\" template) or can reside\n     in the runai namespace (referred to as a \"global\" template)\n\n   capabilities &lt;Object&gt;\n     The capabilities field allows adding a set of unix capabilities to the\n     container running the workload. Capabilities are Linux distinct privileges\n     traditionally associated with superuser which can be independently enabled\n     and disabled. For more information see\n     https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container\n\n   command  &lt;Object&gt;\n     If set, overrides the image's entry point with the supplied command.\n\n   completions  &lt;Object&gt;\n     Used with Hyperparameter Optimization. Specifies the number of successful\n     pods the job should reach to be completed. The Job will be marked as\n     successful once the specified amount of pods has succeeded. The default\n     value for 'completions' is 1. The 'parallelism' flag should be smaller or\n     equal to 'completions' For more information see:\n     https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/\n\n   cpu  &lt;Object&gt;\n     Specifies CPU units to allocate for the created workload (0.5, 1, .etc).\n     The workload will receive at least this amount of CPU. Note that the\n     workload will not be scheduled unless the system can guarantee this amount\n     of CPUs to the workload.\n\n   cpuLimit &lt;Object&gt;\n     Specifies a limit on the number of CPUs consumed by the workload (0.5, 1,\n     .etc). The system guarantees that this workload will not be able to consume\n     more than this amount of CPUs.\n\n   createHomeDir    &lt;Object&gt;\n     Instructs the system to create a temporary home directory for the user\n     within the container. Data stored in this directory will not be saved when\n     the container exits. When the runAsUser flag is set to true, this flag will\n     default to true as well.\n\n   environment  &lt;Object&gt;\n     Specifies environment variables to be set in the container running the\n     created workload.\n\n   exposedUrls  &lt;Object&gt;\n     Specifies a set of exported url (e.g. ingress) from the container running\n     the created workload.\n\n   extendedResources    &lt;Object&gt;\n     Specifies values for extended resources. Extended resources are third-party\n     devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that\n     you want to allocate to your Job. For more information see:\n     https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/\n\n   gitSync  &lt;Object&gt;\n     Specifies git repositories to mount into the container running the\n     workload.\n\n   gpu  &lt;Object&gt;\n     Specifies the number on the number of GPUs to allocate for the created\n     workload. The default is no allocated GPUs. The GPU value can be an integer\n     or a fraction between 0 and 1.\n\n   gpuLimit &lt;Object&gt;\n     Specifies a limit on the GPUs to allocate for this workload (1G, 20M,\n     .etc). Intended to use for Opportunistic jobs (with the smart\n     node-scheduler).\n\n   gpuMemory    &lt;Object&gt;\n     Specifies GPU memory to allocate for the created workload. The workload\n     will receive this amount of memory. Note that the workload will not be\n     scheduled unless the system can guarantee this amount of GPU memory to the\n     workload.\n\n   hostIpc  &lt;Object&gt;\n     Specifies that the created workload will use the host's ipc namespace.\n\n   hostNetwork  &lt;Object&gt;\n     Specifies that the created workload will use the host's network stack\n     inside its container. For more information see the Docker Run Reference at\n     https://docs.docker.com/engine/reference/run/\n\n   image    &lt;Object&gt;\n     Specifies the image to use when creating the container running the\n     workload.\n\n   imagePullPolicy  &lt;Object&gt;\n     Specifies the pull policy of the image when starting a container running\n     the created workload. Options are: always, ifNotPresent, or never. For more\n     information see: https://kubernetes.io/docs/concepts/containers/images\n\n   ingressUrl   &lt;Object&gt;\n     This field is for internal use only.\n\n   labels   &lt;Object&gt;\n     Specifies labels to be set in the container running the created workload.\n\n   largeShm &lt;Object&gt;\n     Specifies a large /dev/shm device to mount into a container running the\n     created workload. SHM is a shared file system mounted on RAM.\n\n   memory   &lt;Object&gt;\n     Specifies the amount of CPU memory to allocate for this workload (1G, 20M,\n     .etc). The workload will receive at least this amount of memory. Note that\n     the workload will not be scheduled unless the system can guarantee this\n     amount of memory to the workload\n\n   memoryLimit  &lt;Object&gt;\n     Specifies a limit on the CPU memory to allocate for this workload (1G, 20M,\n     .etc). The system guarantees that this workload will not be able to consume\n     more than this amount of memory. The workload will receive an error when\n     trying to allocate more memory than this limit.\n\n   migProfile   &lt;Object&gt;\n     Specifies the memory profile to be used for workload running on NVIDIA\n     Multi-Instance GPU (MIG) technology.\n\n   mountPropagation &lt;Object&gt;\n     Allows for sharing volumes mounted by a container to other containers in\n     the same pod, or even to other pods on the same node. The volume mount will\n     receive all subsequent mounts that are mounted to this volume or any of its\n     subdirectories.\n\n   mpi  &lt;Object&gt;\n     This workload produces mpijob\n\n   name &lt;Object&gt;\n     The specific name of the created resource. Either name of namePrefix should\n     be provided, but not both.\n\n   namePrefix   &lt;Object&gt;\n     A prefix used for assigning a name to the created resource. Either name of\n     namePrefix should be provided, but not both.\n\n   nfs  &lt;Object&gt;\n     Specifies nfs volumes to mount into a container running the created\n     workload.\n\n   nodePool &lt;Object&gt;\n     Specifies the NodePool name to be used to schedule this job on - DEPRECATED\n     use NodePools instead\n\n   nodePools    &lt;Object&gt;\n     Specifies the list of node pools to use for scheduling the job, ordered by\n     preference.\n\n   nodeType &lt;Object&gt;\n     Specifies nodes (machines) or a group of nodes on which the workload will\n     run. To use this feature, your Administrator will need to label nodes as\n     explained in the Group Nodes guide at\n     https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag\n     can be used in conjunction with Project-based affinity. In this case, the\n     flag is used to refine the list of allowable node groups set in the\n     Project. For more information see the Projects setup guide at\n     https://docs.run.ai/admin/admin-ui-setup/project-setup.\n\n   parallelism  &lt;Object&gt;\n     Specifies the maximum desired number of pods the workload should run at any\n     given time. The actual number of pods running in a steady state will be\n     less than this number when ((.spec.completions - .status.successful) &lt;\n     .spec.parallelism), i.e. when the work left to do is less than max\n     parallelism. For more information, see:\n     https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/\n\n   podAffinity  &lt;Object&gt;\n     Indicates whether pod affinity scheduling rules apply.\n\n   podAffinitySchedulingRule    &lt;Object&gt;\n     Indicates if we want to use the Pod affinity rule as : the \"hard\"\n     (required) or the \"soft\" (preferred) This field can be specified only if\n     PodAffinity is set to true\n\n   podAffinityTopology  &lt;Object&gt;\n     Specifies the Pod Affinity Topology to be used for scheduling the job This\n     field can be specified only if PodAffinity is set to true\n\n   ports    &lt;Object&gt;\n     Specifies a set of ports exposed from the container running the created\n     workload. Used together with --service-type.\n\n   preemptionLimit  &lt;Object&gt;\n     indicates the number of times the job can be preempted\n\n   processes    &lt;Object&gt;\n     Number of distributed training processes that will be allocated for the\n     created mpijob.\n\n   pvcs &lt;Object&gt;\n     Specifies persistent volume claims to mount into a container running the\n     created workload.\n\n   runAsGid &lt;Object&gt;\n     Specifies the Unix group id with which the container should run. Will be\n     used only if runAsUser is set to true.\n\n   runAsNonRoot &lt;Object&gt;\n     Indicates that the container must run as a non-root user. If true, the\n     Kubelet will validate the image at runtime to ensure that it does not run\n     as UID 0 (root) and fail to start the container if it does. If unset or\n     false, no such validation will be performed.\n\n   runAsUid &lt;Object&gt;\n     Specifies the Unix user id with which the container running the created\n     workload should run. Will be used only if runAsUser is set to true.\n\n   runAsUser    &lt;Object&gt;\n     Limits the container running the created workload to run in the context of\n     a specific non-root user. The user id is provided by the runAsUid field.\n     This would manifest itself in access to operating system resources, in the\n     ownership of new folders created under shared directories, etc.\n     Alternatively, if your cluster is connected to Run:ai via SAML, you can map\n     the container to use the Linux UID/GID which is stored in the\n     organization's directory. For more information see the User Identity guide\n     at https://docs.run.ai/admin/runai-setup/config/non-root-containers/\n\n   s3   &lt;Object&gt;\n     Specifies S3 buckets to mount into the container running the workload\n\n   seccompProfileType   &lt;Object&gt;\n     Indicates which kind of seccomp profile will be applied to the container.\n     Valid options are: RuntimeDefault - the container runtime default profile\n     should be used. Unconfined - no profile should be applied. Localhost is not\n     yet supported by Run:ai.\n\n   serviceType  &lt;Object&gt;\n     Specifies the default service exposure method for ports. The default shall\n     be used for ports which do not specify service type. Options are:\n     LoadBalancer, NodePort or ClusterIP. For more information see the External\n     Access to Containers guide on\n     https://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/\n\n   slotsPerWorker   &lt;Object&gt;\n     Number of slots to allocate per worker in the created mpijob.\n\n   stdin    &lt;Object&gt;\n     Instructs the system to keep stdin open for the container(s) running the\n     created workload, even if nothing is attached.\n\n   supplementalGroups   &lt;Object&gt;\n     ';' separated list of supplemental group IDs. Will be added to the security\n     context of the container running the created workload.\n\n   terminateAfterPreemption &lt;Object&gt;\n     Indicates whether the job should be terminated, by the system, after it has\n     been preempted. Default to false.\n\n   tolerations  &lt;Object&gt;\n     Toleration rules which apply to the pods running the workload. Toleration\n     rules guide (but do not require) the system to which node each pod can be\n     scheduled to or evicted from, based on matching between those rules and the\n     set of taints defined for each Kubernetes node.\n\n   tty  &lt;Object&gt;\n     Instructs the system to allocate a pseudo-TTY for the created workload.\n\n   usage    &lt;string&gt;\n     The intended usage of this workload. possible values are \"Template\": this\n     workload is used as the base for other workloads. \"Submit\": this workload\n     is used for submitting a job and/or other Kubernetes resources.\n\n   userId   &lt;Object&gt;\n     The user ID (\"Subject\" in the jwt-token) of the authenticated user who owns\n     the workload. The data might be used for authentication or authorization\n     purposes.\n\n   username &lt;Object&gt;\n     Display-only field describing the user who owns the workload. The data is\n     not used for authentication or authorization purposes.\n\n   volumes  &lt;Object&gt;\n     Specifies volumes to mount into a container running the created workload.\n\n   workingDir   &lt;Object&gt;\n     Specifies a directory that will be used as the current directory when the\n     container running the created workload starts.\n</code></pre>"},{"location":"developer/deprecated/inference/overview/","title":"Overview","text":"<p>Warning</p> <p>Inference API is deprecated. See Cluster API for its replacement. To read more about Inference see the new Inference Overview.</p>"},{"location":"developer/deprecated/inference/overview/#what-is-inference","title":"What is Inference","text":"<p>Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output. </p> <p>With Inference, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time. </p>"},{"location":"developer/deprecated/inference/overview/#inference-and-gpus","title":"Inference and GPUs","text":"<p>The Inference process is a subset of the original Training algorithm on a single datum (e.g. one sentence or one image), or a small batch. As such, GPU memory requirements are typically smaller than a full-blown Training process. </p> <p>Given that, Inference lends itself nicely to the usage of Run:ai Fractions. You can, for example, run 4 instances of an Inference server on a single GPU, each employing a fourth of the memory. </p>"},{"location":"developer/deprecated/inference/overview/#inference-runai","title":"Inference @Run:ai","text":"<p>Run:ai provides Inference services as an equal part together with the other two Workload types: Train and Build.</p> <ul> <li> <p>Inference is considered a high-priority workload as it is customer-facing. Running an Inference workload (within the Project's quota) will preempt any Run:ai Workload marked as Training.</p> </li> <li> <p>Inference is implemented as a Kubernetes Deployment with a defined number of replicas. The replicas are load-balanced by Kubernetes so that adding more replicas will improve the overall throughput of the system.</p> </li> <li> <p>Multiple replicas will appear in Run:ai as a single Inference workload. The workload will appear in all Run:ai dashboards and views as well as the Command-line interface.</p> </li> <li> <p>Inference workloads can be submitted via Run:ai Command-line interface as well as Kubernetes API/YAML. Internally, spawning an Inference workload also creates a Kubernetes Service. The service is an end-point to which clients can connect. </p> </li> </ul>"},{"location":"developer/deprecated/inference/overview/#see-also","title":"See Also","text":"<ul> <li>To setup Inference, see Inference Setup</li> <li>For running Inference see Inference quick-start</li> </ul>"},{"location":"developer/deprecated/inference/setup/","title":"Inference Setup","text":"<p>Warning</p> <p>Inference API is deprecated. See Cluster API for its replacement.</p> <p>Inference Jobs are an integral part of Run:ai and do not require setting up per se. However, Running multiple production-grade processes on a single GPU is best performed with an NVIDIA technology called Multi-Process Service or MPS</p> <p>By default, MPS is not enabled on GPU nodes.</p>"},{"location":"developer/deprecated/inference/setup/#enable-mps","title":"Enable MPS","text":"<p>To enable the MPS server on all nodes, you must edit the cluster installation values file:</p> <ul> <li>When installing the Run:ai cluster, edit the values file.</li> <li>On an existing installation, use the upgrade cluster instructions to modify the values file.</li> </ul> <p>Use:</p> <pre><code>runai-operator:\n  config:\n    mps-server:\n      enabled: true\n</code></pre> <p>Wait for the MPS server to start running:</p> <pre><code> kubectl get pods -n runai\n</code></pre> <p>When the MPS server pod has started to run, restart the <code>nvidia-device-plugin</code> pods:</p> <pre><code>kubectl rollout restart ds/nvidia-device-plugin-daemonset -n gpu-operator\n</code></pre> <p>To enable the MPS server on selected nodes, please contact Run:ai customer support.</p>"},{"location":"developer/deprecated/inference/setup/#verify-mps-is-enabled","title":"Verify MPS is Enabled","text":"<p>Run:</p> <pre><code>kubectl get pods -n runai --selector=app=runai-mps-server -o wide\n</code></pre> <ul> <li> <p>Verify that all mps-server pods are in <code>Running</code> state.</p> </li> <li> <p>Submit a workload with MPS enabled using the --mps flag.  Then run:</p> </li> </ul> <pre><code>runai list\n</code></pre> <ul> <li>Identify the node on which the workload is running. In the <code>get pods</code> command above find the pod running on the same node and then run:</li> </ul> <pre><code>kubectl logs -n runai runai-mps-server-&lt;name&gt; -f\n</code></pre> <p>You should see activity in the log.</p>"},{"location":"developer/deprecated/inference/submit-via-cli/","title":"Submit an inference Workload","text":"<p>Warning</p> <p>Inference API is deprecated. See Cluster API for its replacement.</p> <p>The easiest way to submit a new Inference workload is using the Run:ai Command-line interface. For additional information see the Inference Quickstart documentation.</p>"},{"location":"developer/deprecated/k8s-api/launch-job-via-kubernetes-api/","title":"Submit a Run:ai Job via Kubernetes API","text":"<p>Warning</p> <p>Researcher Kubernetes API is deprecated. See Cluster API for its replacement.</p> <p>This article is a complementary article to the article on launching jobs via YAML. It shows how to use a programming language and Kubernetes API to submit jobs. </p> <p>The article uses Python, though Kubernetes API is available in several other programming languages. </p>"},{"location":"developer/deprecated/k8s-api/launch-job-via-kubernetes-api/#submit-a-runai-job","title":"Submit a Run:ai Job","text":"<pre><code>from __future__ import print_function\nimport kubernetes\nfrom kubernetes import client, config\nfrom pprint import pprint\nimport json\n\nconfig.load_kube_config()\n\nwith client.ApiClient() as api_client:\n\n    namespace = 'runai-team-a'  # Run:ai project name is prefixed by runai-\n    jobname = 'my-job'\n    username = 'john'  # used in un-authenticated systems only\n    gpus = 1\n\n    body = client.V1Job(api_version=\"run.ai/v1\", kind=\"RunaiJob\")\n    body.metadata = client.V1ObjectMeta(namespace=namespace, name=jobname)\n\n    template = client.V1PodTemplate()\n    template.template = client.V1PodTemplateSpec()\n    template.template.metadata = client.V1ObjectMeta(labels = {'user' : username})\n\n    resource = client.V1ResourceRequirements(limits= {'nvidia.com/gpu' : gpus})\n    container = client.V1Container(\n        name=jobname, image='gcr.io/run-ai-demo/quickstart', resources=resource)\n    template.template.spec = client.V1PodSpec(\n        containers=[container], restart_policy='Never', scheduler_name='runai-scheduler')\n    body.spec = client.V1JobSpec(template=template.template)\n\n    pprint(body)\n\n    try:\n        api_instance = client.CustomObjectsApi(api_client)\n        api_response = api_instance.create_namespaced_custom_object(\n            \"run.ai\", \"v1\", namespace, \"runaijobs\", body)\n        pprint(api_response)\n    except client.rest.ApiException as e:\n        print(\"Exception when calling AppsV1Api-&gt;create_namespaced_job: %s\\n\" % e)\n</code></pre>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/","title":"Submit a Run:ai Job via YAML","text":"<p>Warning</p> <p>Researcher Kubernetes API is deprecated. See Cluster API for its replacement.</p> <p>You can use YAML files to submit jobs directly to Kubernetes. A frequent scenario for using the Kubernetes YAML syntax to submit Jobs is integrations. Researchers may already be working with an existing system that submits Jobs, and want to continue working with the same system. </p>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#terminology","title":"Terminology","text":"<p>We differentiate between three types of Workloads:</p> <ul> <li>Train workloads. Train workloads are characterized by a deep learning session that has a start and an end. A Training session can take anywhere from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. Training workloads typically utilize large percentages of GPU computing power and memory.</li> <li>Build workloads. Build workloads are interactive. They are used by data scientists to write machine learning code and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. </li> <li>Inference workloads. Inference workloads are used for serving models in production. For details on how to submit Inference workloads via YAML see here.</li> </ul> <p>The internal Kubernetes implementation of a Run:ai Job is a CRD (Customer Resource) named <code>RunaiJob</code> which is similar to a Kubernetes Job. </p> <p>Run:ai extends the Kubernetes Scheduler. A Kubernetes Scheduler is the software that determines which workload to start on which node. Run:ai provides a custom scheduler named <code>runai-scheduler</code>.</p> <p>The Run:ai scheduler schedules computing resources by associating Workloads with  Run:ai Projects:</p> <ul> <li>A Project is assigned with a GPU quota through the Run:ai Run:ai User Interface. </li> <li>A workload must be associated with a Project name and will receive resources according to the defined quota for the Project and the currently running Workloads</li> </ul> <p>Internally, Run:ai Projects are implemented as Kubernetes namespaces. The scripts below assume that the code is being run after the relevant namespace has been set. </p>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#submit-workloads","title":"Submit Workloads","text":"<ul> <li><code>&lt;JOB-NAME&gt;</code>. The name of the Job. </li> <li><code>&lt;IMAGE-NAME&gt;</code>. The name of the docker image to use. Example: <code>gcr.io/run-ai-demo/quickstart</code>.</li> <li><code>&lt;USER-NAME&gt;</code>. The name of the user submitting the Job. The name is used for display purposes only when Run:ai is installed in an unauthenticated mode.</li> <li><code>&lt;REQUESTED-GPUs&gt;</code>. An integer number of GPUs you request to be allocated for the Job. Examples: 1, 2.</li> <li><code>&lt;NAMESAPCE&gt;</code>. The name of the Project's namespace. This is usually <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> </ul>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#regular-jobs","title":"Regular Jobs","text":"<p>Copy the following into a file and change the parameters:</p> <pre><code>apiVersion: run.ai/v1\nkind: RunaiJob (* see note below)\nmetadata:\n  name: &lt;JOB-NAME&gt;\n  namespace: &lt;NAMESPACE&gt;\n  labels:\n    priorityClassName: \"build\" (* see note below)\nspec:\n  template:\n    metadata:\n      labels:\n        user: &lt;USER-NAME&gt;\n    spec:\n      containers:\n      - name: &lt;JOB-NAME&gt;\n        image: &lt;IMAGE-NAME&gt;\n        resources:\n          limits:\n            nvidia.com/gpu: &lt;REQUESTED-GPUs&gt;\n      restartPolicy: Never\n      schedulerName: runai-scheduler\n</code></pre> <p>To submit the job, run:</p> <pre><code>kubectl apply -f &lt;FILE-NAME&gt;\n</code></pre> <p>Note</p> <ul> <li>You can use either a regular <code>Job</code> or <code>RunaiJob</code>. The latter is a Run:ai object which solves various Kubernetes Bugs and provides a better naming for multiple pods in Hyper-Parameter Optimization scenarios</li> <li>Using <code>build</code> in the <code>priorityClassName</code> field is equivalent to running a job via the CLI with a '--interactive' flag. To run a Train job, delete this line.</li> <li>The runai submit CLI command includes many more flags. These flags can be correlated with Kubernetes API functions and added to the YAML above. </li> </ul>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#using-fractional-gpus","title":"Using Fractional GPUs","text":"<p>To submit a Job with fractions of a GPU, replace <code>&lt;REQUESTED-GPUs&gt;</code> with a fraction in quotes. e.g. </p> <pre><code>limits:\n  nvidia.com/gpu: \"0.5\"\n</code></pre> <p>where \"0.5\" is the requested GPU fraction.</p>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#mapping-additional-flags","title":"Mapping Additional Flags","text":"<p>Run:ai Command-Line <code>runai submit</code> has a significant number of flags. The easiest way to find out the mapping from a flag to the correct YAML attribute is to use the <code>--dry-run</code> flag.</p> <p>For example, to find the location of the <code>--large-shm</code> flag, run:</p> <pre><code>&gt; runai submit -i ubuntu --large-shm --dry-run\nTemplate YAML file can be found at:\n/var/folders/xb/rnf9b1bx2jg45c7jprv71d9m0000gn/T/job.yaml185826190\n</code></pre>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#delete-workloads","title":"Delete Workloads","text":"<p>To delete a Run:ai workload, delete the Job:</p> <pre><code>kubectl delete runaijob &lt;JOB-NAME&gt;\n</code></pre>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#see-also","title":"See Also","text":"<ul> <li>See how to use the above YAML syntax with Kubernetes API</li> <li>Use the Researcher REST API to submit, list and delete Jobs.</li> </ul>"},{"location":"developer/deprecated/k8s-api/overview/","title":"Overview: Launch a Job via Kubernetes API","text":"<p>Warning</p> <p>Researcher Kubernetes API is deprecated. See Cluster API for its replacement.</p> <p>You can create, submit, list or delete jobs using the Command-line interface or the Run:ai User Interface. </p> <p>To do the same programmatically you can use the Run:ai Researcher REST API. </p> <p>You can also communicate directly with the underlying Kubernetes infrastructure by:</p> <ul> <li>Using YAML files or,</li> <li>By using a variety of programming languages to send requests to Kubernetes. See Submit a Run:ai Job via Kubernetes API for a python sample.</li> </ul>"},{"location":"developer/deprecated/researcher-rest-api/overview/","title":"Researcher REST API","text":"<p>Warning</p> <p>Researcher Kubernetes API is deprecated. See Cluster API for its replacement.</p> <p>The purpose of the Researcher REST API is to provide an easy-to-use programming interface for submitting, listing, and deleting Jobs. </p> <p>There are other APIs that provide the same functionality. Specifically:</p> <ul> <li>If your code is script-based, you may consider using the Run:ai command-line interface.</li> <li>You can communicate directly with the underlying Kubernetes infrastructure by sending YAML files or by using a variety of programming languages to send requests to Kubernetes. See Submit a Run:ai Job via Kubernetes API.</li> </ul> <p>The Researcher REST API is cluster-specific in the sense that if you have multiple GPU clusters, you will have a separate URL per cluster. This <code>&lt;CLUSTER-ENDPOINT&gt;</code> can be found in the Run:ai User Interface, under <code>Clusters</code>. Each cluster will have a separate URL.</p>"},{"location":"developer/deprecated/researcher-rest-api/overview/#authentication","title":"Authentication","text":"<ul> <li>By default, researcher APIs are unauthenticated. To protect researcher API, you must configure researcher authentication.</li> <li>Once configured, you must create a Client Application to make API requests. Use the client application and secret, to obtain a time-bound bearer token (<code>&lt;ACCESS-TOKEN&gt;</code>). For details, see Calling REST APIs.</li> <li>Use the token for subsequent API calls. </li> </ul>"},{"location":"developer/deprecated/researcher-rest-api/overview/#example","title":"Example","text":"<p>Get all the jobs for a project named <code>team-a</code>: </p> <pre><code>curl  'https://&lt;CLUSTER-ENDPOINT&gt;/researcher/api/v1/jobs/team-a' \\\n  -H 'accept: application/json' \\\n--header 'Authorization: Bearer &lt;ACCESS-TOKEN&gt;' \n</code></pre>"},{"location":"developer/deprecated/researcher-rest-api/overview/#researcher-api-scope","title":"Researcher API Scope","text":"<p>The Researcher API provides the following functionality:</p> <ul> <li>Submit a new Job</li> <li>List jobs for specific Projects.</li> <li>Delete an existing Job</li> <li>Get a list of Projects for which you have access to</li> </ul>"},{"location":"developer/deprecated/researcher-rest-api/overview/#researcher-api-documentation","title":"Researcher API Documentation","text":"<p>To review API documentation:</p> <ul> <li>Open the Run:ai user interface</li> <li>Go to <code>Clusters</code></li> <li>Locate your cluster and browse to <code>https://&lt;cluster-url&gt;/researcher/api/docs</code>.</li> <li>When using the <code>Authenticate</code> button, add <code>Bearer &lt;ACCESS TOKEN&gt;</code> (simply adding the access token will not work).</li> </ul> <p>The document uses the Open API specification to describe the API. You can test the API within the document after creating and saving a token.</p>"},{"location":"developer/metrics/metrics/","title":"Metrics","text":""},{"location":"developer/metrics/metrics/#what-are-metrics","title":"What are Metrics","text":"<p>Metrics are numeric measurements recorded over time that are emitted from the Run:ai cluster. Typical metrics involve utilization, allocation, time measurements and so on. Metrics are used in Run:ai dashboards as well as in the Run:ai administration user interface.</p> <p>The purpose of this document is to detail the structure and purpose of metrics emitted by Run:ai to enable customers to create custom dashboards or integrate metric data into other monitoring systems.</p> <p>Run:ai uses Prometheus for collecting and querying metrics.</p> <p>Warning</p> <p>From cluster version 2.17 and onwards, Run:ai will support metrics via the Run:ai API and direct metrics queries (metrics that are queried directly from Prometheus) will be deprecated.</p>"},{"location":"developer/metrics/metrics/#published-runai-metrics","title":"Published Run:ai Metrics","text":"<p>Following is the list of published Run:ai metrics, per cluster version (make sure to pick the right cluster version in the picker at the top of the page):</p> Metric name Labels Measurement Description runai_active_job_cpu_requested_cores {clusterId,  job_name, job_uuid} CPU Cores Workload's requested CPU cores runai_active_job_memory_requested_bytes {clusterId,  job_name, job_uuid} Bytes Workload's requested CPU memory runai_cluster_cpu_utilization {clusterId} 0 to 1 CPU utilization of the entire cluster runai_cluster_memory_used_bytes {clusterId} Bytes Used CPU memory of the entire cluster runai_cluster_memory_utilization {clusterId} 0 to 1 CPU memory utilization of the entire cluster runai_allocated_gpu_count_per_gpu {gpu, clusterId, node} 0/1 Is a GPU hosting a pod runai_last_gpu_utilization_time_per_gpu {gpu, clusterId, node} Unix time Last time GPU was not idle runai_requested_gpu_memory_mb_per_workload {clusterId, job_type, job_uuid, job_name, project, workload_id} MegaBytes Requested GPU memory per workload (0 if not specified by the user) runai_requested_gpus_per_workload {clusterId, workload_type, workload_id, workload_name, project} Double Number of requested GPUs per workload runai_run_time_seconds_per_workload {clusterId, workload_id, workload_name} Seconds Total run time per workload runai_wait_time_seconds_per_workload {clusterId, workload_id, workload_name} Seconds Total wait time per workload runai_node_cpu_requested_cores {clusterId, node} Double Sum of the requested CPU cores of all workloads running in a node runai_node_cpu_utilization {clusterId, node} 0 to 1 CPU utilization per node runai_node_memory_utilization {clusterId, node} 0 to 1 CPU memory utilization per node runai_node_requested_memory_bytes {clusterId, node} Bytes Sum of the requested CPU memory of all workloads running in a node runai_node_used_memory_bytes {clusterId, node} Bytes Used CPU memory per node runai_project_guaranteed_gpus {clusterId, project} Double Guaranteed GPU quota per project runai_project_info {memory_quota, cpu_quota, gpu_guaranteed_quota, clusterId, project, department} N/A Information on CPU, CPU memory, GPU quota per project runai_queue_info {memory_quota, cpu_quota, gpu_guaranteed_quota, clusterId, nodepool, queue_name, department} N/A Information on CPU, CPU memory, GPU quota per project/department per nodepool runai_cpu_limits_per_active_workload {clusterId, job_name , job_uuid} CPU Cores Workloads CPU limit (in number of cores). See link runai_job_cpu_usage {clusterId, workload_id, workload_name, project} Double Workloads CPU usage (in number of cores) runai_memory_limits_per_active_workload {clusterId, job_name, job_uuid} Bytes Workloads CPU memory limit. See link runai_active_job_memory_requested_bytes {clusterId, job_name, job_uuid} Bytes Workloads requested CPU memory. See link runai_job_memory_used_bytes {clusterId, workload_id, workload_name, project} Bytes Workloads used CPU memory runai_mig_mode_gpu_count {clusterId, node} Double Number of GPUs on MIG nodes runai_gpu_utilization_per_gpu {clusterId, gpu, node} % GPU Utilization per GPU runai_gpu_utilization_per_node {clusterId, node} % GPU Utilization per Node runai_gpu_memory_used_mebibytes_per_gpu {clusterId, gpu, node} MiB Used GPU memory per GPU runai_gpu_memory_used_mebibytes_per_node {clusterId, node} MiB Used GPU memory per Node runai_gpu_memory_total_mebibytes_per_gpu {clusterId, gpu, node} MiB Total GPU memory per GPU runai_gpu_memory_total_mebibytes_per_node {clusterId, node} MiB Total GPU memory per Node runai_gpu_count_per_node {clusterId, node, modelName, ready, schedulable} Number Number of GPUs per Node runai_allocated_gpu_count_per_workload {clusterId, workload_id, workload_name, workload_type, user} Double Number of allocated GPUs per Workload runai_allocated_gpu_count_per_project {clusterId, project} Double Number of allocated GPUs per Project runai_gpu_memory_used_mebibytes_per_pod_per_gpu {clusterId, pod_name, pod_uuid, pod_namespace, node, gpu} MiB Used GPU Memory per Pod, per Gpu on which the workload is running runai_gpu_memory_used_mebibytes_per_workload {clusterId, workload_id, workload_name, workload_type, user} MiB Used GPU Memory per Workload runai_gpu_utilization_per_pod_per_gpu {clusterId, pod_name, pod_uuid, pod_namespace, node, gpu} % GPU Utilization per Pod per GPU runai_gpu_utilization_per_workload {clusterId, workload_id, workload_name, workload_type, user} % Average GPU Utilization per Workload runai_gpu_utilization_per_project {clusterId, project} % Average GPU Utilization per Project runai_last_gpu_utilization_time_per_workload {clusterId, workload_id, workload_name, workload_type, user} Seconds (Unix Timestamp) The Last Time (Unix Timestamp) That The Workload Utilized Any Of Its Allocated GPUs runai_gpu_idle_seconds_per_workload {clusterId, workload_id, workload_name, workload_type, user} Seconds Seconds Passed Since The Workload Utilized Any Of Its Allocated GPUs runai_allocated_gpu_count_per_pod {clusterId, pod_name, pod_uuid, pod_namespace, node} Double Number Of Allocated GPUs per Pod runai_allocated_gpu_count_per_node {clusterId, node} Double Number Of Allocated GPUs per Node runai_allocated_millicpus_per_pod {clusterId, pod_name, pod_uuid, pod_namespace, node} Integer Number Of Allocated Millicpus per Pod runai_allocated_memory_per_pod {clusterId, pod_name, pod_uuid, pod_namespace, node} Bytes Allocated Memory per Pod <p>Following is a list of labels appearing in Run:ai metrics:</p> Label Description clusterId Cluster Identifier department Name of Run:ai Department cpu_quota CPU limit per project gpu GPU index gpu_guaranteed_quota Guaranteed GPU quota per project image Name of Docker image namespace_name Namespace deployment_name Deployment name job_name Job name job_type Job type: training, interactive or inference job_uuid Job identifier workload_name Workload name workload_type Workload type: training, interactive or inference workload_uuid Workload identifier pod_name Pod name. A Workload can contain many pods. pod_namespace Pod namespace memory_quota CPU memory limit per project node Node name project Name of Run:ai Project status Workload status: Running, Pending, etc. For more information on Workload statuses see document user User identifier"},{"location":"developer/metrics/metrics/#other-metrics","title":"Other Metrics","text":"<p>Run:ai exports other metrics emitted by NVIDIA and Kubernetes packages, as follows:</p> Metric name Description runai_gpu_utilization_per_gpu GPU utilization kube_node_status_capacity The capacity for different resources of a node kube_node_status_condition The condition of a cluster node kube_pod_container_resource_requests_cpu_cores The number of CPU cores requested by container kube_pod_container_resource_requests_memory_bytes Bytes of memory requested by a container kube_pod_info Information about pod <p>For additional information, see Kubernetes kube-state-metrics and NVIDIA dcgm exporter.</p>"},{"location":"developer/metrics/metrics/#changed-metrics-and-api-mapping","title":"Changed metrics and API mapping","text":"<p>Starting in cluster version 2.17, some of the metrics names have been changed. In addition some Run:ai metrics are available as API endpoints. Using the API endpoints is more efficient and provides an easier way of retrieving metrics in any application. The following table lists the metrics that were changed.</p> Metric name in version 2.16 2.17 Change Description 2.17 API Endpoint runai_active_job_cpu_requested_cores available also via API https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_REQUEST_CORES\" metricType runai_active_job_memory_requested_bytes available also via API https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_MEMORY_REQUEST_BYTES\" metricType runai_cluster_cpu_utilization available also via API https://app.run.ai/api/v2/clusters/{clusterUuid}/metrics ; with \"CPU_UTILIZATION\" metricType runai_cluster_memory_utilization available also via API https://app.run.ai/api/v2/clusters/{clusterUuid}/metrics ; with \"CPU_MEMORY_UTILIZATION\" metricType runai_gpu_utilization_non_fractional_jobs no longer available runai_allocated_gpu_count_per_workload labels changed runai_gpu_utilization_per_pod_per_gpu available also via API https://app.run.ai/api/v1/workloads/{workloadId}/pods/{podId}/metrics ; with \"GPU_UTILIZATION_PER_GPU\" metricType runai_gpu_utilization_per_workload available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"GPU_UTILIZATION\" metricType runai_job_image no longer available runai_job_requested_gpu_memory available also via API and renamed to: \"runai_requested_gpu_memory_mb_per_workload\" with different labels https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"GPU_MEMORY_REQUEST_BYTES\" metricType runai_job_requested_gpus renamed to: \"runai_requested_gpus_per_workload\" with different labels runai_job_total_runtime renamed to: \"runai_run_time_seconds_per_workload\" with different labels runai_job_total_wait_time renamed to: \"runai_wait_time_seconds_per_workload\" with different labels runai_gpu_memory_used_mebibytes_per_workload available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"GPU_MEMORY_USAGE_BYTES\" metricType runai_gpu_memory_used_mebibytes_per_pod_per_gpu available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/pods/{podId}/metrics ; with \"GPU_MEMORY_USAGE_BYTES_PER_GPU\" metricType runai_node_gpu_used_memory_bytes renamed and changed units: \"runai_gpu_memory_used_mebibytes_per_node\" runai_node_total_memory_bytes renamed and changed units: \"runai_gpu_memory_total_mebibytes_per_node\" runai_project_info labels changed runai_active_job_cpu_limits available also via API and renamed to: \"runai_cpu_limits_per_active_workload\" https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_LIMIT_CORES\" metricType runai_job_cpu_usage available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_USAGE_CORES\" metricType runai_active_job_memory_limits available also via API and renamed to: \"runai_memory_limits_per_active_workload\" https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_MEMORY_LIMIT_BYTES\" metricType runai_running_job_memory_requested_bytes was a duplication of \"runai_active_job_memory_requested_bytes\", see above runai_job_memory_used_bytes available also via API and labels changed https://app.run.ai/api/v1/workloads/{workloadId}/metrics ; with \"CPU_MEMORY_USAGE_BYTES\" metricType runai_job_swap_memory_used_bytes no longer available runai_gpu_count_per_node added labels runai_last_gpu_utilization_time_per_workload labels changed runai_gpu_idle_time_per_workload renamed to: \"runai_gpu_idle_seconds_per_workload\" with different labels"},{"location":"developer/metrics/metrics/#create-custom-dashboards","title":"Create custom dashboards","text":"<p>To create custom dashboards based on the above metrics, please contact Run:ai customer support.</p>"},{"location":"home/","title":"Overview","text":"<p>This is an overview of the what's new.</p>"},{"location":"home/components/","title":"Run:ai System Components","text":""},{"location":"home/components/#components","title":"Components","text":"<p>Run:ai is made up of two components:</p> <ul> <li>The Run:ai cluster provides scheduling services and workload management.</li> <li>The Run:ai control plane provides resource management, Workload submission and cluster monitoring.</li> </ul> <p>Technology-wise, both are installed over a Kubernetes Cluster.</p> <p>Run:ai users:</p> <ul> <li>Researchers submit Machine Learning workloads via the Run:ai Console, the Run:ai Command-Line Interface (CLI), or directly by sending YAML files to Kubernetes.</li> <li>Administrators monitor and set priorities via the Run:ai User Interface</li> </ul> <p></p>"},{"location":"home/components/#runai-cluster","title":"Run:ai Cluster","text":"<ul> <li>Run:ai comes with its own Scheduler. The Run:ai scheduler extends the Kubernetes scheduler. It uses business rules to schedule workloads sent by Researchers.</li> <li>Run:ai schedules Workloads. Workloads include the actual researcher code running as a Kubernetes container, together with all the system resources required to run the code, such as user storage, network endpoints to access the container etc.</li> <li>The cluster uses an outbound-only, secure connection to synchronize with the Run:ai control plane. Information includes meta-data sync and various metrics on Workloads, Nodes etc.</li> <li>The Run:ai cluster is installed as a Kubernetes Operator</li> <li>Run:ai is installed in its own Kubernetes namespace named runai</li> <li>Workloads are run in the context of Run:ai Projects. Each Project is mapped to a Kubernetes namespace with its own settings and access control.</li> </ul>"},{"location":"home/components/#runai-control-plane-on-the-cloud","title":"Run:ai Control Plane on the cloud","text":"<p>The Run:ai control plane is used by multiple customers (tenants) to manage resources (such as Projects &amp; Departments), submit Workloads and monitor multiple clusters.</p> <p>A single Run:ai customer (tenant) defined in the control-plane, can manage multiple Run:ai clusters. So a single customer, can manage mutltiple GPU clusters in multiple locations/subnets from a single interface.</p>"},{"location":"home/components/#self-hosted-control-plane","title":"Self-hosted Control-Plane","text":"<p>The Run:ai control plane can also be locally installed. To understand the various installation options see the installation types document.</p>"},{"location":"home/data-privacy-details/","title":"Data Privacy","text":"<p>Run:ai SaaS Cluster installation uses the Run:ai cloud as its control plane. The cluster sends information to the cloud for control as well as analytics. The document below is a run-down of the data that is being sent to the Run:ai cloud.</p> <p>Note</p> <p>If the data detailed below is not in line with your organization's policy, you can choose to install the Run:ai self-hosted version. The self-hosted installation includes the Run:ai control-plane and will not communicate with the cloud. The self-hosted installation has different pricing. </p>"},{"location":"home/data-privacy-details/#data","title":"Data","text":"<p>Following is a list of platform data items that are sent to the Run:ai cloud.</p> Asset Data Details Workload Metrics Workload names, CPU, GPU, and Memory metrics, parameters sent using the <code>runai submit</code> command Workload Assets Workload Assets such as environments, compute resources and data resoruces Resource Credentials Credentials to cluster resources are stored and encrypted using a SHA-512 algorithm. The encryption is tenant-specific Node Metrics Node names and IPs, CPU, GPU, and Memory metrics Cluster Metrics Cluster names, CPU, GPU, and Memory metrics Projects &amp; Departments Names, quota information Users User Run:ai roles, emails and passwords (when single-sign on not used) <p>Run:ai does not send deep-learning artifacts to the cloud. As such any Code, images, container logs, training data, models, checkpoints and the like, stay behind corporate firewalls. </p>"},{"location":"home/data-privacy-details/#see-also","title":"See Also","text":"<p>The Run:ai privacy policy. </p>"},{"location":"home/product-support-policy/","title":"Product Support Policy","text":"<p>The product support levels for the Run:ai software are as follows:</p> Critical Bug Fixes Important Bug Fixes Full support V V Extended support V - End of support - - <ul> <li>Full support period: 12 months from the release date of a Major Version.</li> <li>Extended support period: 6 months after the end of the full support period.</li> <li>End of support: 18 months from the release date of a Major Version</li> </ul> <p>Notes:</p> <ol> <li>Run:ai may extend the support periods and/or otherwise amend this Product Support Levels Policy from time to time at its own discretion.</li> <li>Versioning: Run:ai versioning follows Semantic Version (SemVer) numbering scheme, \u201cMa.Mi.Pa\u201d, where:<ul> <li>Ma.Mi is a major version that contains new features, bug fixes and security updates (\u201cMajor Version\u201d).</li> <li>Pa is a patch level version that is focused on bug fixes and security updates. Run:ai version release dates are listed in theRun:ai product documentation.</li> </ul> </li> <li> <p>Critical Bug: a bug that represents a severity 1 support ticket, as listed in the Run:ai support agreement. Important bug:  a bug that represents a severity 1 or severity 2 support ticket, as listed in the Run:ai support agreement.</p> </li> <li> <p>Run:ai is built from 3 components: Run:ai Control Plane, Run:ai Cluster and Run:ai Command Line Inference (CLI). For full details about Run:ai system components see: https://docs.run.ai/latest/home/components.</p> </li> <li> <p>Run:ai Control Plane, Run:ai Cluster &amp; Run:ai CLI are always released together with the same version number.</p> </li> <li> <p>A supported Run:ai environment is built from:</p> <ul> <li> <p>Run:ai Control Plane of a version equal to or greater than the versions of each of the Run:ai Clusters.</p> </li> <li> <p>Run:ai CLI of a version equal to the version on the Run:ai Cluster.</p> </li> </ul> </li> <li> <p>From time to time, Run:ai may provide API deprecation notices under the product     documentation of the applicable Run:ai version. For full details about Run:ai API deprecation notice and support policy see: https://docs.run.ai/latest/developer/overview-developer/#api-support. </p> </li> </ol> <p>Last update: Aug 6 2024</p>"},{"location":"home/whats-new-2-13/","title":"Run:ai version 2.13","text":""},{"location":"home/whats-new-2-13/#version-2137","title":"Version 2.13.7","text":""},{"location":"home/whats-new-2-13/#release-date","title":"Release date","text":"<p>July 2023</p>"},{"location":"home/whats-new-2-13/#release-content","title":"Release content","text":"<ul> <li>Added filters to the historic quota ratio widget on the Quota management dashboard.</li> </ul>"},{"location":"home/whats-new-2-13/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-11080 Fixed an issue in OpenShift environments where log in via SSO with the <code>kubeadmin</code> user, gets blank pages for every page. RUN-11119 Fixed an issue where values that should be the Order of priority column are in the wrong column. RUN-11120 Fixed an issue where the Projects table does not show correct metrics when Run:ai version 2.13 is paired with a Run:ai 2.8 cluster. RUN-11121 Fixed an issue where the wrong over quota memory alert is shown in the Quota management pane in project edit form. RUN-11272 Fixed an issue in OpenShift environments where the selection in the cluster drop down in the main UI does not match the cluster selected on the login page."},{"location":"home/whats-new-2-13/#version-2134","title":"Version 2.13.4","text":""},{"location":"home/whats-new-2-13/#release-date_1","title":"Release date","text":"<p>July 2023</p>"},{"location":"home/whats-new-2-13/#fixed-issues_1","title":"Fixed issues","text":"Internal ID Description RUN-11089 Fixed an issue when creating an environment, commands in the Runtime settings pane and are not persistent and cannot be found in other assets (for example in a new Training)."},{"location":"home/whats-new-2-13/#version-2131","title":"Version 2.13.1","text":""},{"location":"home/whats-new-2-13/#release-date_2","title":"Release date","text":"<p>July 2023</p>"},{"location":"home/whats-new-2-13/#release-content_1","title":"Release content","text":"<ul> <li>Made an improvement so that occurrences of labels that are not in use anymore are deleted.</li> </ul>"},{"location":"home/whats-new-2-13/#fixed-issues_2","title":"Fixed issues","text":"<p>N/A</p>"},{"location":"home/whats-new-2-13/#version-2130","title":"Version 2.13.0","text":""},{"location":"home/whats-new-2-13/#release-content_2","title":"Release content","text":"<p>This version contains features and fixes from previous versions starting with 2.9. Refer to the prior versions for specific features and fixes. </p> <p>Projects</p> <ul> <li>Improved the Projects UI for ease of use. Projects follows UI upgrades and changes that are designed to make setting up of components and assets easier for administrators and researchers. To configure a project, see Projects.</li> </ul> <p>Dashboards</p> <ul> <li> <p>Added a new dashboard for Quota management, which provides an efficient means to monitor and manage resource utilization within the AI cluster. The dashboard filters the display of resource quotas based on Departments, Projects, and Node pools. For more information, see Quota management dashboard.</p> </li> <li> <p>Added to the Overview dashboard, the ability to filter the cluster by one or more node pools. For more information, see Node pools.</p> </li> </ul> <p>Nodes and Node pools</p> <ul> <li> <p>Run:ai scheduler supports 2 scheduling strategies: Bin Packing (default) and Spread. For more information, see Scheduling strategies. You can configure the scheduling strategy in the node pool level to improve the support of clusters with mixed types of resources and workloads. For configuration information, see Creating new node pools.</p> </li> <li> <p>GPU device level DCGM Metrics are collected per GPU and presented by Run:ai in the Nodes table. Each node contains a list of its embedded GPUs with their respective DCGM metrics. See DCGM Metrics for the list of metrics which are provided by NVidia DCGM and collected by Run:ai. Contact your Run:ai customer representative to enable this feature.</p> </li> </ul> <ul> <li>Added per node pool over-quota priority. Over-quota priority sets the relative amount of additional unused resources that an asset can get above its current quota. For more information, see Over-quota priority.</li> </ul> <ul> <li>Added support of associating workspaces to node pool. The association between workspaces and node pools is done using Compute resources section. In order to associate a compute resource to a node pool, in the Compute resource section, press More settings. Press Add new to add more node pools to the configuration. Drag and drop the node pools to set their priority.</li> </ul> <ul> <li>Added Node pool selection as part of the workload submission form. This allows researchers to quickly determine the list of node pools available and their priority. Priority is set by dragging and dropping them in the desired order of priority. In addition, when the node pool priority list is locked by a policy, the list isn't editable by the Researcher even if the workspace is created from a template or copied from another workspace.</li> </ul> <p>Time limit duration</p> <ul> <li> <p>Improved the behavior of any workload time limit (for example, Idle time limit) so that the time limit will affect existing workloads that were created before the time limit was configured. This is an optional feature which provides help in handling situations where researchers leave sessions open even when they do not need to access the resources. For more information, see Limit duration of interactive training jobs.</p> </li> <li> <p>Improved workspaces time limits. Workspaces that reach a time limit will now transition to a state of <code>stopped</code> so that they can be reactivated later.</p> </li> <li> <p>Added time limits for training jobs per project. Administrators (Department Admin, Editor) can limit the duration of Run:ai Training jobs per Project using a specified time limit value. This capability can assist administrators to limit the duration and resources consumed over time by training jobs in specific projects. Each training job that reaches this duration will be terminated.</p> </li> </ul> <p>Workload assets</p> <ul> <li>Extended the collaboration functionality for any workload asset such as Environment, Compute resource, and some Data source types. These assets are now shared with Departments in the organization in addition to being shared with specific projects, or the entire cluster.</li> </ul> <ul> <li>Added a search box for card galleries in any asset based workload creation form to provide an easy way to search for assets and resources. To filter use the asset name or one of the field values of the card.</li> </ul> <p>PVC data sources</p> <ul> <li>Added support for PVC block storage in the New data source form. In the New data source form for a new PVC data source, in the Volume mode field, select from Filesystem or Block. For more information, see Create a PVC data source.</li> </ul> <p>Credentials</p> <ul> <li>Added Docker registry to the Credentials menu. Users can create docker credentials for use in specific projects for image pulling. To configure credentials, see Configuring credentials.</li> </ul> <p>Policies</p> <ul> <li>Improved policy support by adding <code>DEFAULTS</code> in the <code>items</code> section in the policy. The <code>DEFAULTS</code> section sets the default behavior for items declared in this section. For example, this can be use to limit the submission of workloads only to existing PVCs. For more information and an example, see Policies, Complex values.</li> </ul> <ul> <li>Added support for making a PVC data source available to all projects. In the New data source form, when creating a new PVC data source, select All from the Project pane.</li> </ul> <p>Researcher API</p> <ul> <li>Extended researcher's API to allow stopping and starting of workloads using the API. For more information, see Submitting Workloads via HTTP/REST.</li> </ul> <p>Integrations</p> <ul> <li>Added support for Spark and Elastic jobs. For more information, see Running Spark jobs with Run:ai.</li> </ul> <ul> <li> <p>Added support for Ray jobs. Ray is an open-source unified framework for scaling AI and Python applications. For more information, see Integrate Run:ai with Ray.</p> </li> <li> <p>Added integration with Weights &amp; Biases Sweep to allow data scientists to submit hyperparameter optimization workloads directly from the Run:ai UI. To configure sweep, see Sweep configuration.</p> </li> </ul> <ul> <li>Added support for XGBoost. XGBoost, which stands for Extreme Gradient Boosting, is a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library. It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems. For more information, see runai submit-dist xgboost</li> </ul> <p>Compatability</p> <ul> <li>Added support for multiple OpenShift clusters. For configuration information, see Installing additional Clusters.</li> </ul>"},{"location":"home/whats-new-2-13/#installation","title":"Installation","text":"<ul> <li>The manual process of upgrading Kubernetes CRDs is no longer needed when upgrading to the most recent version (2.13) of Run:ai.</li> <li>From Run:ai 2.12 and above, the control-plane installation has been simplified and no longer requires the creation of a backend values file. Instead, install directly using <code>helm</code> as described in Install the Run:ai Control Plane.  </li> <li>From Run:ai 2.12 and above, the air-gapped, control-plane installation now generates a <code>custom-env.yaml</code> values file during the preparation stage. This is used when installing the control-plane.</li> </ul>"},{"location":"home/whats-new-2-13/#known-issues","title":"Known issues","text":"Internal ID Description RUN-11005 Incorrect error messages when trying to run <code>runai</code> CLI commands in an OpenShift environment. RUN-11009 Incorrect error message when a user without permissions to tries to delete another user."},{"location":"home/whats-new-2-13/#fixed-issues_3","title":"Fixed issues","text":"Internal ID Description RUN-9039 Fixed an issue where in the new job screen, after toggling off the preemptible flag, and a job is submitted, the job still shows as preemptible. RUN-9323 Fixed an issue with a non-scaleable error message when scheduling hundreds of nodes is not successful. RUN-9324 Fixed an issue where the scheduler did not take into consideration the amount of storage so there is no explanation that pvc is not ready. RUN-9902 Fixed an issue in OpenShift environments, where there are no metrics in the dashboard because Prometheus doesn\u2019t have permissions to monitor the <code>runai</code> namespace after an installation or upgrade to 2.9. RUN-9920 Fixed an issue where the <code>canEdit</code> key in a policy is not validated properly for itemized fields when configuring an interactive policy. RUN-10052 Fixed an issue when loading a new job from a template gives an error until there are changes made on the form. RUN-10053 Fixed an issue where the Node pool column is unsearchable in the job list. RUN-10422 Fixed an issue where node details show running workloads that were actually finished (successfully/failed/etc.). RUN-10500 Fixed an issue where jobs are shown as running even though they don't exist in the cluster. RUN-10813 Fixed an issue in adding a <code>data source</code> where the path is case sensitive and didn't allow uppercase."},{"location":"home/whats-new-2-15/","title":"What's New 2.15 - December 3, 2023","text":""},{"location":"home/whats-new-2-15/#release-content","title":"Release Content","text":""},{"location":"home/whats-new-2-15/#researcher","title":"Researcher","text":""},{"location":"home/whats-new-2-15/#jobs-workloads-trainings-and-workspaces","title":"Jobs, Workloads, Trainings, and Workspaces","text":"<ul> <li> <p>Added support to run distributed workloads via the training view in the UI. You can configure distributed training on the following:</p> <ul> <li>Trainings form</li> <li>Environments form</li> </ul> <p>You can select <code>single</code> or <code>multi-node (distributed)</code> training. When configuring distributed training, you will need to select a framework from the list. Supported frameworks now include:</p> <ul> <li>PyTorch</li> <li>Tensorflow</li> <li>XGBoost</li> <li>MPI</li> </ul> <p>For Trainings configuration, see Adding trainings. See your Run:ai representative to enable this feature. For Environments configuration, see Creating an Environment.</p> </li> <li> <p>Preview the new Workloads view. Workloads is a new view for jobs that are running in the AI cluster. The Workloads view provides a more advanced UI than the previous Jobs UI. The new table format provides:</p> <ul> <li>Improved views of the data</li> <li>Improved filters and search</li> <li>More information</li> </ul> <p>Use the toggle at the top of the Jobs page to switch to the Workloads view. For more information, see Workloads.</p> </li> <li> <p>Improved support for Kubeflow Notebooks. Run:ai now supports the scheduling of Kubeflow notebooks with fractional GPUs. Kubeflow notebooks are identified automatically and appear with a dedicated icon in the Jobs UI.</p> </li> <li>Improved the Trainings and Workspaces forms. Now the runtime field for Command and Arguments can be edited directly in the new Workspace or Training creation form.</li> <li>Added new functionality to the Run:ai CLI that allows submitting a workload with multiple service types at the same time in a CSV style format. Both the CLI and the UI now offer the same functionality. For more information, see runai submit.</li> <li>Improved functionality in the <code>runai submit</code> command so that the port for the container is specified using the <code>nodeport</code> flag. For more information, see <code>runai submit</code> --service-type <code>nodeport</code>.</li> </ul>"},{"location":"home/whats-new-2-15/#credentials","title":"Credentials","text":"<ul> <li>Improved Credentials creation. A Run:ai scope can now be added to credentials. For more information, see Credentials.</li> </ul>"},{"location":"home/whats-new-2-15/#environments","title":"Environments","text":"<ul> <li>Added support for workload types when creating a new or editing existing environments. Select from <code>single-node</code> or <code>multi-node (distributed)</code> workloads. The environment is available only on feature forms which are relevant to the workload type selected.</li> </ul>"},{"location":"home/whats-new-2-15/#volumes-and-storage","title":"Volumes and Storage","text":"<ul> <li>Added support for Ephemeral volumes in Workspaces. Ephemeral storage is temporary storage that gets wiped out and lost when the workspace is deleted. Adding Ephemeral storage to a workspace ties that storage to the lifecycle of the Workspace to which it was added. Ephemeral storage is added to the Workspace configuration form in the Volume pane. For configuration information, see Create a new workspace.</li> </ul>"},{"location":"home/whats-new-2-15/#templates","title":"Templates","text":"<ul> <li>Added support for Run:ai a Scope in the template form. For configuration information, see Creating templates.</li> </ul>"},{"location":"home/whats-new-2-15/#deployments","title":"Deployments","text":"<ul> <li>Improvements in the New Deployment form include:<ul> <li>Support for Tolerations. Tolerations guide the system to which node each pod can be scheduled to or evicted by matching between rules and taints defined for each Kubernetes node.</li> <li>Support for Multi-Process Service (MPS). MPS is a service which allows the running of parallel processes on the same GPU, which are all run by the same userid. To enable MPS support, use the toggle switch on the Deployments form.</li> </ul> <p>Note</p> <p>If you do not use the same userid, the processes will run in serial and could possibly degrade performance.</p> </li> </ul>"},{"location":"home/whats-new-2-15/#auto-delete-jobs","title":"Auto Delete Jobs","text":"<ul> <li>Added new functionality to the UI and CLI that provides configuration options to automatically delete jobs after a specified amount of time upon completion. Auto-deletion provides more efficient use of resources and makes it easier for researchers to manage their jobs. For more configuration options in the UI, see Auto deletion (Step 9) in Create a new workspace. For more information on the CLI flag, see --auto-deletion-time-after-completion.</li> </ul>"},{"location":"home/whats-new-2-15/#runai-administrator","title":"Run:ai Administrator","text":""},{"location":"home/whats-new-2-15/#authorization","title":"Authorization","text":"<ul> <li>Run:ai has now revised and updated the Role Based Access Control (RBAC) mechanism, expanding the scope of Kubernetes. Using the new RBAC mechanism makes it easier for administrators to manage access policies across multiple clusters and to define specific access rules over specific scopes for specific users and groups. Along with the revised RBAC mechanism, new user interface views are introduced to support the management of users, groups, and access rules. For more information, see Role based access control.</li> </ul>"},{"location":"home/whats-new-2-15/#policies","title":"Policies","text":"<ul> <li>During Workspaces and Training creation, assets that do not comply with policies cannot be selected. These assets are greyed out and have a button on the cards when the item does not comply with a configured policy. The button displays information about which policies are non-compliant.</li> <li>Added configuration options to Policies in order to prevent the submission of workloads that use data sources of type <code>host path</code>. This prevents data from being stored on the node, so that data is not lost when a node is deleted. For configuration information, see Prevent Data Storage on the Node.</li> <li>Improved flexibility when creating policies which provide the ability to allocate a <code>min</code> and a <code>max</code> value for CPU and GPU memory. For configuration information, see GPU and CPU memory limits in Configuring policies.</li> </ul>"},{"location":"home/whats-new-2-15/#nodes-and-node-pools","title":"Nodes and Node Pools","text":"<ul> <li>Node pools are now enabled by default. There is no need to enable the feature in the settings.</li> </ul>"},{"location":"home/whats-new-2-15/#quotas-and-over-quota","title":"Quotas and Over-Quota","text":"<ul> <li>Improved control over how over-quota is managed by adding the ability to block over-subscription of the quota in Projects or Departments. For more information, see Limit Over-Quota.</li> <li>Improved the scheduler fairness for departments using the <code>over quota priority</code> switch (in Settings). When the feature flag is disabled, over-quota weights are equal to the deserved quota and any excess resources are divided in the same proportion as the in-quota resources. For more information, see Over Quota Priority.</li> <li>Added new functionality to always guarantee in-quota workloads at the expense of inter-Department fairness. Large distributed workloads from one department may preempt in-quota smaller workloads from another department. This new setting in the <code>RunaiConfig</code> file preserves in-quota workloads, even if the department quota or over-quota-fairness is not preserved. For more information, see Scheduler Fairness.</li> </ul>"},{"location":"home/whats-new-2-15/#control-and-visibility","title":"Control and Visibility","text":""},{"location":"home/whats-new-2-15/#dashboards","title":"Dashboards","text":"<ul> <li>To ease the management of AI CPU and cluster resources, a new CPU focused dashboard was added for CPU based environments. The dashboards display specific information for CPU based nodes, node-pools, clusters, or tenants. These dashboards also include additional metrics that are specific to CPU based environments. This will help optimize visual information eliminating the views of empty GPU dashlets. For more information see CPU Dashboard.</li> <li>Improved the Consumption report interface by moving the Cost settings to the General settings menu.</li> <li>Added an additional table to the Consumption dashboard that displays the consumption and cost per department. For more information, see Consumption dashboard.</li> </ul>"},{"location":"home/whats-new-2-15/#nodes","title":"Nodes","text":"<ul> <li>Improved the readability of the Nodes table to include more detailed statuses and descriptions. The added information in the table makes it easier to inspect issues that may impact resource availability in the cluster. For more information, see Node and Node Pool Status.</li> </ul>"},{"location":"home/whats-new-2-15/#ui-enhancements","title":"UI Enhancements","text":"<ul> <li>Added the ability to download a CSV file from any page that contains a table. Downloading a CSV provides a snapshot of the page's history over the course of time, and helps with compliance tracking. All the columns that are selected (displayed) in the table are downloaded to the file.</li> </ul>"},{"location":"home/whats-new-2-15/#installation-and-configuration","title":"Installation and Configuration","text":""},{"location":"home/whats-new-2-15/#cluster-installation-and-configuration","title":"Cluster Installation and configuration","text":"<ul> <li>New cluster wizard for adding and installing new clusters to your system.</li> </ul>"},{"location":"home/whats-new-2-15/#openshift-support","title":"OpenShift Support","text":"<ul> <li>Added support for <code>restricted</code> policy for Pod Security Admission (PSA) on OpenShift only. For more information, see Pod security admission.</li> <li>Added the ability, in OpenShift environments, to configure cluster routes created by Run:ai instead of using the OpenShift certificate. For more information, see the table entry Dedicated certificate for the researcher service route.</li> </ul>"},{"location":"home/whats-new-2-16/","title":"Version 2.16","text":""},{"location":"home/whats-new-2-16/#release-content-january-25-2024","title":"Release Content - January 25, 2024","text":""},{"location":"home/whats-new-2-16/#researcher","title":"Researcher","text":"<ul> <li>Added enterprise level security for researcher tools such as Jupyter Notebooks, VSCode, or any other URL associated with the workload. Using this feature, anyone within the organization requesting access to a specific URL will be redirected to the login page to be authenticated and authorized. This results in protected URLs which cannot be reached from outside the organization. Researchers can enhance the URL privacy by using the Private toggle which means that only the researcher who created the workload can is authorized to access it. The Private toggle is available per tool that uses an external URL as a connection type and is located in the workload creation from in the UI in the environment section. This toggle sets a flag of <code>isPrivate</code> in the <code>connections</code> section of a policy for the connection type <code>ExternalUrl</code>. For more information, see Creating a new Workspace.</li> </ul>"},{"location":"home/whats-new-2-16/#jobs-workloads-and-workspaces","title":"Jobs, Workloads, and Workspaces","text":"<ul> <li>Added the capability view and edit policies directly in the project submission form. Pressing on Policy will open a window that displays the effective policy. For more information, see Viewing Project Policies.</li> </ul> <ul> <li> <p>Running machine learning workloads effectively on Kubernetes can be difficult, but Run:ai makes it easy. The new Workloads experience introduces a simpler and more efficient way to manage machine learning workloads, which will appeal to data scientists and engineers alike. The Workloads experience provides a fast, reliable, and easy to use unified interface.</p> <ul> <li>Fast-query of data from the new workloads service.</li> <li>Reliable data retrieval and presentation in the CLI, UI, and API.</li> <li>Easy to use single unified view with all workload types in one place.</li> </ul> <p>For more information, see Workloads Overview.</p> </li> <li> <p>Changed the workload default auto deletion time after completion value from <code>Never</code> to <code>90 days</code>. This ensures that environments will be cleaned from old data. This field is editable by default, allowing researchers the ability to change the value while submitting a workload. Using workload policies, administrators can increase, decrease, set the default value to <code>never</code>, or even lock access to this value so researchers can not edit it when they submit workloads.</p> </li> </ul>"},{"location":"home/whats-new-2-16/#assets","title":"Assets","text":"<ul> <li>When creating an asset such as data sources, credentials, or others, the scope is limited to the cluster selected at the top of the UI.</li> </ul>"},{"location":"home/whats-new-2-16/#runai-administrator","title":"Run:ai Administrator","text":"<ul> <li>Added the capability for administrators to configure messages to users when they log into the platform. Messages are configured using the Message Editor screen. For more information, see Administrator Messages.</li> </ul>"},{"location":"home/whats-new-2-16/#monitoring-and-analytics","title":"Monitoring and Analytics","text":"<ul> <li> <p>Added to the dashboard updated GPU and CPU resource availability.</p> <ul> <li>Added a chart displaying the number of free GPUs per node. Free GPU are GPUs that have not been allocated to a workload.</li> <li>Added a dashlet that displays the total vs. ready resources for GPUs and CPUs. The dashlet indicates how many total nodes are in the platform, and how many are available. </li> </ul> <p>For more information, see Total and Ready GPU or CPU Nodes.</p> </li> <li> <p>Added additional columns to the consumption report for both Projects and Departments tables. The new columns are:</p> <ul> <li>GPU Idle allocated hours\u2014the portion of time the GPUs spend idle from the total allocation hours.</li> <li>CPU usage hours\u2014the actual usage time of CPU.</li> <li>Memory usage time\u2014the actual usage time of CPU memory.</li> </ul> <p>For more information, see Consumption Dashboard.</p> </li> </ul>"},{"location":"home/whats-new-2-16/#authentication-and-authorization","title":"Authentication and Authorization","text":"<ul> <li>SSO users who have logged into the system will now be visible in the Users table. In addition, added a column to the Users table for the type of user that was created (Local or SSO). For more information, see Adding, Updating, and Deleting Users.</li> </ul>"},{"location":"home/whats-new-2-16/#policies","title":"Policies","text":"<ul> <li> <p>Added new Policy Manager. The new Policy Manager provides administrators the ability to impose restrictions and default vaules on system resources. The new Policy Manager provides a YAML editor for configuration of the policies. Administrators can easily add both Workspace or Training policies. The editor makes it easy to see the configuration that has been applied and provides a quick and easy method to edit the policies. The new Policy Editor* brings other important policy features such as the ability to see non-compliant resources in workloads. For more information, see Policies.</p> </li> <li> <p>Added a new policy manager. Enabling the New Policy Manager provides new tools to discover how resources are not compliant. Non-compliant resources and will appear greyed out and cannot be selected. To see how a resource is not compliant, press on the clipboard icon in the upper right hand corner of the resource. Policies can also be applied to specific scopes within the Run:ai platform. For more information, see Viewing Project Policies.</p> </li> </ul>"},{"location":"home/whats-new-2-16/#control-and-visibility","title":"Control and Visibility","text":"<ul> <li>Improved the clarity of the status column in the Clusters view. Now users have more insight about the actual status of Run:ai on the cluster. Users can now see extended details about the state of the Run:ai installation and services on the cluster, and its connectivity state. For more information, see Cluster status.</li> </ul>"},{"location":"home/whats-new-2-16/#deprecation-notifications","title":"Deprecation Notifications","text":"<p>Deprecation notifications allow you to plan for future changes in the Run:ai Platform. Deprecated features will be available for two versions ahead of the notification. For questions, see your Run:ai representative.</p>"},{"location":"home/whats-new-2-16/#project-migration","title":"Project migration","text":"<ul> <li> <p>Run:ai will be deprecating the migration of projects between departments. This affects:</p> <ul> <li>API\u2014the <code>departmentId</code> field will be marked as deprecated in the<code>put</code> endpoint in the <code>projects</code> category.</li> <li>User Interface\u2014there will no longer be an option to:<ul> <li>migrate projects to another department, when deleting departments.</li> <li>change departments, when editing a project.</li> </ul> </li> </ul> </li> </ul>"},{"location":"home/whats-new-2-16/#api-deprecations","title":"API deprecations","text":""},{"location":"home/whats-new-2-16/#removed-apis-and-api-fields-completed-deprecation","title":"Removed APIs and API fields (completed deprecation)","text":"<p>The following list of API endpoints and fields that have completed their deprecation process and therefore will be changed as follows:</p> Endpoint Change /v1/k8s/clusters The endpoint was removed and is replaced by /api/v1/clusters /v1/k8s/clusters/{uuid} The endpoint was removed and is replaced by /api/v1/clusters/{uuid}"},{"location":"home/whats-new-2-17/","title":"Version 2.17","text":""},{"location":"home/whats-new-2-17/#release-content-april-14-2024","title":"Release Content - April 14, 2024","text":"<ul> <li>Deprecation notifications</li> <li>Breaking changes</li> </ul>"},{"location":"home/whats-new-2-17/#researcher","title":"Researcher","text":""},{"location":"home/whats-new-2-17/#scheduler","title":"Scheduler","text":"<ul> <li> <p>Added functionality to configure over provisioning ratios for node pools running any kind of workload. Over provisioning assumes that workloads are either under utilizing or intermittently using GPUs. This indicates that the real utilization is lower than the actual GPU allocation requested. Over provisioning allows the administrator to condense more workloads on a single GPU than what the workload required. For more information, see Optimize performance with Node Level Scheduler.</p> </li> <li> <p>Added the GPU Resource Optimization feature to the UI. Now you can enable and configure GPU Portion (Fraction) limit and GPU Memory Limit from the UI. For more information, see Compute resources UI with Dynamic Fractions. </p> </li> <li> <p>Added the ability to set Run:ai as the default scheduler for any project or namespace. This provides the administrator the ability to ensure that all workloads in a project or namespace are scheduled using the Run:ai scheduler. For more information, see Setting Run:ai as default scheduler.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#jobs-workloads-and-workspaces","title":"Jobs, Workloads, and Workspaces","text":"<ul> <li> <p>Added to the workload details view, the ability to filter by pod. You can now filter metrics and logs per pod or all the pods. Also, the Workloads table now has additional columns including connections and preemtability adding more at a glance information about the workload. In addition, using the Copy &amp; edit button, you can submit a new workload via CLI based on the selected workload. For more information, see Workloads.</p> </li> <li> <p>Added Inference to workload types. Inference workloads can now be created and managed from the unified Workloads table. The Deployments workload type has been deprecated, and replaced with Inference workloads which are submitted using the workload form. For more information, see Inference and for submitting an Inference workload, see Submitting workloads.</p> </li> <li> <p>Added functionality that supports a single workloads submission selection. Now you can submit workloads by pressing + New workloads in the Workloads table. You can submit the following workloads from this table:</p> <ul> <li>Workspace</li> <li>Training</li> <li>Inference</li> </ul> <p>This improvement phases out the previous version's Workspace and Jobs tables. The Jobs table and submission forms have been deprecated and can be reactivated. To reenable the Jobs table and forms, press Tools &amp; settings, then General, then Workloads, and then Toggle the Jobs view and the Jobs submission buttons. For more information, see Submitting workloads.</p> </li> <li> <p>Added the ability to configure a Kubernetes readiness probe. The readiness probe detects resources and workloads that are ready to receive traffic.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#assets","title":"Assets","text":"<ul> <li> <p>Added the capability to use a ConfigMap as a data source. The ability to use a ConfigMap as a data source can be configured in the Data sources UI, the CLI, and as part of a policy. For more information, see Setup a ConfigMap as a data source, Setup a ConfigMap as a volume using the CLI, or Setup a ConfigMap Resource description fields in training policies.</p> </li> <li> <p>Added a Status column to the Credentials table, and the Data sources table. The Status column displays the state of the resource and provides troubleshooting information about that asset. For more information, see the Credentials table and the Data sources table.</p> </li> <li> <p>Added functionality for asset creation that validates the asset based on version compatibility of the cluster or the control plane within a specific scope. At time of asset creation, invalid scopes will appear greyed out and will show a pop-up with the reason for the invalidation. This improvement is designed to increase the confidence that an asset is created properly and successfully.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#runai-administrator","title":"Run:ai Administrator","text":""},{"location":"home/whats-new-2-17/#configuration-and-administration","title":"Configuration and Administration","text":"<ul> <li> <p>Introducing a new Tools &amp; Settings menu. The new Tools &amp; Settings menu provides a streamlined UI for administrators to configure the Run:ai environment. The new UI is divided into categories that easily identify the areas where the administrator can change settings. The new categories include:</p> <ul> <li>Analytics\u2014features related to analytics and metrics.</li> <li>Resources\u2014features related to resource configuration and allocation.</li> <li>Workloads\u2014features related to configuration and submission of workloads.</li> <li>Security\u2014features related to configuration of SSO (Single Sign On).</li> <li>Notifications\u2014used for system notifications.</li> <li>Cluster authentication\u2014snippets related to Researcher authentication.</li> </ul> <p>Some features are now labeled either Experimental or Legacy. Experimental features are new features in the environment, that may have certain instabilities and may not perform as expected. Legacy features are features that are in the process of being deprecated, and may be removed in future versions.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#clusters","title":"Clusters","text":"<ul> <li> <p>Added new columns to the Clusters table to show Kubernetes distribution and version. This helps administrators view potential compatibility issues that may arise.</p> </li> <li> <p>Improved the location of the cluster filter. The cluster filter has been relocated to filter bar and the drop down cluster filter in the header of the page has been removed. This improvement creates the following:</p> <ul> <li> <p>Filter assets by cluster in the following tables:</p> <ul> <li>Data sources</li> <li>Environments</li> <li>Computer resources</li> <li>Templates</li> <li>Credentials</li> </ul> </li> <li> <p>Creating a new asset, will automatically display only the scope of the selected cluster.</p> </li> <li>Prevention of account (top most level in the Scope) from being selected when creating assets.</li> <li>Enforcement a cluster specific scope. This increases the confidence that an asset is created properly and successfully.</li> </ul> <p>Note</p> <p>This feature is only applicable if the all the clusters are version 2.17 and above.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#monitoring-and-analytics","title":"Monitoring and Analytics","text":"<ul> <li> <p>Improved GPU Overview dashboard. This improvement provides rich and extensive GPU allocation and performance data and now has interactive tiles that provide direct links to the Nodes, Workloads, and Departments tables. Hover over tiles with graphs to show rich data in the selected time frame filter. Tiles with graphs can be downloaded as CSV files. The new dashboard is enabled by default. Use the Go back to legacy view to return to the previous dashboard style. For more information, see Dashboard analysis.</p> </li> <li> <p>Updated the knative and autoscaler metrics. Run:ai currently supports the following metrics:</p> <ul> <li>Throughput</li> <li>Concurrency</li> </ul> <p>For more information, see Autoscaling metrics.</p> </li> <li> <p>Improved availability of metrics by using Run:ai APIs. Using the API endpoints is now the preferred method to retrieve metrics for use in any application. For more information, see Metrics.</p> </li> </ul>"},{"location":"home/whats-new-2-17/#authentication-and-authorization","title":"Authentication and Authorization","text":"<ul> <li> <p>Added new functionality to SAML 2.0 identity provider configuration in the Security category of the General settings. The added functionality assists with troubleshooting SSO configuration and authentication issues that may arise. Now administrators now have the ability to:</p> <ul> <li>View and edit the identity provider settings for SAML 2.0</li> <li>Upload or download the SAML 2.0 identity provider metadata XML file.</li> </ul> </li> </ul> <p>For more information, see SSO UI configuration.</p>"},{"location":"home/whats-new-2-17/#deprecation-notifications","title":"Deprecation Notifications","text":"<p>Deprecation notifications allow you to plan for future changes in the Run:ai Platform.</p>"},{"location":"home/whats-new-2-17/#feature-deprecations","title":"Feature deprecations","text":"<p>Deprecated features will be available for two versions ahead of the notification. For questions, see your Run:ai representative. The following features have been marked for deprecation:</p> <ul> <li>Jobs\u2014the Jobs feature (submission form and view) has been moved to the category of Legacy. To enable them, go to Tools &amp; Settings, General, open the Workloads pane, and then toggle the Jobs view and Job submission switch to the enabled position.</li> <li>Deployments\u2014the Deployments feature has been removed. It has been replaced by Inference workloads. For more information, see Jobs, Workloads, and Workspaces above.</li> <li>Workspaces view\u2014the Workspaces menu has been removed. You can now submit a Workspace workload using the + New workload form from the Workloads table.</li> </ul>"},{"location":"home/whats-new-2-17/#api-support-and-endpoint-deprecations","title":"API support and endpoint deprecations","text":"<p>The endpoints and parameters specified in the API reference are the ones that are officially supported by Run:ai. For more information about Run:ai's API support policy and deprecation process, see Developer overview.</p>"},{"location":"home/whats-new-2-17/#deprecated-apis-and-api-fields","title":"Deprecated APIs and API fields","text":"<p>The following list of API endpoints and fields that have been marked for deprecation:</p>"},{"location":"home/whats-new-2-17/#jobs-and-pods-api","title":"Jobs and Pods API","text":"Deprecated Replacement /v1/k8s/clusters/{uuid}/jobs /api/v1/workloads /v1/k8s/clusters/{uuid}/jobs/count /api/v1/workloads/count /v1/k8s/clusters/{uuid}/jobs/{jobId}/pods /api/v1/workloads/{workloadId}/pods /v1/k8s/clusters/{uuid}/pods /api/v1/workloads/pods"},{"location":"home/whats-new-2-17/#clusters-api","title":"Clusters API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterUuid}/metrics /api/v1/clusters/{clusterUuid}/metrics"},{"location":"home/whats-new-2-17/#authorization-and-authentication-api","title":"Authorization and Authentication API","text":"Deprecated Replacement /v1/k8s/auth/token/exchange /api/v1/token /v1/k8s/auth/oauth/tokens/refresh /api/v1/token /v1/k8s/auth/oauth/apptoken /api/v1/token /v1/k8s/users/roles /api/v1/authorization/roles /v1/k8s/users /api/v1/users /v1/k8s/users/{userId} /api/v1/users/{userId} /v1/k8s/users/{userId}/roles /api/v1/authorization/access-rules /v1/k8s/apps /api/v1/apps /v1/k8s/apps/{clientId} /api/v1/apps/{appId} /v1/k8s/groups /api/v1/authorization/access-rules /v1/k8s/groups/{groupName} /api/v1/authorization/access-rules /v1/k8s/clusters/{clusterId}/departments/{department-id}/access-control /api/v1/authorization/access-rules /api/v1/authorization/access-rules - <code>subjectIdFilter</code> field Use <code>filterBy</code> / <code>sortBy</code> fields /api/v1/authorization/access-rules - <code>scopeType</code> field Use <code>filterBy</code> / <code>sortBy</code> fields /api/v1/authorization/access-rules - <code>roleId</code> field Use <code>filterBy</code> / <code>sortBy</code> fields"},{"location":"home/whats-new-2-17/#projects-api","title":"Projects API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterId}/projects - <code>permissions</code> field /api/v1/authorization/access-rules /v1/k8s/clusters/{clusterId}/projects - <code>resources</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/projects - <code>deservedGpus</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/projects - <code>maxAllowedGpus</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/projects - <code>gpuOverQuotaWeight</code> field Use <code>nodePoolResources</code> field"},{"location":"home/whats-new-2-17/#departments-api","title":"Departments API","text":"Deprecated Replacement /v1/k8s/clusters/{clusterId}/departments - <code>resources</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/departments - <code>deservedGpus</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/departments - <code>allowOverQuota</code> field Use <code>nodePoolResources</code> field /v1/k8s/clusters/{clusterId}/departments - <code>maxAllowedGpus</code> field Use <code>nodePoolResources</code> field"},{"location":"home/whats-new-2-17/#policy-api","title":"Policy API","text":"Deprecated Replacement /api/v1/policy/workspace /api/v2/policy/workspaces /api/v1/policy/training /api/v2/policy/trainings"},{"location":"home/whats-new-2-17/#logo-api","title":"Logo API","text":"Deprecated Replacement /v1/k8s/tenant/{tenantId}/logo /api/v1/logo"},{"location":"home/whats-new-2-17/#removed-apis-and-api-fields-completed-deprecation","title":"Removed APIs and API fields (completed deprecation)","text":"<p>The following list of API endpoints and fields that have completed their deprecation process and therefore will be changed as follows:</p>"},{"location":"home/whats-new-2-17/#assets-api","title":"Assets API","text":"Endpoint Change /api/v1/asset/compute <code>gpuRequest</code> field was removed and is replaced by the following fields:  * <code>gpuDevicesRequest</code> (New and mandatory)  * <code>gpuRequestType</code> (New and mandatory if  <code>gpuDevicesRequest=1</code> otherwise optional for values 0 or greater than 1)  * <code>gpuPortion</code> was changed to <code>gpuPortionRequest</code> and accepts values between 0 and 1 (for example 0.75)  * <code>gpuPortionLimit</code> (New and optional)  * <code>gpuMemory</code> was changed to <code>gpuMemoryRequest</code>  * <code>gpuMemoryLimit</code> (New and optional)"},{"location":"home/whats-new-2-17/#metrics-deprecations","title":"Metrics deprecations","text":"<p>The following metrics are deprecated and replaced by API endpoints. For details about the replacement APIs, see Changed Metrics:</p> Metric runai_active_job_cpu_requested_cores runai_active_job_memory_requested_bytes runai_cluster_cpu_utilization runai_cluster_memory_utilization runai_gpu_utilization_per_pod_per_gpu runai_gpu_utilization_per_workload runai_job_requested_gpu_memory runai_gpu_memory_used_mebibytes_per_workload runai_gpu_memory_used_mebibytes_per_pod_per_gpu runai_active_job_cpu_limits runai_job_cpu_usage runai_active_job_memory_limits runai_job_memory_used_bytes"},{"location":"home/whats-new-2-17/#breaking-changes","title":"Breaking changes","text":"<p>Breaking changes notifications allow you to plan around potential changes that may interfere your current workflow when interfacing with the Run:ai Platform.</p>"},{"location":"home/whats-new-2-17/#metrics","title":"Metrics","text":"<p>Be aware that some names of metrics have been changed. For more information, see Changed Metrics.</p>"},{"location":"home/whats-new-2-8/","title":"Run:ai Version 2.8","text":""},{"location":"home/whats-new-2-8/#release-date","title":"Release Date","text":"<p>November 2022 </p>"},{"location":"home/whats-new-2-8/#release-content","title":"Release Content","text":""},{"location":"home/whats-new-2-8/#node-pools","title":"Node Pools","text":"<p>Node Pools is a new method for managing GPU and CPU resources by grouping the resources into distinct pools. With node pools:</p> <ul> <li>The administrator allocates Project and Department resources from these pools to be used by Workloads. </li> <li>The administrator controls which workloads can use which resources, allowing an optimized utilization of resources according to customer's specific mode of operation. </li> </ul>"},{"location":"home/whats-new-2-8/#user-interface-enhancements","title":"User Interface Enhancements","text":"<ul> <li>The Departments screen has been revamped and new functionality added, including a new and clean look and feel, and improved search and filtering capabilities.</li> <li>The Jobs screen has been split into 2 tabs for ease of use:</li> <li>Current:  (the default tab) consists of all the jobs that currently exist in the cluster. </li> <li>History:  consists of all the jobs that have been deleted from the cluster. Deleting Jobs also deletes their Log (no change).</li> </ul>"},{"location":"home/whats-new-2-8/#installation-improvements","title":"Installation improvements","text":"<p>The Run:ai user interface requires a URL address to the Kubernetes cluster. The requirement is relevant for SaaS installation only. </p> <p>In previous versions of Run:ai the administrator should provide an IP address and Run:ai would automatically create a DNS entry for it and a matching trusted certificate. </p> <p>In version 2.8,  the default is for the Run:ai administrator to provide a DNS and a trusted certificate. </p> <p>The older option still exists but is being deprecated due to complexity.</p>"},{"location":"home/whats-new-2-8/#inference","title":"Inference","text":"<p>The Deployment details page now contains the URL for the Inference service </p>"},{"location":"home/whats-new-2-8/#hyperparameter-optimization-hpo","title":"Hyperparameter Optimization (HPO)","text":"<p>HPO Jobs are now presented as a single line in the Job List rather than a separate line per experiment. </p>"},{"location":"home/whats-new-2-8/#known-issues","title":"Known Issues","text":"Internal ID Description Workaround RUN-6236 The Run:ai access control system prevents setting a role of researcher together with ML engineer or researcher manager at the same time. However, using the UI you can select these two roles by clicking the text near the check None RUN-6218 When installing Run:ai on OpenShift a second time, oauth client secret is incorrect/not updated. As a result, login is not possible Can be performed via manual configuration. Please contact Run:ai support. RUN-6216 In the multi cluster overview, the allocated GPU in the table of each cluster is wrong. The correct number is in the overview dashboard. None RUN-6190 When deleting a cluster, there are leftover pods that are not deleted. No side effects on functionality. Delete the pods manually. RUN-5855 (SaaS version only) The new control plane, versioned 2.8 does not allow the creation of a new deployment on a cluster whose version is lower than 2.8. Upgrade your cluster to 2.8 RUN-5780 It is possible to change runai/node-pool label of a running pod. This is a wrong usage of the system and may cause unexpected behavior. None. RUN-5527 Idle allocated GPU metric is not displayed for MIG workloads in OpenShift. None RUN-5519 When selecting a Job, the GPU memory utilization metrics is not displayed on the right-hand side. This is an NVIDIA DCGM known bug (see:  https://github.com/NVIDIA/dcgm-exporter/issues/103 ) which has been fixed in a later version but was not yet included in the latest NVIDIA GPU Operator Install the suggested version as described by NVIDIA. RUN-5478 Dashboard panels of GPU Allocation/project and Allocated jobs per project metrics:  In rare cases, some metrics reflect the wrong number of GPUs None RUN-5444 Dynamic MIG feature does not work with A-100 with 80GB of memory. None RUN-5424 When a workload is selected in the job list, the GPU tab in the right panel, shows the details of the whole GPUs in the node, instead of the details of the GPUs used by the workload. None RUN-5226 In rare occasions, when there is more than 1 NVIDIA MIG workload, nvidia-smi command to one of the workloads will result with no devices. None RUN-6359 In rare cases, when using fractions and the kubelet service on the scheduled node is down (Kubernetes not running on node)the pending workload will never run, even when the IT problem is solved. Delete the job and re-submit the workload. RUN-6399 Requested GPUs are sometimes displayed in the Job list as 0 for distributed workloads. None. This is a display-only issue RUN-6400 On EKS (Amazon Kubernetes Server), when using runai CLI, every command response starts with an error. No functionality harm. None. The CLI functions as expected."},{"location":"home/whats-new-2-8/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-5676 When Interactive Jupyter notebook workloads that contain passwords are cloned, the password is exposed in the displayed CLI command. RUN-5457 When using the Home environment variable in conjunction with the ran-as-user option in the CLI, the Home environment variable is overwritten with the user's home directory. RUN-5370 It is possible to submit two jobs with the same node-port. RUN-5314 When you apply an inference deployment via a file, the allocated GPUs are displayed as 0 in the deployments list. RUN-5284 When workloads are deleted while the cluster synchronization is down, there might be a non-existent Job shown in the user interface. The Job cannot be deleted. RUN-5160 In some situations, when a Job is deleted, there may be leftover Kubernetes configMaps in the system RUN-5154 In some cases, an error \"failed to load data\" can be seen in the graphs showing on the Job sidebar. RUN-5145 The default Kubernetes \"priority Class\" for deployments is the same as the priority class for interactive jobs. RUN-5039 In some scenarios, Dashboards may show \"found duplicate series for the match group\" error RUN-4941 The scheduler is wrongly trying to schedule jobs on a node, where there are allocated GPU jobs at an \"ImagePullBackoff\" state. This causes an error of \"UnexpectedAdmissionError\" RUN-4574 The role \"Researcher Manager\" is not displayed in the access control list of projects. RUN-4554 Users are trying to login with single-sign-on get a \"review profile\" page. RUN-4464 Single HPO (hyperparameter optimization) workload is displayed in the Job list user interfgace as multiple jobs (one for every pod)."},{"location":"home/whats-new-2-9/","title":"Run:ai Version 2.9","text":""},{"location":"home/whats-new-2-9/#version-299","title":"Version 2.9.9","text":"Internal ID Description RUN-10333 Fixed an issue with allowing a fractional GPU value of 0 when submitting jobs via YAML. RUN-9920 Fixed an issue with policies where the <code>canEdit</code> rule is not validated properly for itemized fields. RUN-9912 Fixed an issue where <code>runai bash</code> does not wait for pods to be ready. RUN-9902 Fixed an issue with Prometheus permissions in OpenShift environments. RUN-9326 Fixed an issue that affected the dashboard where projects created with fractional GPUs, display the number of GPUs rounded down to nearest whole number."},{"location":"home/whats-new-2-9/#version-297","title":"Version 2.9.7","text":""},{"location":"home/whats-new-2-9/#release-date","title":"Release date","text":"<p>May 2023</p>"},{"location":"home/whats-new-2-9/#fixed-issues","title":"Fixed Issues","text":"Internal ID Description RUN-8989 Fixed openshift authentication for users lacking email so that they can submit jobs using the UI. RUN-9488 Fixed certificate error when retrieving dashboards in environments that are using a self-signed certificate."},{"location":"home/whats-new-2-9/#release-date_1","title":"Release Date","text":"<p>February 2023</p>"},{"location":"home/whats-new-2-9/#release-content","title":"Release Content","text":""},{"location":"home/whats-new-2-9/#authentication","title":"Authentication","text":"<p>OpenShift groups</p> <p>Ability to manage access control through IDP groups declaration - groups are managed from the OpenShift platform and integrated into Run:ai platform, as opposed to group management in vanilla k8s with SSO. OpenShift doesn\u2019t need any additional configuration as this comes built-in with regular installation or the upgrade option.</p> <p>UID/GID for SSO users</p> <p>When running a workload through the UI the Run:ai platform now automatically injects the UID and GID into the created container. This has changed from previous versions where the user would enter data in these fields manually. This is designed for environments where UIDs and GIDs are managed in an SSO server, and Run:ai is configured with SSO.   </p> <p>SSO: block access to Run:ai</p> <p>When configuring SSO in the Run:ai platform all users are assigned a new default role. It means an SSO user will not have any access to the Run:ai platform unless a manager explicitly assigns additional roles via the user or group management areas.</p> <p>Run CPU over-quota workloads</p> <p>Added support for CPU workloads to support over-quota - CPU resources fairness was added to the Run:ai scheduler in addition to the GPU fairness that is already supported. The updated fairness algorithm takes into account all resource types (GPU, CPU compute and CPU memory) and is supported regardless of node pool configuration. </p>"},{"location":"home/whats-new-2-9/#runai-workspaces","title":"Run:ai Workspaces","text":"<p>A Run:ai workspace is a simplified, efficient tool for researchers to conduct their experiments, build AI models, access standard MLOps tools, and collaborate with their peers.</p> <p>Run:ai workspaces abstract complex concepts related to running containerized workloads in a Kubernetes environment, such as networking, storage, and secrets, and are built from predefined abstracted setups, that ease and streamline the researcher AI models development. A workspace consists of container images, data sets, resource requests, and all the required tools for the research. They are quickly created with the workspace wizard. For more information see Workspaces.</p>"},{"location":"home/whats-new-2-9/#new-supported-tools-for-researchers","title":"New supported tools for researchers","text":"<p>As part of the introduction of Run:ai workspaces a few new development and research tools were added. The new supported tools are: RStudio, Visual Studio Code, Matlab and Weights and Biases (see full details). This is an addition to adding already supported tools, such as JupyterNotebook and TensorBoard to Run:ai workspaces.</p> <p>Weight and Biases</p> <p>Weights and Biases is a commercial tool that provides experiment tracking, model visualization, and collaboration for machine learning projects. It helps researchers and developers keep track of their experiments, visualize their results, and compare different models to make informed decisions. This integration provides data researchers with connectivity between the running Workspace in Run:ai and the relevant project for experiment tracking. For more information, see Weights and Biases.</p> <p>Node pools enhancements</p> <p>Added additional support to multi-node pools. This new capability allows the researcher to specify a prioritized list of node pools for the Run:ai scheduler to use. Researchers now gain the flexibility to use multiple resource types and maximize the utilization of the system\u2019s GPU and CPU resources. Administrators now have the option to set a default Project (namespace) level with a prioritized list of node pools that a workload will use if the researcher did not set its own priorities.</p> <p>New nodes and node pools Screens</p> <p>Run:ai has revised the nodes table, adding new information fields and graphs. It is now easier to assess how resources are allocated and utilized. Run:ai has also added a new \u2018node pools\u2019 table where Administrators can add a new node pool, update, and delete an existing node pool. In addition, the node pools table presents a large number of metrics and details about each of the node pools. A set of graphs reflect the node pools\u2019 resource status over time according to different criteria.</p> <p>Consumption Dashboard</p> <p>Added a \u201cConsumption\u201d dashboard. When enabled by the \u201cShow Consumption Dashboard\u201d alpha flag under \u201cSettings\u201d, this dashboard allows the admin to review consumption patterns for GPUs, CPUs and RAM over time. You can segregate consumption by over or in-quota allocation in the project or department level. For more information, see Consumption dashboard.</p> <p>Event History (Audit Log UI)</p> <p>Added the option for Administrators to view the system\u2019s Audit Log via the Run:ai user interface. Configuration changes and other administrative operations (login/logout etc) are saved in an Audit Log facility. Administrators can browse through the Admin Log (Event History), download as a JSON or CSV, filter specific date periods, set multiple criteria filters, and decide which information fields to view.</p> <p>Idle jobs timeout policy</p> <p>Added an option \u2018Editor\u2019 so that Administrators can terminate idle workloads by setting the criteria of \u2018idle time\u2019 per project so that the editor can identify and terminate idle Training and Interactive (build) workloads. This is used for maximizing and maintaining system sanitation.</p>"},{"location":"home/whats-new-2-9/#installation-enhancements","title":"Installation Enhancements","text":""},{"location":"home/whats-new-2-9/#cluster-upgrade","title":"Cluster Upgrade","text":"<p>Cluster upgrade to 2.9 requires uninstalling and then installing. No data is lost during the process. For more information see cluster upgrade.</p> <p>Using an IP address for a cluster URL is no longer available in this version. You must use a domain name.</p>"},{"location":"home/whats-new-2-9/#cluster-prerequisites","title":"Cluster Prerequisites","text":"<ul> <li> <p>Prometheus is no longer installed together with Run:ai. You must install the Prometheus stack before installing Run:ai. This is designed for organizations that already have Prometheus installed in the cluster. The Run:ai installation configures the existing Prometheus with a custom set of rules designed to extract metrics from the cluster.</p> </li> <li> <p>NGINX is no longer installed together with Run:ai. You must install an Ingress controller before installing Run:ai. This is designed for organizations that already have an ingress controller installed. The Run:ai installation creates NGINX rules to work with the controller.</p> </li> <li> <p>List of Run:ai installation Prerequisites can be found here.</p> </li> <li> <p>The Run:ai installation now performs a series of checks to verify the installation's validity. When the installation is complete, verify by reviewing the following in the log file:</p> <ul> <li>Are all mandatory prerequisites met?</li> <li>Are optional prerequisites met?</li> <li>Does the cluster have connectivity to the Run:ai control plane?</li> <li>Does Run:ai support the underlying Kubernetes version?</li> </ul> </li> </ul>"},{"location":"home/whats-new-2-9/#control-plane-upgrade","title":"Control Plane Upgrade","text":"<p>A special process is required to upgrade the control-plane to version 2.9. </p>"},{"location":"home/whats-new-2-9/#control-plane-prerequisites","title":"Control plane Prerequisites","text":"<ul> <li> <p>Run:ai control plane installation no longer installs NGINX. You must pre-install an ingress controller.</p> </li> <li> <p>The default persistent storage is now a default storage class preconfigured in Kubernetes rather than the older NFS assumptions. NFS flags in <code>runai-adm</code> generate-values still exist for backward compatibility.</p> </li> </ul>"},{"location":"home/whats-new-2-9/#other","title":"Other","text":"<p>Cluster Wizard has been simplified for environments with multiple clusters   in a self-hosted configuration. Clusters are now easier to configure. Choose a cluster location: </p> <ul> <li>Same as Control Plane. </li> <li>Remote to Control Plane. </li> </ul>"},{"location":"home/whats-new-2-9/#new-supported-software","title":"New Supported Software","text":"<ul> <li>Run:ai now supports Kubernetes 1.25 and 1.26.</li> <li>Run:ai now supports OpenShift 4.11</li> <li>Run:ai now supports Dynamic MIG with NVIDIA H100 hardware</li> <li>The Run:ai command-line interface now supports Microsoft Windows. See Install the Run:ai Command-line Interface.</li> </ul>"},{"location":"home/whats-new-2-9/#known-issues","title":"Known Issues","text":"Internal ID Description Workaround RUN-7874 When a project is not connected to a namespace - new job returns \"malformed URL\" None RUN-7617 Cannot delete Node affinity from project after it was created Remove it using the API."},{"location":"home/whats-new-2-9/#fixed-issues_1","title":"Fixed Issues","text":"Internal ID Description RUN-7776 user does not exist in the UI due to pagination limitation RUN-6995 Group Mapping from SSO Group to Researcher Manager Role no working RUN-6460 S3 Fail (read/write in Jupyter notebook) RUN-6445 Project can be created with deleted node pool RUN-6400 EKS - Every command response in runai CLI starts with an error. No functionality harm RUN-6399 Requested GPU is always 0 for MPI jobs, making also other metrics wrong RUN-6359 Job gets UnexpectedAdmissionError race condition with Kubelet RUN-6272 runai pod which owner is not RunaiJob - Do not allow deletion, suspension, cloning RUN-6218 When installing Run:ai on OpenShift a second time, oauth client secret is incorrect/not updated RUN-6216 Multi cluster: allocated GPU is wrong as a result of metric not counting jobs in error RUN-6029 CLI Submit git sync severe bug RUN-6027 [Security Issue] Job submitted with github sync -- Password is displayed in the UI RUN-5822 Environment Variables in the UI do not honor the \"canRemove:false\" attribute in Policy RUN-5676 Security issue with \"Clone Job\" functionality RUN-5527 Metrics (MIG - OCP): GPU Idle Allocated GPUs show No Data RUN-5478 # of GPUs is higher than existing GPUs in the cluster RUN-5444 MIG doesn't work on A100 - 80GB RUN-5424 Deployment GPUs tab shows all the GPUs on the node instead of the ones in use by the deployment RUN-5370 Can submit job with the same node port + imagePullpolicy RUN-5226 MIG job can't see device after submitting a different mig job RUN-4869 S3 jobs run forever with NotReady state RUN-4244 Run:ai Alertmanager shows false positive errors on Agent"},{"location":"home/changelog/hotfixes-2-13/","title":"Changelog Version 2.13","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.13.</p>"},{"location":"home/changelog/hotfixes-2-13/#version-21348-march-14-2024","title":"Version 2.13.48 - March 14, 2024","text":"Internal ID Description RUN-16787 Fixed an issue after an upgrade to 2.13 where distributed PyTorch jobs were not able to run due to PVCs being assigned to only worker pods. RUN-16626 Fixed an issue in SSO environments, where Workspaces created using a template were assigned the template creator's UID/GID and not the Workspace creator's UID/GID. RUN-16357 Fixed an issue where pressing the Project link in Jobs screen redirects the view to the Projects of a different cluster in multi-cluster environments."},{"location":"home/changelog/hotfixes-2-13/#version-21343-february-15-2024","title":"Version 2.13.43 - February 15, 2024","text":"Internal ID Description RUN-14946 Fixed an issue where Dashboards are displaying the hidden Grafana path."},{"location":"home/changelog/hotfixes-2-13/#version-21337","title":"Version 2.13.37","text":"Internal ID Description RUN-13300 Fixed an issue where projects will appear with a status of empty while waiting for the project controller to update its status. This was caused because the cluster-sync works faster than the project controller."},{"location":"home/changelog/hotfixes-2-13/#version-21335-december-19-2023","title":"Version 2.13.35 - December 19, 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content","title":"Release content","text":"<ul> <li>Added the ability to set node affinity for Prometheus.</li> </ul>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-14472 Fixed an issue where template updates were not being applied to the workload. RUN-14434 Fixed an issue where <code>runai_allocated_gpu_count_per_gpu</code> was multiplied by seven. RUN-13956 Fixed an issue where editing templates failed. RUN-13825 Fixed an issue when deleting a job that is allocated a fraction of a GPU, an associated configmap is not deleted. RUN-13343 Fixed an issue in pod status calculation."},{"location":"home/changelog/hotfixes-2-13/#version-21331","title":"Version 2.13.31","text":"Internal ID Description RUN-11367 Fixed an issue where a double click on SSO Users redirects to a blank screen. RUN-10560 Fixed an issue where the <code>RunaiDaemonSetRolloutStuck</code> alert did not work."},{"location":"home/changelog/hotfixes-2-13/#version-21325","title":"Version 2.13.25","text":"Internal ID Description RUN-13171 Fixed an issue when a cluster is not connected the actions in the Workspace and Training pages are still enabled. After the corrections, the actions will be disabled."},{"location":"home/changelog/hotfixes-2-13/#version-21321","title":"Version 2.13.21","text":"Internal ID Description RUN-12563 Fixed an issue where users are unable to login after upgrading the control plane from 2.9.16 to 2.13.16. To correct the issue, secrets need to be upgraded manually in keycloak."},{"location":"home/changelog/hotfixes-2-13/#version-21320-september-28-2023","title":"Version 2.13.20 - September 28, 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content_1","title":"Release content","text":"<ul> <li>Added the prevention of selecting tenant or department scopes for credentials, and the prevention of selecting s3, PVC, and Git data sources if the cluster version does not support these.</li> <li>Quota management is now enabled by default.</li> </ul> Internal ID Description RUN-12923 Fixed an issue in upgrading due to a misconfigured Docker image for airgapped systems in 2.13.19. The helm chart contained an error, and the image is not used even though it is packaged as part of the tar. RUN-12928, RUN-12968 Fixed an issue in upgrading Prometheus due to a misconfigured image for airgapped systems in 2.13.19. The helm chart contained an error, and the image is not used even though it is packaged as part of the tar. RUN-12751 Fixed an issue when upgrading from 2.9 to 2.13 results with a missing engine-config file. RUN-12717 Fixed an issue where the user that is logged in as researcher manager can't see the clusters. RUN-12642 Fixed an issue where assets-sync could not restart due to failing to get token from control plane. RUN-12191 Fixed an issue where there was a timeout while waiting for the <code>runai_allocated_gpu_count_per_project</code> metric to return values. RUN-10474 Fixed an issue where the <code>runai-conatiner-toolkit-exporter</code> DaemonSet fails to start."},{"location":"home/changelog/hotfixes-2-13/#version-21319-september-27-2023","title":"Version 2.13.19 - September 27, 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content_2","title":"Release content","text":"<ul> <li>Added the ability to identify Kubeflow notebooks and display them in the Jobs table.</li> <li>Added the ability to schedule Kubelow workloads.</li> <li>Added functionality that displays Jobs that only belong to the user that is logged in.</li> <li> Added and refined alerts to the state of Run:ai components, schedule latency, and warnings for out of memory on Jobs.</li> <li>Added the ability to work with restricted PSA policy.</li> </ul>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues_1","title":"Fixed issues","text":"Internal ID Description RUN-12650 Fixed an issue that used an incorrect metric in analytics GPU ALLOCATION PER NODE panel. Now the correct allocation is in percentage. RUN-12602 Fixed an issue in <code>runaiconfig</code> where the <code>WorkloadServices</code> spec has memory requests/limits and cpu requests/limits and gets overwritten with the system default. RUN-12585 Fixed an issue where the workload-controller creates a delay in running jobs. RUN-12031 Fixed an issue when upgrading from 2.9 to 2.13 where the Scheduler pod fails to upgrade due to the change of owner. RUN-11091 Fixed an issue where the Departments feature is disabled, you are not able to schedule non-preemable jobs."},{"location":"home/changelog/hotfixes-2-13/#version-21313","title":"Version 2.13.13","text":"Internal ID Description RUN-11321 Fixed an issue where metrics always showed CPU Memory Utilization and CPU Compute Utilization as 0. RUN-11307 Fixed an issue where node affinity might change mid way through a job. Node affinity in now calculated only once at job submission. RUN-11129 Fixed an issue where CRDs are not automatically upgraded when upgrading from 2.9 to 2.13."},{"location":"home/changelog/hotfixes-2-13/#version-21312-august-7-2023","title":"Version 2.13.12 - August 7, 2023","text":"Internal ID Description RUN-11476 Fixed an issue with analytics node pool filter in Allocated GPUs per Project panel."},{"location":"home/changelog/hotfixes-2-13/#version-21311","title":"Version 2.13.11","text":"Internal ID Description RUN-11408 Added to the Run:ai job-controller 2 configurable parameters <code>QPS</code> and <code>Burst</code> which are applied as environment variables in the job-controller Deployment object."},{"location":"home/changelog/hotfixes-2-13/#version-2137-july-2023","title":"Version 2.13.7 - July 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content_3","title":"Release content","text":"<ul> <li>Added filters to the historic quota ratio widget on the Quota management dashboard.</li> </ul>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues_2","title":"Fixed issues","text":"Internal ID Description RUN-11080 Fixed an issue in OpenShift environments where log in via SSO with the <code>kubeadmin</code> user, gets blank pages for every page. RUN-11119 Fixed an issue where values that should be the Order of priority column are in the wrong column. RUN-11120 Fixed an issue where the Projects table does not show correct metrics when Run:ai version 2.13 is paired with a Run:ai 2.8 cluster. RUN-11121 Fixed an issue where the wrong over quota memory alert is shown in the Quota management pane in project edit form. RUN-11272 Fixed an issue in OpenShift environments where the selection in the cluster drop down in the main UI does not match the cluster selected on the login page."},{"location":"home/changelog/hotfixes-2-13/#version-2134","title":"Version 2.13.4","text":""},{"location":"home/changelog/hotfixes-2-13/#release-date","title":"Release date","text":"<p>July 2023</p>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues_3","title":"Fixed issues","text":"Internal ID Description RUN-11089 Fixed an issue when creating an environment, commands in the Runtime settings pane and are not persistent and cannot be found in other assets (for example in a new Training)."},{"location":"home/changelog/hotfixes-2-13/#version-2131-july-2023","title":"Version 2.13.1 - July 2023","text":""},{"location":"home/changelog/hotfixes-2-13/#release-content_4","title":"Release content","text":"<ul> <li>Made an improvement so that occurrences of labels that are not in use anymore are deleted.</li> </ul>"},{"location":"home/changelog/hotfixes-2-13/#fixed-issues_4","title":"Fixed issues","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-15/","title":"Changelog Version 2.15","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.15.</p>"},{"location":"home/changelog/hotfixes-2-15/#version-2159-february-5-2024","title":"Version 2.15.9 - February 5, 2024","text":""},{"location":"home/changelog/hotfixes-2-15/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-15296 Fixed an issue where the <code>resources</code> parameter was deprecated in the Projects and Departments API."},{"location":"home/changelog/hotfixes-2-15/#version-2154-january-5-2024","title":"Version 2.15.4 - January 5, 2024","text":""},{"location":"home/changelog/hotfixes-2-15/#fixed-issues_1","title":"Fixed issues","text":"Internal ID Description RUN-15026 Fixed an issue in workloads that were built on a cluster that does not support the NFS field. RUN-14907 Fixed an issue after an upgrade where the Analytics dashboard was missing the time ranges from before the upgrade. RUN-14903 Fixed an issue where internal operations were exposed to the customer audit log. RUN-14062 Fixed an issue in the Overview dashboard where the content for the Running Workload per Type panel did not fit."},{"location":"home/changelog/hotfixes-2-15/#version-2152-february-5-2024","title":"Version 2.15.2 - February 5, 2024","text":""},{"location":"home/changelog/hotfixes-2-15/#fixed-issues_2","title":"Fixed issues","text":"Internal ID Description RUN-14434 Fixed an issue where the Allocated GPUs metric was multiplied by seven."},{"location":"home/changelog/hotfixes-2-15/#version-2151-december-17-2023","title":"Version 2.15.1 - December 17, 2023","text":""},{"location":"home/changelog/hotfixes-2-15/#release-content","title":"Release content","text":"<ul> <li> <p>Added environment variables for customizable QPS and burst support.</p> </li> <li> <p>Added the ability to support running multiple Prometheus replicas.</p> </li> </ul>"},{"location":"home/changelog/hotfixes-2-15/#fixed-issues_3","title":"Fixed issues","text":"Internal ID Description RUN-14292 Fixed an issue where BCM installations were failing due to missing <code>create cluster</code> permissions. RUN-14289 Fixed an issue where metrics were not working due to an incorrect parameter in the cluster-config file. RUN-14198 Fixed an issue in services where multi nodepool jobs were not scheduled due to an unassigned nodepool status. RUN-14191 Fixed an issue where a consolidation failure would cause unnecessary evictions. RUN-14154 Fixed an issue in the New cluster form, whefre the dropdown listed versions that were incompatible with the installed control plane. RUN-13956 Fixed an issue in the Jobs table where templates were not edited successfully. RUN-13891 Fixed an issue where Ray job statuses were shown as empty. RUN-13825 Fixed an issue where GPU sharing configmaps were not deleted. RUN-13628 Fixed an issue where the <code>pre-install</code> pod failed to run <code>pre-install</code> tasks due to the request being denied (Unauthorized). RUN-13550 Fixed an issue where environments were not recovering from a node restart due to a missing GPU runtime class for containerized nodes. RUN-11895 Fixed an issue where the wrong amount of GPU memory usage was shown (is now MB). RUN-11681 Fixed an issue in OpenShift environments where some metrics were not shown on dashboards when the GPU Operator from the RedHat marketplace was installed."},{"location":"home/changelog/hotfixes-2-15/#version-2150","title":"Version 2.15.0","text":""},{"location":"home/changelog/hotfixes-2-15/#fixed-issues_4","title":"Fixed issues","text":"Internal ID Description RUN-13456 Fixed an issue where the Researcher L1 role did not have permissions to create and manage credentials. RUN-13282 Fixed an issue where Workspace logs crashed unexpectedly after restarting. RUN-13121 Fixed an issue in not being able to launch jobs using the API after an upgrade overrode a change in keycloak for applications which have a custom mapping to an email. RUN-13103 Fixed an issue in the Workspaces and Trainings table where the action buttons were not greyed out for users with only the view role. RUN-12993 Fixed an issue where Prometheus was reporting metrics even though the cluster was disconnected. RUN-12978 Fixed an issue after an upgrade, where permissions fail to sync to a project due to a missing application name in the CRD. RUN-12900 Fixed an issue in the Projects table, when sorting by Allocated GPUs, the projects were displayed alphabetically and not numerically. RUN-12846 Fixed an issue after a control-plane upgrade, where GPU, CPU, and Memory Cost fields (in the Consumption Reports) were missing when not using Grafana. RUN-12824 Fixed an issue where airgapped environments tried to pull an image from gcr.io (Internet). RUN-12769 Fixed an issue where SSO users were unable to see projects in Job Form unless the group they belong to was added directly to the project. RUN-12602 Fixed an issue in the documentation where the <code>WorkloadServices</code> configuration in the <code>runaiconfig</code> file was incorrect. RUN-12528 Fixed an issue where the Workspace duration scheduling rule was suspending workspaces regardless of the configured duration. RUN-12298 Fixed an issue where projects were not shown in the Projects table due to the API not sanitizing the project name at time of creation. RUN-12157 Fixed an issue where querying pods completion time returned a negative number. RUN-10560 Fixed an issue where no Prometheus alerts were sent due to a misconfiguration of the parameter <code>RunaiDaemonSetRolloutStuck</code>."},{"location":"home/changelog/hotfixes-2-16/","title":"Changelog Version 2.16","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.16.</p>"},{"location":"home/changelog/hotfixes-2-16/#version-21665","title":"Version 2.16.65","text":"Internal ID Description RUN-21448 Fixed an issue with degraded workload so the condition would reflect the actual state. RUN-20680 Fixed an issue where the workload page did not present the requested GPU."},{"location":"home/changelog/hotfixes-2-16/#version-21657","title":"Version 2.16.57","text":"Internal ID Description RUN-20388 Fixed an issue where cluster-sync caused a memory leak."},{"location":"home/changelog/hotfixes-2-16/#version-21625","title":"Version 2.16.25","text":"Internal ID Description RUN-17241 Fixed an issue where the nodes page showed nodes as not ready due to \"tookit not installed\"."},{"location":"home/changelog/hotfixes-2-16/#version-21621","title":"Version 2.16.21","text":"Internal ID Description RUN-16463 Fixed an issue after a cluster upgrade to v2.16, where some metrics of pre-existing workloads were displayed incorrectly in the Overview Dashboard."},{"location":"home/changelog/hotfixes-2-16/#version-21618","title":"Version 2.16.18","text":"Internal ID Description RUN-16486 Fixed an issue in the Workloads creation form where the GPU fields of the compute resource tiles were showing no data."},{"location":"home/changelog/hotfixes-2-16/#version-21616","title":"Version 2.16.16","text":"Internal ID Description RUN-16340 Fixed an issue in the Workloads table where filters were not saved correctly."},{"location":"home/changelog/hotfixes-2-16/#version-21615","title":"Version 2.16.15","text":""},{"location":"home/changelog/hotfixes-2-16/#release-content","title":"Release content","text":"<ul> <li>Implemented a new Workloads API to support the Workloads feature.</li> </ul>"},{"location":"home/changelog/hotfixes-2-16/#fixed-issues","title":"Fixed issues","text":"Internal ID Description RUN-16070 Fixed an issue where missing metrics caused the Nodepools table to appear empty."},{"location":"home/changelog/hotfixes-2-16/#version-21614","title":"Version 2.16.14","text":""},{"location":"home/changelog/hotfixes-2-16/#release-content_1","title":"Release content","text":"<p>*Improved overall performance by slowing down metrics updates from 10 seconds to 30 seconds.</p>"},{"location":"home/changelog/hotfixes-2-16/#fixed-issues_1","title":"Fixed issues","text":"Internal ID Description RUN-16255 Fixed an issue in the Analytics dashboard where the GPU Allocation per Node and GPU Memory Allocation per Node panels were displaying incorrect data. RUN-16035 Fixed an issue in the Workloads table where completed pods continue to be counted in the requested resources column."},{"location":"home/changelog/hotfixes-2-16/#version-21612","title":"Version 2.16.12","text":""},{"location":"home/changelog/hotfixes-2-16/#fixed-issues_2","title":"Fixed issues","text":"Internal ID Description RUN-16110 Fixed an issue where creating a training workload (single or multi-node) with a new PVC or Volume, resulted in the Workloads table showing the workload in the Unknown/Pending status. RUN-16086 Fixed an issue in airgapped environments where incorrect installation commands were shown when upgrading to V2.15."},{"location":"home/changelog/hotfixes-2-16/#version-21611","title":"Version 2.16.11","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-16/#version-2169","title":"Version 2.16.9","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-16/#version-2168","title":"Version 2.16.8","text":""},{"location":"home/changelog/hotfixes-2-16/#release-content_2","title":"Release content","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-16/#version-2167","title":"Version 2.16.7","text":""},{"location":"home/changelog/hotfixes-2-16/#release-content_3","title":"Release content","text":"<ul> <li>Added an API endpoint that retrieves data from a workloads's pod.</li> </ul>"},{"location":"home/changelog/hotfixes-2-16/#fixed-issues_3","title":"Fixed issues","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-16/#version-2166","title":"Version 2.16.6","text":"<p>N/A</p>"},{"location":"home/changelog/hotfixes-2-17/","title":"Changelog Version 2.17","text":"<p>The following is a list of the known and fixed issues for Run:ai V2.17.</p>"},{"location":"home/changelog/hotfixes-2-17/#version-21763","title":"Version 2.17.63","text":"Internal ID Description RUN-21448 Fixed an issue where a degraded workload was stuck and could not be released."},{"location":"home/changelog/hotfixes-2-17/#version-21746","title":"Version 2.17.46","text":"Internal ID Description RUN-20136 Updated postgres version."},{"location":"home/changelog/hotfixes-2-17/#version-21743","title":"Version 2.17.43","text":"Internal ID Description RUN-19949 Fixed an issue where runai submit arguments were not parsed correctly to the command."},{"location":"home/changelog/hotfixes-2-17/#version-21741","title":"Version 2.17.41","text":"Internal ID Description RUN-19870 Added debug logs to cluster-sync"},{"location":"home/changelog/hotfixes-2-17/#version-21726","title":"Version 2.17.26","text":"Internal ID Description RUN-19189 Fixed an issue in cluster-sync that sometimes caused unnecessary sync process to the control-plane."},{"location":"home/changelog/hotfixes-2-17/#version-21725","title":"Version 2.17.25","text":"Internal ID Description RUN-16357 Fixed an issue where the Project button in the Jobs screen redirects to the Projects page but on the wrong cluster."},{"location":"home/changelog/hotfixes-2-17/#version-21710","title":"Version 2.17.10","text":"Internal ID Description RUN-18065 Fixed an issue where the legacy job sumbission configuration was not available in the Settings page"},{"location":"home/changelog/hotfixes-2-17/#version-2170","title":"Version 2.17.0","text":"Internal ID Description RUN-20010 Fixed an issue of reduced permissions that run:ai grants users"},{"location":"snippets/common-submit-cli-commands/","title":"Common submit cli commands","text":""},{"location":"snippets/common-submit-cli-commands/#naming-and-shortcuts","title":"Naming and Shortcuts","text":""},{"location":"snippets/common-submit-cli-commands/#-job-name-prefix-string","title":"--job-name-prefix <code>&lt;string&gt;</code>","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"snippets/common-submit-cli-commands/#-name-string","title":"--name <code>&lt;string&gt;</code>","text":"<p>The name of the Job.</p>"},{"location":"snippets/common-submit-cli-commands/#-template-string","title":"--template <code>&lt;string&gt;</code>","text":"<p>Load default values from a workload.</p>"},{"location":"snippets/common-submit-cli-commands/#container-definition","title":"Container Definition","text":""},{"location":"snippets/common-submit-cli-commands/#-add-capability-stringarray","title":"--add-capability <code>&lt;stringArray&gt;</code>","text":"<p>Add linux capabilities to the container.</p>"},{"location":"snippets/common-submit-cli-commands/#-a-annotation-stringarray","title":"-a | --annotation <code>&lt;stringArray&gt;</code>","text":"<p>Set annotations variables in the container.</p>"},{"location":"snippets/common-submit-cli-commands/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true.</p>"},{"location":"snippets/common-submit-cli-commands/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example:</p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"snippets/common-submit-cli-commands/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"snippets/common-submit-cli-commands/#-e-stringarray-environment","title":"-e <code>&lt;stringArray&gt;  | --environment</code>`  <p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>","text":""},{"location":"snippets/common-submit-cli-commands/#-image-string-i-string","title":"--image <code>&lt;string&gt;</code> | -i <code>&lt;string&gt;</code>  <p>Image to use when creating the container for this Job</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-image-pull-policy-string","title":"--image-pull-policy <code>&lt;string&gt;</code>  <p>Pulling policy of the image when starting a container. Options are:</p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-l-label-stringarray","title":"-l | --label <code>&lt;stringArray&gt;</code>  <p>Set labels variables in the container.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-preferred-pod-topology-key-string","title":"--preferred-pod-topology-key <code>&lt;string&gt;</code>  <p>If possible, all pods of this job will be scheduled onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-required-pod-topology-key-string","title":"--required-pod-topology-key <code>&lt;string&gt;</code>  <p>Enforce scheduling pods of this job onto nodes that have a label with this key and identical values.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-stdin","title":"--stdin  <p>Keep stdin open for the container(s) in the pod, even if nothing is attached.is attached.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-t-tty","title":"-t | --tty  <p>Allocate a pseudo-TTY.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-working-dir-string","title":"--working-dir <code>&lt;string&gt;</code>  <p>Starts the container with the specified directory as the current directory.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#resource-allocation","title":"Resource Allocation","text":""},{"location":"snippets/common-submit-cli-commands/#-cpu-double","title":"--cpu <code>&lt;double&gt;</code>  <p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-cpu-limit-double","title":"--cpu-limit <code>&lt;double&gt;</code>  <p>Limitations on the number of CPUs consumed by the Job (for example 0.5, 1). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-extended-resource","title":"--extended-resource `  <p>Request access to extended resource, syntax <code>&lt;resource-name&gt; = &lt; resource_quantity &gt;</code></p>","text":""},{"location":"snippets/common-submit-cli-commands/#-g-gpu-float","title":"-g | --gpu <code>&lt;float&gt;</code>  <p>GPU units to allocate for the Job (0.5, 1).</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-gpu-memory","title":"--gpu-memory  <p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-memory-string","title":"--memory <code>&lt;string&gt;</code>  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-memory-limit","title":"--memory-limit `  <p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-mig-profile-string","title":"--mig-profile <code>&lt;string&gt;</code>  <p>MIG profile to allocate for the job (1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb)</p>","text":""},{"location":"snippets/common-submit-cli-commands/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"snippets/common-submit-cli-commands/#-backoff-limit-int","title":"--backoff-limit <code>&lt;int&gt;</code>  <p>The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the <code>--interactive</code> flag is not specified).</p>","text":""},{"location":"snippets/common-submit-cli-commands/#storage","title":"Storage","text":""},{"location":"snippets/common-submit-cli-commands/#-git-sync-stringarray","title":"--git-sync <code>&lt;stringArray&gt;</code>  <p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-large-shm","title":"--large-shm  <p>Mount a large /dev/shm device.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-mount-propagation","title":"--mount-propagation  <p>Enable HostToContainer mount propagation for all container volumes</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-nfs-server-string","title":"--nfs-server <code>&lt;string&gt;</code>  <p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"snippets/common-submit-cli-commands/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>  <p>Mount a persistent volume claim into a container.</p>  <p>Note</p> <p>This option is being deprecated from version 2.10 and above. To mount existing or newly created Persistent Volume Claim (PVC), use the parameters <code>--pvc-exists</code> and <code>--pvc-new</code>.</p>  <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p>  <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-pvc-exists-string","title":"--pvc-exists <code>&lt;string&gt;</code>  <p>Mount a persistent volume. You must include a <code>claimname</code> and <code>path</code>.</p> <ul> <li>claim name\u2014The name of the persistent colume claim. Can be obtained by running </li> </ul> <p><code>kubectl get storageclasses.storage.k8s.io</code></p> <ul> <li>path\u2014the path internal to the container where the storage will be mounted</li> </ul> <p>Use the format:</p> <p><code>claimname=&lt;CLAIM_NAME&gt;,path=&lt;PATH&gt;</code></p>","text":""},{"location":"snippets/common-submit-cli-commands/#-pvc-new-string","title":"--pvc-new  <code>&lt;string&gt;</code>  <p>Mount a persistent volume claim (PVC). If the PVC does not exist, it will be created based on the parameters entered. If a PVC exists, it will be used with its defined attributes and the parameters in the command will be ignored.</p> <ul> <li>claim name\u2014The name of the persistent colume claim.</li> <li>storage class\u2014A storage class name that can be obtained by running</li> </ul>  <p><code>kubectl get storageclasses.storage.k8s.io.</code></p> <p><code>storageclass</code> may be omitted if there is a single storage class in the system, or you are using the default storage class.</p>  <ul> <li>size\u2014The volume size you want to allocate for the PVC when creating it. See Kubernetes documentation to specify volume sizes.</li> <li>accessmode\u2014The description ofthedesired volume capabilities for the PVC.</li> <li>ro\u2014Mount the PVC with read-only access.</li> <li>ephemeral\u2014The PVC will be created as volatile temporary storage which is only present during the running lifetime of the job.</li> </ul> <p>Use the format:</p> <p><code>storageclass=  &lt;storageclass&gt;,size= &lt;size&gt;, path= &lt;path&gt;, ro, accessmode-rwm</code></p>","text":""},{"location":"snippets/common-submit-cli-commands/#-s3-string","title":"--s3 <code>&lt;string&gt;</code>  <p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>","text":""},{"location":"snippets/common-submit-cli-commands/#-v-volume-sourcecontainer_mount_pathronfs-host","title":"-v | --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'  <p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code></p> <p>Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code></p> <p>Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#network","title":"Network","text":""},{"location":"snippets/common-submit-cli-commands/#-address-string","title":"--address <code>&lt;string&gt;</code>  <p>Comma separated list of IP addresses to listen to when running with --service-type portforward (default: localhost)</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-host-ipc","title":"--host-ipc  <p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-host-network","title":"--host-network  <p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-port-stringarray","title":"--port <code>&lt;stringArray&gt;</code>  <p>Expose ports from the Job container.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-s-service-type-string","title":"-s | --service-type <code>&lt;string&gt;</code>  <p>External access type to interactive jobs. Options are: portforward, loadbalancer, nodeport, ingress.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#access-control","title":"Access Control","text":""},{"location":"snippets/common-submit-cli-commands/#-allow-privilege-escalation","title":"--allow-privilege-escalation  <p>Allow the job to gain additional privileges after start.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-run-as-user","title":"--run-as-user  <p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#scheduling","title":"Scheduling","text":""},{"location":"snippets/common-submit-cli-commands/#-node-pools-string","title":"--node-pools <code>&lt;string&gt;</code>  <p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-node-type-string","title":"--node-type <code>&lt;string&gt;</code>  <p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-toleration-string","title":"--toleration <code>&lt;string&gt;</code>  <p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string:</p> <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre>","text":""},{"location":"snippets/common-submit-cli-commands/#global-flags","title":"Global Flags","text":""},{"location":"snippets/common-submit-cli-commands/#-loglevel-string","title":"--loglevel (string)  <p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-project-p-string","title":"--project | -p (string)  <p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>","text":""},{"location":"snippets/common-submit-cli-commands/#-help-h","title":"--help | -h  <p>Show help text.</p>","text":""},{"location":"snippets/snippets-policies/","title":"Snippets policies","text":""},{"location":"snippets/snippets-policies/#configurable-fields","title":"Configurable Fields","text":"<p>The following parameters can be configured in the policy manager.</p> <p>Note</p> <p>In the tables below, when a Type has <code>null</code> as an option, you can choose to either not use the Field or use the value <code>null</code> in the policy YAML.</p>"},{"location":"snippets/snippets-policies/#defaults","title":"Defaults","text":"<p>The <code>defaults</code> section of the policy file is...</p> Field Type Description <code>environment</code> <code>object</code> or <code>null</code> Environment fields that can be overridden when creating a workload. <code>compute</code> <code>object</code> or <code>null</code> Compute resources requested. <code>hostPath</code> <code>object</code> or <code>null</code> Volumes resource definitions. <code>nfs</code> <code>object</code> or <code>null</code> NFS volume definitions. <code>pvc</code> <code>object</code> or <code>null</code> PVC definitions. <code>git</code> <code>object</code> or <code>null</code> Git repository definitions. <code>s3</code> <code>object</code> or <code>null</code> S3 resource definitions. <code>configmap</code> <code>object</code> or <code>null</code> ConfigMap definitions. <code>imposedAssets</code> <code>object</code> or <code>null</code> A list of asset to be imposed on the workloads created in org units affected by this policy."},{"location":"snippets/snippets-policies/#environment-fields","title":"Environment Fields","text":"Field Type Description <code>command</code> <code>string</code> or <code>null</code> (non-empty) A command sent to the server used as the entry point of the container running the workspace. <code>args</code> <code>string</code> or <code>null</code> (non-empty) Arguments applied to the command that the container running the workspace executes. <code>environmentVariables</code> <code>array of objects</code> or <code>null</code> or <code>null</code> An array of environment variables to populate into the container running the workspace. <code>runAsUid</code> <code>integer</code>  or <code>null</code> The userid to run the entrypoint of the container. Default to the (optional) value specified in the environment asset <code>runAsUid</code> field. Can be provided only when the source uid/gid of the environment asset is not <code>fromTheImage</code>, and <code>overrideUidGidInWorkspace</code> is enabled. <code>runAsGid</code> <code>integer</code>  or <code>null</code> \u00a0The group id to run the entrypoint of the container. Default to the (optional) value specified in the environment asset runAsGid field. Can be provided only when the source uid/gid of the environment asset is not <code>fromTheImage</code>, and <code>overrideUidGidInWorkspace</code> is enabled. <code>supplementalGroups</code> <code>string</code> or <code>null</code> Comma seperated list of groups that the user running the container belongs to, in addition to the group indicated by <code>runAsGid</code>. Can be provided only when the source uid/gid of the environment asset is not <code>fromTheImage</code>, and <code>overrideUidGidInWorkspace</code> is enabled. Empty string implies reverting to the supplementary groups of the image. <code>image</code> <code>string</code> or <code>null</code> (non-empty) Docker image name.\u00a0Image name is mandatory for creating a workspace. See Images <code>imagePullPolicy</code> <code>string</code> or <code>null</code>\u00a0(non-empty) Image pull policy.  Select from:\u00a0<code>Always</code>,\u00a0<code>Never</code>, or\u00a0<code>IfNotPresent</code>. Defaults to Always if <code>latest tag</code> is specified, or <code>IfNotPresent</code> otherwise. <code>workingDir</code> <code>string</code> or <code>null</code> (non-empty) The container's working directory. If not specified, the container runtime default will be used, which might be configured in the container image. <code>hostIpc</code> <code>boolean</code> or <code>null</code> Enable host IPC. Defaults to <code>false</code>. <code>hostNetwork</code> <code>boolean</code> or <code>null</code> Enable host networking. Default to <code>false</code>. <code>connections</code> <code>array of\u00a0objects</code> List of connections that either expose ports from the container (each port is associated with a tool that the container runs), or URL's to be used for connecting to an external tool that is related to the action of the container (such as Weights &amp; Biases). <code>createHomeDir</code> <code>boolean</code> or <code>null</code> Create a home directory for the container. <code>allowPrivilegeEscalation</code> <code>boolean</code> or <code>null</code> Allow the container running the workload and all launched processes to gain additional privileges after the workload starts. For more information, see User Identity in Container. <code>uidGidSource</code> <code>string</code> or <code>null</code> Indicate the way to determine the user and group ids of the container. Choose from:  <code>fromTheImage</code>\u2014user and group ids are determined by the docker image that the container runs (Default). <code>custom</code>\u2014user and group ids can be specified in the environment asset and/or the workspace creation request.  <code>idpToken</code>\u2014user and group ids are determined according to the identity provider (idp) access token. This option is intended for internal use of the environment UI form. For more information see User Identity guide. <code>overrideUidGidInWorkspace</code> <code>boolean</code> Allow specifying uid/gid as part of create workspace. This is relevant only for custom uigGidSource. Default:\u00a0false <code>capabilities</code> <code>array of\u00a0strings</code> or <code>null</code> The POSIX capabilities to add when running containers. Defaults to the default set of capabilities granted by the container runtime. Choose from: <code>AUDIT CONTROL</code>, <code>AUDIT READ</code>, <code>AUDIT WRITE</code>, <code>BLOCK SUSPEND</code>, <code>CHOWN</code>, <code>DAC OVERRIDE</code>, <code>DAC READ SEARCH</code>, <code>FOWNER</code>, <code>FSETID</code>, <code>IPC LOCK</code>, <code>IPC OWNER</code>, <code>KILL</code>, <code>LEASE</code>, <code>LINUX IMMUTABLE</code>, <code>MAC ADMIN</code>, <code>MAC OVERRIDE</code>, <code>MKNOD</code>, <code>NET ADMIN</code>, <code>NET BIND SERVICE</code>, <code>NET BROADCAST</code>, <code>NET RAW</code>, <code>SETGID</code>, <code>SETFCAP</code>, <code>SETPCAP</code>, <code>SETUID</code>, <code>SYS ADMIN</code>, <code>SYS BOOT</code>, <code>SYS CHROOT</code>, <code>SYS MODULE</code>, <code>SYS NICE</code>, <code>SYS PACCT</code>, <code>SYS PTRACE</code>, <code>SYS RAWIO</code>, <code>SYS RESOURCE</code>, <code>SYS TIME</code>, <code>SYS TTY CONFIG</code>, <code>SYSLOG</code>, <code>WAKE ALARM</code>. <code>seccompProfileType</code> <code>string</code> or <code>null</code> Indicates which kind of seccomp profile will be applied to the container. Choose from: <code>Runtime</code> (default)\u2014the container runtime default profile should be used.  <code>Unconfined</code>&amp;mdashno profile should be applied.  <code>Localhost</code> is not yet supported by Run:ai. <code>runAsNonRoot</code> <code>boolean</code> or <code>null</code> Indicates that the container must run as a non-root user."},{"location":"snippets/snippets-policies/#environment-variables","title":"Environment Variables","text":"Field Type Description <code>name</code> (required) <code>string</code> (non-empty) The name of the environment variable. <code>value</code> (required) <code>string</code> The value to set the environment variable to. <code>deleted</code> <code>boolean</code> Exclude this environment variable from the workload. This is necessary in case the variable definition is inherited from a policy."},{"location":"snippets/snippets-policies/#connections-variables","title":"Connections Variables","text":"Field Type Description <code>namerequired</code> <code>string</code> (non-empty) A unique name of this connection. This name correlates between the connection information specified at the environment asset, to the information about the connection as specified in <code>SpecificEnv</code> for a specific workspace. <code>isExternal</code> <code>boolean</code> Internal tools (<code>isExternal=false</code>) are tools that run as part of the container. External tools (<code>isExternal=true</code>) run outside the container, typically in the cloud. Default:\u00a0false. <code>internalToolInfo</code> <code>object</code> or <code>null</code> Information about the internal tool. <code>externalToolInfo</code> <code>object</code> or <code>null</code> Information about the external tool."},{"location":"snippets/snippets-policies/#internal-tool-variables","title":"Internal Tool Variables","text":"Field Type Description <code>toolType</code> (required) <code>string</code>\u00a0(non-empty) The type of the internal tool. This runs within the container and exposes ports associated with the tool using <code>NodePort</code>, <code>LoadBalancer</code> or <code>ExternalUrl</code>. Choose from: <code>jupyter-notebook</code>, <code>pycharm</code>, <code>visual-studio-code</code>, <code>tensorboard</code>,\u00a0<code>rstudio</code>,\u00a0<code>mlflow</code>,\u00a0<code>custom</code>, or\u00a0<code>matlab</code>. <code>connectionType</code> (required) <code>string</code>\u00a0(non-empty) The type of connection that exposes the container port. Choose from: <code>LoadBalancer</code>,\u00a0<code>NodePort</code>, or <code>ExternalUrl</code>. <code>containerPort</code> (required) <code>integer</code> The port within the container that the connection exposes. <code>nodePortInfo</code> <code>object</code> or <code>null</code> Use the <code>isCustomPort</code> variable (<code>boolean</code>) to ensute that the node port is provided in the specific env of the workspace. Use the default <code>false</code> to ensure the node port is auto generated by the system. <code>externalUrlInfo</code> <code>object</code> or <code>null</code> Use the <code>isCustomUrl</code> variable (boolean) to indicate whether the external url is provided in the specific env of the workspace. Use the default <code>false</code>to ensure the external url is auto generated by the system.  Use the <code>externalUrl</code> variable (<code>string</code> or <code>null</code> - non-empty) to decalre the default value for the external url. You can override it in the specific env of the workspace."},{"location":"snippets/snippets-policies/#external-tool-variables","title":"External Tool Variables","text":"Field Type Description <code>toolType</code> (required) <code>string</code>\u00a0(non-empty) The type of external tool that is associated with the connection. External tools typically run in the cloud and require an external url to connect to it. Choose from <code>wandb</code> or <code>comet</code>. <code>externalUrl</code> (required) <code>string</code>\u00a0(non-empty) The external url for connecting to the external tool. The url can include environment variables that will be replaced with the values provided when the workspace is created."},{"location":"snippets/snippets-policies/#compute-resource-fields","title":"Compute Resource Fields","text":"Field Type Description <code>gpuDevicesRequest</code> <code>integer</code> or <code>null</code> Requested number of GPU devices. Currently if more than one device is requested, it is not possible to provide values for gpuMemory/migProfile/gpuPortion. <code>gpuRequestType</code> <code>string</code> or <code>null</code> (GpuRequestType)\u00a0non-empty Enum:\u00a0\"portion\"\u00a0\"memory\"\u00a0\"migProfile\"Whether the request for GPU resources is stated in terms of portion, memory or mig profile. If gpuDevicesRequest &gt; 1, only portion with gpuPortionRequest 1 is supported. If gpuDeviceRequest = 1, request type can be stated as portion, memory or migProfile. <code>gpuPortionRequest</code> <code>number</code> or <code>null</code> Required if and only if gpuRequestType is portion. States the portion of the GPU to allocate for the created workload, per GPU device, between 0 and 1. The default is no allocated GPUs. <code>gpuPortionLimit</code> <code>number</code> or <code>null</code> Limitations on the portion consumed by the workload, per GPU device. The system guarantees The puPotionLimit must be no less than the gpuPortionRequest. <code>gpuMemoryRequest</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ Required if and only if gpuRequestType is memory. States the GPU memory to allocate for the created workload, per GPU device. Note that the workload will not be scheduled unless the system can guarantee this amount of GPU memory to the workload. <code>gpuMemoryLimit</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ Limitation on the memory consumed by the workload, per GPU device. The system guarantees The gpuMemoryLimit must be no less than gpuMemoryRequest. <code>migProfile</code> <code>string</code> or <code>null</code>\u00a0(MigProfile)\u00a0non-empty Enum:\u00a0\"1g.5gb\"\u00a0\"1g.10gb\"\u00a0\"2g.10gb\"\u00a0\"2g.20gb\"\u00a0\"3g.20gb\"\u00a0\"3g.40gb\"\u00a0\"4g.20gb\"\u00a0\"4g.40gb\"\u00a0\"7g.40gb\"\u00a0\"7g.80gb\"Required if and only if gpuRequestType is migProfile. States the memory profile to be used for workload running on NVIDIA Multi-Instance GPU (MIG) technology. <code>cpuCoreRequest</code> <code>number</code> or <code>null</code> CPU units to allocate for the created workload (0.5, 1, .etc). The workload will receive at least this amount of CPU. Note that the workload will not be scheduled unless the system can guarantee this amount of CPUs to the workload. <code>cpuCoreLimit</code> <code>number</code> or <code>null</code> Limitations on the number of CPUs consumed by the workload (0.5, 1, .etc). The system guarantees that this workload will not be able to consume more than this amount of CPUs. <code>cpuMemoryRequest</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The amount of CPU memory to allocate for this workload (1G, 20M, .etc). The workload will receive at least this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of memory to the workload <code>cpuMemoryLimit</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ Limitations on the CPU memory to allocate for this workload (1G, 20M, .etc). The system guarantees that this workload will not be able to consume more than this amount of memory. The workload will receive an error when trying to allocate more memory than this limit. <code>largeShmRequest</code> <code>boolean</code> or <code>null</code> A large /dev/shm device to mount into a container running the created workload. An shm is a shared file system mounted on RAM. <code>extendedResources</code> <code>Array</code> An array of\u00a0objects or null or null\u00a0(ExtendedResources) - Set of extended resources with their quantity."},{"location":"snippets/snippets-policies/#extended-resources-array","title":"Extended Resources Array","text":"Field Type Description <code>resource required</code> <code>string</code> non-empty The name of the extended resource. <code>quantity required</code> <code>string</code> non-empty The requested quantity for the given resource. <code>deleted</code> <code>boolean</code> Whether to exclude this extended resource from the workload. This is necessary in case the extended resource definition is inherited from a policy."},{"location":"snippets/snippets-policies/#hostpath-resource-fields","title":"Hostpath Resource Fields","text":"Field Type Description <code>pathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty Local path within the controller to which the host volume will be mapped. Path is mandatory for creating a workspace. <code>readOnly</code> <code>boolean</code> or <code>null</code> Default:\u00a0true Whether to force the volume to be mounted with read-only permissions. Defaults to false. <code>mountPathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty The path that the host volume will be mounted to when in use. MountPath is mandatory for creating a workspace."},{"location":"snippets/snippets-policies/#nfs-description-fields","title":"NFS Description Fields","text":"Field Type Description <code>pathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty Path that is exported by the NFS server. More info at\u00a0https://kubernetes.io/docs/concepts/storage/volumes#nfs. Path is mandatory for creating a workspace. <code>readOnly</code> <code>boolean</code> or <code>null</code> Default:\u00a0true Whether to force the NFS export to be mounted with read-only permissions. <code>serverrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty The hostname or IP address of the NFS server. Server is mandatory for creating a workspace. <code>mountPathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty The path that the NFS volume will be mounted to when in use. MountPath is mandatory for creating a workspace."},{"location":"snippets/snippets-policies/#pvc-description-fields","title":"PVC Description Fields","text":"Field Type Description <code>existingPvc</code> <code>boolean</code> or <code>null</code> Default:\u00a0false Whether to assume that the PVC exists. If set to true, PVC is assumed to exist. If set to false, the PVC will be create if it does not exist. <code>claimNamerequired</code> <code>string</code> or <code>null</code>\u00a0non-empty A given name for the PVC. Allowed referencing it across workspaces. ClaimName is mandatory for creating a workspace. <code>pathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty Local path within the workspace to which the PVC bucket will be mapped. Path is mandatory for creating a workspace. <code>readOnly</code> <code>boolean</code> or <code>null</code> Default:\u00a0true Whether the path to the PVC permits only read access. <code>ephemeral</code> <code>boolean</code> or <code>null</code> Default:\u00a0false Whether the PVC is ephemeral. If set to true, the PVC will be deleted when the workspace is stopped. <code>claimInfo</code> <code>object</code> or <code>null</code> ClaimInfo Claim information for the newly created PVC. The information should not be provided when attempting to use existing PVC."},{"location":"snippets/snippets-policies/#claim-info","title":"Claim Info","text":"Field Type Description <code>sizerequired</code> <code>string</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ Requested size for the PVC. Mandatory when existingPvc is false. <code>storageClass</code> <code>string</code> or <code>null</code>\u00a0non-empty Storage class name to associate with the PVC. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. For more information see Storage classes. <code>accessModes</code> <code>object</code> or <code>null</code> AccessModes Requested access mode(s) for the newly created PVC. <code>volumeMode</code> <code>string</code> or <code>null</code> Enum:\u00a0\"Filesystem\"\u00a0\"Block\"The volume mode required by the claim, either Filesystem (default) or Block."},{"location":"snippets/snippets-policies/#access-modes","title":"Access Modes","text":"Field Type Description <code>readWriteOnce</code> <code>boolean</code> or <code>null</code> Default:\u00a0true Requesting claim that can be mounted in read/write mode to exactly one host. This is the default access mode. <code>readOnlyMany</code> <code>boolean</code> or <code>null</code> Default:\u00a0false Requesting claim that can be mounted in read-only mode to many hosts. <code>readWriteMany</code> <code>boolean</code> or <code>null</code> Default:\u00a0false Requesting claim that can be mounted in read/write mode to many hosts."},{"location":"snippets/snippets-policies/#git-repository-description-fields","title":"Git Repository Description Fields","text":"Field Type Description <code>repositoryrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty URL to a remote git repository. The content of this repository will be mapped to the container running the workload. Repository name is mandatory for creating a workspace. <code>branch</code> <code>string</code> or <code>null</code>\u00a0non-empty Specific branch to synchronize the repository from. <code>revision</code> <code>string</code> or <code>null</code>\u00a0non-empty Specific revision to synchronize the repository from. <code>pathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty Local path within the workspace to which the S3 bucket will be mapped. Path is mandatory for creating a workspace. <code>passwordAssetId</code> <code>string</code> or <code>null</code>\u00a0non-empty ID of credentials asset of type password. Needed for non public repository which requires authentication."},{"location":"snippets/snippets-policies/#s3-resource-description-fields","title":"S3 Resource Description Fields","text":"Field Type Description <code>bucketrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty The name of the bucket Bucket name is mandatory for creating a workspace. <code>pathrequired</code> <code>string</code> or <code>null</code>\u00a0non-empty Local path within the workspace to which the S3 bucket will be mapped. Path is mandatory for creating a workspace. <code>accessKeyAssetId</code> <code>string</code> or <code>null</code>\u00a0non-empty ID of credentials asset of type access-key, for private S3 buckets. <code>url</code> <code>string</code> or <code>null</code>\u00a0non-empty The url of the S3 service provider. The default is the URL of the Amazon AWS S3 service."},{"location":"snippets/snippets-policies/#configmap-resource-description-fields","title":"ConfigMap Resource Description Fields","text":"Field Type Description <code>json:\"configMap\"</code> <code>string</code> The name of the ConfigMap. ConfigMap is mandatory for creating a workspace. <code>json:\"mountPath\"</code> <code>string</code> Local path within the workspace to which the ConfigMap will be mapped. ClaimName is mandatory for creating a workspace."},{"location":"snippets/snippets-policies/#workspace","title":"Workspace","text":"Field Type Description <code>nodeType</code> <code>string</code> or <code>null</code>\u00a0non-empty Nodes or a group of nodes on which the workload will run. To use this feature, your Administrator will need to label nodes as explained in Group Nodes. This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information, see Projects. <code>nodePools</code> <code>Array of\u00a0strings</code> or <code>null</code> A prioritize list of node pools for the scheduler to run the workspace on. The scheduler will always try to use the first node pool before moving to the next one when the fist is not available. <code>allowOverQuota</code> <code>boolean</code> or <code>null</code> Whether to allow the workspace to exceed the quota of the project. <code>annotations</code> <code>Array of\u00a0objects</code> or <code>null</code> Annotations Set of annotations to populate into the container running the workspace. <code>labels</code> <code>Array of\u00a0objects</code> or <code>null</code> Labels Set of labels to populate into the container running the workspace. <code>autoDeletionTimeAfterCompletionSeconds</code> <code>integer</code> or <code>null</code> \u00a0Specifies the duration after which a finished workload (Completed or Failed) will be automatically deleted. <code>terminateAfterPreemption</code> <code>boolean</code> or <code>null</code> Indicates whether the job should be terminated, by the system, after it has been preempted."},{"location":"snippets/snippets-policies/#annotations","title":"Annotations","text":"Field Type Description <code>namerequired</code> <code>string</code>\u00a0non-empty The name of the annotation. <code>valuerequired</code> <code>string</code> The value to set the annotation to. <code>deleted</code> <code>boolean</code> Whether to exclude this annotation from the workload. This is necessary in case the annotation definition is inherited from a policy."},{"location":"snippets/snippets-policies/#labels","title":"Labels","text":"Field Type Description <code>namerequired</code> <code>string</code>\u00a0non-empty The name of the annotation. <code>valuerequired</code> <code>string</code> The value to set the annotation to. <code>deleted</code> <code>boolean</code> Whether to exclude this annotation from the workload. This is necessary in case the annotation definition is inherited from a policy."},{"location":"snippets/snippets-policies/#imposed-assets","title":"Imposed Assets","text":"Field Type Description <code>datasources</code> <code>Array of strings</code> or <code>null</code> --"},{"location":"snippets/snippets-policies/#rules","title":"Rules","text":"<p>The <code>rules</code> section of the policy file is...</p> Field Type Description <code>environment</code> <code>object</code> or <code>null</code> Rules Environment fields fields that can be overridden when creating a workload. <code>compute</code> <code>object</code> or <code>null</code> Compute resources requested. <code>hostPath</code> <code>object</code> or <code>null</code> Volumes resource definitions. <code>nfs</code> <code>object</code> or <code>null</code> NFS volume definitions. <code>pvc</code> <code>object</code> or <code>null</code> PVC definitions. <code>git</code> <code>object</code> or <code>null</code> Git repository definitions. <code>s3</code> <code>object</code> or <code>null</code> S3 resource definitions. <code>imposedAssets</code> <code>object</code> or <code>null</code> A list of asset to be imposed on the workloads created in org units affected by this policy."},{"location":"snippets/snippets-policies/#rules-environment-fields","title":"Rules Environment fields","text":"Field Type Description <code>allowPrivilegeEscalation</code> <code>object</code> or <code>null</code> Allow Privilege Escalation -- <code>args</code> <code>object</code> or <code>null</code> (StringRulesOptional) -- <code>capabilities</code> <code>object</code> or <code>null</code> (ArrayRules) -- <code>command</code> <code>object</code> or <code>null</code> (StringRulesOptional) -- <code>createHomeDir</code> <code>object</code> or <code>null</code> (BooleanRules) -- <code>environmentVariables</code> <code>object</code> or <code>null</code> (EnvironmentVariablesRules) -- <code>hostIpc</code> <code>object</code> or <code>null</code> (BooleanRules) -- <code>hostNetwork</code> <code>object</code> or <code>null</code> (BooleanRules) -- <code>image</code> <code>object</code> or <code>null</code> (StringRules) -- <code>imagePullPolicy</code> <code>object</code> or <code>null</code> (ImagePullPolicyRules) -- <code>overrideUidGidInWorkspace</code> <code>object</code> or <code>null</code> (BooleanRulesOptional) -- <code>runAsUid</code> <code>object</code> or <code>null</code> (IntegerRulesOptional) -- <code>runAsGid</code> <code>object</code> or <code>null</code> (IntegerRulesOptional) -- <code>supplementalGroups</code> <code>object</code> or <code>null</code> (StringRulesOptional) -- <code>uidGidSource</code> <code>object</code> or <code>null</code> (StringRules) -- <code>workingDir</code> <code>object</code> or <code>null</code> (StringRules) -- <code>runAsNonRoot</code> <code>object</code> or <code>null</code> (BooleanRules) -- <code>seccompProfileType</code> <code>object</code> or <code>null</code> (StringRules) --"},{"location":"snippets/snippets-policies/#allow-privilege-escalation","title":"Allow Privilege Escalation","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable."},{"location":"snippets/snippets-policies/#source-of-rule","title":"Source of Rule","text":"Field Type Description <code>scoperequired</code> <code>string</code>\u00a0(Scope) Enum:\u00a0\"tenant\"\u00a0\"department\"\u00a0\"project\"The scope that the policy relates to. <code>projectId</code> <code>integer</code> or <code>null</code>\u00a0(ProjectId) The id of the project. Must be specified for project scoped assets. <code>departmentId</code> <code>string</code> or <code>null</code> \u00a0(DepartmentId)\u00a0non-emptyThe id of the department. Must be specified for department scoped policies."},{"location":"snippets/snippets-policies/#args","title":"Args","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#capabilities","title":"Capabilities","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable."},{"location":"snippets/snippets-policies/#command","title":"Command","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#create-home-dir","title":"Create Home Dir","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable."},{"location":"snippets/snippets-policies/#rules-environment-variables","title":"Rules Environment Variables","text":"Field Type Description <code>itemRules</code> <code>object</code> or <code>null</code> <code>sourceOfRule</code><code>object</code> or <code>null</code> Source Of RuleThis field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests.<code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is permitted to add items. Default to true.<code>locked</code><code>Array of\u00a0strings</code>Set of keys for items that are \"locked\", i.e. cannot be removed or deleted."},{"location":"snippets/snippets-policies/#host-ipc","title":"Host IPC","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#host-network","title":"Host Network","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#image","title":"Image","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#image-pull-policy","title":"Image Pull Policy","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#override-uidgid-in-workspace","title":"Override Uid/Gid In Workspace","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"snippets/snippets-policies/#run-as-uid","title":"Run as Uid","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable. <code>min</code> <code>integer</code> or <code>null</code> \u00a0The minimum value that the field can be assigned to. <code>max</code> <code>integer</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>integer</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=10, step=2 implies that the values the field can hold are 2, 4, 6, 8 and 10."},{"location":"snippets/snippets-policies/#run-as-gid","title":"Run as Gid","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable. <code>min</code> <code>integer</code> or <code>null</code> \u00a0The minimum value that the field can be assigned to. <code>max</code> <code>integer</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>integer</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=10, step=2 implies that the values the field can hold are 2, 4, 6, 8 and 10."},{"location":"snippets/snippets-policies/#supplemental-groups","title":"Supplemental Groups","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#uidgid-source","title":"Uid/Gid Source","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#working-dir","title":"Working Dir","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#run-as-non-root","title":"Run as Non-root","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"snippets/snippets-policies/#seccomp-profile-type","title":"Seccomp Profile Type","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#rules-compute-fields","title":"Rules Compute Fields","text":""},{"location":"snippets/snippets-policies/#cpu-core-request","title":"CPU Core Request","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>number</code> or <code>null</code> The minimum value that the field can be assigned to. <code>max</code> <code>number</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>number</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=3, step=0.25 implies that the values the field can hold are 2, 2.25, 2.5 and 3."},{"location":"snippets/snippets-policies/#cpu-core-limit","title":"CPU Core Limit","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>number</code> or <code>null</code> The minimum value that the field can be assigned to. <code>max</code> <code>number</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>number</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=3, step=0.25 implies that the values the field can hold are 2, 2.25, 2.5 and 3."},{"location":"snippets/snippets-policies/#cpu-memory-request","title":"CPU Memory Request","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The minimum value that the field can be assigned to. <code>max</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The maximum value that the field can be assigned to."},{"location":"snippets/snippets-policies/#cpu-memory-limit","title":"CPU Memory Limit","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The minimum value that the field can be assigned to. <code>max</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The maximum value that the field can be assigned to."},{"location":"snippets/snippets-policies/#extended-resources","title":"Extended Resources","text":"Field Type Description <code>itemRules</code> <code>object</code> or <code>null</code>\u00a0(ItemRules) <code>members</code> <code>object</code> or <code>null</code> the Quantity of rules. Use the following table for the fields and values."},{"location":"snippets/snippets-policies/#quantity","title":"Quantity","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#large-shm-request","title":"Large Shm Request","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"snippets/snippets-policies/#gpu-request-type","title":"GPU Request Type","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code> Set of options that the value of the field must be chosen from."},{"location":"snippets/snippets-policies/#mig-profile","title":"Mig Profile","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of Objects</code> or <code>null</code> Set of options that the value of the field must be chosen from. Required if and only if gpuRequestType is migProfile. States the memory profile to be used for workload running on NVIDIA Multi-Instance GPU (MIG) technology.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value."},{"location":"snippets/snippets-policies/#gpu-devices-request","title":"GPU Devices Request","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable. <code>min</code> <code>integer</code> or <code>null</code> \u00a0The minimum value that the field can be assigned to. <code>max</code> <code>integer</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>integer</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=10, step=2 implies that the values the field can hold are 2, 4, 6, 8 and 10."},{"location":"snippets/snippets-policies/#gpu-portion-request","title":"GPU Portion Request","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>number</code> or <code>null</code> The minimum value that the field can be assigned to. <code>max</code> <code>number</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>number</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=3, step=0.25 implies that the values the field can hold are 2, 2.25, 2.5 and 3."},{"location":"snippets/snippets-policies/#gpu-portion-limit","title":"GPU Portion Limit","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>number</code> or <code>null</code> The minimum value that the field can be assigned to. <code>max</code> <code>number</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>number</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=3, step=0.25 implies that the values the field can hold are 2, 2.25, 2.5 and 3."},{"location":"snippets/snippets-policies/#gpu-memory-request","title":"GPU Memory Request","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The minimum value that the field can be assigned to. <code>max</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The maximum value that the field can be assigned to."},{"location":"snippets/snippets-policies/#gpu-memory-limit","title":"GPU Memory Limit","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>min</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The minimum value that the field can be assigned to. <code>max</code> <code>string</code> or <code>null</code> ^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$ The maximum value that the field can be assigned to."},{"location":"snippets/snippets-policies/#host-path","title":"Host Path","text":"Field Type Description <code>path</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>readOnly</code> <code>object</code> or <code>null</code>(BooleanRules) -- <code>mountPath</code> <code>object</code> or <code>null</code> (StringRules) --"},{"location":"snippets/snippets-policies/#path","title":"Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#read-only","title":"Read Only","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"snippets/snippets-policies/#mount-path","title":"Mount Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#nfs","title":"NFS","text":"Field Type Description <code>path</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>readOnly</code> <code>object</code> or <code>null</code>\u00a0(BooleanRules) -- <code>server</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>mountPath</code> <code>object</code> or <code>null</code>\u00a0(StringRules) --"},{"location":"snippets/snippets-policies/#path_1","title":"Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#read-only_1","title":"Read Only","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"snippets/snippets-policies/#server","title":"Server","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#mount-path_1","title":"Mount Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#pvc","title":"PVC","text":"Field Type Description <code>claimName</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>path</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>readOnly</code> <code>object</code> or <code>null</code>\u00a0(BooleanRules) -- <code>claimInfo</code> <code>object</code> or <code>null</code>\u00a0(ClaimInfoRules) --"},{"location":"snippets/snippets-policies/#claim-name","title":"Claim Name","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#path_2","title":"Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#read-only_2","title":"Read Only","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"snippets/snippets-policies/#claim-info_1","title":"Claim Info","text":"Field Type Description <code>size</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>storageClass</code> <code>object</code> or <code>null</code>\u00a0(StringRules) --"},{"location":"snippets/snippets-policies/#size","title":"Size","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#storage-class","title":"Storage Class","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#git","title":"Git","text":"Field Type Description <code>repository</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>branch</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>revision</code> <code>object</code> or <code>null</code> (StringRules) <code>path</code> <code>object</code> or <code>null</code> (StringRules) --"},{"location":"snippets/snippets-policies/#repository","title":"Repository","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#branch","title":"Branch","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#revision","title":"Revision","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#path_3","title":"Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#s3","title":"S3","text":"Field Type Description <code>bucket</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>path</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>url</code> <code>object</code> or <code>null</code>\u00a0(StringRules) --"},{"location":"snippets/snippets-policies/#bucket","title":"Bucket","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#path_4","title":"Path","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#url","title":"URL","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#imposed-assets_1","title":"Imposed Assets","text":"Field Type Description <code>datasources</code> <code>object</code> or <code>null</code> Use any of the fields in the following table:"},{"location":"snippets/snippets-policies/#data-sources","title":"Data Sources","text":"Field Type Description <code>hostPath</code> <code>object</code> or <code>null</code> <code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is possible to add non-imposed assets in the workload <code>nfs</code> <code>object</code> or <code>null</code> <code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is possible to add non-imposed assets in the workload <code>pvc</code> <code>object</code> or <code>null</code> <code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is possible to add non-imposed assets in the workload <code>git</code> <code>object</code> or <code>null</code> <code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is possible to add non-imposed assets in the workload <code>s3</code> <code>object</code> or <code>null</code> <code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is possible to add non-imposed assets in the workload"},{"location":"snippets/snippets-policies/#workspace_1","title":"Workspace","text":"Field Type Description <code>nodeType</code> <code>object</code> or <code>null</code>\u00a0(StringRules) -- <code>nodePools</code> <code>object</code> or <code>null</code> (ArrayRules) -- <code>allowOverQuota</code> <code>object</code> or <code>null</code>\u00a0(BooleanRules) -- <code>annotations</code> <code>object</code> or <code>null</code>\u00a0(AnnotationsRules) -- <code>labels</code> <code>object</code> or <code>null</code>\u00a0(LabelsRules) -- <code>autoDeletionTimeAfterCompletionSeconds</code> <code>object</code> or <code>null</code>\u00a0(IntegerRules) -- <code>terminateAfterPreemption</code> <code>object</code> or <code>null</code>\u00a0(BooleanRules) --"},{"location":"snippets/snippets-policies/#node-type","title":"Node Type","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true <code>options</code> <code>Array of\u00a0objects</code> or <code>null</code>\u00a0(StringOption)Set of options that the value of the field must be chosen from. <code>value required</code><code>string</code>The value that the field should hold.<code>displayed</code><code>string</code> or <code>null</code>Textual description of the value. to be used by user interface applications."},{"location":"snippets/snippets-policies/#node-pools","title":"Node Pools","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"snippets/snippets-policies/#allowed-over-quota","title":"Allowed Over-quota","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"},{"location":"snippets/snippets-policies/#annotations_1","title":"Annotations","text":"Field Type Description <code>itemRules</code> <code>object</code> or <code>null</code> <code>sourceOfRule</code><code>object</code> or <code>null</code> Source Of RuleThis field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests.<code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is permitted to add items. Default to true.<code>locked</code><code>Array of\u00a0strings</code>Set of keys for items that are \"locked\", i.e. cannot be removed or deleted."},{"location":"snippets/snippets-policies/#labels_1","title":"Labels","text":"Field Type Description <code>itemRules</code> <code>object</code> or <code>null</code> <code>sourceOfRule</code><code>object</code> or <code>null</code> Source Of RuleThis field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests.<code>canAdd</code><code>boolean</code> or <code>null</code>Whether it is permitted to add items. Default to true.<code>locked</code><code>Array of\u00a0strings</code>Set of keys for items that are \"locked\", i.e. cannot be removed or deleted."},{"location":"snippets/snippets-policies/#auto-deletion-time-after-completion-seconds","title":"Auto Deletion Time After Completion Seconds","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Default = true Whether the value of the field is editable. <code>min</code> <code>integer</code> or <code>null</code> \u00a0The minimum value that the field can be assigned to. <code>max</code> <code>integer</code> or <code>null</code> The maximum value that the field can be assigned to. <code>step</code> <code>integer</code> or <code>null</code> The minimal difference between two values the field can be assigned to. For example, min=2, max=10, step=2 implies that the values the field can hold are 2, 4, 6, 8 and 10."},{"location":"snippets/snippets-policies/#time-after-preemption","title":"Time after Preemption","text":"Field Type Description <code>sourceOfRule</code> <code>object</code> or <code>null</code> Source Of Rule This field is used by the system along with effective rules, in order to specify the org unit from which this effective rule has been derived. It should be left empty when sending apply policy requests. <code>required</code> <code>boolean</code> or <code>null</code> Default = false Whether the field is mandatory. <code>canEdit</code> <code>boolean</code> or <code>null</code> Whether the value of the field is editable, default to true"}]}